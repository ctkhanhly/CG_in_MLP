Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : LBFGS
Learning rate                         : 0.01
Total number of function evaluations  : 3006
Total number of iterations            : 1147
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 30.2069034576416
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 51.310041505284744%
Percentage of parameters < 1e-7       : 51.310041505284744%
Percentage of parameters < 1e-6       : 51.311032084872856%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5548651382501648
Loss at iteration [2]: 1.2628303575560018
Loss at iteration [3]: 1.251091384187284
Loss at iteration [4]: 1.1903841286366663
Loss at iteration [5]: 1.141302694375167
Loss at iteration [6]: 1.1003550621785312
Loss at iteration [7]: 1.0267370671474716
Loss at iteration [8]: 0.9682605321986958
Loss at iteration [9]: 0.9325944023469249
Loss at iteration [10]: 0.8980272457097372
Loss at iteration [11]: 0.8614330888482034
Loss at iteration [12]: 0.8349447067979122
Loss at iteration [13]: 0.8088901011786788
Loss at iteration [14]: 0.7918281711368824
Loss at iteration [15]: 0.7836513908692876
Loss at iteration [16]: 0.7748343804650046
Loss at iteration [17]: 0.7637151902176277
Loss at iteration [18]: 0.7426615909570699
Loss at iteration [19]: 0.7180839103576337
Loss at iteration [20]: 0.6924959920964237
Loss at iteration [21]: 0.6673096098594611
Loss at iteration [22]: 0.6447696528459815
Loss at iteration [23]: 0.6128716837985063
Loss at iteration [24]: 0.5884836168152526
Loss at iteration [25]: 0.5679584935857126
Loss at iteration [26]: 0.5381579141402046
Loss at iteration [27]: 0.5066955443631982
Loss at iteration [28]: 0.4785527186724253
Loss at iteration [29]: 0.44863781180458223
Loss at iteration [30]: 0.4203376288665028
Loss at iteration [31]: 0.39572683792663554
Loss at iteration [32]: 0.3743493991909389
Loss at iteration [33]: 0.356029210978299
Loss at iteration [34]: 0.3376068081169437
Loss at iteration [35]: 0.3185962380555058
Loss at iteration [36]: 0.29873575160981236
Loss at iteration [37]: 0.2786695569282415
Loss at iteration [38]: 0.2524081275527465
Loss at iteration [39]: 0.2307310887565379
Loss at iteration [40]: 0.20933983066592585
Loss at iteration [41]: 0.19495402915371565
Loss at iteration [42]: 0.17696009145630834
Loss at iteration [43]: 0.15236380278798564
Loss at iteration [44]: 0.1417340407985411
Loss at iteration [45]: 0.13173596099258605
Loss at iteration [46]: 0.1214034198299842
Loss at iteration [47]: 0.11160209867675869
Loss at iteration [48]: 0.10222892460869049
Loss at iteration [49]: 0.09633222492194504
Loss at iteration [50]: 0.09109444166018962
Loss at iteration [51]: 0.08552381475390614
Loss at iteration [52]: 0.07861829837240321
Loss at iteration [53]: 0.06904774467685926
Loss at iteration [54]: 0.06571338695071326
Loss at iteration [55]: 0.061561890769031895
Loss at iteration [56]: 0.05952530087847736
Loss at iteration [57]: 0.05621574323832397
Loss at iteration [58]: 0.054584605499104766
Loss at iteration [59]: 0.05196951158942756
Loss at iteration [60]: 0.049238799098407844
Loss at iteration [61]: 0.047944243892894596
Loss at iteration [62]: 0.0471861997510641
Loss at iteration [63]: 0.04602755446425862
Loss at iteration [64]: 0.04406903311533672
Loss at iteration [65]: 0.041812876659053796
Loss at iteration [66]: 0.04016801643725506
Loss at iteration [67]: 0.03796193302688529
Loss at iteration [68]: 0.03666628244973618
Loss at iteration [69]: 0.035616130610839625
Loss at iteration [70]: 0.03465337165636423
Loss at iteration [71]: 0.033738072700461474
Loss at iteration [72]: 0.03315679790068179
Loss at iteration [73]: 0.032438527645146105
Loss at iteration [74]: 0.031971583580165025
Loss at iteration [75]: 0.03141853315162849
Loss at iteration [76]: 0.030698979707639597
Loss at iteration [77]: 0.030358921005511113
Loss at iteration [78]: 0.029901289072089887
Loss at iteration [79]: 0.029401230159433075
Loss at iteration [80]: 0.02909530742055295
Loss at iteration [81]: 0.028897439128862234
Loss at iteration [82]: 0.028710469315389904
Loss at iteration [83]: 0.028461550754770505
Loss at iteration [84]: 0.028123161412770357
Loss at iteration [85]: 0.02793849332513514
Loss at iteration [86]: 0.027606225230197578
Loss at iteration [87]: 0.027340330784976063
Loss at iteration [88]: 0.02698151059141193
Loss at iteration [89]: 0.026478328444885253
Loss at iteration [90]: 0.026041530198987486
Loss at iteration [91]: 0.02562271049763143
Loss at iteration [92]: 0.025466791446750476
Loss at iteration [93]: 0.025287996548378783
Loss at iteration [94]: 0.025140769727584927
Loss at iteration [95]: 0.025039287297391008
Loss at iteration [96]: 0.02494166259705893
Loss at iteration [97]: 0.024890186605583457
Loss at iteration [98]: 0.02482009795649624
Loss at iteration [99]: 0.024667580360990142
Loss at iteration [100]: 0.024586421324482407
Loss at iteration [101]: 0.02447361740369234
Loss at iteration [102]: 0.02433419443244726
Loss at iteration [103]: 0.02411557624500864
Loss at iteration [104]: 0.02383104768873446
Loss at iteration [105]: 0.023664732963127415
Loss at iteration [106]: 0.02350464379130401
Loss at iteration [107]: 0.02338563084089387
Loss at iteration [108]: 0.023318085718397087
Loss at iteration [109]: 0.023225482917178675
Loss at iteration [110]: 0.02320587376613445
Loss at iteration [111]: 0.02316112093865529
Loss at iteration [112]: 0.023059199397543598
Loss at iteration [113]: 0.022956785166981818
Loss at iteration [114]: 0.02282580073926032
Loss at iteration [115]: 0.022740585739861142
Loss at iteration [116]: 0.0227024577073451
