Model name                            : MLP_Multistep
The number of input features          : 5
The number of output features         : 2
Optimizer name                        : LBFGS
Learning rate                         : 0.01
Total number of function evaluations  : 3017
Total number of iterations            : 1101
Max number of iterations              : 3000
Number of samples in training data    : 122
Number of samples in tests data       : 52
Total training time                   : 27.02464270591736
Total number of parameters            : 202302
Percentage of parameters < 1e-9       : 50.37468734861741%
Percentage of parameters < 1e-7       : 50.37518165910372%
Percentage of parameters < 1e-6       : 50.37617028007632%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5469086495606932
Loss at iteration [2]: 0.6358389964890916
Loss at iteration [3]: 0.5815252763568793
Loss at iteration [4]: 0.49684745619457366
Loss at iteration [5]: 0.37685457766665376
Loss at iteration [6]: 0.3008310636479846
Loss at iteration [7]: 0.2751197478246761
Loss at iteration [8]: 0.25696858984559745
Loss at iteration [9]: 0.24744882645882313
Loss at iteration [10]: 0.23498106837886257
Loss at iteration [11]: 0.2222143449998082
Loss at iteration [12]: 0.20738784034645558
Loss at iteration [13]: 0.169146454425004
Loss at iteration [14]: 0.13237108533466369
Loss at iteration [15]: 0.10487498680654063
Loss at iteration [16]: 0.0870632562381418
Loss at iteration [17]: 0.07756284291022879
Loss at iteration [18]: 0.06942230726135588
Loss at iteration [19]: 0.06381312940959431
Loss at iteration [20]: 0.05894844828013419
Loss at iteration [21]: 0.056804323585583696
Loss at iteration [22]: 0.055067144570790996
Loss at iteration [23]: 0.05375315522354288
Loss at iteration [24]: 0.052714490842456795
Loss at iteration [25]: 0.05222735349779707
Loss at iteration [26]: 0.051672855078561925
Loss at iteration [27]: 0.051450800700930825
Loss at iteration [28]: 0.051352126162889505
Loss at iteration [29]: 0.05131437432411338
Loss at iteration [30]: 0.05128493152684134
Loss at iteration [31]: 0.051247538780175746
Loss at iteration [32]: 0.051234632674511615
Loss at iteration [33]: 0.05121089621144902
Loss at iteration [34]: 0.05118679754550631
Loss at iteration [35]: 0.05116994696119081
Loss at iteration [36]: 0.051153110673747566
Loss at iteration [37]: 0.05114527298957528
Loss at iteration [38]: 0.05113661614674907
Loss at iteration [39]: 0.05112785519996538
Loss at iteration [40]: 0.051121734302707805
Loss at iteration [41]: 0.05111635754989568
Loss at iteration [42]: 0.05111086600230627
Loss at iteration [43]: 0.05110375694097201
Loss at iteration [44]: 0.0510937930848174
Loss at iteration [45]: 0.05108975519511441
Loss at iteration [46]: 0.05108413852490333
Loss at iteration [47]: 0.05107619403770292
Loss at iteration [48]: 0.0510719649773122
Loss at iteration [49]: 0.05106358366466458
Loss at iteration [50]: 0.05105605122845031
Loss at iteration [51]: 0.051045980150819126
Loss at iteration [52]: 0.05103902830943582
Loss at iteration [53]: 0.051031741609511856
Loss at iteration [54]: 0.051024816146173
Loss at iteration [55]: 0.05101988231290871
Loss at iteration [56]: 0.051016558572905095
Loss at iteration [57]: 0.05101261303729619
Loss at iteration [58]: 0.051011291816354505
Loss at iteration [59]: 0.05100157453439027
Loss at iteration [60]: 0.05099662730485376
Loss at iteration [61]: 0.05099258683967576
Loss at iteration [62]: 0.05098712865291147
Loss at iteration [63]: 0.05098428276957006
Loss at iteration [64]: 0.05098235261232037
Loss at iteration [65]: 0.05097876776774202
Loss at iteration [66]: 0.050973245383290205
Loss at iteration [67]: 0.05096616640768025
Loss at iteration [68]: 0.05096357856593939
Loss at iteration [69]: 0.05096170821098785
Loss at iteration [70]: 0.05095983657939193
Loss at iteration [71]: 0.05095402741173146
Loss at iteration [72]: 0.0509491582902033
Loss at iteration [73]: 0.05094711130719181
Loss at iteration [74]: 0.05094391830623099
Loss at iteration [75]: 0.05094170957492499
Loss at iteration [76]: 0.050939071604060984
Loss at iteration [77]: 0.050932552022454926
Loss at iteration [78]: 0.05092302414555495
Loss at iteration [79]: 0.050918873186354546
Loss at iteration [80]: 0.05091167505822499
Loss at iteration [81]: 0.05090750932305743
Loss at iteration [82]: 0.05090526224267928
Loss at iteration [83]: 0.05090187856748758
Loss at iteration [84]: 0.05089774087727329
Loss at iteration [85]: 0.050891954556492945
Loss at iteration [86]: 0.050887565379684856
Loss at iteration [87]: 0.05088402202348087
Loss at iteration [88]: 0.05088112363891939
Loss at iteration [89]: 0.05087964950404145
Loss at iteration [90]: 0.050877953857090256
Loss at iteration [91]: 0.05087655462927135
Loss at iteration [92]: 0.05087554659496669
Loss at iteration [93]: 0.05087210138320882
Loss at iteration [94]: 0.05086740655571006
Loss at iteration [95]: 0.05086641876483659
Loss at iteration [96]: 0.05086401838310561
Loss at iteration [97]: 0.05086118938046857
Loss at iteration [98]: 0.05085805328742439
Loss at iteration [99]: 0.05085384819935606
Loss at iteration [100]: 0.050849961233801075
Loss at iteration [101]: 0.0508468515559516
Loss at iteration [102]: 0.05084433484039737
Loss at iteration [103]: 0.050843386211202504
Loss at iteration [104]: 0.05084207984825081
Loss at iteration [105]: 0.05084067399182279
Loss at iteration [106]: 0.05083992826345549
Loss at iteration [107]: 0.050838168996316116
Loss at iteration [108]: 0.0508359145624924
Loss at iteration [109]: 0.05083405655063565
Loss at iteration [110]: 0.0508316214718486
Loss at iteration [111]: 0.050828742757734
Loss at iteration [112]: 0.050825763876694775
Loss at iteration [113]: 0.05082216104760351
Loss at iteration [114]: 0.050820472162887494
Loss at iteration [115]: 0.05081871781430062
Loss at iteration [116]: 0.05081747240585838
Loss at iteration [117]: 0.05081596914880244
