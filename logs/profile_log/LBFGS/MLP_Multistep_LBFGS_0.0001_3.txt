Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : LBFGS
Learning rate                         : 0.0001
Total number of function evaluations  : 3019
Total number of iterations            : 626
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 19.638264894485474
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 50.22535685629662%
Percentage of parameters < 1e-7       : 50.22585214609067%
Percentage of parameters < 1e-6       : 50.22634743588473%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5922332690023109
Loss at iteration [2]: 1.2853465109601103
Loss at iteration [3]: 1.2651251491621507
Loss at iteration [4]: 1.2609926351808167
Loss at iteration [5]: 1.2564674211781335
Loss at iteration [6]: 1.2340115892027999
Loss at iteration [7]: 1.2014252743384055
Loss at iteration [8]: 1.1572079482939277
Loss at iteration [9]: 1.1213108134079233
Loss at iteration [10]: 1.0959977102579332
Loss at iteration [11]: 1.068833733082769
Loss at iteration [12]: 1.0350110376591248
Loss at iteration [13]: 0.9973728314277046
Loss at iteration [14]: 0.9661917942617533
Loss at iteration [15]: 0.9442116585736128
Loss at iteration [16]: 0.9240963824040906
Loss at iteration [17]: 0.8943045634268224
Loss at iteration [18]: 0.8704839581041272
Loss at iteration [19]: 0.8512979047886717
Loss at iteration [20]: 0.8405886103237886
Loss at iteration [21]: 0.832412797130924
Loss at iteration [22]: 0.8226875267762325
Loss at iteration [23]: 0.8126625766059594
Loss at iteration [24]: 0.8009661674469473
Loss at iteration [25]: 0.7925564747694808
Loss at iteration [26]: 0.7875945867888291
Loss at iteration [27]: 0.7830665052959486
Loss at iteration [28]: 0.7786000994608072
Loss at iteration [29]: 0.7725830426831612
Loss at iteration [30]: 0.7706642217551462
Loss at iteration [31]: 0.7662876718989959
Loss at iteration [32]: 0.7557513902553162
Loss at iteration [33]: 0.7393851301872737
Loss at iteration [34]: 0.7268713024679845
Loss at iteration [35]: 0.7145380260884475
Loss at iteration [36]: 0.7015662190107284
Loss at iteration [37]: 0.6884226483678215
Loss at iteration [38]: 0.6793155281074252
Loss at iteration [39]: 0.656525097889575
Loss at iteration [40]: 0.6401920222857096
Loss at iteration [41]: 0.6150396335558305
Loss at iteration [42]: 0.5936698607385177
Loss at iteration [43]: 0.5745264664324237
Loss at iteration [44]: 0.5644951971036973
Loss at iteration [45]: 0.5514635975442851
Loss at iteration [46]: 0.5216044660092992
Loss at iteration [47]: 0.4962980238206967
Loss at iteration [48]: 0.4778686008882546
Loss at iteration [49]: 0.44980747689586204
Loss at iteration [50]: 0.42679139809523914
Loss at iteration [51]: 0.39782560463322575
Loss at iteration [52]: 0.3729810308906705
Loss at iteration [53]: 0.36611199414205803
Loss at iteration [54]: 0.35373725060975686
Loss at iteration [55]: 0.3400577184500108
Loss at iteration [56]: 0.3228193460364246
Loss at iteration [57]: 0.2974025135304764
Loss at iteration [58]: 0.27926963332806193
Loss at iteration [59]: 0.26816966215815613
Loss at iteration [60]: 0.256133025629018
Loss at iteration [61]: 0.24988193401204836
Loss at iteration [62]: 0.2431741569768101
Loss at iteration [63]: 0.23339291981408358
Loss at iteration [64]: 0.22309455998433803
Loss at iteration [65]: 0.2151009774833601
Loss at iteration [66]: 0.20357882369151742
Loss at iteration [67]: 0.19430514979750835
Loss at iteration [68]: 0.1850618283822953
Loss at iteration [69]: 0.17952835840061002
Loss at iteration [70]: 0.17392603766079506
Loss at iteration [71]: 0.16923397120262385
Loss at iteration [72]: 0.16403409026337662
Loss at iteration [73]: 0.15484077389764067
Loss at iteration [74]: 0.1483558362543055
Loss at iteration [75]: 0.14512756499223706
Loss at iteration [76]: 0.14121727894256095
Loss at iteration [77]: 0.13584776364543963
Loss at iteration [78]: 0.13080702455027854
Loss at iteration [79]: 0.12771761830566242
Loss at iteration [80]: 0.12466035035198927
Loss at iteration [81]: 0.12209165053341176
Loss at iteration [82]: 0.11911399331400972
Loss at iteration [83]: 0.1173272972063412
Loss at iteration [84]: 0.11416753513656318
Loss at iteration [85]: 0.11054035198828993
Loss at iteration [86]: 0.10410429514663387
Loss at iteration [87]: 0.10118204366964169
Loss at iteration [88]: 0.09884530114182984
Loss at iteration [89]: 0.09752706458312276
Loss at iteration [90]: 0.0970583763162642
Loss at iteration [91]: 0.09672709930528466
Loss at iteration [92]: 0.09565268173837604
Loss at iteration [93]: 0.09386168765280441
Loss at iteration [94]: 0.09275855910701916
Loss at iteration [95]: 0.091413264798855
Loss at iteration [96]: 0.08995038275903583
Loss at iteration [97]: 0.08877768387256058
Loss at iteration [98]: 0.08770446335804073
Loss at iteration [99]: 0.08708319415371102
Loss at iteration [100]: 0.0857221018486038
Loss at iteration [101]: 0.08507226060832082
Loss at iteration [102]: 0.0845319566045305
Loss at iteration [103]: 0.08331074882431848
Loss at iteration [104]: 0.08188760926846067
Loss at iteration [105]: 0.08089350474357028
Loss at iteration [106]: 0.08053613874360926
Loss at iteration [107]: 0.07983284943159803
Loss at iteration [108]: 0.07915568310856874
Loss at iteration [109]: 0.07876757783935058
Loss at iteration [110]: 0.0782829404840552
Loss at iteration [111]: 0.07793982940297112
Loss at iteration [112]: 0.07761869424435713
Loss at iteration [113]: 0.07709435934074964
