Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : LBFGS
Learning rate                         : 0.1
Total number of function evaluations  : 1976
Total number of iterations            : 1308
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 30.817380666732788
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 51.326881358282726%
Percentage of parameters < 1e-7       : 51.326881358282726%
Percentage of parameters < 1e-6       : 51.32787193787085%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5926918084324162
Loss at iteration [2]: 1.2580081711841506
Loss at iteration [3]: 1.180505582688242
Loss at iteration [4]: 1.0790511134908483
Loss at iteration [5]: 0.9584760157714691
Loss at iteration [6]: 0.8888460802460957
Loss at iteration [7]: 0.8429266745949856
Loss at iteration [8]: 0.787925385337026
Loss at iteration [9]: 0.7482644506078646
Loss at iteration [10]: 0.7243584412637397
Loss at iteration [11]: 0.6906301670384754
Loss at iteration [12]: 0.6496729215798784
Loss at iteration [13]: 0.583183072769858
Loss at iteration [14]: 0.529870223677748
Loss at iteration [15]: 0.46358169875541144
Loss at iteration [16]: 0.40930440203227986
Loss at iteration [17]: 0.354537059115585
Loss at iteration [18]: 0.3012136635289525
Loss at iteration [19]: 0.2701453010543795
Loss at iteration [20]: 0.24712516342332835
Loss at iteration [21]: 0.2029189030315345
Loss at iteration [22]: 0.17075776843285292
Loss at iteration [23]: 0.14697892071338564
Loss at iteration [24]: 0.12539607775924883
Loss at iteration [25]: 0.11223095218978343
Loss at iteration [26]: 0.10786074558266673
Loss at iteration [27]: 0.10160962756674055
Loss at iteration [28]: 0.09468232540023365
Loss at iteration [29]: 0.09023924468058171
Loss at iteration [30]: 0.08209223841767234
Loss at iteration [31]: 0.07688525534008714
Loss at iteration [32]: 0.07147184313908889
Loss at iteration [33]: 0.06533845038214299
Loss at iteration [34]: 0.06096243081345808
Loss at iteration [35]: 0.05714084163397345
Loss at iteration [36]: 0.052734734299899035
Loss at iteration [37]: 0.05003603835288608
Loss at iteration [38]: 0.0478797230295144
Loss at iteration [39]: 0.04588104370423574
Loss at iteration [40]: 0.04416128326604183
Loss at iteration [41]: 0.04278459638021156
Loss at iteration [42]: 0.04138160003178734
Loss at iteration [43]: 0.04044946492581286
Loss at iteration [44]: 0.03896703338635701
Loss at iteration [45]: 0.037649804462079835
Loss at iteration [46]: 0.03681725580523573
Loss at iteration [47]: 0.03624223505431737
Loss at iteration [48]: 0.035507842012867656
Loss at iteration [49]: 0.034820294836110474
Loss at iteration [50]: 0.03256353490802819
Loss at iteration [51]: 0.031226931336758957
Loss at iteration [52]: 0.03080212865383531
Loss at iteration [53]: 0.030323457328628366
Loss at iteration [54]: 0.03006388403855894
Loss at iteration [55]: 0.0296628848784701
Loss at iteration [56]: 0.02920828069319741
Loss at iteration [57]: 0.028600436943858113
Loss at iteration [58]: 0.02830102657089549
Loss at iteration [59]: 0.027584820136451534
Loss at iteration [60]: 0.027114650118570848
Loss at iteration [61]: 0.026842288954618085
Loss at iteration [62]: 0.02659879516033578
Loss at iteration [63]: 0.026403809135653523
Loss at iteration [64]: 0.02628706632183265
Loss at iteration [65]: 0.026082160986430823
Loss at iteration [66]: 0.02598446114480296
Loss at iteration [67]: 0.025876368078874844
Loss at iteration [68]: 0.02569871731639342
Loss at iteration [69]: 0.025574148728399606
Loss at iteration [70]: 0.0254398717924917
Loss at iteration [71]: 0.025313568298273425
Loss at iteration [72]: 0.025228218408505868
Loss at iteration [73]: 0.02502563715981209
Loss at iteration [74]: 0.024775262167325614
Loss at iteration [75]: 0.02456506298637841
Loss at iteration [76]: 0.024430659379234426
Loss at iteration [77]: 0.024322212329409974
Loss at iteration [78]: 0.02424391537793347
