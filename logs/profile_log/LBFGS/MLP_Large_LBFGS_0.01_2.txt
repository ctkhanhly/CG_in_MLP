Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : LBFGS
Learning rate                         : 0.01
Total number of function evaluations  : 3018
Total number of iterations            : 1103
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 80.39888525009155
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.916293740562836%
Percentage of parameters < 1e-7       : 49.916293740562836%
Percentage of parameters < 1e-6       : 49.91704000736317%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0543148273750724
Loss at iteration [2]: 0.009160661608362073
Loss at iteration [3]: 0.002612679056084835
Loss at iteration [4]: 0.0025268487984258115
Loss at iteration [5]: 0.0024972306038040515
Loss at iteration [6]: 0.0024822588577406877
Loss at iteration [7]: 0.002472425581736454
Loss at iteration [8]: 0.0024588514067078794
Loss at iteration [9]: 0.00244829546835086
Loss at iteration [10]: 0.0024370052571416226
Loss at iteration [11]: 0.0024247691362061334
Loss at iteration [12]: 0.0024082381646431633
Loss at iteration [13]: 0.002394906213107431
Loss at iteration [14]: 0.002384690747431393
Loss at iteration [15]: 0.0023704409569624187
Loss at iteration [16]: 0.002353257256271339
Loss at iteration [17]: 0.00233385066455237
Loss at iteration [18]: 0.0023222138829155636
Loss at iteration [19]: 0.0023087562501348667
Loss at iteration [20]: 0.0022959656820630402
Loss at iteration [21]: 0.002280661622363895
Loss at iteration [22]: 0.0022722277797659796
Loss at iteration [23]: 0.00225672869641538
Loss at iteration [24]: 0.002241951337501929
Loss at iteration [25]: 0.002226460750148784
Loss at iteration [26]: 0.002212371992611839
Loss at iteration [27]: 0.002202869175126843
Loss at iteration [28]: 0.0021956974919269566
Loss at iteration [29]: 0.002186366251526919
Loss at iteration [30]: 0.0021825348116857614
Loss at iteration [31]: 0.0021767641389592468
Loss at iteration [32]: 0.0021713571725063313
Loss at iteration [33]: 0.0021644464820304273
Loss at iteration [34]: 0.002158174778412613
Loss at iteration [35]: 0.0021478841135371424
Loss at iteration [36]: 0.0021364654512817036
Loss at iteration [37]: 0.0021250946154178605
Loss at iteration [38]: 0.0021182769069113664
Loss at iteration [39]: 0.0021113506177283786
Loss at iteration [40]: 0.002106331887141205
Loss at iteration [41]: 0.002101216535800953
Loss at iteration [42]: 0.002095480698424715
Loss at iteration [43]: 0.002087029096048953
Loss at iteration [44]: 0.002080061741172254
Loss at iteration [45]: 0.002069950710819136
Loss at iteration [46]: 0.0020604304858361368
Loss at iteration [47]: 0.0020481072461872884
Loss at iteration [48]: 0.0020333658912470172
Loss at iteration [49]: 0.0020211134102713435
Loss at iteration [50]: 0.0020125366849641016
Loss at iteration [51]: 0.002003893311893058
Loss at iteration [52]: 0.001993609831732786
Loss at iteration [53]: 0.001982508523520847
Loss at iteration [54]: 0.0019682645025519492
Loss at iteration [55]: 0.0019537034199106082
Loss at iteration [56]: 0.0019386081894203011
Loss at iteration [57]: 0.0019112065076585204
Loss at iteration [58]: 0.001899454334509891
Loss at iteration [59]: 0.0018893565027186423
Loss at iteration [60]: 0.0018792495503222111
Loss at iteration [61]: 0.0018703309910093752
Loss at iteration [62]: 0.0018592940553092965
Loss at iteration [63]: 0.001849168427546024
Loss at iteration [64]: 0.0018391442297522145
Loss at iteration [65]: 0.0018243734836677235
Loss at iteration [66]: 0.0018080649636171326
Loss at iteration [67]: 0.0017938415153369428
Loss at iteration [68]: 0.0017821841229961374
Loss at iteration [69]: 0.0017720889684524216
Loss at iteration [70]: 0.001765890498519282
Loss at iteration [71]: 0.0017576201330654602
Loss at iteration [72]: 0.0017494065047655655
Loss at iteration [73]: 0.001741780126350719
Loss at iteration [74]: 0.0017319815128010493
Loss at iteration [75]: 0.0017255561672141271
Loss at iteration [76]: 0.0017193475119339397
Loss at iteration [77]: 0.001714192787354314
Loss at iteration [78]: 0.0017071020978894037
Loss at iteration [79]: 0.001701379869040799
Loss at iteration [80]: 0.001696700672886916
Loss at iteration [81]: 0.001692038093103916
Loss at iteration [82]: 0.0016835630539233433
Loss at iteration [83]: 0.0016746437248288074
Loss at iteration [84]: 0.0016674564658361159
Loss at iteration [85]: 0.0016610086023888355
Loss at iteration [86]: 0.0016537337207369993
Loss at iteration [87]: 0.0016476717655322648
Loss at iteration [88]: 0.0016433929334470348
Loss at iteration [89]: 0.0016399101869167757
Loss at iteration [90]: 0.0016365414634560375
Loss at iteration [91]: 0.0016330419066226324
Loss at iteration [92]: 0.0016288134034722474
Loss at iteration [93]: 0.0016251099622078464
Loss at iteration [94]: 0.0016221243689435445
Loss at iteration [95]: 0.0016182954692259353
Loss at iteration [96]: 0.0016134543829143106
Loss at iteration [97]: 0.0016103718183291434
Loss at iteration [98]: 0.0016063840621415768
Loss at iteration [99]: 0.001604736094793907
Loss at iteration [100]: 0.0016017204264951012
Loss at iteration [101]: 0.0015984187991105638
Loss at iteration [102]: 0.001594808775294402
Loss at iteration [103]: 0.0015910962622374088
Loss at iteration [104]: 0.0015855457220364721
Loss at iteration [105]: 0.001579375198034391
Loss at iteration [106]: 0.0015740547068289327
Loss at iteration [107]: 0.0015700779948114028
Loss at iteration [108]: 0.0015648206140915651
Loss at iteration [109]: 0.0015599533727997672
Loss at iteration [110]: 0.0015534857266867516
Loss at iteration [111]: 0.0015475640263004254
Loss at iteration [112]: 0.0015404399323925171
Loss at iteration [113]: 0.0015338702562591388
Loss at iteration [114]: 0.001528104330763821
Loss at iteration [115]: 0.0015197940090428573
Loss at iteration [116]: 0.0015123957057075563
Loss at iteration [117]: 0.0015080662817855178
