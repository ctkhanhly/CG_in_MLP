Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : LBFGS
Learning rate                         : 0.001
Total number of function evaluations  : 3011
Total number of iterations            : 772
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 21.193803548812866
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 50.02029682874427%
Percentage of parameters < 1e-7       : 50.02029682874427%
Percentage of parameters < 1e-6       : 50.02029682874427%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.9907776988379573
Loss at iteration [2]: 0.022068657879050043
Loss at iteration [3]: 0.007681387945297637
Loss at iteration [4]: 0.0031278978827668953
Loss at iteration [5]: 0.00264632538102508
Loss at iteration [6]: 0.0025253191880677964
Loss at iteration [7]: 0.002497643266093327
Loss at iteration [8]: 0.0024667011201614584
Loss at iteration [9]: 0.0024608877644546824
Loss at iteration [10]: 0.002449461179703097
Loss at iteration [11]: 0.0024385845990012147
Loss at iteration [12]: 0.0024297030037142326
Loss at iteration [13]: 0.0024247677308961114
Loss at iteration [14]: 0.0024021550503128034
Loss at iteration [15]: 0.0023937370882127417
Loss at iteration [16]: 0.002385038399099299
Loss at iteration [17]: 0.002379261509783174
Loss at iteration [18]: 0.002358598767078939
Loss at iteration [19]: 0.0023420643375557226
Loss at iteration [20]: 0.002331400832372022
Loss at iteration [21]: 0.0023211389133073316
Loss at iteration [22]: 0.002305943489383348
Loss at iteration [23]: 0.0022858571013998365
Loss at iteration [24]: 0.0022748295577938314
Loss at iteration [25]: 0.002256870150197953
Loss at iteration [26]: 0.002250818681137833
Loss at iteration [27]: 0.002241512722628549
Loss at iteration [28]: 0.002233947896260316
Loss at iteration [29]: 0.0022234738626079474
Loss at iteration [30]: 0.002215095707951358
Loss at iteration [31]: 0.0022079258910388407
Loss at iteration [32]: 0.002198470253075575
Loss at iteration [33]: 0.0021931210222883232
Loss at iteration [34]: 0.002187318206215769
Loss at iteration [35]: 0.0021823652362861893
Loss at iteration [36]: 0.002177999792712274
Loss at iteration [37]: 0.002173772708194391
Loss at iteration [38]: 0.0021684441055045307
Loss at iteration [39]: 0.002164807403293713
Loss at iteration [40]: 0.0021615031677563287
Loss at iteration [41]: 0.0021569254031782825
Loss at iteration [42]: 0.0021514959219843775
Loss at iteration [43]: 0.0021468608900198655
Loss at iteration [44]: 0.0021427168725896887
Loss at iteration [45]: 0.0021383512012033944
Loss at iteration [46]: 0.0021349141537360286
Loss at iteration [47]: 0.002131644139305145
Loss at iteration [48]: 0.002128347182048261
Loss at iteration [49]: 0.0021236661407468015
Loss at iteration [50]: 0.0021133944927248355
Loss at iteration [51]: 0.0021055883202277114
Loss at iteration [52]: 0.0020954443292859927
Loss at iteration [53]: 0.002088096430380216
Loss at iteration [54]: 0.0020782112691181175
Loss at iteration [55]: 0.002066160278305791
Loss at iteration [56]: 0.002054011448058561
Loss at iteration [57]: 0.0020457041194320495
Loss at iteration [58]: 0.0020346450099952783
Loss at iteration [59]: 0.002022392027265886
Loss at iteration [60]: 0.002010671448017378
Loss at iteration [61]: 0.00199508226373157
Loss at iteration [62]: 0.0019832585883056454
Loss at iteration [63]: 0.0019657851209324484
Loss at iteration [64]: 0.0019516318840079841
Loss at iteration [65]: 0.0019391074884682012
Loss at iteration [66]: 0.0019267591447420854
Loss at iteration [67]: 0.0019139861471580604
Loss at iteration [68]: 0.0019006838806421926
Loss at iteration [69]: 0.0018826211428627133
Loss at iteration [70]: 0.0018664805854288007
Loss at iteration [71]: 0.0018563442089551362
Loss at iteration [72]: 0.001841440373163008
Loss at iteration [73]: 0.0018280977659615846
Loss at iteration [74]: 0.0018174320366152897
Loss at iteration [75]: 0.0018041194092285064
Loss at iteration [76]: 0.0017951672149129655
Loss at iteration [77]: 0.001788202983721924
Loss at iteration [78]: 0.0017775762921110356
Loss at iteration [79]: 0.0017699961067813368
Loss at iteration [80]: 0.0017606363356884216
Loss at iteration [81]: 0.0017545481449443125
Loss at iteration [82]: 0.0017499971074997434
Loss at iteration [83]: 0.001744395480041854
Loss at iteration [84]: 0.0017329814704389566
Loss at iteration [85]: 0.001727740448103198
Loss at iteration [86]: 0.0017217940926880309
Loss at iteration [87]: 0.001716882309341379
Loss at iteration [88]: 0.0017108489828448945
Loss at iteration [89]: 0.0017072958629635283
Loss at iteration [90]: 0.001702868697762952
Loss at iteration [91]: 0.0016983934262933252
Loss at iteration [92]: 0.001693606309066438
Loss at iteration [93]: 0.0016883781593780842
Loss at iteration [94]: 0.0016813068042663651
Loss at iteration [95]: 0.0016748978629416592
Loss at iteration [96]: 0.0016694533234208704
Loss at iteration [97]: 0.0016624364216586062
Loss at iteration [98]: 0.0016545005007613499
Loss at iteration [99]: 0.0016445382107703004
Loss at iteration [100]: 0.0016366663707064026
Loss at iteration [101]: 0.0016311025496202273
Loss at iteration [102]: 0.001628817466891756
Loss at iteration [103]: 0.001626531825782322
Loss at iteration [104]: 0.001623246298759385
Loss at iteration [105]: 0.0016170065398159333
Loss at iteration [106]: 0.001612008832665282
Loss at iteration [107]: 0.0016086605628858948
Loss at iteration [108]: 0.0016059873859100577
Loss at iteration [109]: 0.0015991126407374724
Loss at iteration [110]: 0.0015963689705181601
Loss at iteration [111]: 0.0015927424954823297
Loss at iteration [112]: 0.001588128582601256
Loss at iteration [113]: 0.0015848317168170901
Loss at iteration [114]: 0.0015823486090772644
