Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : LBFGS
Learning rate                         : 0.1
Total number of function evaluations  : 3023
Total number of iterations            : 2095
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 38.120564699172974
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.96782210077128%
Percentage of parameters < 1e-7       : 49.96782210077128%
Percentage of parameters < 1e-6       : 49.968812189978316%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0409449746459651
Loss at iteration [2]: 0.008795394538766127
Loss at iteration [3]: 0.0026418852944285476
Loss at iteration [4]: 0.0025017849488144896
Loss at iteration [5]: 0.0024649417628618145
Loss at iteration [6]: 0.002443061273658011
Loss at iteration [7]: 0.0024157565766840675
Loss at iteration [8]: 0.002390534927202161
Loss at iteration [9]: 0.002376195813981091
Loss at iteration [10]: 0.0023552616448219714
Loss at iteration [11]: 0.002308401060190429
Loss at iteration [12]: 0.0022623822260852567
Loss at iteration [13]: 0.002233459973773343
Loss at iteration [14]: 0.0022106663394034563
Loss at iteration [15]: 0.0021907891552056385
Loss at iteration [16]: 0.002175826871928893
Loss at iteration [17]: 0.0021656719325379133
Loss at iteration [18]: 0.0021487393649005505
Loss at iteration [19]: 0.002138706002803574
Loss at iteration [20]: 0.002128165325529552
Loss at iteration [21]: 0.0021211832194586596
Loss at iteration [22]: 0.0021131629289535417
Loss at iteration [23]: 0.0021044625375545695
Loss at iteration [24]: 0.002097130211100637
Loss at iteration [25]: 0.002084531106539474
Loss at iteration [26]: 0.002073345234963884
Loss at iteration [27]: 0.0020584391916776777
Loss at iteration [28]: 0.0020485590792035744
Loss at iteration [29]: 0.0020370326909219133
Loss at iteration [30]: 0.0020264437929960787
Loss at iteration [31]: 0.0020165478697618438
Loss at iteration [32]: 0.002001867789330757
Loss at iteration [33]: 0.001987860734578263
Loss at iteration [34]: 0.001967827167378642
Loss at iteration [35]: 0.0019499104313597684
Loss at iteration [36]: 0.0019397592958597528
Loss at iteration [37]: 0.001923715770976255
Loss at iteration [38]: 0.0019142945376970364
Loss at iteration [39]: 0.001904940149383209
Loss at iteration [40]: 0.0018932438223628165
Loss at iteration [41]: 0.0018780799862216254
Loss at iteration [42]: 0.00186229999857956
Loss at iteration [43]: 0.0018369141050581896
Loss at iteration [44]: 0.0018189637420714768
Loss at iteration [45]: 0.0018104765694506144
Loss at iteration [46]: 0.0017989495245176626
Loss at iteration [47]: 0.001785306158339638
Loss at iteration [48]: 0.001776965647846972
Loss at iteration [49]: 0.001768287551933786
Loss at iteration [50]: 0.0017572816954843314
Loss at iteration [51]: 0.0017456936948656771
Loss at iteration [52]: 0.001736718566733235
Loss at iteration [53]: 0.0017294662273897826
Loss at iteration [54]: 0.0017205752885484226
Loss at iteration [55]: 0.0017108369019551055
Loss at iteration [56]: 0.0016917461159888488
Loss at iteration [57]: 0.0016824152390203621
Loss at iteration [58]: 0.0016771538490791518
Loss at iteration [59]: 0.0016696791186952976
Loss at iteration [60]: 0.0016607180106901112
Loss at iteration [61]: 0.0016527078291720604
Loss at iteration [62]: 0.0016456428921024558
Loss at iteration [63]: 0.0016363835853366827
Loss at iteration [64]: 0.0016290399964437017
Loss at iteration [65]: 0.0016230242869599515
Loss at iteration [66]: 0.001615504390101177
Loss at iteration [67]: 0.001608829902690825
Loss at iteration [68]: 0.001599031459587451
Loss at iteration [69]: 0.001590566069721482
Loss at iteration [70]: 0.0015834893816973519
Loss at iteration [71]: 0.0015790110234602524
Loss at iteration [72]: 0.0015758341145644171
Loss at iteration [73]: 0.0015713225856095294
Loss at iteration [74]: 0.0015685547269829456
Loss at iteration [75]: 0.001563464112877088
Loss at iteration [76]: 0.0015593655613560082
Loss at iteration [77]: 0.0015528919968200439
Loss at iteration [78]: 0.0015465828621483817
Loss at iteration [79]: 0.001537611314238155
Loss at iteration [80]: 0.0015299142887697545
Loss at iteration [81]: 0.0015184088325553473
Loss at iteration [82]: 0.0015116024325931906
Loss at iteration [83]: 0.0015043921464764566
Loss at iteration [84]: 0.0014950114604122225
Loss at iteration [85]: 0.0014834133561850472
Loss at iteration [86]: 0.0014761850783820121
Loss at iteration [87]: 0.0014701159742745796
Loss at iteration [88]: 0.0014625659248713914
Loss at iteration [89]: 0.0014556581763781294
Loss at iteration [90]: 0.0014483981511071494
Loss at iteration [91]: 0.001443521250019211
Loss at iteration [92]: 0.0014381953038263785
Loss at iteration [93]: 0.001432172220098116
Loss at iteration [94]: 0.00142576965365677
Loss at iteration [95]: 0.0014185073767927386
Loss at iteration [96]: 0.0014112119912336627
Loss at iteration [97]: 0.0013990107645489593
Loss at iteration [98]: 0.0013890540031088847
Loss at iteration [99]: 0.0013802293135941543
Loss at iteration [100]: 0.0013753014518810187
Loss at iteration [101]: 0.0013701457115793284
Loss at iteration [102]: 0.0013638844496222558
Loss at iteration [103]: 0.001357829689918494
Loss at iteration [104]: 0.0013530021698846637
Loss at iteration [105]: 0.0013488904489122953
Loss at iteration [106]: 0.0013439589393507913
Loss at iteration [107]: 0.0013391206814793468
Loss at iteration [108]: 0.0013349033071822808
Loss at iteration [109]: 0.0013310462149952644
Loss at iteration [110]: 0.001326179036624041
Loss at iteration [111]: 0.0013207972360793545
Loss at iteration [112]: 0.0013176771696125935
Loss at iteration [113]: 0.001314080176523806
Loss at iteration [114]: 0.0013114092348605875
Loss at iteration [115]: 0.001309236819882474
Loss at iteration [116]: 0.0013068718329464827
Loss at iteration [117]: 0.0013035958457626301
Loss at iteration [118]: 0.0013000406309477393
Loss at iteration [119]: 0.0012968431736005532
Loss at iteration [120]: 0.0012931250236464427
