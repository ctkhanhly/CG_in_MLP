Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : LBFGS
Learning rate                         : 0.001
Total number of function evaluations  : 3009
Total number of iterations            : 768
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 67.29668855667114
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.96380606018393%
Percentage of parameters < 1e-7       : 49.964054815784046%
Percentage of parameters < 1e-6       : 49.96480108258437%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0131487816273577
Loss at iteration [2]: 0.014037504351782833
Loss at iteration [3]: 0.004051293476345409
Loss at iteration [4]: 0.0026171461746074196
Loss at iteration [5]: 0.0025440858193402473
Loss at iteration [6]: 0.002517288802596427
Loss at iteration [7]: 0.002503204627176297
Loss at iteration [8]: 0.002485573274743746
Loss at iteration [9]: 0.00247421077012399
Loss at iteration [10]: 0.0024620761992489963
Loss at iteration [11]: 0.002452235810776145
Loss at iteration [12]: 0.002439704074046385
Loss at iteration [13]: 0.0024320926713650155
Loss at iteration [14]: 0.002427199440954685
Loss at iteration [15]: 0.002417666398232254
Loss at iteration [16]: 0.002406429467598955
Loss at iteration [17]: 0.002396203249839786
Loss at iteration [18]: 0.0023874739034208045
Loss at iteration [19]: 0.0023725358760899144
Loss at iteration [20]: 0.0023602443391071344
Loss at iteration [21]: 0.0023496345926016285
Loss at iteration [22]: 0.0023387453927140207
Loss at iteration [23]: 0.0023285070507337723
Loss at iteration [24]: 0.0023122938512645163
Loss at iteration [25]: 0.0022947263588215046
Loss at iteration [26]: 0.002285501915252127
Loss at iteration [27]: 0.0022770212629540923
Loss at iteration [28]: 0.0022629603662205956
Loss at iteration [29]: 0.0022475852131808734
Loss at iteration [30]: 0.002236592012282965
Loss at iteration [31]: 0.0022268095791222013
Loss at iteration [32]: 0.0022189344573252993
Loss at iteration [33]: 0.0022053382367941245
Loss at iteration [34]: 0.002196782310056675
Loss at iteration [35]: 0.0021916296605244737
Loss at iteration [36]: 0.0021875206428970196
Loss at iteration [37]: 0.002184319617690039
Loss at iteration [38]: 0.002179515699643461
Loss at iteration [39]: 0.0021745914143575214
Loss at iteration [40]: 0.0021682468678256447
Loss at iteration [41]: 0.002163041975161004
Loss at iteration [42]: 0.002158695274512234
Loss at iteration [43]: 0.002154069102227502
Loss at iteration [44]: 0.0021489084908318584
Loss at iteration [45]: 0.002146435522013322
Loss at iteration [46]: 0.0021427962213129725
Loss at iteration [47]: 0.0021360927545963
Loss at iteration [48]: 0.0021288541043596385
Loss at iteration [49]: 0.002124297961450199
Loss at iteration [50]: 0.002119446061119628
Loss at iteration [51]: 0.0021138981830376865
Loss at iteration [52]: 0.00210767164100128
Loss at iteration [53]: 0.002099941089005372
Loss at iteration [54]: 0.0020936193733681174
Loss at iteration [55]: 0.0020825381776607243
Loss at iteration [56]: 0.002077089072072576
Loss at iteration [57]: 0.0020728226479582836
Loss at iteration [58]: 0.00206615983536744
Loss at iteration [59]: 0.0020587410186030837
Loss at iteration [60]: 0.0020513499621601454
Loss at iteration [61]: 0.002046226071132469
Loss at iteration [62]: 0.002038376288552
Loss at iteration [63]: 0.0020317130049539705
Loss at iteration [64]: 0.0020261625759631276
Loss at iteration [65]: 0.0020186864506157713
Loss at iteration [66]: 0.0020131541684897053
Loss at iteration [67]: 0.0020085750400745434
Loss at iteration [68]: 0.0020018145500374515
Loss at iteration [69]: 0.0019970105052731515
Loss at iteration [70]: 0.0019917219121185006
Loss at iteration [71]: 0.00198865475547537
Loss at iteration [72]: 0.001983965553991345
Loss at iteration [73]: 0.0019784742455678807
Loss at iteration [74]: 0.00196955209766655
Loss at iteration [75]: 0.0019623097598849546
Loss at iteration [76]: 0.0019570453681721062
Loss at iteration [77]: 0.001951764571174706
Loss at iteration [78]: 0.001944641693917909
Loss at iteration [79]: 0.001936740786840312
Loss at iteration [80]: 0.0019272773178979523
Loss at iteration [81]: 0.001917055355268197
Loss at iteration [82]: 0.0019051157094636087
Loss at iteration [83]: 0.001889033334421678
Loss at iteration [84]: 0.001879017445661135
Loss at iteration [85]: 0.0018665880246109545
Loss at iteration [86]: 0.0018494374217195683
Loss at iteration [87]: 0.0018363594290611658
Loss at iteration [88]: 0.001821123052161635
Loss at iteration [89]: 0.001811183091940203
Loss at iteration [90]: 0.001805940504428944
Loss at iteration [91]: 0.0017993109962525199
Loss at iteration [92]: 0.0017946957123057704
Loss at iteration [93]: 0.0017877481712797456
Loss at iteration [94]: 0.001780943316243277
Loss at iteration [95]: 0.0017732538722488875
Loss at iteration [96]: 0.0017650344608395316
Loss at iteration [97]: 0.0017584085537547336
Loss at iteration [98]: 0.0017535067850144319
Loss at iteration [99]: 0.0017468983298298821
Loss at iteration [100]: 0.0017415444289760075
Loss at iteration [101]: 0.0017348324414382468
Loss at iteration [102]: 0.0017311017116665116
Loss at iteration [103]: 0.0017269834400524995
Loss at iteration [104]: 0.0017232089108327653
Loss at iteration [105]: 0.0017198927362670547
Loss at iteration [106]: 0.0017159108331791365
Loss at iteration [107]: 0.001713186710198818
Loss at iteration [108]: 0.0017090946702599905
Loss at iteration [109]: 0.001705965000168301
Loss at iteration [110]: 0.0017030903767554645
Loss at iteration [111]: 0.0017004155456785363
Loss at iteration [112]: 0.0016981775788990005
Loss at iteration [113]: 0.0016954844897601432
