Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : LBFGS
Learning rate                         : 0.01
Total number of function evaluations  : 3022
Total number of iterations            : 1110
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 27.951364755630493
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.43218383976396%
Percentage of parameters < 1e-7       : 49.43218383976396%
Percentage of parameters < 1e-6       : 49.433173928971%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0611679414956265
Loss at iteration [2]: 0.015252032893399619
Loss at iteration [3]: 0.0028811579245664972
Loss at iteration [4]: 0.002542056240453145
Loss at iteration [5]: 0.0024791845616871874
Loss at iteration [6]: 0.0024545806750154663
Loss at iteration [7]: 0.0024379830851925543
Loss at iteration [8]: 0.002422067894391335
Loss at iteration [9]: 0.0024042765705115394
Loss at iteration [10]: 0.0023838122032138116
Loss at iteration [11]: 0.0023657603932958344
Loss at iteration [12]: 0.0023523482349907996
Loss at iteration [13]: 0.0023329094114730256
Loss at iteration [14]: 0.002318096005869433
Loss at iteration [15]: 0.0023029504133247674
Loss at iteration [16]: 0.0022920459628924094
Loss at iteration [17]: 0.0022794947156305036
Loss at iteration [18]: 0.0022663180336535056
Loss at iteration [19]: 0.002252565318342552
Loss at iteration [20]: 0.002240127829126535
Loss at iteration [21]: 0.0022270226317433243
Loss at iteration [22]: 0.002214973320453452
Loss at iteration [23]: 0.002201656221326814
Loss at iteration [24]: 0.0021901296772500484
Loss at iteration [25]: 0.0021797532028442093
Loss at iteration [26]: 0.0021689481993135175
Loss at iteration [27]: 0.0021606457636153967
Loss at iteration [28]: 0.0021532793816113637
Loss at iteration [29]: 0.00214631743232906
Loss at iteration [30]: 0.0021391606599668006
Loss at iteration [31]: 0.0021279939360781376
Loss at iteration [32]: 0.002110720795670326
Loss at iteration [33]: 0.002090492023914597
Loss at iteration [34]: 0.0020808543483466888
Loss at iteration [35]: 0.002070662722687183
Loss at iteration [36]: 0.002056314635443376
Loss at iteration [37]: 0.0020435387284410797
Loss at iteration [38]: 0.002036653842346074
Loss at iteration [39]: 0.00202761073640892
Loss at iteration [40]: 0.00201380860833215
Loss at iteration [41]: 0.001997079940360875
Loss at iteration [42]: 0.001987631480955063
Loss at iteration [43]: 0.001977832695602404
Loss at iteration [44]: 0.001968742733379401
Loss at iteration [45]: 0.0019626708320768013
Loss at iteration [46]: 0.0019574566624647326
Loss at iteration [47]: 0.0019497012749202813
Loss at iteration [48]: 0.0019429432090174812
Loss at iteration [49]: 0.0019351215786047391
Loss at iteration [50]: 0.001926677128352522
Loss at iteration [51]: 0.0019181485973238802
Loss at iteration [52]: 0.0019018229922864234
Loss at iteration [53]: 0.0018843700057376523
Loss at iteration [54]: 0.0018739587199818014
Loss at iteration [55]: 0.0018604036761350595
Loss at iteration [56]: 0.001846036982129813
Loss at iteration [57]: 0.0018355496572050477
Loss at iteration [58]: 0.001827681583110833
Loss at iteration [59]: 0.0018204814667170822
Loss at iteration [60]: 0.0018130312235256313
Loss at iteration [61]: 0.0018033199016603493
Loss at iteration [62]: 0.0017948771454010712
Loss at iteration [63]: 0.0017874001400688163
Loss at iteration [64]: 0.0017841375082386745
Loss at iteration [65]: 0.001778765361433981
Loss at iteration [66]: 0.0017705610184167977
Loss at iteration [67]: 0.0017637221049450523
Loss at iteration [68]: 0.0017586413627076365
Loss at iteration [69]: 0.001753544244967937
Loss at iteration [70]: 0.0017479000122424674
Loss at iteration [71]: 0.0017442353433745875
Loss at iteration [72]: 0.0017398071802086064
Loss at iteration [73]: 0.001735949317342911
Loss at iteration [74]: 0.0017324561132103988
Loss at iteration [75]: 0.0017277940953310245
Loss at iteration [76]: 0.001723613133930748
Loss at iteration [77]: 0.0017200457825332386
Loss at iteration [78]: 0.0017167361149516562
Loss at iteration [79]: 0.0017138059994954047
Loss at iteration [80]: 0.0017106984375244317
Loss at iteration [81]: 0.00170664529928265
Loss at iteration [82]: 0.0016993055358453246
Loss at iteration [83]: 0.0016923627255530686
Loss at iteration [84]: 0.0016848774710962282
Loss at iteration [85]: 0.001680110553300624
Loss at iteration [86]: 0.0016763299209265907
Loss at iteration [87]: 0.0016708312820824685
Loss at iteration [88]: 0.001666519305435008
Loss at iteration [89]: 0.0016628335856265035
Loss at iteration [90]: 0.0016568451818796432
Loss at iteration [91]: 0.0016519074537218114
Loss at iteration [92]: 0.001646227463058026
Loss at iteration [93]: 0.0016412141701621643
Loss at iteration [94]: 0.0016373922845111075
Loss at iteration [95]: 0.0016342220902823437
Loss at iteration [96]: 0.0016305573558612406
Loss at iteration [97]: 0.0016241339381545192
Loss at iteration [98]: 0.0016197000721939014
Loss at iteration [99]: 0.0016147899470387403
Loss at iteration [100]: 0.0016075929244531817
Loss at iteration [101]: 0.0016001542247079616
Loss at iteration [102]: 0.001595234404324799
Loss at iteration [103]: 0.0015897942734997609
Loss at iteration [104]: 0.0015843890604090123
Loss at iteration [105]: 0.0015786193399467826
Loss at iteration [106]: 0.0015730641776679664
Loss at iteration [107]: 0.0015662532906214883
Loss at iteration [108]: 0.0015586062511399907
Loss at iteration [109]: 0.0015540366168692817
Loss at iteration [110]: 0.0015481285814751035
Loss at iteration [111]: 0.0015437846752846417
Loss at iteration [112]: 0.0015408383700094219
Loss at iteration [113]: 0.0015390632892867495
Loss at iteration [114]: 0.0015366468793263919
Loss at iteration [115]: 0.0015324948694642846
Loss at iteration [116]: 0.001528748680006914
Loss at iteration [117]: 0.0015259708204898815
