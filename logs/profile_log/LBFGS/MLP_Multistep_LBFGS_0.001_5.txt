Model name                            : MLP_Multistep
The number of input features          : 5
The number of output features         : 2
Optimizer name                        : LBFGS
Learning rate                         : 0.001
Total number of function evaluations  : 3003
Total number of iterations            : 790
Max number of iterations              : 3000
Number of samples in training data    : 122
Number of samples in tests data       : 52
Total training time                   : 20.779568195343018
Total number of parameters            : 202302
Percentage of parameters < 1e-9       : 50.01235776215756%
Percentage of parameters < 1e-7       : 50.01235776215756%
Percentage of parameters < 1e-6       : 50.01235776215756%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.550499015297882
Loss at iteration [2]: 1.0024214226823442
Loss at iteration [3]: 0.6410718830167433
Loss at iteration [4]: 0.5744694241006938
Loss at iteration [5]: 0.5436993307386057
Loss at iteration [6]: 0.4564124602172763
Loss at iteration [7]: 0.35819055625506885
Loss at iteration [8]: 0.3073517762379629
Loss at iteration [9]: 0.27986126146286605
Loss at iteration [10]: 0.26021245543524946
Loss at iteration [11]: 0.24846468662680124
Loss at iteration [12]: 0.2361109530750705
Loss at iteration [13]: 0.22568697480417332
Loss at iteration [14]: 0.21893172418737455
Loss at iteration [15]: 0.2107450641181811
Loss at iteration [16]: 0.205595247568131
Loss at iteration [17]: 0.18785873261384578
Loss at iteration [18]: 0.16388861750847106
Loss at iteration [19]: 0.13248183170904348
Loss at iteration [20]: 0.12075830959572147
Loss at iteration [21]: 0.1056458651048416
Loss at iteration [22]: 0.09383701559462203
Loss at iteration [23]: 0.08704356904194702
Loss at iteration [24]: 0.08205494570329007
Loss at iteration [25]: 0.0781427702945333
Loss at iteration [26]: 0.07276572932491651
Loss at iteration [27]: 0.06758587876031238
Loss at iteration [28]: 0.06297472778148637
Loss at iteration [29]: 0.06065673131888054
Loss at iteration [30]: 0.05795137256362045
Loss at iteration [31]: 0.05556959781167932
Loss at iteration [32]: 0.0547059438158855
Loss at iteration [33]: 0.053610016801595245
Loss at iteration [34]: 0.05255757686819547
Loss at iteration [35]: 0.05198070471206608
Loss at iteration [36]: 0.05158549408768297
Loss at iteration [37]: 0.05137579604711174
Loss at iteration [38]: 0.05128474809610638
Loss at iteration [39]: 0.0512523990695765
Loss at iteration [40]: 0.05123541996788429
Loss at iteration [41]: 0.05122419055329979
Loss at iteration [42]: 0.05121381349934172
Loss at iteration [43]: 0.05119397742351303
Loss at iteration [44]: 0.05118135726126913
Loss at iteration [45]: 0.051173275465132576
Loss at iteration [46]: 0.051157148858455265
Loss at iteration [47]: 0.05114309744615993
Loss at iteration [48]: 0.05113432758447231
Loss at iteration [49]: 0.05112520026529117
Loss at iteration [50]: 0.05111563218994251
Loss at iteration [51]: 0.051112318752865024
Loss at iteration [52]: 0.05110645025024303
Loss at iteration [53]: 0.05110205570146128
Loss at iteration [54]: 0.05109956731336045
Loss at iteration [55]: 0.05109494059109557
Loss at iteration [56]: 0.051090867433596206
Loss at iteration [57]: 0.05108573459791479
Loss at iteration [58]: 0.05107934209920197
Loss at iteration [59]: 0.05107662123739497
Loss at iteration [60]: 0.051069485085045226
Loss at iteration [61]: 0.05106364130443435
Loss at iteration [62]: 0.05105168790923055
Loss at iteration [63]: 0.05104360794334424
Loss at iteration [64]: 0.051040583313954856
Loss at iteration [65]: 0.051037188562821516
Loss at iteration [66]: 0.05103319518237463
Loss at iteration [67]: 0.05102763875857779
Loss at iteration [68]: 0.051021819632096184
Loss at iteration [69]: 0.05101633651493778
Loss at iteration [70]: 0.051006956372466855
Loss at iteration [71]: 0.05100151169161417
Loss at iteration [72]: 0.050994953711545554
Loss at iteration [73]: 0.05099158886739596
Loss at iteration [74]: 0.0509894141584842
Loss at iteration [75]: 0.05098599326301854
Loss at iteration [76]: 0.050982098631440154
Loss at iteration [77]: 0.05097825728498726
Loss at iteration [78]: 0.05097479032198922
Loss at iteration [79]: 0.05097267615954255
Loss at iteration [80]: 0.05097054330572071
Loss at iteration [81]: 0.05096776759605985
Loss at iteration [82]: 0.050964639958589936
Loss at iteration [83]: 0.0509631589370607
Loss at iteration [84]: 0.05096010735071136
Loss at iteration [85]: 0.05095734181421776
Loss at iteration [86]: 0.050954724320861684
Loss at iteration [87]: 0.05095194915746907
Loss at iteration [88]: 0.05094946298707734
Loss at iteration [89]: 0.05094536941406156
Loss at iteration [90]: 0.050935042220372456
Loss at iteration [91]: 0.05092920698143807
Loss at iteration [92]: 0.05092378382025403
Loss at iteration [93]: 0.050917267309026994
Loss at iteration [94]: 0.050913773833926694
Loss at iteration [95]: 0.05091120208096555
Loss at iteration [96]: 0.050910099792490594
Loss at iteration [97]: 0.05090905868628345
Loss at iteration [98]: 0.050907130131459775
Loss at iteration [99]: 0.05090354885365675
Loss at iteration [100]: 0.05089852415911736
Loss at iteration [101]: 0.05089252453764158
Loss at iteration [102]: 0.05088959384666234
Loss at iteration [103]: 0.05088675064819393
Loss at iteration [104]: 0.05088540088154737
Loss at iteration [105]: 0.05088414594163994
Loss at iteration [106]: 0.05088188696280642
Loss at iteration [107]: 0.05088037066717274
Loss at iteration [108]: 0.05087960706323103
Loss at iteration [109]: 0.050878038923930675
Loss at iteration [110]: 0.05087670783323256
Loss at iteration [111]: 0.05087427812247386
Loss at iteration [112]: 0.05087090400624101
Loss at iteration [113]: 0.05086953028256142
Loss at iteration [114]: 0.0508666934507879
