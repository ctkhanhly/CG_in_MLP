Model name                            : MLP_Multistep
The number of input features          : 5
The number of output features         : 2
Optimizer name                        : LBFGS
Learning rate                         : 0.0001
Total number of function evaluations  : 3020
Total number of iterations            : 616
Max number of iterations              : 3000
Number of samples in training data    : 122
Number of samples in tests data       : 52
Total training time                   : 17.105231285095215
Total number of parameters            : 202302
Percentage of parameters < 1e-9       : 50.002471552431516%
Percentage of parameters < 1e-7       : 50.002471552431516%
Percentage of parameters < 1e-6       : 50.00296586291781%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5212738874665368
Loss at iteration [2]: 1.1438942292731173
Loss at iteration [3]: 0.8232488507604622
Loss at iteration [4]: 0.659470436707394
Loss at iteration [5]: 0.584188576912264
Loss at iteration [6]: 0.5616992368178909
Loss at iteration [7]: 0.5410569773065619
Loss at iteration [8]: 0.47747119636529745
Loss at iteration [9]: 0.37645330306303654
Loss at iteration [10]: 0.34648233182854227
Loss at iteration [11]: 0.30994364030943183
Loss at iteration [12]: 0.2953662466266859
Loss at iteration [13]: 0.2863635071704634
Loss at iteration [14]: 0.27603600698546304
Loss at iteration [15]: 0.26395619876068555
Loss at iteration [16]: 0.25760753380323614
Loss at iteration [17]: 0.25573647632264107
Loss at iteration [18]: 0.2507399283154711
Loss at iteration [19]: 0.2460011829835455
Loss at iteration [20]: 0.24097058747337033
Loss at iteration [21]: 0.23673132897472407
Loss at iteration [22]: 0.23042130010920137
Loss at iteration [23]: 0.225088349203636
Loss at iteration [24]: 0.21315447256115097
Loss at iteration [25]: 0.20203421197447216
Loss at iteration [26]: 0.19131466074426345
Loss at iteration [27]: 0.16904640114941405
Loss at iteration [28]: 0.15039428614070835
Loss at iteration [29]: 0.1314454690098244
Loss at iteration [30]: 0.11877722776781882
Loss at iteration [31]: 0.11056883319357909
Loss at iteration [32]: 0.10682484389115192
Loss at iteration [33]: 0.10339666885040888
Loss at iteration [34]: 0.09857845908279518
Loss at iteration [35]: 0.0952011145356396
Loss at iteration [36]: 0.09208334740754019
Loss at iteration [37]: 0.08870600079661045
Loss at iteration [38]: 0.08404760008544292
Loss at iteration [39]: 0.08269107198646483
Loss at iteration [40]: 0.07948862092622858
Loss at iteration [41]: 0.07806436468845135
Loss at iteration [42]: 0.0725748104273713
Loss at iteration [43]: 0.06843592829309514
Loss at iteration [44]: 0.0641478399879554
Loss at iteration [45]: 0.06273705530009505
Loss at iteration [46]: 0.06149992574786184
Loss at iteration [47]: 0.058978390033117
Loss at iteration [48]: 0.057731254342170864
Loss at iteration [49]: 0.0558154736449733
Loss at iteration [50]: 0.05489933181112914
Loss at iteration [51]: 0.05384747028224255
Loss at iteration [52]: 0.05301654845542885
Loss at iteration [53]: 0.052480345902243146
Loss at iteration [54]: 0.05219963613270646
Loss at iteration [55]: 0.051999409169042846
Loss at iteration [56]: 0.05193396429242881
Loss at iteration [57]: 0.051880303099117
Loss at iteration [58]: 0.051793157719471766
Loss at iteration [59]: 0.05168849937008925
Loss at iteration [60]: 0.051619023879724446
Loss at iteration [61]: 0.05154037531976121
Loss at iteration [62]: 0.05150215799335531
Loss at iteration [63]: 0.05144326341926516
Loss at iteration [64]: 0.05139836079726297
Loss at iteration [65]: 0.051319374378291704
Loss at iteration [66]: 0.05130033704664313
Loss at iteration [67]: 0.05127325192516058
Loss at iteration [68]: 0.05125077971653452
Loss at iteration [69]: 0.05123887549300133
Loss at iteration [70]: 0.051232483587797875
Loss at iteration [71]: 0.05122712362203663
Loss at iteration [72]: 0.051218925241579796
Loss at iteration [73]: 0.05121626786717282
Loss at iteration [74]: 0.05121404204417313
Loss at iteration [75]: 0.05121010346843344
Loss at iteration [76]: 0.05120443756493758
Loss at iteration [77]: 0.05118944572856185
Loss at iteration [78]: 0.05116915204662721
Loss at iteration [79]: 0.0511545672445005
Loss at iteration [80]: 0.051130850667801445
Loss at iteration [81]: 0.05111363988673754
Loss at iteration [82]: 0.05110774348862587
Loss at iteration [83]: 0.0510963259392984
Loss at iteration [84]: 0.05108811980552881
Loss at iteration [85]: 0.051083878914825784
Loss at iteration [86]: 0.05107705285466818
Loss at iteration [87]: 0.0510739429632633
Loss at iteration [88]: 0.05107256685388631
Loss at iteration [89]: 0.05107095657264557
Loss at iteration [90]: 0.051058974840416306
Loss at iteration [91]: 0.0510497245543344
Loss at iteration [92]: 0.051038398571145414
Loss at iteration [93]: 0.05103061874904801
Loss at iteration [94]: 0.05101791034205445
Loss at iteration [95]: 0.05101170849514482
Loss at iteration [96]: 0.05100703477597539
Loss at iteration [97]: 0.05100275943138944
Loss at iteration [98]: 0.050997943707361194
Loss at iteration [99]: 0.05099496529992189
Loss at iteration [100]: 0.05099215118028371
Loss at iteration [101]: 0.05098688010664552
Loss at iteration [102]: 0.05098056616943598
Loss at iteration [103]: 0.05097721054367818
Loss at iteration [104]: 0.05097344830059341
Loss at iteration [105]: 0.05096953810410408
Loss at iteration [106]: 0.05096803272733449
Loss at iteration [107]: 0.050965729875452934
Loss at iteration [108]: 0.0509623653521546
Loss at iteration [109]: 0.05095772817298878
Loss at iteration [110]: 0.05095286122588535
Loss at iteration [111]: 0.05094890407524608
Loss at iteration [112]: 0.05094676356915338
Loss at iteration [113]: 0.050943904329800276
