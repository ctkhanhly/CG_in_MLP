Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : SGD
Learning rate                         : 0.4
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 0.19862771034240723
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 58.60345692614928%
Percentage of parameters < 1e-7       : 58.60345692614928%
Percentage of parameters < 1e-6       : 58.60394880522573%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5682866002596756
Loss at iteration [2]: 1.220989395951094
Loss at iteration [3]: 1.1126414383189944
Loss at iteration [4]: 1.759670918226901
***** Warning: Loss has increased *****
Loss at iteration [5]: 1.1190254513054378
Loss at iteration [6]: 1.361343924770547
***** Warning: Loss has increased *****
Loss at iteration [7]: 1.5912737870010931
***** Warning: Loss has increased *****
Loss at iteration [8]: 1.3358288659921649
Loss at iteration [9]: 1.1156795816280813
Loss at iteration [10]: 0.8579101176865813
Loss at iteration [11]: 0.736583006314156
Loss at iteration [12]: 0.8951785016261998
***** Warning: Loss has increased *****
Loss at iteration [13]: 2.595725467579145
***** Warning: Loss has increased *****
Loss at iteration [14]: 4.067811872988233
***** Warning: Loss has increased *****
Loss at iteration [15]: 2.1458136712589284
Loss at iteration [16]: 1.4666607341324953
Loss at iteration [17]: 1.373284181103835
Loss at iteration [18]: 1.3635044901238371
Loss at iteration [19]: 1.2249147004452559
Loss at iteration [20]: 1.0965394111295403
Loss at iteration [21]: 0.9040239969737847
Loss at iteration [22]: 0.807394942380548
Loss at iteration [23]: 0.6932300655577429
Loss at iteration [24]: 0.6195761253651677
Loss at iteration [25]: 0.5743680766239867
Loss at iteration [26]: 0.9401059515307006
***** Warning: Loss has increased *****
Loss at iteration [27]: 5.021698702842356
***** Warning: Loss has increased *****
Loss at iteration [28]: 2.642246617936487
Loss at iteration [29]: 1.5402014652626945
Loss at iteration [30]: 1.553103956542674
***** Warning: Loss has increased *****
Loss at iteration [31]: 1.5432401570824912
Loss at iteration [32]: 1.5264296430126527
Loss at iteration [33]: 1.5064332001360456
Loss at iteration [34]: 1.4765861949197947
Loss at iteration [35]: 1.4247313284821974
Loss at iteration [36]: 1.3397896566283465
Loss at iteration [37]: 1.1679882257510232
Loss at iteration [38]: 0.961448034874971
Loss at iteration [39]: 0.7390083276024517
Loss at iteration [40]: 0.6357019512016281
Loss at iteration [41]: 1.0601435354169937
***** Warning: Loss has increased *****
Loss at iteration [42]: 3.401358741655226
***** Warning: Loss has increased *****
Loss at iteration [43]: 1.6003879034474398
Loss at iteration [44]: 1.542401969528134
Loss at iteration [45]: 1.5810190951487602
***** Warning: Loss has increased *****
Loss at iteration [46]: 1.5732878434942772
Loss at iteration [47]: 1.5468083635573346
Loss at iteration [48]: 1.5355342112698591
Loss at iteration [49]: 1.5358219603752026
***** Warning: Loss has increased *****
Loss at iteration [50]: 1.53648253310713
***** Warning: Loss has increased *****
Loss at iteration [51]: 1.5351306300999403
Loss at iteration [52]: 1.5334981361037052
Loss at iteration [53]: 1.5321143169900104
Loss at iteration [54]: 1.5302520022531596
Loss at iteration [55]: 1.5275994749134225
Loss at iteration [56]: 1.523166265569815
Loss at iteration [57]: 1.5158184867136841
Loss at iteration [58]: 1.5033550627285621
Loss at iteration [59]: 1.4829713481580742
Loss at iteration [60]: 1.4421315959466718
Loss at iteration [61]: 1.3630497274095248
Loss at iteration [62]: 1.2182688904237982
Loss at iteration [63]: 0.9974358418140581
Loss at iteration [64]: 0.8899290162797284
Loss at iteration [65]: 0.8750603623207353
Loss at iteration [66]: 0.773596503975263
Loss at iteration [67]: 0.8083996276562314
***** Warning: Loss has increased *****
Loss at iteration [68]: 0.807076350093472
Loss at iteration [69]: 1.2150024759632037
***** Warning: Loss has increased *****
Loss at iteration [70]: 0.7918395057281552
Loss at iteration [71]: 0.7615073975047923
Loss at iteration [72]: 0.742830228536859
Loss at iteration [73]: 0.7143913421601895
Loss at iteration [74]: 0.7025972680963969
Loss at iteration [75]: 0.6957227962383808
Loss at iteration [76]: 0.6868535111002771
Loss at iteration [77]: 0.6770436058551672
Loss at iteration [78]: 0.6727415845186229
Loss at iteration [79]: 0.6602571542792475
Loss at iteration [80]: 0.6536153778547873
Loss at iteration [81]: 0.6335562903555145
Loss at iteration [82]: 0.6176892612651501
Loss at iteration [83]: 0.5900700515451018
Loss at iteration [84]: 0.563950956768032
Loss at iteration [85]: 0.5347968584534091
Loss at iteration [86]: 0.5036861157720626
Loss at iteration [87]: 0.46858267469653037
Loss at iteration [88]: 0.4540966456329576
Loss at iteration [89]: 1.8781265007465626
***** Warning: Loss has increased *****
Loss at iteration [90]: 2.8721804449383206
***** Warning: Loss has increased *****
Loss at iteration [91]: 1.5373811548670395
Loss at iteration [92]: 1.5662740790957488
***** Warning: Loss has increased *****
Loss at iteration [93]: 1.5583215163521227
Loss at iteration [94]: 1.5183723465351764
Loss at iteration [95]: 1.4821677986176272
Loss at iteration [96]: 1.4280856231156398
Loss at iteration [97]: 1.3213523347058456
Loss at iteration [98]: 1.1732618912258876
Loss at iteration [99]: 1.0543416651194302
Loss at iteration [100]: 0.9065347138089496
Loss at iteration [101]: 0.779875069787918
Loss at iteration [102]: 0.7591634829625757
Loss at iteration [103]: 0.6887228462243352
Loss at iteration [104]: 0.8306283487609338
***** Warning: Loss has increased *****
Loss at iteration [105]: 3.0004403583168173
***** Warning: Loss has increased *****
Loss at iteration [106]: 3.1789933233605656
***** Warning: Loss has increased *****
Loss at iteration [107]: 2.5734385438508265
Loss at iteration [108]: 1.7656621852376573
Loss at iteration [109]: 1.5345193838914428
Loss at iteration [110]: 1.5827613680341466
***** Warning: Loss has increased *****
Loss at iteration [111]: 1.5959756572131607
***** Warning: Loss has increased *****
Loss at iteration [112]: 1.5585663123939029
Loss at iteration [113]: 1.5337558536125118
Loss at iteration [114]: 1.5282096376322796
Loss at iteration [115]: 1.526174973235112
Loss at iteration [116]: 1.5187599911622578
Loss at iteration [117]: 1.5048995306920812
Loss at iteration [118]: 1.4832280177128718
Loss at iteration [119]: 1.4436415414074435
Loss at iteration [120]: 1.3712775257817345
Loss at iteration [121]: 1.2841465499603333
Loss at iteration [122]: 1.204520917962827
Loss at iteration [123]: 1.1140202418283087
Loss at iteration [124]: 1.023199751783502
Loss at iteration [125]: 0.9279944821638583
Loss at iteration [126]: 0.8302030912268001
Loss at iteration [127]: 0.7398333504004513
Loss at iteration [128]: 0.6796997915754441
Loss at iteration [129]: 0.6361215557380445
Loss at iteration [130]: 0.5849390888556846
Loss at iteration [131]: 0.6402402374612919
***** Warning: Loss has increased *****
Loss at iteration [132]: 1.2878796394466012
***** Warning: Loss has increased *****
Loss at iteration [133]: 3.365170788415505
***** Warning: Loss has increased *****
Loss at iteration [134]: 2.256386262694391
Loss at iteration [135]: 1.6153614749350464
Loss at iteration [136]: 1.1980372701740258
Loss at iteration [137]: 1.0572988403825327
Loss at iteration [138]: 1.6940851505442474
***** Warning: Loss has increased *****
Loss at iteration [139]: 1.5111890633952145
Loss at iteration [140]: 1.2272650211898049
Loss at iteration [141]: 0.888310063645509
Loss at iteration [142]: 0.9146111607785125
***** Warning: Loss has increased *****
Loss at iteration [143]: 0.8485909431161213
Loss at iteration [144]: 0.7376552340187158
Loss at iteration [145]: 0.6214507589629954
Loss at iteration [146]: 0.5466522899239131
Loss at iteration [147]: 0.5258880317917083
Loss at iteration [148]: 0.7660558539399279
***** Warning: Loss has increased *****
Loss at iteration [149]: 3.535701421902651
***** Warning: Loss has increased *****
Loss at iteration [150]: 1.7371623225399475
Loss at iteration [151]: 1.7733198747140935
***** Warning: Loss has increased *****
Loss at iteration [152]: 1.6325766472141567
Loss at iteration [153]: 1.5391045053327033
Loss at iteration [154]: 1.5234994719537522
Loss at iteration [155]: 1.5136146679958118
Loss at iteration [156]: 1.4774972904210844
Loss at iteration [157]: 1.4091053117414352
Loss at iteration [158]: 1.318196889460832
Loss at iteration [159]: 1.272387496433077
Loss at iteration [160]: 1.190207387479582
Loss at iteration [161]: 1.117729865385427
Loss at iteration [162]: 1.0628596865045323
Loss at iteration [163]: 1.0103194917513507
Loss at iteration [164]: 0.9687810391327487
Loss at iteration [165]: 0.9858258472084902
***** Warning: Loss has increased *****
Loss at iteration [166]: 0.9087850469996305
Loss at iteration [167]: 1.0137691685287298
***** Warning: Loss has increased *****
Loss at iteration [168]: 0.8456015420464384
Loss at iteration [169]: 0.757337945330881
Loss at iteration [170]: 0.7498616510597135
Loss at iteration [171]: 0.7394702150296241
Loss at iteration [172]: 0.7292707973675954
Loss at iteration [173]: 0.7208247570710371
Loss at iteration [174]: 0.7136253513286615
Loss at iteration [175]: 0.7073981305946612
Loss at iteration [176]: 0.7024293170448502
Loss at iteration [177]: 0.6984156992042051
Loss at iteration [178]: 0.6953481915513153
Loss at iteration [179]: 0.6930201557157103
Loss at iteration [180]: 0.6913724301747182
Loss at iteration [181]: 0.6903450762536415
Loss at iteration [182]: 0.6906083365376275
***** Warning: Loss has increased *****
Loss at iteration [183]: 0.69493663026986
***** Warning: Loss has increased *****
Loss at iteration [184]: 0.7129415533907785
***** Warning: Loss has increased *****
Loss at iteration [185]: 0.7635377770705077
***** Warning: Loss has increased *****
Loss at iteration [186]: 0.8632528157742324
***** Warning: Loss has increased *****
Loss at iteration [187]: 0.7235735308177046
Loss at iteration [188]: 0.7396548092085887
***** Warning: Loss has increased *****
Loss at iteration [189]: 0.6995125626032337
Loss at iteration [190]: 0.698973387578126
Loss at iteration [191]: 0.6825717086328349
Loss at iteration [192]: 0.6852905204468608
***** Warning: Loss has increased *****
Loss at iteration [193]: 0.6702491737196384
Loss at iteration [194]: 0.6426999189295344
Loss at iteration [195]: 0.6339240562883195
Loss at iteration [196]: 0.6243815142128202
Loss at iteration [197]: 0.6094152460808728
Loss at iteration [198]: 0.5945338911226246
Loss at iteration [199]: 0.5796765448516316
Loss at iteration [200]: 0.5636083939322132
Loss at iteration [201]: 0.5474976735282975
Loss at iteration [202]: 0.5229772875877831
Loss at iteration [203]: 2.4717283453269943
***** Warning: Loss has increased *****
Loss at iteration [204]: 25.121723089857717
***** Warning: Loss has increased *****
Loss at iteration [205]: 4.147759699289413
Loss at iteration [206]: 7.651281249567745
***** Warning: Loss has increased *****
Loss at iteration [207]: 1.3849165252209266
Loss at iteration [208]: 2.9410868636825764
***** Warning: Loss has increased *****
Loss at iteration [209]: 18.739032138392506
***** Warning: Loss has increased *****
Loss at iteration [210]: 6.741403982675457
Loss at iteration [211]: 12.501162476275631
***** Warning: Loss has increased *****
Loss at iteration [212]: 4.640818536205072
Loss at iteration [213]: 2.2052752089865906
Loss at iteration [214]: 18.40493179729453
***** Warning: Loss has increased *****
Loss at iteration [215]: 5074441609253.201
***** Warning: Loss has increased *****
