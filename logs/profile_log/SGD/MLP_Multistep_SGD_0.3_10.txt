Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : SGD
Learning rate                         : 0.3
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 1.1093029975891113
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 54.821398707341785%
Percentage of parameters < 1e-7       : 54.821890586418235%
Percentage of parameters < 1e-6       : 54.82238246549468%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.585799210743297
Loss at iteration [2]: 1.3492844353551954
Loss at iteration [3]: 1.0920909044134242
Loss at iteration [4]: 0.8894900016963764
Loss at iteration [5]: 0.9256511041803994
***** Warning: Loss has increased *****
Loss at iteration [6]: 1.4699704116950258
***** Warning: Loss has increased *****
Loss at iteration [7]: 2.036535367215732
***** Warning: Loss has increased *****
Loss at iteration [8]: 1.5889074846553264
Loss at iteration [9]: 1.4335499864016683
Loss at iteration [10]: 1.2824775269955961
Loss at iteration [11]: 1.0314746810280637
Loss at iteration [12]: 0.8769379261263366
Loss at iteration [13]: 0.725926921986115
Loss at iteration [14]: 0.6304185911972917
Loss at iteration [15]: 0.6379404146279479
***** Warning: Loss has increased *****
Loss at iteration [16]: 1.2440222870571924
***** Warning: Loss has increased *****
Loss at iteration [17]: 1.884884519165295
***** Warning: Loss has increased *****
Loss at iteration [18]: 1.6727806119285746
Loss at iteration [19]: 1.6574477652263404
Loss at iteration [20]: 1.48368363012788
Loss at iteration [21]: 1.2590744981222544
Loss at iteration [22]: 0.9818458887507501
Loss at iteration [23]: 0.8542282931434546
Loss at iteration [24]: 0.8337107292781967
Loss at iteration [25]: 0.7074648353947942
Loss at iteration [26]: 0.8621041839573249
***** Warning: Loss has increased *****
Loss at iteration [27]: 0.9070993716509517
***** Warning: Loss has increased *****
Loss at iteration [28]: 1.8322937941763202
***** Warning: Loss has increased *****
Loss at iteration [29]: 1.4925217543078935
Loss at iteration [30]: 0.7557054219661368
Loss at iteration [31]: 1.5313931091529167
***** Warning: Loss has increased *****
Loss at iteration [32]: 1.7787008446107309
***** Warning: Loss has increased *****
Loss at iteration [33]: 1.7054573519608915
Loss at iteration [34]: 1.4446124558814588
Loss at iteration [35]: 1.28972986979152
Loss at iteration [36]: 1.1068364300792402
Loss at iteration [37]: 1.0085945057078445
Loss at iteration [38]: 0.8860523655693912
Loss at iteration [39]: 0.7889519387351996
Loss at iteration [40]: 0.7045056048224
Loss at iteration [41]: 0.6561742820009661
Loss at iteration [42]: 0.7620622793281601
***** Warning: Loss has increased *****
Loss at iteration [43]: 1.7827840848810037
***** Warning: Loss has increased *****
Loss at iteration [44]: 0.707047102417197
Loss at iteration [45]: 2.368372489717103
***** Warning: Loss has increased *****
Loss at iteration [46]: 2.5178376779599216
***** Warning: Loss has increased *****
Loss at iteration [47]: 2.1989512748469227
Loss at iteration [48]: 1.7377762534785215
Loss at iteration [49]: 1.502673548940293
Loss at iteration [50]: 1.3957176961333753
Loss at iteration [51]: 1.2325549010876315
Loss at iteration [52]: 0.9139005817951781
Loss at iteration [53]: 0.9188317851884775
***** Warning: Loss has increased *****
Loss at iteration [54]: 1.0959458871358856
***** Warning: Loss has increased *****
Loss at iteration [55]: 0.6794053481507534
Loss at iteration [56]: 0.8904458504520408
***** Warning: Loss has increased *****
Loss at iteration [57]: 1.3802798567305459
***** Warning: Loss has increased *****
Loss at iteration [58]: 1.0726021830933241
Loss at iteration [59]: 0.7021942011978471
Loss at iteration [60]: 0.6340899257664329
Loss at iteration [61]: 0.6472183288182108
***** Warning: Loss has increased *****
Loss at iteration [62]: 0.5389340661016633
Loss at iteration [63]: 0.48186821510665
Loss at iteration [64]: 0.4423821637241306
Loss at iteration [65]: 0.6308597027549326
***** Warning: Loss has increased *****
Loss at iteration [66]: 2.4011914577351057
***** Warning: Loss has increased *****
Loss at iteration [67]: 3.313774202821755
***** Warning: Loss has increased *****
Loss at iteration [68]: 1.840987924326175
Loss at iteration [69]: 1.608883582538134
Loss at iteration [70]: 1.5425098869718337
Loss at iteration [71]: 1.5346795739587973
Loss at iteration [72]: 1.5304104970245276
Loss at iteration [73]: 1.5138224111815062
Loss at iteration [74]: 1.4857810249004422
Loss at iteration [75]: 1.441999860053714
Loss at iteration [76]: 1.366009771854374
Loss at iteration [77]: 1.2299202582889857
Loss at iteration [78]: 1.0358042973070771
Loss at iteration [79]: 0.8894529319879432
Loss at iteration [80]: 0.7999842877863377
Loss at iteration [81]: 0.7711289068069417
Loss at iteration [82]: 0.7427571807176715
Loss at iteration [83]: 0.751723514762308
***** Warning: Loss has increased *****
Loss at iteration [84]: 1.052942042092299
***** Warning: Loss has increased *****
Loss at iteration [85]: 1.2653254610637135
***** Warning: Loss has increased *****
Loss at iteration [86]: 1.9447665039545348
***** Warning: Loss has increased *****
Loss at iteration [87]: 1.7280781581689157
Loss at iteration [88]: 1.4852916317206877
Loss at iteration [89]: 1.387351120115301
Loss at iteration [90]: 1.2610126015329064
Loss at iteration [91]: 1.00969397882816
Loss at iteration [92]: 0.9818127748110039
Loss at iteration [93]: 0.8670755258371576
Loss at iteration [94]: 0.6985238855218597
Loss at iteration [95]: 0.6821952983341093
Loss at iteration [96]: 0.7837947215126393
***** Warning: Loss has increased *****
Loss at iteration [97]: 0.9353976555170544
***** Warning: Loss has increased *****
Loss at iteration [98]: 1.7870676133970653
***** Warning: Loss has increased *****
Loss at iteration [99]: 1.355101952699023
Loss at iteration [100]: 0.9109818951198665
Loss at iteration [101]: 0.7996373236324353
Loss at iteration [102]: 0.7439915401998205
Loss at iteration [103]: 0.6121923858461221
Loss at iteration [104]: 0.4872209323946985
Loss at iteration [105]: 0.50019718152844
***** Warning: Loss has increased *****
Loss at iteration [106]: 2.0099187617362295
***** Warning: Loss has increased *****
Loss at iteration [107]: 4.323564622784558
***** Warning: Loss has increased *****
Loss at iteration [108]: 1.5932116915103143
Loss at iteration [109]: 1.6045455768987582
***** Warning: Loss has increased *****
Loss at iteration [110]: 1.5944306548414346
Loss at iteration [111]: 1.5304266156196031
Loss at iteration [112]: 1.447639500232954
Loss at iteration [113]: 1.3377840213523151
Loss at iteration [114]: 1.2212172700135229
Loss at iteration [115]: 1.1441377261526338
Loss at iteration [116]: 1.0497709164000406
Loss at iteration [117]: 0.9488454394028103
Loss at iteration [118]: 0.8550998469149972
Loss at iteration [119]: 0.7832542254497906
Loss at iteration [120]: 0.7721313143020894
Loss at iteration [121]: 0.92800561537322
***** Warning: Loss has increased *****
Loss at iteration [122]: 1.2540206893585657
***** Warning: Loss has increased *****
Loss at iteration [123]: 2.1755137120393915
***** Warning: Loss has increased *****
Loss at iteration [124]: 1.966456951448197
Loss at iteration [125]: 1.659432490225027
Loss at iteration [126]: 1.5294177193033358
Loss at iteration [127]: 1.5055102809503966
Loss at iteration [128]: 1.4885049230493586
Loss at iteration [129]: 1.4415654143143035
Loss at iteration [130]: 1.3559753221254218
Loss at iteration [131]: 1.2011857507879193
Loss at iteration [132]: 0.9758246566552038
Loss at iteration [133]: 0.8246707310714746
Loss at iteration [134]: 0.7201939461423327
Loss at iteration [135]: 0.672011129006677
Loss at iteration [136]: 0.6326431406963098
Loss at iteration [137]: 0.713616623418309
***** Warning: Loss has increased *****
Loss at iteration [138]: 1.6591459244492779
***** Warning: Loss has increased *****
Loss at iteration [139]: 0.8451466559920239
Loss at iteration [140]: 0.7873355408917138
Loss at iteration [141]: 0.683743019381998
Loss at iteration [142]: 0.6545924887473631
Loss at iteration [143]: 0.6608264746206914
***** Warning: Loss has increased *****
Loss at iteration [144]: 0.5787944136028244
Loss at iteration [145]: 0.5738415865716477
Loss at iteration [146]: 0.5588156644213229
Loss at iteration [147]: 0.700271780598725
***** Warning: Loss has increased *****
Loss at iteration [148]: 0.616308931107133
Loss at iteration [149]: 0.8215264946581595
***** Warning: Loss has increased *****
Loss at iteration [150]: 0.4524867705368335
Loss at iteration [151]: 0.520489928092092
***** Warning: Loss has increased *****
Loss at iteration [152]: 0.7874551849602992
***** Warning: Loss has increased *****
Loss at iteration [153]: 0.4953045270026615
Loss at iteration [154]: 0.4377028495163115
Loss at iteration [155]: 0.38934978843382567
Loss at iteration [156]: 0.3338537982248912
Loss at iteration [157]: 0.46322598840492063
***** Warning: Loss has increased *****
Loss at iteration [158]: 3.3922271072499917
***** Warning: Loss has increased *****
Loss at iteration [159]: 2.7649124627453197
Loss at iteration [160]: 1.815174843966297
Loss at iteration [161]: 1.6086218221080697
Loss at iteration [162]: 1.5041727977694421
Loss at iteration [163]: 1.4523825163840287
Loss at iteration [164]: 1.311047485447184
Loss at iteration [165]: 1.0366402273159787
Loss at iteration [166]: 1.0583666483165146
***** Warning: Loss has increased *****
Loss at iteration [167]: 0.9091426899317676
Loss at iteration [168]: 0.809736372866591
Loss at iteration [169]: 0.8166297711362897
***** Warning: Loss has increased *****
Loss at iteration [170]: 0.861526268867383
***** Warning: Loss has increased *****
Loss at iteration [171]: 0.7430180040937763
Loss at iteration [172]: 0.7145146066467402
Loss at iteration [173]: 0.7033487937881641
Loss at iteration [174]: 0.6947677440525887
Loss at iteration [175]: 0.699253492037113
***** Warning: Loss has increased *****
Loss at iteration [176]: 0.7493867716942261
***** Warning: Loss has increased *****
Loss at iteration [177]: 0.7967184885564718
***** Warning: Loss has increased *****
Loss at iteration [178]: 1.352045192564304
***** Warning: Loss has increased *****
Loss at iteration [179]: 0.654527637486294
Loss at iteration [180]: 1.1901094182993142
***** Warning: Loss has increased *****
Loss at iteration [181]: 2.179644190099906
***** Warning: Loss has increased *****
Loss at iteration [182]: 2.0298223226356495
Loss at iteration [183]: 1.694181483209125
Loss at iteration [184]: 1.5271075969719208
Loss at iteration [185]: 1.4884226700303063
Loss at iteration [186]: 1.471814157883584
Loss at iteration [187]: 1.4248678519924947
Loss at iteration [188]: 1.3435468803577169
Loss at iteration [189]: 1.2115456932370525
Loss at iteration [190]: 1.001327663862498
Loss at iteration [191]: 0.9207436496429594
Loss at iteration [192]: 0.8333159237368337
Loss at iteration [193]: 0.7748486067747137
Loss at iteration [194]: 0.747169382213801
Loss at iteration [195]: 0.7253783453162167
Loss at iteration [196]: 0.7004636431608553
Loss at iteration [197]: 0.6770051357251406
Loss at iteration [198]: 0.6349894689573302
Loss at iteration [199]: 0.5860015347395948
Loss at iteration [200]: 0.5481228004712604
Loss at iteration [201]: 0.5558491427233223
***** Warning: Loss has increased *****
Loss at iteration [202]: 0.6814037401758528
***** Warning: Loss has increased *****
Loss at iteration [203]: 0.8358454052801718
***** Warning: Loss has increased *****
Loss at iteration [204]: 1.7958735782279052
***** Warning: Loss has increased *****
Loss at iteration [205]: 1.2035085750221148
Loss at iteration [206]: 1.1927690771870478
Loss at iteration [207]: 1.123015963051734
Loss at iteration [208]: 0.9110928356167775
Loss at iteration [209]: 0.8160576553251369
Loss at iteration [210]: 0.7502421721913106
Loss at iteration [211]: 0.7511027340609526
***** Warning: Loss has increased *****
Loss at iteration [212]: 0.7290919930425372
Loss at iteration [213]: 0.7240685921742699
Loss at iteration [214]: 0.7138452501235761
Loss at iteration [215]: 0.7093078918742207
Loss at iteration [216]: 0.7040497110711483
Loss at iteration [217]: 0.6998621166388399
Loss at iteration [218]: 0.6959560432549882
Loss at iteration [219]: 0.6923904192748632
Loss at iteration [220]: 0.6882498252289041
Loss at iteration [221]: 0.6849152431586213
Loss at iteration [222]: 0.6817380302585191
Loss at iteration [223]: 0.6780208133537146
Loss at iteration [224]: 0.6745371638241525
Loss at iteration [225]: 0.6713930493748373
Loss at iteration [226]: 0.6678397949543432
Loss at iteration [227]: 0.6638285056454188
Loss at iteration [228]: 0.6603426841128892
Loss at iteration [229]: 0.6563319365382175
Loss at iteration [230]: 0.6520036583786398
Loss at iteration [231]: 0.6466955440581164
Loss at iteration [232]: 0.6412859175003705
Loss at iteration [233]: 0.6354967408633615
Loss at iteration [234]: 0.6294708401621667
Loss at iteration [235]: 0.6237917642764049
Loss at iteration [236]: 0.6186507227017045
Loss at iteration [237]: 0.6105588145594536
Loss at iteration [238]: 0.6033414269264663
Loss at iteration [239]: 0.5953710103259477
Loss at iteration [240]: 0.5870922383752392
Loss at iteration [241]: 0.5785383340504252
Loss at iteration [242]: 0.5698508866316467
Loss at iteration [243]: 0.5612276188367785
Loss at iteration [244]: 0.5524901302132467
Loss at iteration [245]: 0.5437400648739498
Loss at iteration [246]: 0.5356334533007807
Loss at iteration [247]: 0.5294043937596751
Loss at iteration [248]: 0.5255276742056955
Loss at iteration [249]: 0.5266911086806243
***** Warning: Loss has increased *****
Loss at iteration [250]: 0.5328240366013602
***** Warning: Loss has increased *****
Loss at iteration [251]: 0.5679945170980972
***** Warning: Loss has increased *****
Loss at iteration [252]: 0.6147652648875195
***** Warning: Loss has increased *****
Loss at iteration [253]: 0.6298240947314753
***** Warning: Loss has increased *****
Loss at iteration [254]: 0.5138237240344957
Loss at iteration [255]: 0.4821358100691091
Loss at iteration [256]: 0.4798956364235886
Loss at iteration [257]: 0.43115747352530065
Loss at iteration [258]: 0.4650420180189909
***** Warning: Loss has increased *****
Loss at iteration [259]: 0.9385366639626355
***** Warning: Loss has increased *****
Loss at iteration [260]: 1.7011398381947065
***** Warning: Loss has increased *****
Loss at iteration [261]: 0.9899885885022209
Loss at iteration [262]: 0.8915721789176896
Loss at iteration [263]: 0.7455833842097918
Loss at iteration [264]: 0.7182000978508565
Loss at iteration [265]: 0.6716240164563924
Loss at iteration [266]: 0.6224864752700187
Loss at iteration [267]: 0.5833495541125364
Loss at iteration [268]: 0.551882168594141
Loss at iteration [269]: 0.5141177720545509
Loss at iteration [270]: 0.48359699283103735
Loss at iteration [271]: 0.47686851478468645
Loss at iteration [272]: 0.47390185747489866
Loss at iteration [273]: 0.4793497439902427
***** Warning: Loss has increased *****
Loss at iteration [274]: 0.3956971066566901
Loss at iteration [275]: 0.44057595240964625
***** Warning: Loss has increased *****
Loss at iteration [276]: 0.38097870632045483
Loss at iteration [277]: 0.4978570942826489
***** Warning: Loss has increased *****
Loss at iteration [278]: 0.35735411496370134
Loss at iteration [279]: 0.46771471560682343
***** Warning: Loss has increased *****
Loss at iteration [280]: 0.34567781655908725
Loss at iteration [281]: 0.5055014329541723
***** Warning: Loss has increased *****
Loss at iteration [282]: 0.35938244745777725
Loss at iteration [283]: 0.5870846802372675
***** Warning: Loss has increased *****
Loss at iteration [284]: 0.4032929593882133
Loss at iteration [285]: 0.7539012152723312
***** Warning: Loss has increased *****
Loss at iteration [286]: 0.45666638135456655
Loss at iteration [287]: 0.6685461457329007
***** Warning: Loss has increased *****
Loss at iteration [288]: 0.2883101022194294
Loss at iteration [289]: 0.5257860736872549
***** Warning: Loss has increased *****
Loss at iteration [290]: 1.0367166816962086
***** Warning: Loss has increased *****
Loss at iteration [291]: 0.836845714920521
Loss at iteration [292]: 0.8259746033890867
Loss at iteration [293]: 0.7328286706278663
Loss at iteration [294]: 0.6981279197644976
Loss at iteration [295]: 0.6656369521943669
Loss at iteration [296]: 0.6403811657759969
Loss at iteration [297]: 0.6181793614653912
Loss at iteration [298]: 0.5952939594729788
Loss at iteration [299]: 0.5724736938230176
Loss at iteration [300]: 0.5494708795951648
Loss at iteration [301]: 0.5253338733063706
Loss at iteration [302]: 0.5029131856127143
Loss at iteration [303]: 0.4858686403276225
Loss at iteration [304]: 0.5138489363876261
***** Warning: Loss has increased *****
Loss at iteration [305]: 0.6432288278935925
***** Warning: Loss has increased *****
Loss at iteration [306]: 0.8739365904672767
***** Warning: Loss has increased *****
Loss at iteration [307]: 0.8292249626893519
Loss at iteration [308]: 0.7791072182579865
Loss at iteration [309]: 0.7207568723686103
Loss at iteration [310]: 0.5067859546243983
Loss at iteration [311]: 0.488401339997661
Loss at iteration [312]: 0.44477614493556394
Loss at iteration [313]: 0.3902578082120668
Loss at iteration [314]: 0.3664905319882589
Loss at iteration [315]: 0.32543074360645835
Loss at iteration [316]: 0.38295660412771215
***** Warning: Loss has increased *****
Loss at iteration [317]: 0.4443972596737054
***** Warning: Loss has increased *****
Loss at iteration [318]: 0.33457706132050147
Loss at iteration [319]: 0.31595405907209034
Loss at iteration [320]: 0.3159568542280855
***** Warning: Loss has increased *****
Loss at iteration [321]: 0.29510699347410335
Loss at iteration [322]: 0.6901092462553131
***** Warning: Loss has increased *****
Loss at iteration [323]: 1.1154002246476615
***** Warning: Loss has increased *****
Loss at iteration [324]: 0.8397537125270648
Loss at iteration [325]: 0.8784744527710509
***** Warning: Loss has increased *****
Loss at iteration [326]: 0.7557186065174543
Loss at iteration [327]: 0.7143614627552071
Loss at iteration [328]: 0.6869276473266327
Loss at iteration [329]: 0.6734091548820891
Loss at iteration [330]: 0.6463450257447619
Loss at iteration [331]: 0.6317500029254657
Loss at iteration [332]: 0.6120091734757273
Loss at iteration [333]: 0.5941933210215111
Loss at iteration [334]: 0.5745055355665551
Loss at iteration [335]: 0.5555317027654574
Loss at iteration [336]: 0.5355224283892732
Loss at iteration [337]: 0.5153384521423127
Loss at iteration [338]: 0.4947519790526177
Loss at iteration [339]: 0.4738100021594207
Loss at iteration [340]: 0.45260661328994234
Loss at iteration [341]: 0.4322520006807665
Loss at iteration [342]: 0.4140359850436922
Loss at iteration [343]: 0.42282825074106256
***** Warning: Loss has increased *****
Loss at iteration [344]: 0.7360313068631302
***** Warning: Loss has increased *****
Loss at iteration [345]: 1.0388987389084303
***** Warning: Loss has increased *****
Loss at iteration [346]: 0.8863360277699731
Loss at iteration [347]: 0.8405713049806661
Loss at iteration [348]: 0.6319164956905791
Loss at iteration [349]: 0.5148064526577952
Loss at iteration [350]: 0.4705869397483968
Loss at iteration [351]: 0.41645122850821725
Loss at iteration [352]: 0.37765956685366553
Loss at iteration [353]: 0.35500032196127507
Loss at iteration [354]: 0.3357813994066715
Loss at iteration [355]: 0.3258596330904755
Loss at iteration [356]: 0.3185589474247637
Loss at iteration [357]: 0.30970582262404406
Loss at iteration [358]: 0.3117080199290031
***** Warning: Loss has increased *****
Loss at iteration [359]: 0.3338591396317475
***** Warning: Loss has increased *****
Loss at iteration [360]: 0.3503261739852408
***** Warning: Loss has increased *****
Loss at iteration [361]: 0.33308151861126617
Loss at iteration [362]: 0.2977323072695157
Loss at iteration [363]: 0.3938839263717344
***** Warning: Loss has increased *****
Loss at iteration [364]: 0.5568090819526464
***** Warning: Loss has increased *****
Loss at iteration [365]: 0.3470604139374148
Loss at iteration [366]: 0.2879394355478999
Loss at iteration [367]: 0.36218779164092973
***** Warning: Loss has increased *****
Loss at iteration [368]: 0.41055337486631105
***** Warning: Loss has increased *****
Loss at iteration [369]: 0.33956534300643704
Loss at iteration [370]: 0.3230225873197173
Loss at iteration [371]: 0.31952443192258795
Loss at iteration [372]: 0.3040496986505332
Loss at iteration [373]: 0.3431187636098817
***** Warning: Loss has increased *****
Loss at iteration [374]: 0.35243193523203953
***** Warning: Loss has increased *****
Loss at iteration [375]: 0.38444715969143406
***** Warning: Loss has increased *****
Loss at iteration [376]: 0.36215575446717496
Loss at iteration [377]: 0.28017884789218656
Loss at iteration [378]: 0.3469827602937844
***** Warning: Loss has increased *****
Loss at iteration [379]: 0.33471265429396085
Loss at iteration [380]: 0.400492111644179
***** Warning: Loss has increased *****
Loss at iteration [381]: 0.34451212813709575
Loss at iteration [382]: 0.29297144149838783
Loss at iteration [383]: 0.3762622137297209
***** Warning: Loss has increased *****
Loss at iteration [384]: 0.348859236102082
Loss at iteration [385]: 0.44336841176704134
***** Warning: Loss has increased *****
Loss at iteration [386]: 0.36582198108135777
Loss at iteration [387]: 0.2841795202721967
Loss at iteration [388]: 0.4495375207029557
***** Warning: Loss has increased *****
Loss at iteration [389]: 0.5548384998957886
***** Warning: Loss has increased *****
Loss at iteration [390]: 0.36731718462061763
Loss at iteration [391]: 0.3111560882816381
Loss at iteration [392]: 0.3221863312436407
***** Warning: Loss has increased *****
Loss at iteration [393]: 0.30978517603384015
Loss at iteration [394]: 0.31149517610573496
***** Warning: Loss has increased *****
Loss at iteration [395]: 0.3078497996369355
Loss at iteration [396]: 0.30779203151710366
Loss at iteration [397]: 0.3067281790523
Loss at iteration [398]: 0.30581054796041035
Loss at iteration [399]: 0.30525627055767557
Loss at iteration [400]: 0.30518243056654065
Loss at iteration [401]: 0.3045336066050889
Loss at iteration [402]: 0.3043145542380669
Loss at iteration [403]: 0.30364508573422316
Loss at iteration [404]: 0.3036114611953639
Loss at iteration [405]: 0.30294642520875953
Loss at iteration [406]: 0.3025564064129477
Loss at iteration [407]: 0.3016768206720234
Loss at iteration [408]: 0.3017121671298758
***** Warning: Loss has increased *****
Loss at iteration [409]: 0.3008397770379407
Loss at iteration [410]: 0.30037104164221523
Loss at iteration [411]: 0.30010603288205845
Loss at iteration [412]: 0.2991263526797634
Loss at iteration [413]: 0.29944924456721494
***** Warning: Loss has increased *****
Loss at iteration [414]: 0.2979264789070427
Loss at iteration [415]: 0.29835650571571215
***** Warning: Loss has increased *****
Loss at iteration [416]: 0.2973630736557362
Loss at iteration [417]: 0.29555914218975016
Loss at iteration [418]: 0.2954012536562194
Loss at iteration [419]: 0.27922421565749855
Loss at iteration [420]: 0.27720170556372686
Loss at iteration [421]: 0.2466147601550323
Loss at iteration [422]: 0.2460975521549735
Loss at iteration [423]: 0.2866934182593533
***** Warning: Loss has increased *****
Loss at iteration [424]: 0.24058726090720253
Loss at iteration [425]: 0.2381810762867445
Loss at iteration [426]: 0.2733397328000006
***** Warning: Loss has increased *****
Loss at iteration [427]: 0.3576319030497279
***** Warning: Loss has increased *****
Loss at iteration [428]: 0.3382804016053516
Loss at iteration [429]: 0.31433901208351017
Loss at iteration [430]: 0.30221863418465006
Loss at iteration [431]: 0.3040871947159349
***** Warning: Loss has increased *****
Loss at iteration [432]: 0.30074420674314584
Loss at iteration [433]: 0.2954133763537018
Loss at iteration [434]: 0.294745863133461
Loss at iteration [435]: 0.29112837855328216
Loss at iteration [436]: 0.29219308469476574
***** Warning: Loss has increased *****
Loss at iteration [437]: 0.2892125800759571
Loss at iteration [438]: 0.2873573709752711
Loss at iteration [439]: 0.28436148972563685
Loss at iteration [440]: 0.28544299034578646
***** Warning: Loss has increased *****
Loss at iteration [441]: 0.28050596649075343
Loss at iteration [442]: 0.28161356206099164
***** Warning: Loss has increased *****
Loss at iteration [443]: 0.27778850303995517
Loss at iteration [444]: 0.269038816785413
Loss at iteration [445]: 0.2450678867314295
Loss at iteration [446]: 0.23363085152681812
Loss at iteration [447]: 0.2282281019317171
Loss at iteration [448]: 0.22772814765274255
Loss at iteration [449]: 0.23580901031183935
***** Warning: Loss has increased *****
Loss at iteration [450]: 0.2258749625110764
Loss at iteration [451]: 0.22815476136563817
***** Warning: Loss has increased *****
Loss at iteration [452]: 0.22277034629406592
Loss at iteration [453]: 0.22518202467984
***** Warning: Loss has increased *****
Loss at iteration [454]: 0.2210087507323911
Loss at iteration [455]: 0.2213529834889391
***** Warning: Loss has increased *****
Loss at iteration [456]: 0.22094668684833937
Loss at iteration [457]: 0.22608032069776648
***** Warning: Loss has increased *****
Loss at iteration [458]: 0.22012796602235815
Loss at iteration [459]: 0.2182202418756713
Loss at iteration [460]: 0.21770926804319132
Loss at iteration [461]: 0.21741306911023808
Loss at iteration [462]: 0.2170408345244571
Loss at iteration [463]: 0.21701350361971553
Loss at iteration [464]: 0.21811504597691075
***** Warning: Loss has increased *****
Loss at iteration [465]: 0.2258765279436741
***** Warning: Loss has increased *****
Loss at iteration [466]: 0.2595282572183871
***** Warning: Loss has increased *****
Loss at iteration [467]: 0.22895588635548372
Loss at iteration [468]: 0.21955539735416768
Loss at iteration [469]: 0.21702730770117348
Loss at iteration [470]: 0.2173458323709028
***** Warning: Loss has increased *****
Loss at iteration [471]: 0.21703436247934121
Loss at iteration [472]: 0.21666451672579828
Loss at iteration [473]: 0.21660265629895264
Loss at iteration [474]: 0.216263692254049
Loss at iteration [475]: 0.21600095550561846
Loss at iteration [476]: 0.21577765999312098
Loss at iteration [477]: 0.21567026207256035
Loss at iteration [478]: 0.21560386438167814
Loss at iteration [479]: 0.21556322543476408
Loss at iteration [480]: 0.21552938570167587
Loss at iteration [481]: 0.21549896546217479
Loss at iteration [482]: 0.21546947599178282
Loss at iteration [483]: 0.21544612375108624
Loss at iteration [484]: 0.21542716301283257
Loss at iteration [485]: 0.2154191671821983
Loss at iteration [486]: 0.21543989551555223
***** Warning: Loss has increased *****
Loss at iteration [487]: 0.21544495901377944
***** Warning: Loss has increased *****
Loss at iteration [488]: 0.2155227932139863
***** Warning: Loss has increased *****
Loss at iteration [489]: 0.2155183225511015
Loss at iteration [490]: 0.21563163633434193
***** Warning: Loss has increased *****
Loss at iteration [491]: 0.21557256445154513
Loss at iteration [492]: 0.21562881433424458
***** Warning: Loss has increased *****
Loss at iteration [493]: 0.21550993978320193
Loss at iteration [494]: 0.21545798573074645
Loss at iteration [495]: 0.21536543870385558
Loss at iteration [496]: 0.21530842285202964
Loss at iteration [497]: 0.21528691023684404
Loss at iteration [498]: 0.21528072585952893
Loss at iteration [499]: 0.21527563050657442
Loss at iteration [500]: 0.21526907084794714
Loss at iteration [501]: 0.21526226830775363
Loss at iteration [502]: 0.21525632863352764
Loss at iteration [503]: 0.2152511189561443
Loss at iteration [504]: 0.2152463718066206
Loss at iteration [505]: 0.21524182872993783
Loss at iteration [506]: 0.2152374062211148
Loss at iteration [507]: 0.21523310579023153
Loss at iteration [508]: 0.2152289554179283
Loss at iteration [509]: 0.2152249032184327
Loss at iteration [510]: 0.2152209707881743
Loss at iteration [511]: 0.21521714607936776
Loss at iteration [512]: 0.21521338983936575
Loss at iteration [513]: 0.21520970061691097
Loss at iteration [514]: 0.21520612554401458
Loss at iteration [515]: 0.21520257302497056
Loss at iteration [516]: 0.21519910419928143
Loss at iteration [517]: 0.21519568658501367
Loss at iteration [518]: 0.2151923082794958
Loss at iteration [519]: 0.21518900732408444
Loss at iteration [520]: 0.21518571993013305
Loss at iteration [521]: 0.2151825055161504
Loss at iteration [522]: 0.21517931329067064
Loss at iteration [523]: 0.21517614455259143
Loss at iteration [524]: 0.215173037624751
Loss at iteration [525]: 0.2151699350894937
Loss at iteration [526]: 0.2151669046237939
Loss at iteration [527]: 0.21516388871946693
Loss at iteration [528]: 0.21516088652983334
Loss at iteration [529]: 0.21515794490222243
Loss at iteration [530]: 0.21515499310825215
Loss at iteration [531]: 0.2151520882103612
Loss at iteration [532]: 0.21514919462463256
Loss at iteration [533]: 0.2151463077364381
Loss at iteration [534]: 0.21514348739213077
Loss at iteration [535]: 0.21514064395653745
Loss at iteration [536]: 0.21513783032713735
Loss at iteration [537]: 0.2151350346597644
Loss at iteration [538]: 0.21513224958064905
Loss at iteration [539]: 0.21512949459191769
Loss at iteration [540]: 0.21512674042024274
Loss at iteration [541]: 0.21512403995487284
Loss at iteration [542]: 0.21512130773339405
Loss at iteration [543]: 0.21511862028044665
Loss at iteration [544]: 0.21511593286023614
Loss at iteration [545]: 0.21511324237050866
Loss at iteration [546]: 0.2151106198937351
Loss at iteration [547]: 0.21510795988705383
Loss at iteration [548]: 0.21510532324029377
Loss at iteration [549]: 0.21510270081521737
Loss at iteration [550]: 0.21510009510281933
Loss at iteration [551]: 0.21509750014758708
Loss at iteration [552]: 0.21509491678036502
Loss at iteration [553]: 0.2150923596667755
Loss at iteration [554]: 0.21508979122762423
Loss at iteration [555]: 0.21508726037842968
Loss at iteration [556]: 0.2150847031879582
Loss at iteration [557]: 0.21508220709504264
Loss at iteration [558]: 0.21507968884975348
Loss at iteration [559]: 0.21507716256107146
Loss at iteration [560]: 0.2150746998422668
Loss at iteration [561]: 0.21507220078839984
Loss at iteration [562]: 0.21506972848461253
Loss at iteration [563]: 0.21506726190843686
Loss at iteration [564]: 0.21506480680069157
Loss at iteration [565]: 0.2150623610390848
Loss at iteration [566]: 0.21505993377722832
Loss at iteration [567]: 0.21505751949564078
Loss at iteration [568]: 0.21505509655273097
Loss at iteration [569]: 0.2150527231342716
Loss at iteration [570]: 0.21505031098798824
Loss at iteration [571]: 0.2150479469088109
Loss at iteration [572]: 0.21504557053697482
Loss at iteration [573]: 0.21504318675212744
Loss at iteration [574]: 0.21504084724536068
Loss at iteration [575]: 0.21503848996888833
Loss at iteration [576]: 0.21503618094425347
Loss at iteration [577]: 0.21503384426048075
Loss at iteration [578]: 0.21503155132621832
Loss at iteration [579]: 0.21502922683229483
Loss at iteration [580]: 0.21502695859685406
Loss at iteration [581]: 0.2150246704037929
Loss at iteration [582]: 0.21502237212596917
Loss at iteration [583]: 0.2150201223670245
Loss at iteration [584]: 0.21501788647862702
Loss at iteration [585]: 0.21501573430827794
Loss at iteration [586]: 0.2150135644058951
Loss at iteration [587]: 0.2150113979737251
Loss at iteration [588]: 0.21500927519277635
Loss at iteration [589]: 0.21500710233374126
Loss at iteration [590]: 0.2150050373191439
Loss at iteration [591]: 0.21500288438630838
Loss at iteration [592]: 0.21500077125231418
Loss at iteration [593]: 0.2149986867639725
Loss at iteration [594]: 0.21499657444047426
Loss at iteration [595]: 0.21499451378130258
Loss at iteration [596]: 0.21499242060113094
Loss at iteration [597]: 0.21499036719048817
Loss at iteration [598]: 0.21498831459315196
Loss at iteration [599]: 0.21498627037803625
Loss at iteration [600]: 0.21498423714692505
Loss at iteration [601]: 0.2149822165422231
Loss at iteration [602]: 0.2149802207626914
Loss at iteration [603]: 0.2149781859319057
Loss at iteration [604]: 0.2149763689501281
Loss at iteration [605]: 0.21497441969759626
Loss at iteration [606]: 0.21497283433323205
Loss at iteration [607]: 0.21497096077846528
Loss at iteration [608]: 0.21496968597216248
Loss at iteration [609]: 0.2149675070263765
Loss at iteration [610]: 0.2149659837114199
Loss at iteration [611]: 0.21496346060212168
Loss at iteration [612]: 0.21496101514220853
Loss at iteration [613]: 0.21495886553404048
Loss at iteration [614]: 0.2149569801508962
Loss at iteration [615]: 0.21495510628593353
Loss at iteration [616]: 0.21495325741491073
Loss at iteration [617]: 0.21495132177174237
Loss at iteration [618]: 0.21494953087114566
Loss at iteration [619]: 0.21494765396585283
Loss at iteration [620]: 0.21494580719294912
Loss at iteration [621]: 0.21494420676382905
Loss at iteration [622]: 0.21494251651474147
Loss at iteration [623]: 0.2149420659608579
Loss at iteration [624]: 0.21494023670554174
Loss at iteration [625]: 0.2149395370835378
Loss at iteration [626]: 0.214936968796944
Loss at iteration [627]: 0.21493412651425223
Loss at iteration [628]: 0.21493181018403232
Loss at iteration [629]: 0.21492997585682524
Loss at iteration [630]: 0.2149284599812124
Loss at iteration [631]: 0.21492689349340796
Loss at iteration [632]: 0.21492536632593084
Loss at iteration [633]: 0.21492354039663733
Loss at iteration [634]: 0.21492197978322178
Loss at iteration [635]: 0.21492013503644444
Loss at iteration [636]: 0.2149182682045523
Loss at iteration [637]: 0.21491657256035573
Loss at iteration [638]: 0.21491490642754574
Loss at iteration [639]: 0.21491325700102193
Loss at iteration [640]: 0.21491174246115766
Loss at iteration [641]: 0.21491026033293592
Loss at iteration [642]: 0.21490996659113826
Loss at iteration [643]: 0.21490856111453094
Loss at iteration [644]: 0.21490853405972182
Loss at iteration [645]: 0.21490622733994913
Loss at iteration [646]: 0.21490543678041327
Loss at iteration [647]: 0.21490256535838853
Loss at iteration [648]: 0.21489981693978932
Loss at iteration [649]: 0.21489775808719297
Loss at iteration [650]: 0.2148962487782873
Loss at iteration [651]: 0.21489498282359457
Loss at iteration [652]: 0.21489366614972846
Loss at iteration [653]: 0.21489227978095196
Loss at iteration [654]: 0.21489061867385942
Loss at iteration [655]: 0.21488888548551052
Loss at iteration [656]: 0.21488731472720254
Loss at iteration [657]: 0.21488590006226127
Loss at iteration [658]: 0.2148846163591547
Loss at iteration [659]: 0.21488326984662864
Loss at iteration [660]: 0.21488195353584472
Loss at iteration [661]: 0.2148805597155668
Loss at iteration [662]: 0.21487900472380977
Loss at iteration [663]: 0.21487748619137528
Loss at iteration [664]: 0.21487609256696125
Loss at iteration [665]: 0.21487477401837643
Loss at iteration [666]: 0.21487409168897037
Loss at iteration [667]: 0.2148730470922883
Loss at iteration [668]: 0.21487427528332947
***** Warning: Loss has increased *****
Loss at iteration [669]: 0.21487255966900562
Loss at iteration [670]: 0.21487255526414914
Loss at iteration [671]: 0.21486952006792873
Loss at iteration [672]: 0.2148666096842327
Loss at iteration [673]: 0.21486450948394373
Loss at iteration [674]: 0.2148631270867309
Loss at iteration [675]: 0.21486188866266387
Loss at iteration [676]: 0.2148606717299538
Loss at iteration [677]: 0.21485948238740454
Loss at iteration [678]: 0.2148581484575841
Loss at iteration [679]: 0.21485710327742277
Loss at iteration [680]: 0.2148557584826346
Loss at iteration [681]: 0.2148544604019307
Loss at iteration [682]: 0.21485318356922442
Loss at iteration [683]: 0.2148519415577061
Loss at iteration [684]: 0.2148507990200739
Loss at iteration [685]: 0.21484961971268504
Loss at iteration [686]: 0.2148484541657826
Loss at iteration [687]: 0.21484728464384625
Loss at iteration [688]: 0.21484612194656044
Loss at iteration [689]: 0.21484510738551002
Loss at iteration [690]: 0.2148440014699467
Loss at iteration [691]: 0.2148435871096786
Loss at iteration [692]: 0.2148426226427466
Loss at iteration [693]: 0.21484376450150253
***** Warning: Loss has increased *****
Loss at iteration [694]: 0.21484197608712594
Loss at iteration [695]: 0.2148396581062822
Loss at iteration [696]: 0.21483773116082666
Loss at iteration [697]: 0.21483661835967408
Loss at iteration [698]: 0.21483555885630187
Loss at iteration [699]: 0.21483471894604314
Loss at iteration [700]: 0.21483385965083227
Loss at iteration [701]: 0.21483278034075098
Loss at iteration [702]: 0.21483195192870427
Loss at iteration [703]: 0.21483088759367733
Loss at iteration [704]: 0.2148297680864445
Loss at iteration [705]: 0.21482875428441386
Loss at iteration [706]: 0.2148277521096292
Loss at iteration [707]: 0.214826821425464
Loss at iteration [708]: 0.21482612571377854
Loss at iteration [709]: 0.2148254344916321
Loss at iteration [710]: 0.2148263911430842
***** Warning: Loss has increased *****
Loss at iteration [711]: 0.2148253887851572
Loss at iteration [712]: 0.21482734619809804
***** Warning: Loss has increased *****
Loss at iteration [713]: 0.2148249279084582
Loss at iteration [714]: 0.21482198037957523
Loss at iteration [715]: 0.2148199851757413
Loss at iteration [716]: 0.21481889196094367
Loss at iteration [717]: 0.2148180761653998
Loss at iteration [718]: 0.21481717505966422
Loss at iteration [719]: 0.21481621883189567
Loss at iteration [720]: 0.21481531783925262
Loss at iteration [721]: 0.21481453701518274
Loss at iteration [722]: 0.21481366362288956
Loss at iteration [723]: 0.2148128072533615
Loss at iteration [724]: 0.21481192303511756
Loss at iteration [725]: 0.21481110771753184
Loss at iteration [726]: 0.2148103621533463
Loss at iteration [727]: 0.2148095416298879
Loss at iteration [728]: 0.21480894937200917
Loss at iteration [729]: 0.21480805814842577
Loss at iteration [730]: 0.2148071697351841
Loss at iteration [731]: 0.21480630207732082
Loss at iteration [732]: 0.21480602452387734
Loss at iteration [733]: 0.21480507419014822
Loss at iteration [734]: 0.21480416576657457
Loss at iteration [735]: 0.21480320098626304
Loss at iteration [736]: 0.21480258076329697
Loss at iteration [737]: 0.21480169097814342
Loss at iteration [738]: 0.21480092041837842
Loss at iteration [739]: 0.21480018300835377
Loss at iteration [740]: 0.21480052962601162
***** Warning: Loss has increased *****
Loss at iteration [741]: 0.21479976974485765
Loss at iteration [742]: 0.2148005837698251
***** Warning: Loss has increased *****
Loss at iteration [743]: 0.21479930507399583
Loss at iteration [744]: 0.2147974766970337
Loss at iteration [745]: 0.21479599743642538
Loss at iteration [746]: 0.21479518985096913
Loss at iteration [747]: 0.21479449240027007
Loss at iteration [748]: 0.21479388712207542
Loss at iteration [749]: 0.21479324423252366
Loss at iteration [750]: 0.21479256482779488
Loss at iteration [751]: 0.2147919362680353
Loss at iteration [752]: 0.2147911793109829
Loss at iteration [753]: 0.21479035312924447
Loss at iteration [754]: 0.21478960101974454
Loss at iteration [755]: 0.21478893093018303
Loss at iteration [756]: 0.21478824776802766
Loss at iteration [757]: 0.21478775251573298
Loss at iteration [758]: 0.21478820817681898
***** Warning: Loss has increased *****
Loss at iteration [759]: 0.21478763411228077
Loss at iteration [760]: 0.21478991753664115
***** Warning: Loss has increased *****
Loss at iteration [761]: 0.2147883695271462
Loss at iteration [762]: 0.21478600597592215
Loss at iteration [763]: 0.21478418291969456
Loss at iteration [764]: 0.21478330016900476
Loss at iteration [765]: 0.2147827205066929
Loss at iteration [766]: 0.21478208701246884
Loss at iteration [767]: 0.21478142209954462
Loss at iteration [768]: 0.21478080255369572
Loss at iteration [769]: 0.21478014972443704
Loss at iteration [770]: 0.21477954758441542
Loss at iteration [771]: 0.21477902092574636
Loss at iteration [772]: 0.21477840424390338
Loss at iteration [773]: 0.21477782515171817
Loss at iteration [774]: 0.2147771924271525
Loss at iteration [775]: 0.2147765862730437
Loss at iteration [776]: 0.21477609716104856
Loss at iteration [777]: 0.21477627970454055
***** Warning: Loss has increased *****
Loss at iteration [778]: 0.21477582069600934
Loss at iteration [779]: 0.21477790028323052
***** Warning: Loss has increased *****
Loss at iteration [780]: 0.21477679266511335
Loss at iteration [781]: 0.2147750477730014
Loss at iteration [782]: 0.21477334304673165
Loss at iteration [783]: 0.2147723542268707
Loss at iteration [784]: 0.21477175233155077
Loss at iteration [785]: 0.21477122308479654
Loss at iteration [786]: 0.2147706735609455
Loss at iteration [787]: 0.21477014085562388
Loss at iteration [788]: 0.21476957923464537
Loss at iteration [789]: 0.21476908271029765
Loss at iteration [790]: 0.21476860161002595
Loss at iteration [791]: 0.21476806828091555
Loss at iteration [792]: 0.2147675836717982
Loss at iteration [793]: 0.21476702784746382
Loss at iteration [794]: 0.21476653676948027
Loss at iteration [795]: 0.2147661241315068
Loss at iteration [796]: 0.21476674791535094
***** Warning: Loss has increased *****
Loss at iteration [797]: 0.21476632073352236
Loss at iteration [798]: 0.21476899567833055
***** Warning: Loss has increased *****
Loss at iteration [799]: 0.21476802991478242
Loss at iteration [800]: 0.21476605545606894
Loss at iteration [801]: 0.2147640473275501
Loss at iteration [802]: 0.21476294588521477
Loss at iteration [803]: 0.2147623893617521
Loss at iteration [804]: 0.21476192634239022
Loss at iteration [805]: 0.21476147903100046
Loss at iteration [806]: 0.2147609966775217
Loss at iteration [807]: 0.21476048773419742
Loss at iteration [808]: 0.21476002784262174
Loss at iteration [809]: 0.2147595652999449
Loss at iteration [810]: 0.21475912556109752
Loss at iteration [811]: 0.21475875094994737
Loss at iteration [812]: 0.21475832268816455
Loss at iteration [813]: 0.21475787815670302
Loss at iteration [814]: 0.21475744641301414
Loss at iteration [815]: 0.21475699527545195
Loss at iteration [816]: 0.21475659969159583
Loss at iteration [817]: 0.21475624065498117
Loss at iteration [818]: 0.2147558996585513
Loss at iteration [819]: 0.21475649621908585
***** Warning: Loss has increased *****
Loss at iteration [820]: 0.21475612503270086
Loss at iteration [821]: 0.2147588800807119
***** Warning: Loss has increased *****
Loss at iteration [822]: 0.21475792073910874
Loss at iteration [823]: 0.21475610882505
Loss at iteration [824]: 0.21475425727283357
Loss at iteration [825]: 0.21475323382805037
Loss at iteration [826]: 0.21475264911818023
Loss at iteration [827]: 0.21475229890521377
Loss at iteration [828]: 0.21475187826735578
Loss at iteration [829]: 0.21475151713373752
Loss at iteration [830]: 0.2147511130395661
Loss at iteration [831]: 0.21475070015976233
Loss at iteration [832]: 0.2147503102372312
Loss at iteration [833]: 0.2147500146283084
Loss at iteration [834]: 0.21474966073350552
Loss at iteration [835]: 0.21474930967813058
Loss at iteration [836]: 0.2147489045268753
Loss at iteration [837]: 0.21474851106794934
Loss at iteration [838]: 0.2147481713167065
Loss at iteration [839]: 0.21474791457921916
Loss at iteration [840]: 0.2147475630378487
Loss at iteration [841]: 0.21474722832533905
Loss at iteration [842]: 0.214746815409522
Loss at iteration [843]: 0.21474644124904518
Loss at iteration [844]: 0.21474610269526206
Loss at iteration [845]: 0.21474626187949983
***** Warning: Loss has increased *****
Loss at iteration [846]: 0.21474600211821515
Loss at iteration [847]: 0.2147478590074551
***** Warning: Loss has increased *****
Loss at iteration [848]: 0.21474745441956303
Loss at iteration [849]: 0.21474626160375662
Loss at iteration [850]: 0.21474484327024437
Loss at iteration [851]: 0.21474401683179317
Loss at iteration [852]: 0.21474353811635843
Loss at iteration [853]: 0.2147432081162936
Loss at iteration [854]: 0.21474290888329545
Loss at iteration [855]: 0.2147425838594127
Loss at iteration [856]: 0.21474225188162674
Loss at iteration [857]: 0.21474200611051097
Loss at iteration [858]: 0.2147417659448135
Loss at iteration [859]: 0.21474143518858496
Loss at iteration [860]: 0.21474114303396188
Loss at iteration [861]: 0.21474077724791096
Loss at iteration [862]: 0.2147404732917986
Loss at iteration [863]: 0.21474017139477602
Loss at iteration [864]: 0.21473993615253875
Loss at iteration [865]: 0.21473964868068954
Loss at iteration [866]: 0.21473934097728972
Loss at iteration [867]: 0.21473902062012928
Loss at iteration [868]: 0.21473874194629236
Loss at iteration [869]: 0.21473844112945253
Loss at iteration [870]: 0.21473869345289143
***** Warning: Loss has increased *****
Loss at iteration [871]: 0.21473852850051875
Loss at iteration [872]: 0.21473846779141442
Loss at iteration [873]: 0.214737969093479
Loss at iteration [874]: 0.2147373920402063
Loss at iteration [875]: 0.21473692071845962
Loss at iteration [876]: 0.21473659458092698
Loss at iteration [877]: 0.21473630885403458
Loss at iteration [878]: 0.2147360356772836
Loss at iteration [879]: 0.21473580169820947
Loss at iteration [880]: 0.21473553397191536
Loss at iteration [881]: 0.21473628364296254
***** Warning: Loss has increased *****
Loss at iteration [882]: 0.2147361447577225
Loss at iteration [883]: 0.2147379408367643
***** Warning: Loss has increased *****
Loss at iteration [884]: 0.21473725227879184
Loss at iteration [885]: 0.21473591046150783
Loss at iteration [886]: 0.2147346346045413
Loss at iteration [887]: 0.2147339972382197
Loss at iteration [888]: 0.2147336279152642
Loss at iteration [889]: 0.21473341815515218
Loss at iteration [890]: 0.2147331925589197
Loss at iteration [891]: 0.2147329304340459
Loss at iteration [892]: 0.21473266762581325
Loss at iteration [893]: 0.21473245189811863
Loss at iteration [894]: 0.214732282449636
Loss at iteration [895]: 0.21473206729078706
Loss at iteration [896]: 0.2147318048847779
Loss at iteration [897]: 0.21473153974014775
Loss at iteration [898]: 0.21473131156769493
Loss at iteration [899]: 0.21473107829502158
Loss at iteration [900]: 0.21473084589979807
Loss at iteration [901]: 0.21473065831339241
Loss at iteration [902]: 0.21473044961021365
Loss at iteration [903]: 0.21473029039424393
Loss at iteration [904]: 0.21473009375782626
Loss at iteration [905]: 0.21472986179468176
Loss at iteration [906]: 0.21472962416862135
Loss at iteration [907]: 0.2147293836227352
Loss at iteration [908]: 0.2147291629424251
Loss at iteration [909]: 0.21472897294284835
Loss at iteration [910]: 0.2147288458944397
Loss at iteration [911]: 0.21472865590086457
Loss at iteration [912]: 0.21472847424415784
Loss at iteration [913]: 0.21472822451848775
Loss at iteration [914]: 0.21472800902450176
Loss at iteration [915]: 0.21472780213815895
Loss at iteration [916]: 0.2147275936250265
Loss at iteration [917]: 0.21472741928679678
Loss at iteration [918]: 0.2147275932409739
***** Warning: Loss has increased *****
Loss at iteration [919]: 0.2147275258151464
Loss at iteration [920]: 0.21472943637343647
***** Warning: Loss has increased *****
Loss at iteration [921]: 0.2147290767335549
Loss at iteration [922]: 0.21472811077338555
Loss at iteration [923]: 0.21472696449589718
Loss at iteration [924]: 0.21472633881243317
Loss at iteration [925]: 0.21472600908978293
Loss at iteration [926]: 0.2147258230614199
Loss at iteration [927]: 0.2147256432002463
Loss at iteration [928]: 0.21472548562393923
Loss at iteration [929]: 0.21472528149679584
Loss at iteration [930]: 0.2147251164449009
Loss at iteration [931]: 0.21472493988188404
Loss at iteration [932]: 0.2147247575888911
Loss at iteration [933]: 0.2147245867921339
Loss at iteration [934]: 0.21472442821599683
Loss at iteration [935]: 0.21472431872447928
Loss at iteration [936]: 0.21472419268011464
Loss at iteration [937]: 0.2147240251096068
Loss at iteration [938]: 0.21472381971224055
Loss at iteration [939]: 0.21472365856274456
Loss at iteration [940]: 0.21472346776425055
Loss at iteration [941]: 0.21472330575401738
Loss at iteration [942]: 0.21472314709800108
Loss at iteration [943]: 0.2147229877135426
Loss at iteration [944]: 0.21472290381271494
Loss at iteration [945]: 0.214722773252012
Loss at iteration [946]: 0.2147226212980159
Loss at iteration [947]: 0.21472244528937878
Loss at iteration [948]: 0.2147222670699732
Loss at iteration [949]: 0.2147220960426501
Loss at iteration [950]: 0.21472194585177046
Loss at iteration [951]: 0.21472179467349792
Loss at iteration [952]: 0.21472166170417167
Loss at iteration [953]: 0.2147219131635637
***** Warning: Loss has increased *****
Loss at iteration [954]: 0.21472193497520856
***** Warning: Loss has increased *****
Loss at iteration [955]: 0.21472383586176452
***** Warning: Loss has increased *****
Loss at iteration [956]: 0.2147235011155877
Loss at iteration [957]: 0.21472254762568943
Loss at iteration [958]: 0.21472144056031503
Loss at iteration [959]: 0.21472088379841395
Loss at iteration [960]: 0.2147206158495689
Loss at iteration [961]: 0.21472048557941542
Loss at iteration [962]: 0.21472035834286585
Loss at iteration [963]: 0.21472022437688018
Loss at iteration [964]: 0.2147200764170182
Loss at iteration [965]: 0.21471993752879603
Loss at iteration [966]: 0.21471979883221684
Loss at iteration [967]: 0.21471966624058536
Loss at iteration [968]: 0.21471955154951364
Loss at iteration [969]: 0.21471941199692132
Loss at iteration [970]: 0.21471930175946077
Loss at iteration [971]: 0.21471917798322082
Loss at iteration [972]: 0.2147190539905064
Loss at iteration [973]: 0.21471897371320556
Loss at iteration [974]: 0.21471887662140587
Loss at iteration [975]: 0.21471874705808686
Loss at iteration [976]: 0.21471861569784106
Loss at iteration [977]: 0.21471846462325586
Loss at iteration [978]: 0.21471835185775168
Loss at iteration [979]: 0.21471823208494836
Loss at iteration [980]: 0.21471811050639064
Loss at iteration [981]: 0.2147179884083278
Loss at iteration [982]: 0.21471788009625847
Loss at iteration [983]: 0.2147177652379975
Loss at iteration [984]: 0.2147177139301751
Loss at iteration [985]: 0.2147176087485247
Loss at iteration [986]: 0.21471750447731577
Loss at iteration [987]: 0.21471737787418116
Loss at iteration [988]: 0.21471725032314715
Loss at iteration [989]: 0.21471713164512707
Loss at iteration [990]: 0.21471701666478551
Loss at iteration [991]: 0.2147169081119997
Loss at iteration [992]: 0.21471681402587744
Loss at iteration [993]: 0.21471670238447946
Loss at iteration [994]: 0.2147166110098022
Loss at iteration [995]: 0.21471651049950843
Loss at iteration [996]: 0.21471644493002243
Loss at iteration [997]: 0.21471639774500784
Loss at iteration [998]: 0.2147169494192189
***** Warning: Loss has increased *****
Loss at iteration [999]: 0.2147168878573335
Loss at iteration [1000]: 0.21471659705604895
Loss at iteration [1001]: 0.21471618224334324
Loss at iteration [1002]: 0.21471592537014902
Loss at iteration [1003]: 0.2147157567396189
Loss at iteration [1004]: 0.21471564380827457
Loss at iteration [1005]: 0.21471555648568566
Loss at iteration [1006]: 0.2147154634189498
Loss at iteration [1007]: 0.21471536854872372
Loss at iteration [1008]: 0.21471528015820182
Loss at iteration [1009]: 0.2147151858922036
Loss at iteration [1010]: 0.21471510174839878
Loss at iteration [1011]: 0.2147150035105349
Loss at iteration [1012]: 0.21471492593522448
Loss at iteration [1013]: 0.2147148730168495
Loss at iteration [1014]: 0.2147147970327221
Loss at iteration [1015]: 0.2147147142211393
Loss at iteration [1016]: 0.21471460879583804
Loss at iteration [1017]: 0.21471451355639384
Loss at iteration [1018]: 0.21471442415149097
Loss at iteration [1019]: 0.21471433555764677
Loss at iteration [1020]: 0.2147142521881569
Loss at iteration [1021]: 0.21471416966772783
Loss at iteration [1022]: 0.2147140868734447
Loss at iteration [1023]: 0.214714033821892
Loss at iteration [1024]: 0.21471436293639945
***** Warning: Loss has increased *****
Loss at iteration [1025]: 0.21471432868103266
Loss at iteration [1026]: 0.214715999861224
***** Warning: Loss has increased *****
Loss at iteration [1027]: 0.21471567185163198
Loss at iteration [1028]: 0.21471484707023544
Loss at iteration [1029]: 0.2147139991202158
Loss at iteration [1030]: 0.21471361457626884
Loss at iteration [1031]: 0.21471344407462023
Loss at iteration [1032]: 0.214713374716664
Loss at iteration [1033]: 0.21471330890536447
Loss at iteration [1034]: 0.2147132219784227
Loss at iteration [1035]: 0.21471313736680175
Loss at iteration [1036]: 0.21471305820053946
Loss at iteration [1037]: 0.21471298238293401
Loss at iteration [1038]: 0.2147129205873797
Loss at iteration [1039]: 0.21471284480518288
Loss at iteration [1040]: 0.21471277518099333
Loss at iteration [1041]: 0.21471270407989682
Loss at iteration [1042]: 0.2147126313200222
Loss at iteration [1043]: 0.21471257172147012
Loss at iteration [1044]: 0.21471249991084762
Loss at iteration [1045]: 0.21471243211618707
Loss at iteration [1046]: 0.21471236687072423
Loss at iteration [1047]: 0.21471229603519934
Loss at iteration [1048]: 0.2147122383898371
Loss at iteration [1049]: 0.21471216905740748
Loss at iteration [1050]: 0.21471210403076918
Loss at iteration [1051]: 0.21471204025046225
Loss at iteration [1052]: 0.21471197245945986
Loss at iteration [1053]: 0.21471191865484796
Loss at iteration [1054]: 0.21471188704923294
Loss at iteration [1055]: 0.21471183675506697
Loss at iteration [1056]: 0.21471177541980063
Loss at iteration [1057]: 0.21471170100635015
Loss at iteration [1058]: 0.2147116254026224
Loss at iteration [1059]: 0.21471155679368276
Loss at iteration [1060]: 0.21471149778534215
Loss at iteration [1061]: 0.21471143220395622
Loss at iteration [1062]: 0.21471137683322333
Loss at iteration [1063]: 0.21471131798402449
Loss at iteration [1064]: 0.21471125627205315
Loss at iteration [1065]: 0.21471120447773698
Loss at iteration [1066]: 0.21471114340323763
Loss at iteration [1067]: 0.214711090259169
Loss at iteration [1068]: 0.21471103263008848
Loss at iteration [1069]: 0.21471097364692845
Loss at iteration [1070]: 0.2147109299575739
Loss at iteration [1071]: 0.21471090394110473
Loss at iteration [1072]: 0.21471085865844358
Loss at iteration [1073]: 0.21471080180808108
Loss at iteration [1074]: 0.21471073731899712
Loss at iteration [1075]: 0.21471067103203237
Loss at iteration [1076]: 0.21471060980192008
Loss at iteration [1077]: 0.21471055603614725
Loss at iteration [1078]: 0.21471050128617802
Loss at iteration [1079]: 0.21471045065225064
Loss at iteration [1080]: 0.2147103966875565
Loss at iteration [1081]: 0.21471034839482342
Loss at iteration [1082]: 0.2147102977802264
Loss at iteration [1083]: 0.2147102466320865
Loss at iteration [1084]: 0.21471020183375067
Loss at iteration [1085]: 0.21471014888749282
Loss at iteration [1086]: 0.21471010457985823
Loss at iteration [1087]: 0.21471005539226262
Loss at iteration [1088]: 0.2147100050389568
Loss at iteration [1089]: 0.2147100004419068
Loss at iteration [1090]: 0.21470999073017163
Loss at iteration [1091]: 0.21471049793824828
***** Warning: Loss has increased *****
Loss at iteration [1092]: 0.21471047533549203
Loss at iteration [1093]: 0.21471026644977836
Loss at iteration [1094]: 0.21470994465418056
Loss at iteration [1095]: 0.21470976062495498
Loss at iteration [1096]: 0.21470965584739857
Loss at iteration [1097]: 0.2147096085626087
Loss at iteration [1098]: 0.21470956412264847
Loss at iteration [1099]: 0.21470951846209668
Loss at iteration [1100]: 0.21470947756371042
Loss at iteration [1101]: 0.214709430912419
Loss at iteration [1102]: 0.21470938691593874
Loss at iteration [1103]: 0.21470934317849236
Loss at iteration [1104]: 0.21470929622824006
Loss at iteration [1105]: 0.2147092565633621
Loss at iteration [1106]: 0.2147092126812997
Loss at iteration [1107]: 0.21470917535747308
Loss at iteration [1108]: 0.2147091307152104
Loss at iteration [1109]: 0.21470909148779382
Loss at iteration [1110]: 0.21470904923106682
Loss at iteration [1111]: 0.2147090074357967
Loss at iteration [1112]: 0.21470896845739312
Loss at iteration [1113]: 0.2147089276744649
Loss at iteration [1114]: 0.21470889074209104
Loss at iteration [1115]: 0.21470885186693112
Loss at iteration [1116]: 0.2147088153723576
Loss at iteration [1117]: 0.21470881139343406
Loss at iteration [1118]: 0.2147087835947258
Loss at iteration [1119]: 0.2147087408227398
Loss at iteration [1120]: 0.21470868782550828
Loss at iteration [1121]: 0.2147086412454287
Loss at iteration [1122]: 0.21470859753290156
Loss at iteration [1123]: 0.21470855952563628
Loss at iteration [1124]: 0.21470852165644227
Loss at iteration [1125]: 0.2147084888547381
Loss at iteration [1126]: 0.2147084512259667
Loss at iteration [1127]: 0.214708417519369
Loss at iteration [1128]: 0.21470838262522923
Loss at iteration [1129]: 0.21470834661890723
Loss at iteration [1130]: 0.21470831239391436
Loss at iteration [1131]: 0.21470827818086863
Loss at iteration [1132]: 0.21470824418346077
Loss at iteration [1133]: 0.21470820903710114
Loss at iteration [1134]: 0.21470817936039102
Loss at iteration [1135]: 0.21470814381646303
Loss at iteration [1136]: 0.21470811166122225
Loss at iteration [1137]: 0.2147080781549627
Loss at iteration [1138]: 0.21470804773815114
Loss at iteration [1139]: 0.21470828554487995
***** Warning: Loss has increased *****
Loss at iteration [1140]: 0.21470827055365926
Loss at iteration [1141]: 0.2147082027901958
Loss at iteration [1142]: 0.21470803865993093
Loss at iteration [1143]: 0.21470794469677887
Loss at iteration [1144]: 0.2147078707189602
Loss at iteration [1145]: 0.2147078303916357
Loss at iteration [1146]: 0.21470780260787592
Loss at iteration [1147]: 0.2147077721352137
Loss at iteration [1148]: 0.21470773836054655
Loss at iteration [1149]: 0.21470770966203864
Loss at iteration [1150]: 0.21470767738225727
Loss at iteration [1151]: 0.21470764697536127
Loss at iteration [1152]: 0.21470761642765454
Loss at iteration [1153]: 0.2147075897374025
Loss at iteration [1154]: 0.21470758981408927
***** Warning: Loss has increased *****
