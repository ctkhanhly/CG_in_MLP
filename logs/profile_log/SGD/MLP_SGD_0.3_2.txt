Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : SGD
Learning rate                         : 0.3
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 0.6485850811004639
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 56.14894902030672%
Percentage of parameters < 1e-7       : 56.14894902030672%
Percentage of parameters < 1e-6       : 56.14894902030672%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.9361305705134577
Loss at iteration [2]: 0.10254302613101807
Loss at iteration [3]: 1.0446986357457388
***** Warning: Loss has increased *****
Loss at iteration [4]: 1.0532515456635816
***** Warning: Loss has increased *****
Loss at iteration [5]: 1.5079095957478632
***** Warning: Loss has increased *****
Loss at iteration [6]: 1.3687631453806965
Loss at iteration [7]: 1.1745229607310435
Loss at iteration [8]: 0.83774443412544
Loss at iteration [9]: 0.38896498938594687
Loss at iteration [10]: 0.32057977467686455
Loss at iteration [11]: 2.082295877236409
***** Warning: Loss has increased *****
Loss at iteration [12]: 0.9999658784654123
Loss at iteration [13]: 1.1351241385661341
***** Warning: Loss has increased *****
Loss at iteration [14]: 1.0355717577073176
Loss at iteration [15]: 0.8319308152701416
Loss at iteration [16]: 0.5295591608971065
Loss at iteration [17]: 0.43233284087849655
Loss at iteration [18]: 1.4354456500732817
***** Warning: Loss has increased *****
Loss at iteration [19]: 0.2415489991159704
Loss at iteration [20]: 1.629117693547774
***** Warning: Loss has increased *****
Loss at iteration [21]: 1.9040194608859964
***** Warning: Loss has increased *****
Loss at iteration [22]: 1.0235794131627536
Loss at iteration [23]: 0.49062409271725527
Loss at iteration [24]: 0.3511445763648779
Loss at iteration [25]: 1.077613244153971
***** Warning: Loss has increased *****
Loss at iteration [26]: 0.21947710847510954
Loss at iteration [27]: 2.7653159643595355
***** Warning: Loss has increased *****
Loss at iteration [28]: 1.8864397017971704
Loss at iteration [29]: 1.4414115700437764
Loss at iteration [30]: 1.0150110181563725
Loss at iteration [31]: 1.0443513039523253
***** Warning: Loss has increased *****
Loss at iteration [32]: 1.06057316042236
***** Warning: Loss has increased *****
Loss at iteration [33]: 1.0027965388212587
Loss at iteration [34]: 0.9778070084742757
Loss at iteration [35]: 0.962242439878046
Loss at iteration [36]: 0.9169351402039078
Loss at iteration [37]: 0.8204911383425373
Loss at iteration [38]: 0.6282747466003703
Loss at iteration [39]: 0.42844417739734825
Loss at iteration [40]: 0.2892235724818145
Loss at iteration [41]: 0.26714614574407114
Loss at iteration [42]: 0.5194108428383816
***** Warning: Loss has increased *****
Loss at iteration [43]: 1.7630774927783859
***** Warning: Loss has increased *****
Loss at iteration [44]: 3.1389438186966676
***** Warning: Loss has increased *****
Loss at iteration [45]: 1.6826585118582462
Loss at iteration [46]: 0.9693513996667236
Loss at iteration [47]: 1.065282666936229
***** Warning: Loss has increased *****
Loss at iteration [48]: 0.8392835928958648
Loss at iteration [49]: 0.28578420884617817
Loss at iteration [50]: 0.6649899590677285
***** Warning: Loss has increased *****
Loss at iteration [51]: 1.5080762545858406
***** Warning: Loss has increased *****
Loss at iteration [52]: 1.2856749430989343
Loss at iteration [53]: 1.011560890903204
Loss at iteration [54]: 1.029061213457504
***** Warning: Loss has increased *****
Loss at iteration [55]: 1.0429249369681028
***** Warning: Loss has increased *****
Loss at iteration [56]: 1.0102453941673635
Loss at iteration [57]: 1.0001519947252786
Loss at iteration [58]: 1.0038171626239714
***** Warning: Loss has increased *****
Loss at iteration [59]: 1.0024232431999922
Loss at iteration [60]: 1.0001421159930155
Loss at iteration [61]: 1.000077025375419
Loss at iteration [62]: 1.0001890467527925
***** Warning: Loss has increased *****
Loss at iteration [63]: 0.9998867525858671
Loss at iteration [64]: 0.9996926155332533
Loss at iteration [65]: 0.9995803388967386
Loss at iteration [66]: 0.9993775146833885
Loss at iteration [67]: 0.9990796307937126
Loss at iteration [68]: 0.9986753783242691
Loss at iteration [69]: 0.998091155117527
Loss at iteration [70]: 0.9972334973942996
Loss at iteration [71]: 0.9959658772628398
Loss at iteration [72]: 0.9940625355212984
Loss at iteration [73]: 0.9911442512433752
Loss at iteration [74]: 0.9865733294590884
Loss at iteration [75]: 0.9792107957597476
Loss at iteration [76]: 0.9669233154329632
Loss at iteration [77]: 0.9454648743232517
Loss at iteration [78]: 0.9061603563589665
Loss at iteration [79]: 0.8315470508909121
Loss at iteration [80]: 0.6931771881465576
Loss at iteration [81]: 0.49244047461990337
Loss at iteration [82]: 0.34319357471006495
Loss at iteration [83]: 0.2642102920219392
Loss at iteration [84]: 0.24818220933171733
Loss at iteration [85]: 0.4450644005717906
***** Warning: Loss has increased *****
Loss at iteration [86]: 0.9749753086150351
***** Warning: Loss has increased *****
Loss at iteration [87]: 3.068557231485232
***** Warning: Loss has increased *****
Loss at iteration [88]: 1.659948471821227
Loss at iteration [89]: 0.9926841047387397
Loss at iteration [90]: 1.1280022275636203
***** Warning: Loss has increased *****
Loss at iteration [91]: 1.0387013679238803
Loss at iteration [92]: 0.7430022976749726
Loss at iteration [93]: 0.46638921015388585
Loss at iteration [94]: 0.29455616669553963
Loss at iteration [95]: 0.4746599224021064
***** Warning: Loss has increased *****
Loss at iteration [96]: 0.20524559480830948
Loss at iteration [97]: 0.3368737753003352
***** Warning: Loss has increased *****
Loss at iteration [98]: 0.38480300223751673
***** Warning: Loss has increased *****
Loss at iteration [99]: 1.211305364102839
***** Warning: Loss has increased *****
Loss at iteration [100]: 0.8700484815201787
Loss at iteration [101]: 0.4538210764495346
Loss at iteration [102]: 0.5502699986611476
***** Warning: Loss has increased *****
Loss at iteration [103]: 0.7366976029173841
***** Warning: Loss has increased *****
Loss at iteration [104]: 0.3552408380274015
Loss at iteration [105]: 1.4206374586392279
***** Warning: Loss has increased *****
Loss at iteration [106]: 1.6593249426740038
***** Warning: Loss has increased *****
Loss at iteration [107]: 1.2882490862418001
Loss at iteration [108]: 1.0059637267495531
Loss at iteration [109]: 1.0395888784021154
***** Warning: Loss has increased *****
Loss at iteration [110]: 1.0473812505895248
***** Warning: Loss has increased *****
Loss at iteration [111]: 1.0092990824723407
Loss at iteration [112]: 1.0004887182713074
Loss at iteration [113]: 1.004634160410245
***** Warning: Loss has increased *****
Loss at iteration [114]: 1.0025244746232522
Loss at iteration [115]: 1.000126552371912
Loss at iteration [116]: 1.0002282952216344
***** Warning: Loss has increased *****
Loss at iteration [117]: 1.000367727003896
***** Warning: Loss has increased *****
Loss at iteration [118]: 1.000095365758237
Loss at iteration [119]: 1.0000019679779382
Loss at iteration [120]: 1.0000325906167056
***** Warning: Loss has increased *****
Loss at iteration [121]: 1.0000224486205376
Loss at iteration [122]: 1.0000028191623678
Loss at iteration [123]: 1.0000020717753397
Loss at iteration [124]: 1.0000035647621377
***** Warning: Loss has increased *****
Loss at iteration [125]: 1.0000016480355387
Loss at iteration [126]: 1.0000006897314493
Loss at iteration [127]: 1.0000008389334456
***** Warning: Loss has increased *****
Loss at iteration [128]: 1.0000007570396763
Loss at iteration [129]: 1.0000005524808773
Loss at iteration [130]: 1.0000004849206796
Loss at iteration [131]: 1.0000004506556126
Loss at iteration [132]: 1.0000003907777668
Loss at iteration [133]: 1.0000003354931881
Loss at iteration [134]: 1.0000002902702776
Loss at iteration [135]: 1.0000002446966794
Loss at iteration [136]: 1.0000001988868648
Loss at iteration [137]: 1.0000001692068528
Loss at iteration [138]: 1.0000001422349507
Loss at iteration [139]: 1.000000116323729
Loss at iteration [140]: 1.0000000911052898
Loss at iteration [141]: 1.0000000699855784
Loss at iteration [142]: 1.000000056009873
Loss at iteration [143]: 1.0000000434449028
Loss at iteration [144]: 1.0000000316082827
Loss at iteration [145]: 1.0000000201452834
Loss at iteration [146]: 1.0000000088722953
Loss at iteration [147]: 1.000000000474758
Loss at iteration [148]: 1.0000000000000402
