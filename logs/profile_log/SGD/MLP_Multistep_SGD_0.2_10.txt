Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : SGD
Learning rate                         : 0.2
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 1.2250010967254639
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 51.68911274852191%
Percentage of parameters < 1e-7       : 51.68911274852191%
Percentage of parameters < 1e-6       : 51.68960462759835%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5984559521164694
Loss at iteration [2]: 1.4526992554249991
Loss at iteration [3]: 1.2821805104691002
Loss at iteration [4]: 1.067524651170341
Loss at iteration [5]: 0.8824265826839633
Loss at iteration [6]: 0.7274614309513888
Loss at iteration [7]: 0.6643772659122085
Loss at iteration [8]: 0.646161802830543
Loss at iteration [9]: 0.7305371657148376
***** Warning: Loss has increased *****
Loss at iteration [10]: 1.6581540124112473
***** Warning: Loss has increased *****
Loss at iteration [11]: 2.2266126136977142
***** Warning: Loss has increased *****
Loss at iteration [12]: 1.628152972159113
Loss at iteration [13]: 1.510109592487103
Loss at iteration [14]: 1.4355108347010856
Loss at iteration [15]: 1.3270924040943282
Loss at iteration [16]: 1.1576249951841067
Loss at iteration [17]: 0.9562287812238918
Loss at iteration [18]: 0.8202015455356487
Loss at iteration [19]: 0.6877109761464297
Loss at iteration [20]: 0.5898409891123795
Loss at iteration [21]: 0.49959290984015314
Loss at iteration [22]: 0.4410836677235244
Loss at iteration [23]: 0.41780605138087556
Loss at iteration [24]: 0.4671657900346918
***** Warning: Loss has increased *****
Loss at iteration [25]: 0.9638790935582249
***** Warning: Loss has increased *****
Loss at iteration [26]: 2.7599711974667493
***** Warning: Loss has increased *****
Loss at iteration [27]: 1.046409782422704
Loss at iteration [28]: 1.0868168698012408
***** Warning: Loss has increased *****
Loss at iteration [29]: 1.0353907530896655
Loss at iteration [30]: 0.9587680560684574
Loss at iteration [31]: 0.8880383239845834
Loss at iteration [32]: 0.8180702871058563
Loss at iteration [33]: 0.7559442468867426
Loss at iteration [34]: 0.7124046871684406
Loss at iteration [35]: 0.6777909848540684
Loss at iteration [36]: 0.6507727299285051
Loss at iteration [37]: 0.6261919384418834
Loss at iteration [38]: 0.5995181455977323
Loss at iteration [39]: 0.5735437283064521
Loss at iteration [40]: 0.547054797026845
Loss at iteration [41]: 0.5191227186949354
Loss at iteration [42]: 0.4891815341039407
Loss at iteration [43]: 0.45946643549094907
Loss at iteration [44]: 0.428194014043981
Loss at iteration [45]: 0.4037062330202943
Loss at iteration [46]: 0.39978179886749626
Loss at iteration [47]: 0.514207452110727
***** Warning: Loss has increased *****
Loss at iteration [48]: 0.7640546513873864
***** Warning: Loss has increased *****
Loss at iteration [49]: 1.773970599735042
***** Warning: Loss has increased *****
Loss at iteration [50]: 0.7154183415580563
Loss at iteration [51]: 0.8567066976806663
***** Warning: Loss has increased *****
Loss at iteration [52]: 0.5516554175761604
Loss at iteration [53]: 0.473186220565824
Loss at iteration [54]: 0.3466543165669244
Loss at iteration [55]: 0.30148637974052034
Loss at iteration [56]: 0.31719292076180444
***** Warning: Loss has increased *****
Loss at iteration [57]: 0.2414200425829911
Loss at iteration [58]: 0.23526323457684645
Loss at iteration [59]: 0.23846494691160058
***** Warning: Loss has increased *****
Loss at iteration [60]: 0.24073204704839088
***** Warning: Loss has increased *****
Loss at iteration [61]: 0.2518208158160425
***** Warning: Loss has increased *****
Loss at iteration [62]: 0.26102514229158935
***** Warning: Loss has increased *****
Loss at iteration [63]: 0.2743384520438384
***** Warning: Loss has increased *****
Loss at iteration [64]: 0.27093429168221933
Loss at iteration [65]: 0.2575814596491894
Loss at iteration [66]: 0.23776477157846562
Loss at iteration [67]: 0.22107615752589935
Loss at iteration [68]: 0.2241525779690345
***** Warning: Loss has increased *****
Loss at iteration [69]: 0.22267072460955423
Loss at iteration [70]: 0.21951551261855157
Loss at iteration [71]: 0.21934169081020613
Loss at iteration [72]: 0.21921335295013025
Loss at iteration [73]: 0.218637650763525
Loss at iteration [74]: 0.21838215365330776
Loss at iteration [75]: 0.21825095759907953
Loss at iteration [76]: 0.2180865780700127
Loss at iteration [77]: 0.21793904245121087
Loss at iteration [78]: 0.21782332102541685
Loss at iteration [79]: 0.21771548847979597
Loss at iteration [80]: 0.21761799810649238
Loss at iteration [81]: 0.21752310063168867
Loss at iteration [82]: 0.2174358352495287
Loss at iteration [83]: 0.21735429654690164
Loss at iteration [84]: 0.21727108856681618
Loss at iteration [85]: 0.21718041362677307
Loss at iteration [86]: 0.2170883962383538
Loss at iteration [87]: 0.21699028238215348
Loss at iteration [88]: 0.21689104479256657
Loss at iteration [89]: 0.21678575537490954
Loss at iteration [90]: 0.2166727851799559
Loss at iteration [91]: 0.216552920478981
Loss at iteration [92]: 0.21643235032420302
Loss at iteration [93]: 0.21631907731035982
Loss at iteration [94]: 0.21621455739466644
Loss at iteration [95]: 0.21612053378454502
Loss at iteration [96]: 0.216033597702519
Loss at iteration [97]: 0.21595363053838582
Loss at iteration [98]: 0.21588722233024732
Loss at iteration [99]: 0.2158181993753341
Loss at iteration [100]: 0.21575610805356202
Loss at iteration [101]: 0.2156941303249106
Loss at iteration [102]: 0.21563658268019406
Loss at iteration [103]: 0.2155891794575146
Loss at iteration [104]: 0.21554201794723016
Loss at iteration [105]: 0.21550772301080962
Loss at iteration [106]: 0.2154654911161918
Loss at iteration [107]: 0.21542441241665586
Loss at iteration [108]: 0.21538279608636707
Loss at iteration [109]: 0.21534009892172026
Loss at iteration [110]: 0.21530688289844796
Loss at iteration [111]: 0.21527023658824895
Loss at iteration [112]: 0.21524140170619868
Loss at iteration [113]: 0.21521896399353643
Loss at iteration [114]: 0.21521139656085167
Loss at iteration [115]: 0.2152344813651729
***** Warning: Loss has increased *****
Loss at iteration [116]: 0.21521647143839395
Loss at iteration [117]: 0.2152285447289286
***** Warning: Loss has increased *****
Loss at iteration [118]: 0.21515747008972053
Loss at iteration [119]: 0.21513859420781167
Loss at iteration [120]: 0.2150857165346751
Loss at iteration [121]: 0.21505438762905613
Loss at iteration [122]: 0.21503150849448766
Loss at iteration [123]: 0.21502499811341957
Loss at iteration [124]: 0.21503096856554693
***** Warning: Loss has increased *****
Loss at iteration [125]: 0.2150453771532174
***** Warning: Loss has increased *****
Loss at iteration [126]: 0.2150418490589455
Loss at iteration [127]: 0.21504769098172838
***** Warning: Loss has increased *****
Loss at iteration [128]: 0.21500922706728212
Loss at iteration [129]: 0.2150022566516196
Loss at iteration [130]: 0.2149630988822617
Loss at iteration [131]: 0.2149492684941447
Loss at iteration [132]: 0.21492627948183035
Loss at iteration [133]: 0.2149221398810956
Loss at iteration [134]: 0.2149152516531714
Loss at iteration [135]: 0.21493120892189765
***** Warning: Loss has increased *****
Loss at iteration [136]: 0.21492941390531722
Loss at iteration [137]: 0.2149659936597651
***** Warning: Loss has increased *****
Loss at iteration [138]: 0.21493739571689816
Loss at iteration [139]: 0.2149609504381394
***** Warning: Loss has increased *****
Loss at iteration [140]: 0.21490724021709995
Loss at iteration [141]: 0.21489477697399764
Loss at iteration [142]: 0.21486273377680987
Loss at iteration [143]: 0.21484747538119575
Loss at iteration [144]: 0.21484024080273514
Loss at iteration [145]: 0.21483786250131554
Loss at iteration [146]: 0.21484879632661047
***** Warning: Loss has increased *****
Loss at iteration [147]: 0.21487349214165183
***** Warning: Loss has increased *****
Loss at iteration [148]: 0.21488731316797743
***** Warning: Loss has increased *****
Loss at iteration [149]: 0.21492942068714488
***** Warning: Loss has increased *****
Loss at iteration [150]: 0.2148924874634124
Loss at iteration [151]: 0.21490256644982497
***** Warning: Loss has increased *****
Loss at iteration [152]: 0.214844930147176
Loss at iteration [153]: 0.21482477785954868
Loss at iteration [154]: 0.21480491494814086
Loss at iteration [155]: 0.21479726116412204
Loss at iteration [156]: 0.214797020630998
Loss at iteration [157]: 0.21480761665875375
***** Warning: Loss has increased *****
Loss at iteration [158]: 0.21482831736803626
***** Warning: Loss has increased *****
Loss at iteration [159]: 0.2148385053458059
***** Warning: Loss has increased *****
Loss at iteration [160]: 0.214865191646645
***** Warning: Loss has increased *****
Loss at iteration [161]: 0.21483576033272736
Loss at iteration [162]: 0.21483690998233354
***** Warning: Loss has increased *****
Loss at iteration [163]: 0.21479623535400372
Loss at iteration [164]: 0.21478471209009609
Loss at iteration [165]: 0.2147701288931341
Loss at iteration [166]: 0.2147637805201368
Loss at iteration [167]: 0.2147628352263708
Loss at iteration [168]: 0.21477182420607938
***** Warning: Loss has increased *****
Loss at iteration [169]: 0.21479173241875937
***** Warning: Loss has increased *****
Loss at iteration [170]: 0.21480385647031855
***** Warning: Loss has increased *****
Loss at iteration [171]: 0.21483758756906232
***** Warning: Loss has increased *****
Loss at iteration [172]: 0.21480756338683984
Loss at iteration [173]: 0.21480477254196012
Loss at iteration [174]: 0.2147654522145388
Loss at iteration [175]: 0.21475452852100244
Loss at iteration [176]: 0.21474334690470098
Loss at iteration [177]: 0.21473907965904088
Loss at iteration [178]: 0.21474221971701857
***** Warning: Loss has increased *****
Loss at iteration [179]: 0.21474912709093885
***** Warning: Loss has increased *****
Loss at iteration [180]: 0.21476438861158467
***** Warning: Loss has increased *****
Loss at iteration [181]: 0.21476336789571365
Loss at iteration [182]: 0.21477386701735554
***** Warning: Loss has increased *****
Loss at iteration [183]: 0.21475344174624217
Loss at iteration [184]: 0.21475000502514444
Loss at iteration [185]: 0.21473351853251332
Loss at iteration [186]: 0.21473017694246901
Loss at iteration [187]: 0.21472342999606858
Loss at iteration [188]: 0.21472097202878934
Loss at iteration [189]: 0.21471970397779636
Loss at iteration [190]: 0.21472142203471087
***** Warning: Loss has increased *****
Loss at iteration [191]: 0.21472510199797226
***** Warning: Loss has increased *****
Loss at iteration [192]: 0.21473416902743472
***** Warning: Loss has increased *****
Loss at iteration [193]: 0.21473184509180965
Loss at iteration [194]: 0.21473807961528707
***** Warning: Loss has increased *****
Loss at iteration [195]: 0.21472535542017454
Loss at iteration [196]: 0.21472336353522004
Loss at iteration [197]: 0.21471305264841722
Loss at iteration [198]: 0.2147100401419149
Loss at iteration [199]: 0.2147061697317686
Loss at iteration [200]: 0.21470488968299314
Loss at iteration [201]: 0.21470451585514128
Loss at iteration [202]: 0.21470648926979619
***** Warning: Loss has increased *****
Loss at iteration [203]: 0.21470835286964
***** Warning: Loss has increased *****
Loss at iteration [204]: 0.21471396419873448
***** Warning: Loss has increased *****
Loss at iteration [205]: 0.2147104538432984
Loss at iteration [206]: 0.21471237330296372
***** Warning: Loss has increased *****
Loss at iteration [207]: 0.21470412308680442
Loss at iteration [208]: 0.21470266178210515
Loss at iteration [209]: 0.21469705133457678
Loss at iteration [210]: 0.21469509827000055
Loss at iteration [211]: 0.21469295872369648
Loss at iteration [212]: 0.21469260972645512
Loss at iteration [213]: 0.21469276722657726
***** Warning: Loss has increased *****
Loss at iteration [214]: 0.2146948827861128
***** Warning: Loss has increased *****
Loss at iteration [215]: 0.21469518050792605
***** Warning: Loss has increased *****
Loss at iteration [216]: 0.2146984621093966
***** Warning: Loss has increased *****
Loss at iteration [217]: 0.21469476505131918
Loss at iteration [218]: 0.21469539830190548
***** Warning: Loss has increased *****
Loss at iteration [219]: 0.21468995550500086
Loss at iteration [220]: 0.2146884824291758
Loss at iteration [221]: 0.21468511544082886
Loss at iteration [222]: 0.21468352927185352
Loss at iteration [223]: 0.21468234788002724
Loss at iteration [224]: 0.2146821343099497
Loss at iteration [225]: 0.21468242275341815
***** Warning: Loss has increased *****
Loss at iteration [226]: 0.21468404705574057
***** Warning: Loss has increased *****
Loss at iteration [227]: 0.21468420735270216
***** Warning: Loss has increased *****
Loss at iteration [228]: 0.21468676179026008
***** Warning: Loss has increased *****
Loss at iteration [229]: 0.21468367046186657
Loss at iteration [230]: 0.21468384068855617
***** Warning: Loss has increased *****
Loss at iteration [231]: 0.21467963972236873
Loss at iteration [232]: 0.21467838483294627
Loss at iteration [233]: 0.21467600584894733
Loss at iteration [234]: 0.21467466540256272
Loss at iteration [235]: 0.21467378862626754
Loss at iteration [236]: 0.21467341856046737
Loss at iteration [237]: 0.21467354978662512
***** Warning: Loss has increased *****
Loss at iteration [238]: 0.2146745388302181
***** Warning: Loss has increased *****
Loss at iteration [239]: 0.21467488542266228
***** Warning: Loss has increased *****
Loss at iteration [240]: 0.21467698660651247
***** Warning: Loss has increased *****
Loss at iteration [241]: 0.21467473611815044
Loss at iteration [242]: 0.21467486227300722
***** Warning: Loss has increased *****
Loss at iteration [243]: 0.21467141403993514
Loss at iteration [244]: 0.21467016296717864
Loss at iteration [245]: 0.2146683163534182
Loss at iteration [246]: 0.21466728369289179
Loss at iteration [247]: 0.21466664817424871
Loss at iteration [248]: 0.21466625403179432
Loss at iteration [249]: 0.21466632179734407
***** Warning: Loss has increased *****
Loss at iteration [250]: 0.21466695663119292
***** Warning: Loss has increased *****
Loss at iteration [251]: 0.21466722809071484
***** Warning: Loss has increased *****
Loss at iteration [252]: 0.2146685524844373
***** Warning: Loss has increased *****
Loss at iteration [253]: 0.2146668752957275
Loss at iteration [254]: 0.2146669482975824
***** Warning: Loss has increased *****
Loss at iteration [255]: 0.2146643631630239
Loss at iteration [256]: 0.21466330318631216
Loss at iteration [257]: 0.21466191701043802
Loss at iteration [258]: 0.21466110175164635
Loss at iteration [259]: 0.21466054629488657
Loss at iteration [260]: 0.21466009287926507
Loss at iteration [261]: 0.2146597034853103
Loss at iteration [262]: 0.21465943038867943
Loss at iteration [263]: 0.21465957597708893
***** Warning: Loss has increased *****
Loss at iteration [264]: 0.21466035949946638
***** Warning: Loss has increased *****
Loss at iteration [265]: 0.2146598364903464
Loss at iteration [266]: 0.2146603832087912
***** Warning: Loss has increased *****
Loss at iteration [267]: 0.21465872879996115
Loss at iteration [268]: 0.21465806042973065
Loss at iteration [269]: 0.2146565733866713
Loss at iteration [270]: 0.21465575867871284
Loss at iteration [271]: 0.21465515210680236
Loss at iteration [272]: 0.21465477595975285
Loss at iteration [273]: 0.21465467261546356
Loss at iteration [274]: 0.21465482138970035
***** Warning: Loss has increased *****
Loss at iteration [275]: 0.214655552599526
***** Warning: Loss has increased *****
Loss at iteration [276]: 0.21465500999785148
Loss at iteration [277]: 0.21465524538396405
***** Warning: Loss has increased *****
Loss at iteration [278]: 0.21465363029476647
Loss at iteration [279]: 0.21465294373016186
Loss at iteration [280]: 0.21465184021201822
Loss at iteration [281]: 0.21465119374603134
Loss at iteration [282]: 0.21465078781952374
Loss at iteration [283]: 0.21465052666212645
Loss at iteration [284]: 0.21465045851450681
Loss at iteration [285]: 0.21465043413908738
Loss at iteration [286]: 0.21465081581205447
***** Warning: Loss has increased *****
Loss at iteration [287]: 0.21465022128735817
Loss at iteration [288]: 0.21465016917712287
Loss at iteration [289]: 0.21464899423059516
Loss at iteration [290]: 0.21464836451324884
Loss at iteration [291]: 0.21464766337245167
Loss at iteration [292]: 0.2146472072599632
Loss at iteration [293]: 0.21464684491051572
Loss at iteration [294]: 0.21464650658491136
Loss at iteration [295]: 0.21464619033950352
Loss at iteration [296]: 0.2146459447285593
Loss at iteration [297]: 0.21464581612475872
Loss at iteration [298]: 0.21464570452028303
Loss at iteration [299]: 0.21464593299205523
***** Warning: Loss has increased *****
Loss at iteration [300]: 0.21464545658687528
Loss at iteration [301]: 0.214645408048721
Loss at iteration [302]: 0.21464453826612237
Loss at iteration [303]: 0.21464396352387716
Loss at iteration [304]: 0.2146434227466846
Loss at iteration [305]: 0.2146430630325787
Loss at iteration [306]: 0.21464275340186392
Loss at iteration [307]: 0.21464249741398927
Loss at iteration [308]: 0.2146423233493615
Loss at iteration [309]: 0.2146421632730997
Loss at iteration [310]: 0.21464216129590688
Loss at iteration [311]: 0.2146418035566696
Loss at iteration [312]: 0.21464172154626854
Loss at iteration [313]: 0.2146411081551763
Loss at iteration [314]: 0.21464067732880535
Loss at iteration [315]: 0.21464022443879938
Loss at iteration [316]: 0.2146398916071552
Loss at iteration [317]: 0.21463960668275117
Loss at iteration [318]: 0.2146393416522561
Loss at iteration [319]: 0.21463909045274396
Loss at iteration [320]: 0.2146388776358023
Loss at iteration [321]: 0.21463873237145684
Loss at iteration [322]: 0.21463853303525188
Loss at iteration [323]: 0.21463847465455807
Loss at iteration [324]: 0.2146381281847919
Loss at iteration [325]: 0.21463789876250586
Loss at iteration [326]: 0.21463742641486808
Loss at iteration [327]: 0.21463706858956696
Loss at iteration [328]: 0.21463676231796175
Loss at iteration [329]: 0.21463650582004548
Loss at iteration [330]: 0.21463627480017083
Loss at iteration [331]: 0.21463606318485087
Loss at iteration [332]: 0.21463590272812993
Loss at iteration [333]: 0.21463569664672622
Loss at iteration [334]: 0.2146355525008037
Loss at iteration [335]: 0.2146352647161569
Loss at iteration [336]: 0.21463504125592583
Loss at iteration [337]: 0.21463469291484522
Loss at iteration [338]: 0.21463439920190924
Loss at iteration [339]: 0.21463413208230223
Loss at iteration [340]: 0.21463389353190027
Loss at iteration [341]: 0.21463366547160412
Loss at iteration [342]: 0.2146334406673283
Loss at iteration [343]: 0.21463321703410398
Loss at iteration [344]: 0.21463299485890244
Loss at iteration [345]: 0.21463277650101084
Loss at iteration [346]: 0.21463256289897653
Loss at iteration [347]: 0.2146323760860443
Loss at iteration [348]: 0.21463219538507566
Loss at iteration [349]: 0.21463206581280656
Loss at iteration [350]: 0.21463183498515
Loss at iteration [351]: 0.21463164726963557
Loss at iteration [352]: 0.21463136719933507
Loss at iteration [353]: 0.21463112366465834
Loss at iteration [354]: 0.2146308834051834
Loss at iteration [355]: 0.21463066617922857
Loss at iteration [356]: 0.21463045860161037
Loss at iteration [357]: 0.2146302562660909
Loss at iteration [358]: 0.21463005960802323
Loss at iteration [359]: 0.21462987526504432
Loss at iteration [360]: 0.21462970193623068
Loss at iteration [361]: 0.2146295148916116
Loss at iteration [362]: 0.2146293464985683
Loss at iteration [363]: 0.21462913655859395
Loss at iteration [364]: 0.21462895548972302
Loss at iteration [365]: 0.21462872732320154
Loss at iteration [366]: 0.21462851015952342
Loss at iteration [367]: 0.21462830310063682
Loss at iteration [368]: 0.21462810493855622
Loss at iteration [369]: 0.21462791298504177
Loss at iteration [370]: 0.2146277241272759
Loss at iteration [371]: 0.21462754053699556
Loss at iteration [372]: 0.21462736305455615
Loss at iteration [373]: 0.21462719679961056
Loss at iteration [374]: 0.21462703435238653
Loss at iteration [375]: 0.21462687239772552
Loss at iteration [376]: 0.21462668608441696
Loss at iteration [377]: 0.21462651345522896
Loss at iteration [378]: 0.21462630590035342
Loss at iteration [379]: 0.21462611872007054
Loss at iteration [380]: 0.21462592677622866
Loss at iteration [381]: 0.21462574469341408
Loss at iteration [382]: 0.2146255699931511
Loss at iteration [383]: 0.2146253964301477
Loss at iteration [384]: 0.21462522819196905
Loss at iteration [385]: 0.2146250629841831
Loss at iteration [386]: 0.21462490746201987
Loss at iteration [387]: 0.21462474518696562
Loss at iteration [388]: 0.21462458744907453
Loss at iteration [389]: 0.21462441687527709
Loss at iteration [390]: 0.21462426348616995
Loss at iteration [391]: 0.2146240733016077
Loss at iteration [392]: 0.2146238924895495
Loss at iteration [393]: 0.21462371654565027
Loss at iteration [394]: 0.2146235496439799
Loss at iteration [395]: 0.21462338460651326
Loss at iteration [396]: 0.21462322145448515
Loss at iteration [397]: 0.2146230611870187
Loss at iteration [398]: 0.21462290212782348
Loss at iteration [399]: 0.21462274762037734
Loss at iteration [400]: 0.21462259851032767
Loss at iteration [401]: 0.21462245862810897
Loss at iteration [402]: 0.21462232607974366
Loss at iteration [403]: 0.21462216158954786
Loss at iteration [404]: 0.21462200231534573
Loss at iteration [405]: 0.21462182137824362
Loss at iteration [406]: 0.21462165238523098
Loss at iteration [407]: 0.21462149204049963
Loss at iteration [408]: 0.2146213347914322
Loss at iteration [409]: 0.21462118239218414
Loss at iteration [410]: 0.21462103385328216
Loss at iteration [411]: 0.21462088808360275
Loss at iteration [412]: 0.21462074390818017
Loss at iteration [413]: 0.2146206030552017
Loss at iteration [414]: 0.21462045994166878
Loss at iteration [415]: 0.2146203204243817
Loss at iteration [416]: 0.21462017380006898
Loss at iteration [417]: 0.2146200283720292
Loss at iteration [418]: 0.21461987526388887
Loss at iteration [419]: 0.21461972189920436
Loss at iteration [420]: 0.2146195753545924
Loss at iteration [421]: 0.21461943191833846
Loss at iteration [422]: 0.2146192911341132
Loss at iteration [423]: 0.21461914954675193
Loss at iteration [424]: 0.21461900979668302
Loss at iteration [425]: 0.2146188757536409
Loss at iteration [426]: 0.21461874128172378
Loss at iteration [427]: 0.2146186143399301
Loss at iteration [428]: 0.21461848502303574
Loss at iteration [429]: 0.21461835961958442
Loss at iteration [430]: 0.21461822029269964
Loss at iteration [431]: 0.21461809352646685
Loss at iteration [432]: 0.21461794579905863
Loss at iteration [433]: 0.21461780430719848
Loss at iteration [434]: 0.2146176608318379
Loss at iteration [435]: 0.21461752136405415
Loss at iteration [436]: 0.21461738681945533
Loss at iteration [437]: 0.21461725392901584
Loss at iteration [438]: 0.21461712227063115
Loss at iteration [439]: 0.2146169888942746
Loss at iteration [440]: 0.21461686049921241
Loss at iteration [441]: 0.21461673240764823
Loss at iteration [442]: 0.2146166114243234
Loss at iteration [443]: 0.21461649081805495
Loss at iteration [444]: 0.21461637483491158
Loss at iteration [445]: 0.2146162481815388
Loss at iteration [446]: 0.21461613283936382
Loss at iteration [447]: 0.2146159961135179
Loss at iteration [448]: 0.21461586550265735
Loss at iteration [449]: 0.21461572698394146
Loss at iteration [450]: 0.21461559100808633
Loss at iteration [451]: 0.214615462101524
Loss at iteration [452]: 0.21461533520294185
Loss at iteration [453]: 0.21461521010958443
Loss at iteration [454]: 0.21461508708035912
Loss at iteration [455]: 0.2146149631003567
Loss at iteration [456]: 0.21461484285589227
Loss at iteration [457]: 0.2146147240949399
Loss at iteration [458]: 0.21461460942383723
Loss at iteration [459]: 0.21461449892418716
Loss at iteration [460]: 0.21461438542986924
Loss at iteration [461]: 0.2146142752165333
Loss at iteration [462]: 0.21461414831639664
Loss at iteration [463]: 0.21461402888803152
Loss at iteration [464]: 0.21461389466399794
Loss at iteration [465]: 0.21461377150146266
Loss at iteration [466]: 0.2146136464564057
Loss at iteration [467]: 0.214613521835772
Loss at iteration [468]: 0.21461340213884253
Loss at iteration [469]: 0.2146132822721158
Loss at iteration [470]: 0.2146131627838924
Loss at iteration [471]: 0.21461304388017569
Loss at iteration [472]: 0.21461292831136064
Loss at iteration [473]: 0.2146128156386035
Loss at iteration [474]: 0.21461270362912
Loss at iteration [475]: 0.21461259693668353
Loss at iteration [476]: 0.21461249551322267
Loss at iteration [477]: 0.21461238560808468
Loss at iteration [478]: 0.21461228739958454
Loss at iteration [479]: 0.21461216655509802
Loss at iteration [480]: 0.21461205316776663
Loss at iteration [481]: 0.2146119298715253
Loss at iteration [482]: 0.2146118088662736
Loss at iteration [483]: 0.21461168979786527
Loss at iteration [484]: 0.214611578678875
Loss at iteration [485]: 0.2146114644997192
Loss at iteration [486]: 0.2146113547062532
Loss at iteration [487]: 0.21461124510641005
Loss at iteration [488]: 0.2146111342507167
Loss at iteration [489]: 0.21461102862554174
Loss at iteration [490]: 0.21461092619594588
Loss at iteration [491]: 0.2146108324575704
Loss at iteration [492]: 0.21461073616650214
Loss at iteration [493]: 0.21461064224307871
Loss at iteration [494]: 0.21461053146851117
Loss at iteration [495]: 0.2146104236475328
Loss at iteration [496]: 0.2146103023764648
Loss at iteration [497]: 0.2146101842424436
Loss at iteration [498]: 0.2146100683503985
Loss at iteration [499]: 0.21460995842057656
Loss at iteration [500]: 0.21460985357032014
Loss at iteration [501]: 0.21460974851658954
Loss at iteration [502]: 0.21460964525588322
Loss at iteration [503]: 0.21460954763705803
Loss at iteration [504]: 0.21460945355635497
Loss at iteration [505]: 0.2146093608836344
Loss at iteration [506]: 0.2146092772464272
Loss at iteration [507]: 0.2146091729672358
Loss at iteration [508]: 0.21460907669644133
Loss at iteration [509]: 0.21460896322040454
Loss at iteration [510]: 0.2146088680275013
Loss at iteration [511]: 0.21460875690896486
Loss at iteration [512]: 0.2146086490252257
Loss at iteration [513]: 0.2146085421490877
Loss at iteration [514]: 0.21460844042825444
Loss at iteration [515]: 0.21460834213708285
Loss at iteration [516]: 0.21460824651932317
Loss at iteration [517]: 0.2146081511557232
Loss at iteration [518]: 0.21460805863093216
Loss at iteration [519]: 0.21460796858447578
Loss at iteration [520]: 0.21460787924614844
Loss at iteration [521]: 0.21460778526176094
Loss at iteration [522]: 0.2146076839988133
Loss at iteration [523]: 0.2146075865458001
Loss at iteration [524]: 0.2146074807629329
Loss at iteration [525]: 0.21460737737007218
Loss at iteration [526]: 0.21460727625954293
Loss at iteration [527]: 0.214607180115546
Loss at iteration [528]: 0.21460708317022928
Loss at iteration [529]: 0.21460698839588616
Loss at iteration [530]: 0.21460689687650258
Loss at iteration [531]: 0.21460680680187935
Loss at iteration [532]: 0.21460672483815332
Loss at iteration [533]: 0.2146066458047665
Loss at iteration [534]: 0.21460656028779246
Loss at iteration [535]: 0.21460646826474292
Loss at iteration [536]: 0.21460636217971113
Loss at iteration [537]: 0.21460626748816541
Loss at iteration [538]: 0.2146061643978316
Loss at iteration [539]: 0.21460606277465089
Loss at iteration [540]: 0.21460596703199447
Loss at iteration [541]: 0.21460587128792497
Loss at iteration [542]: 0.21460577833846733
Loss at iteration [543]: 0.2146056889712004
Loss at iteration [544]: 0.21460560183111568
Loss at iteration [545]: 0.2146055153904661
Loss at iteration [546]: 0.21460543605136354
Loss at iteration [547]: 0.21460535548231136
Loss at iteration [548]: 0.21460526911486916
Loss at iteration [549]: 0.2146051847085357
Loss at iteration [550]: 0.2146050902716267
Loss at iteration [551]: 0.21460500252015918
Loss at iteration [552]: 0.21460489589984533
Loss at iteration [553]: 0.21460479136113425
Loss at iteration [554]: 0.2146046949781175
Loss at iteration [555]: 0.21460460257000422
Loss at iteration [556]: 0.21460451114359647
Loss at iteration [557]: 0.2146044217196875
Loss at iteration [558]: 0.21460433458310302
Loss at iteration [559]: 0.21460424729679325
Loss at iteration [560]: 0.2146041675532457
Loss at iteration [561]: 0.2146040906154576
Loss at iteration [562]: 0.21460401030848544
Loss at iteration [563]: 0.21460393472351263
Loss at iteration [564]: 0.21460384152179415
Loss at iteration [565]: 0.21460375375799767
Loss at iteration [566]: 0.21460365339604126
Loss at iteration [567]: 0.21460354722381098
Loss at iteration [568]: 0.2146034497780472
Loss at iteration [569]: 0.21460335512440082
Loss at iteration [570]: 0.21460326423485698
Loss at iteration [571]: 0.21460317529182107
Loss at iteration [572]: 0.2146030862993616
Loss at iteration [573]: 0.21460299920898288
Loss at iteration [574]: 0.21460291606451662
Loss at iteration [575]: 0.21460284236336435
Loss at iteration [576]: 0.21460277709274078
Loss at iteration [577]: 0.21460269917487704
Loss at iteration [578]: 0.21460262710183442
Loss at iteration [579]: 0.21460252655588008
Loss at iteration [580]: 0.2146024365856113
Loss at iteration [581]: 0.21460234250758692
Loss at iteration [582]: 0.21460224471130893
Loss at iteration [583]: 0.21460214923236157
Loss at iteration [584]: 0.21460206150680472
Loss at iteration [585]: 0.21460197720682023
Loss at iteration [586]: 0.21460189203027202
Loss at iteration [587]: 0.2146018103534224
Loss at iteration [588]: 0.21460173179744862
Loss at iteration [589]: 0.21460165826804745
Loss at iteration [590]: 0.21460159108770224
Loss at iteration [591]: 0.214601523806213
Loss at iteration [592]: 0.21460144425584654
Loss at iteration [593]: 0.21460136516328063
Loss at iteration [594]: 0.2146012704951716
Loss at iteration [595]: 0.2146011777492917
Loss at iteration [596]: 0.21460108593334384
Loss at iteration [597]: 0.21460099612976885
Loss at iteration [598]: 0.21460090526167963
Loss at iteration [599]: 0.21460081881318172
Loss at iteration [600]: 0.21460073607485422
Loss at iteration [601]: 0.21460065362840727
Loss at iteration [602]: 0.2146005754927105
Loss at iteration [603]: 0.21460049862761726
Loss at iteration [604]: 0.2146004314912668
Loss at iteration [605]: 0.214600370357715
Loss at iteration [606]: 0.21460029713713838
Loss at iteration [607]: 0.21460022980749774
Loss at iteration [608]: 0.2146001400722853
Loss at iteration [609]: 0.21460005573585114
Loss at iteration [610]: 0.21459995835403994
Loss at iteration [611]: 0.2145998622381747
Loss at iteration [612]: 0.21459976965849026
Loss at iteration [613]: 0.21459968528421844
Loss at iteration [614]: 0.21459960120402632
Loss at iteration [615]: 0.21459952021169001
Loss at iteration [616]: 0.21459944085030783
Loss at iteration [617]: 0.2145993677574998
Loss at iteration [618]: 0.2145992953308197
Loss at iteration [619]: 0.2145992397150582
Loss at iteration [620]: 0.21459917251263028
Loss at iteration [621]: 0.2145991084295144
Loss at iteration [622]: 0.2145990220911629
Loss at iteration [623]: 0.21459893999218338
Loss at iteration [624]: 0.21459883931982812
Loss at iteration [625]: 0.21459874495277165
Loss at iteration [626]: 0.21459865669766864
Loss at iteration [627]: 0.21459857020572062
Loss at iteration [628]: 0.21459848936565087
Loss at iteration [629]: 0.2145984077198962
Loss at iteration [630]: 0.21459832723490938
Loss at iteration [631]: 0.21459824970418867
Loss at iteration [632]: 0.21459818046316806
Loss at iteration [633]: 0.21459812092720412
Loss at iteration [634]: 0.21459805981166477
Loss at iteration [635]: 0.21459800325708972
Loss at iteration [636]: 0.2145979127805718
Loss at iteration [637]: 0.2145978318064014
Loss at iteration [638]: 0.21459774186991146
Loss at iteration [639]: 0.21459765239192732
Loss at iteration [640]: 0.21459756042123382
Loss at iteration [641]: 0.21459747210840513
Loss at iteration [642]: 0.21459738872400366
Loss at iteration [643]: 0.21459730746904557
Loss at iteration [644]: 0.2145972288242889
Loss at iteration [645]: 0.21459715223196515
Loss at iteration [646]: 0.21459707738942732
Loss at iteration [647]: 0.21459701266289025
Loss at iteration [648]: 0.214596953147918
Loss at iteration [649]: 0.2145968932749853
Loss at iteration [650]: 0.2145968332064484
Loss at iteration [651]: 0.21459674827488165
Loss at iteration [652]: 0.21459667175198363
Loss at iteration [653]: 0.21459657904648397
Loss at iteration [654]: 0.21459648508781037
Loss at iteration [655]: 0.21459639604218433
Loss at iteration [656]: 0.21459631003187526
Loss at iteration [657]: 0.2145962296338649
Loss at iteration [658]: 0.21459615053082323
Loss at iteration [659]: 0.2145960719900722
Loss at iteration [660]: 0.2145959959669928
Loss at iteration [661]: 0.21459592249212925
Loss at iteration [662]: 0.21459585334069867
Loss at iteration [663]: 0.21459579741865117
Loss at iteration [664]: 0.21459573925273892
Loss at iteration [665]: 0.21459568451445682
Loss at iteration [666]: 0.2145956005060765
Loss at iteration [667]: 0.21459553097265505
Loss at iteration [668]: 0.21459543510811144
Loss at iteration [669]: 0.2145953553607919
Loss at iteration [670]: 0.2145952641700996
Loss at iteration [671]: 0.21459517514295387
Loss at iteration [672]: 0.21459509027079698
Loss at iteration [673]: 0.21459501182342486
Loss at iteration [674]: 0.21459493632532947
Loss at iteration [675]: 0.21459486197764557
Loss at iteration [676]: 0.2145947954481208
Loss at iteration [677]: 0.21459473754040512
Loss at iteration [678]: 0.21459468385109476
Loss at iteration [679]: 0.21459463038884943
Loss at iteration [680]: 0.21459454889595084
Loss at iteration [681]: 0.2145944705168939
Loss at iteration [682]: 0.21459437555084532
Loss at iteration [683]: 0.21459428554093496
Loss at iteration [684]: 0.21459419976151312
Loss at iteration [685]: 0.21459411891588637
Loss at iteration [686]: 0.21459404568488
Loss at iteration [687]: 0.21459397241301248
Loss at iteration [688]: 0.21459390048996024
Loss at iteration [689]: 0.21459383342110117
Loss at iteration [690]: 0.2145937794310257
Loss at iteration [691]: 0.21459373778845106
Loss at iteration [692]: 0.21459368218790248
Loss at iteration [693]: 0.2145936206607888
Loss at iteration [694]: 0.21459353845314894
Loss at iteration [695]: 0.21459345721066903
Loss at iteration [696]: 0.21459336820680303
Loss at iteration [697]: 0.21459328218988752
Loss at iteration [698]: 0.21459319764431403
Loss at iteration [699]: 0.21459312171019831
Loss at iteration [700]: 0.2145930468151709
Loss at iteration [701]: 0.2145929768273492
Loss at iteration [702]: 0.2145929054914608
Loss at iteration [703]: 0.21459283765156884
Loss at iteration [704]: 0.2145927769915123
Loss at iteration [705]: 0.21459273009970756
Loss at iteration [706]: 0.21459268517271876
Loss at iteration [707]: 0.21459263683925694
Loss at iteration [708]: 0.2145925497169254
Loss at iteration [709]: 0.2145924741438669
Loss at iteration [710]: 0.21459238608468292
Loss at iteration [711]: 0.21459230223352926
Loss at iteration [712]: 0.21459222377983567
Loss at iteration [713]: 0.21459214424091208
Loss at iteration [714]: 0.21459206862044417
Loss at iteration [715]: 0.21459199825572936
Loss at iteration [716]: 0.2145919303616661
Loss at iteration [717]: 0.21459186360089147
Loss at iteration [718]: 0.2145918056183576
Loss at iteration [719]: 0.21459175706194838
Loss at iteration [720]: 0.21459170575748993
Loss at iteration [721]: 0.21459165829674134
Loss at iteration [722]: 0.21459158103667575
Loss at iteration [723]: 0.21459150930348764
Loss at iteration [724]: 0.21459142837714312
Loss at iteration [725]: 0.2145913501444913
Loss at iteration [726]: 0.21459126868451076
Loss at iteration [727]: 0.2145911897651553
Loss at iteration [728]: 0.21459111677781945
Loss at iteration [729]: 0.21459104851269642
Loss at iteration [730]: 0.21459098510548216
Loss at iteration [731]: 0.21459092245295847
Loss at iteration [732]: 0.21459086943716163
Loss at iteration [733]: 0.2145908180517839
Loss at iteration [734]: 0.21459077726479886
Loss at iteration [735]: 0.2145907283756754
Loss at iteration [736]: 0.21459067302472074
Loss at iteration [737]: 0.21459059038227826
Loss at iteration [738]: 0.21459050982439828
Loss at iteration [739]: 0.21459043384138834
Loss at iteration [740]: 0.2145903587986477
Loss at iteration [741]: 0.214590284205295
Loss at iteration [742]: 0.2145902151936732
Loss at iteration [743]: 0.2145901477033689
Loss at iteration [744]: 0.21459008129280083
Loss at iteration [745]: 0.21459001620090423
Loss at iteration [746]: 0.21458995903213707
Loss at iteration [747]: 0.21458990704842584
Loss at iteration [748]: 0.2145898636224805
Loss at iteration [749]: 0.21458980789542187
Loss at iteration [750]: 0.21458975639326242
Loss at iteration [751]: 0.21458969382254614
Loss at iteration [752]: 0.21458963880267196
Loss at iteration [753]: 0.21458956554505462
Loss at iteration [754]: 0.21458948991674243
Loss at iteration [755]: 0.21458940128785445
Loss at iteration [756]: 0.21458932433112476
Loss at iteration [757]: 0.21458925373935786
Loss at iteration [758]: 0.21458918991376577
Loss at iteration [759]: 0.21458912461750737
Loss at iteration [760]: 0.2145890599022537
Loss at iteration [761]: 0.21458900039749831
Loss at iteration [762]: 0.21458894733197897
Loss at iteration [763]: 0.2145889043192923
Loss at iteration [764]: 0.21458886718753975
Loss at iteration [765]: 0.21458881943117783
Loss at iteration [766]: 0.21458876605408977
Loss at iteration [767]: 0.21458867990722005
Loss at iteration [768]: 0.2145886113118555
Loss at iteration [769]: 0.21458852793814873
Loss at iteration [770]: 0.2145884475776305
Loss at iteration [771]: 0.21458837779316373
Loss at iteration [772]: 0.2145883088905133
Loss at iteration [773]: 0.2145882443216833
Loss at iteration [774]: 0.2145881817056652
Loss at iteration [775]: 0.21458811978457124
Loss at iteration [776]: 0.21458806397115365
Loss at iteration [777]: 0.2145880206754023
Loss at iteration [778]: 0.2145879844369203
Loss at iteration [779]: 0.21458792791313835
Loss at iteration [780]: 0.21458787359008796
Loss at iteration [781]: 0.21458780528825994
Loss at iteration [782]: 0.21458773397071682
Loss at iteration [783]: 0.21458765768003535
Loss at iteration [784]: 0.21458758452097992
Loss at iteration [785]: 0.21458750807178026
Loss at iteration [786]: 0.2145874362535743
Loss at iteration [787]: 0.21458736919730348
Loss at iteration [788]: 0.21458730781121485
Loss at iteration [789]: 0.21458724271430993
Loss at iteration [790]: 0.2145871814849361
Loss at iteration [791]: 0.21458712069457248
Loss at iteration [792]: 0.21458706847394318
Loss at iteration [793]: 0.2145870282871338
Loss at iteration [794]: 0.21458699613190607
Loss at iteration [795]: 0.21458694407098072
Loss at iteration [796]: 0.21458688945681387
Loss at iteration [797]: 0.21458680867431923
Loss at iteration [798]: 0.2145867342644727
Loss at iteration [799]: 0.2145866595821361
Loss at iteration [800]: 0.21458659290112955
Loss at iteration [801]: 0.21458651708121548
Loss at iteration [802]: 0.2145864485757459
Loss at iteration [803]: 0.2145863868426344
Loss at iteration [804]: 0.21458632454403195
Loss at iteration [805]: 0.21458626137029116
Loss at iteration [806]: 0.2145862059046149
Loss at iteration [807]: 0.21458616101690722
Loss at iteration [808]: 0.2145861291087664
Loss at iteration [809]: 0.2145860801218243
Loss at iteration [810]: 0.2145860331309301
Loss at iteration [811]: 0.21458595998666233
Loss at iteration [812]: 0.21458588904643844
Loss at iteration [813]: 0.21458581523515394
Loss at iteration [814]: 0.2145857370572286
Loss at iteration [815]: 0.21458565889381973
Loss at iteration [816]: 0.21458558853168996
Loss at iteration [817]: 0.21458552497334885
Loss at iteration [818]: 0.21458546473156953
Loss at iteration [819]: 0.2145854041808516
Loss at iteration [820]: 0.21458534768974047
Loss at iteration [821]: 0.21458530084828284
Loss at iteration [822]: 0.21458526473649328
Loss at iteration [823]: 0.21458523300667692
Loss at iteration [824]: 0.21458518828669343
Loss at iteration [825]: 0.21458514138215565
Loss at iteration [826]: 0.21458506364298316
Loss at iteration [827]: 0.21458498542388513
Loss at iteration [828]: 0.2145849026097879
Loss at iteration [829]: 0.2145848341498402
Loss at iteration [830]: 0.21458476592219317
Loss at iteration [831]: 0.21458470199989904
Loss at iteration [832]: 0.2145846407148421
Loss at iteration [833]: 0.21458458324993146
Loss at iteration [834]: 0.21458452364582023
Loss at iteration [835]: 0.21458446829796674
Loss at iteration [836]: 0.2145844208436499
Loss at iteration [837]: 0.2145843824427417
Loss at iteration [838]: 0.2145843479813006
Loss at iteration [839]: 0.21458431627987182
Loss at iteration [840]: 0.21458426268019917
Loss at iteration [841]: 0.21458421114115442
Loss at iteration [842]: 0.21458412747948646
Loss at iteration [843]: 0.2145840527389572
Loss at iteration [844]: 0.21458397677844704
Loss at iteration [845]: 0.21458390514822911
Loss at iteration [846]: 0.21458383896559194
Loss at iteration [847]: 0.2145837742419671
Loss at iteration [848]: 0.21458371447401675
Loss at iteration [849]: 0.21458365506997454
Loss at iteration [850]: 0.2145835976949126
Loss at iteration [851]: 0.21458354086183398
Loss at iteration [852]: 0.21458348355821655
Loss at iteration [853]: 0.21458343038091068
Loss at iteration [854]: 0.21458338486380388
Loss at iteration [855]: 0.21458334756525205
Loss at iteration [856]: 0.21458331095114516
Loss at iteration [857]: 0.21458327922182868
Loss at iteration [858]: 0.21458322820105166
Loss at iteration [859]: 0.214583173180079
Loss at iteration [860]: 0.21458309279049242
Loss at iteration [861]: 0.21458301967818838
Loss at iteration [862]: 0.21458293594223365
Loss at iteration [863]: 0.21458286301217885
Loss at iteration [864]: 0.21458280115592285
Loss at iteration [865]: 0.214582743041727
Loss at iteration [866]: 0.2145826860102202
Loss at iteration [867]: 0.21458262942614292
Loss at iteration [868]: 0.21458257343756829
Loss at iteration [869]: 0.21458251724242244
Loss at iteration [870]: 0.21458246398235037
Loss at iteration [871]: 0.21458242632655627
Loss at iteration [872]: 0.21458239915114596
Loss at iteration [873]: 0.21458236369491293
Loss at iteration [874]: 0.21458233073851313
Loss at iteration [875]: 0.2145822716455393
Loss at iteration [876]: 0.2145822085801929
Loss at iteration [877]: 0.21458212831416607
Loss at iteration [878]: 0.21458205329118074
Loss at iteration [879]: 0.2145819815165879
Loss at iteration [880]: 0.21458191488444792
Loss at iteration [881]: 0.21458185133530738
Loss at iteration [882]: 0.21458179155727783
Loss at iteration [883]: 0.21458173464504657
Loss at iteration [884]: 0.21458168053609789
Loss at iteration [885]: 0.21458163351088141
Loss at iteration [886]: 0.2145816004729787
Loss at iteration [887]: 0.21458158263060356
Loss at iteration [888]: 0.2145815429172066
Loss at iteration [889]: 0.21458149567957574
Loss at iteration [890]: 0.21458142234301966
Loss at iteration [891]: 0.21458135738304673
Loss at iteration [892]: 0.2145812778859803
Loss at iteration [893]: 0.21458120824501734
Loss at iteration [894]: 0.2145811376982728
Loss at iteration [895]: 0.21458107500023915
Loss at iteration [896]: 0.21458101835288962
Loss at iteration [897]: 0.21458096106651475
Loss at iteration [898]: 0.21458090976483393
Loss at iteration [899]: 0.2145808628271392
Loss at iteration [900]: 0.21458082248536095
Loss at iteration [901]: 0.214580791120172
Loss at iteration [902]: 0.21458076526654785
Loss at iteration [903]: 0.21458073142662643
Loss at iteration [904]: 0.2145806578654471
Loss at iteration [905]: 0.21458059821240408
Loss at iteration [906]: 0.21458051410059067
Loss at iteration [907]: 0.21458044374865673
Loss at iteration [908]: 0.21458037881711833
Loss at iteration [909]: 0.2145803144237427
Loss at iteration [910]: 0.21458025368674166
Loss at iteration [911]: 0.21458019557612332
Loss at iteration [912]: 0.21458013684982527
Loss at iteration [913]: 0.214580081576341
Loss at iteration [914]: 0.2145800262753287
Loss at iteration [915]: 0.2145799759679259
Loss at iteration [916]: 0.21457992757454528
Loss at iteration [917]: 0.2145798887258788
Loss at iteration [918]: 0.21457986305291735
Loss at iteration [919]: 0.21457982617301433
Loss at iteration [920]: 0.21457979283817544
Loss at iteration [921]: 0.2145797263743613
Loss at iteration [922]: 0.21457966035153636
Loss at iteration [923]: 0.21457958065208704
Loss at iteration [924]: 0.21457951493563102
Loss at iteration [925]: 0.21457945251207727
Loss at iteration [926]: 0.21457939426378123
Loss at iteration [927]: 0.21457933450611835
Loss at iteration [928]: 0.2145792751505262
Loss at iteration [929]: 0.21457921575134895
Loss at iteration [930]: 0.21457915773329375
Loss at iteration [931]: 0.21457910706950406
Loss at iteration [932]: 0.21457905672915029
Loss at iteration [933]: 0.21457901818124864
Loss at iteration [934]: 0.21457898979126888
Loss at iteration [935]: 0.21457896683153502
Loss at iteration [936]: 0.2145789460342979
Loss at iteration [937]: 0.21457888058789112
Loss at iteration [938]: 0.21457881826350683
Loss at iteration [939]: 0.21457874147540343
Loss at iteration [940]: 0.21457867470440434
Loss at iteration [941]: 0.21457860798752806
Loss at iteration [942]: 0.21457854400365312
Loss at iteration [943]: 0.21457848645916647
Loss at iteration [944]: 0.21457842986842499
Loss at iteration [945]: 0.21457837751401054
Loss at iteration [946]: 0.21457832607619678
Loss at iteration [947]: 0.21457828080610747
Loss at iteration [948]: 0.21457824635297634
Loss at iteration [949]: 0.21457822065197316
Loss at iteration [950]: 0.2145782050122308
Loss at iteration [951]: 0.21457816034456156
Loss at iteration [952]: 0.21457811971661084
Loss at iteration [953]: 0.2145780601973251
Loss at iteration [954]: 0.21457800274383748
Loss at iteration [955]: 0.21457792724780042
Loss at iteration [956]: 0.21457786229664016
Loss at iteration [957]: 0.21457779655399886
Loss at iteration [958]: 0.21457773510381264
Loss at iteration [959]: 0.21457768226845442
Loss at iteration [960]: 0.2145776299825697
Loss at iteration [961]: 0.2145775815684296
Loss at iteration [962]: 0.2145775337963827
Loss at iteration [963]: 0.2145774914119986
Loss at iteration [964]: 0.214577454068048
Loss at iteration [965]: 0.21457743133247006
Loss at iteration [966]: 0.21457740870884914
Loss at iteration [967]: 0.21457737041321173
Loss at iteration [968]: 0.21457733636811732
Loss at iteration [969]: 0.214577280953817
Loss at iteration [970]: 0.21457722589059766
Loss at iteration [971]: 0.21457715066692287
Loss at iteration [972]: 0.21457708574817616
Loss at iteration [973]: 0.2145770161606846
Loss at iteration [974]: 0.21457695510348884
Loss at iteration [975]: 0.2145769016085913
Loss at iteration [976]: 0.21457685264539925
Loss at iteration [977]: 0.21457680361035225
Loss at iteration [978]: 0.21457675418906225
Loss at iteration [979]: 0.2145767067071221
Loss at iteration [980]: 0.21457665943006982
Loss at iteration [981]: 0.21457661853059806
Loss at iteration [982]: 0.21457659764380596
Loss at iteration [983]: 0.21457658247845157
Loss at iteration [984]: 0.21457657056167143
Loss at iteration [985]: 0.21457655384686136
Loss at iteration [986]: 0.2145764817392708
Loss at iteration [987]: 0.21457642193894239
Loss at iteration [988]: 0.21457633801573706
Loss at iteration [989]: 0.21457627289801903
Loss at iteration [990]: 0.21457620457897478
Loss at iteration [991]: 0.21457614488076274
Loss at iteration [992]: 0.2145760890507336
Loss at iteration [993]: 0.2145760374635985
Loss at iteration [994]: 0.21457598954844034
Loss at iteration [995]: 0.21457594104932895
Loss at iteration [996]: 0.2145758952103313
Loss at iteration [997]: 0.21457585241409477
Loss at iteration [998]: 0.2145758205908846
Loss at iteration [999]: 0.21457580964167822
Loss at iteration [1000]: 0.21457579639169613
Loss at iteration [1001]: 0.2145757572456348
Loss at iteration [1002]: 0.214575715016316
Loss at iteration [1003]: 0.21457565524310784
Loss at iteration [1004]: 0.21457558987142222
Loss at iteration [1005]: 0.21457551948713954
Loss at iteration [1006]: 0.21457545707777406
Loss at iteration [1007]: 0.21457539036069
Loss at iteration [1008]: 0.21457533214415417
Loss at iteration [1009]: 0.2145752805077708
Loss at iteration [1010]: 0.21457523191910444
Loss at iteration [1011]: 0.2145751846885054
Loss at iteration [1012]: 0.21457513681798981
Loss at iteration [1013]: 0.21457509554773246
Loss at iteration [1014]: 0.21457505933225773
Loss at iteration [1015]: 0.21457503340949727
Loss at iteration [1016]: 0.2145750165857003
Loss at iteration [1017]: 0.21457498584838855
Loss at iteration [1018]: 0.2145749494272277
Loss at iteration [1019]: 0.2145748917313952
Loss at iteration [1020]: 0.21457483872578392
Loss at iteration [1021]: 0.21457477659906538
Loss at iteration [1022]: 0.2145747240957652
Loss at iteration [1023]: 0.21457465700424144
Loss at iteration [1024]: 0.21457459603608572
Loss at iteration [1025]: 0.2145745362270592
Loss at iteration [1026]: 0.21457448186478625
Loss at iteration [1027]: 0.21457443035641818
Loss at iteration [1028]: 0.21457438095277245
Loss at iteration [1029]: 0.21457433349333735
Loss at iteration [1030]: 0.2145742861194019
Loss at iteration [1031]: 0.21457424202651035
Loss at iteration [1032]: 0.21457420703228378
Loss at iteration [1033]: 0.21457418082393648
Loss at iteration [1034]: 0.21457415871922142
Loss at iteration [1035]: 0.21457414193071006
Loss at iteration [1036]: 0.21457410194388477
Loss at iteration [1037]: 0.21457406700714043
Loss at iteration [1038]: 0.21457401406514232
Loss at iteration [1039]: 0.21457396059242334
Loss at iteration [1040]: 0.2145738841038215
Loss at iteration [1041]: 0.21457381840540682
Loss at iteration [1042]: 0.21457374861160758
Loss at iteration [1043]: 0.21457368870923454
Loss at iteration [1044]: 0.21457363697840404
Loss at iteration [1045]: 0.21457358786767822
Loss at iteration [1046]: 0.21457354039749157
Loss at iteration [1047]: 0.2145734939718605
Loss at iteration [1048]: 0.21457345723570054
Loss at iteration [1049]: 0.21457342673580712
Loss at iteration [1050]: 0.2145734014878732
Loss at iteration [1051]: 0.21457338046128263
Loss at iteration [1052]: 0.21457334589054647
Loss at iteration [1053]: 0.21457331171283583
Loss at iteration [1054]: 0.21457325538998237
Loss at iteration [1055]: 0.21457320741685532
Loss at iteration [1056]: 0.21457313766045766
Loss at iteration [1057]: 0.2145730788359281
Loss at iteration [1058]: 0.21457301475914312
Loss at iteration [1059]: 0.21457296097466974
Loss at iteration [1060]: 0.21457290561738468
Loss at iteration [1061]: 0.21457285279117486
Loss at iteration [1062]: 0.21457280338530701
Loss at iteration [1063]: 0.21457275583100935
Loss at iteration [1064]: 0.21457271289031016
Loss at iteration [1065]: 0.21457267573003144
Loss at iteration [1066]: 0.21457264998496003
Loss at iteration [1067]: 0.2145726324816586
Loss at iteration [1068]: 0.21457260360272357
Loss at iteration [1069]: 0.21457257245237976
Loss at iteration [1070]: 0.21457252271608918
Loss at iteration [1071]: 0.21457247417033723
Loss at iteration [1072]: 0.21457240589064458
Loss at iteration [1073]: 0.21457234912113402
Loss at iteration [1074]: 0.2145722871646929
Loss at iteration [1075]: 0.21457223295890152
Loss at iteration [1076]: 0.21457217951900104
Loss at iteration [1077]: 0.21457212388782287
Loss at iteration [1078]: 0.21457206990486744
Loss at iteration [1079]: 0.21457202109248724
Loss at iteration [1080]: 0.21457197665446082
Loss at iteration [1081]: 0.21457194383216505
Loss at iteration [1082]: 0.214571923480232
Loss at iteration [1083]: 0.21457191720759097
Loss at iteration [1084]: 0.21457189765769674
Loss at iteration [1085]: 0.21457185477823357
Loss at iteration [1086]: 0.21457180929938216
Loss at iteration [1087]: 0.21457173971365406
Loss at iteration [1088]: 0.214571680247825
Loss at iteration [1089]: 0.2145716087984397
Loss at iteration [1090]: 0.21457154929752884
Loss at iteration [1091]: 0.2145714892479098
Loss at iteration [1092]: 0.21457143885388338
Loss at iteration [1093]: 0.21457138840345089
Loss at iteration [1094]: 0.2145713426170201
Loss at iteration [1095]: 0.21457129976293174
Loss at iteration [1096]: 0.21457126068546434
Loss at iteration [1097]: 0.21457122845392093
Loss at iteration [1098]: 0.21457120542904878
Loss at iteration [1099]: 0.21457118472946987
Loss at iteration [1100]: 0.21457115765908222
Loss at iteration [1101]: 0.21457110909111365
Loss at iteration [1102]: 0.2145710621080803
Loss at iteration [1103]: 0.21457100162404752
Loss at iteration [1104]: 0.2145709491572146
Loss at iteration [1105]: 0.2145708914508429
Loss at iteration [1106]: 0.214570836937846
Loss at iteration [1107]: 0.21457078034337415
Loss at iteration [1108]: 0.21457072544961975
Loss at iteration [1109]: 0.21457067541703062
Loss at iteration [1110]: 0.21457062750965614
Loss at iteration [1111]: 0.21457057860191403
Loss at iteration [1112]: 0.21457053325666958
Loss at iteration [1113]: 0.21457049392254632
Loss at iteration [1114]: 0.21457046337352706
Loss at iteration [1115]: 0.21457044805952682
Loss at iteration [1116]: 0.21457043798924896
Loss at iteration [1117]: 0.214570418249975
Loss at iteration [1118]: 0.21457038835369435
Loss at iteration [1119]: 0.21457032253099742
Loss at iteration [1120]: 0.21457026071330182
Loss at iteration [1121]: 0.21457018044883877
Loss at iteration [1122]: 0.2145701241689375
Loss at iteration [1123]: 0.21457005959500133
Loss at iteration [1124]: 0.2145700066452883
Loss at iteration [1125]: 0.2145699569153972
Loss at iteration [1126]: 0.21456991171841328
Loss at iteration [1127]: 0.2145698726513569
Loss at iteration [1128]: 0.21456983509760652
Loss at iteration [1129]: 0.2145698104192329
Loss at iteration [1130]: 0.2145697843044381
Loss at iteration [1131]: 0.21456976985968643
Loss at iteration [1132]: 0.21456974616672939
Loss at iteration [1133]: 0.21456968939511106
Loss at iteration [1134]: 0.2145696443016877
Loss at iteration [1135]: 0.21456958199340256
Loss at iteration [1136]: 0.21456952672678162
Loss at iteration [1137]: 0.21456946952888997
Loss at iteration [1138]: 0.21456941803467258
Loss at iteration [1139]: 0.21456935998644663
Loss at iteration [1140]: 0.2145693016213511
Loss at iteration [1141]: 0.2145692506992325
Loss at iteration [1142]: 0.214569203295824
Loss at iteration [1143]: 0.21456916022839423
Loss at iteration [1144]: 0.21456911733475378
Loss at iteration [1145]: 0.21456907969810227
Loss at iteration [1146]: 0.21456904876769256
Loss at iteration [1147]: 0.21456902103864411
Loss at iteration [1148]: 0.21456899802778767
Loss at iteration [1149]: 0.21456898071141403
Loss at iteration [1150]: 0.21456895870803436
Loss at iteration [1151]: 0.21456890424004257
Loss at iteration [1152]: 0.2145688568586076
Loss at iteration [1153]: 0.21456879307479115
Loss at iteration [1154]: 0.21456873781413346
Loss at iteration [1155]: 0.21456867137971108
Loss at iteration [1156]: 0.2145686189021781
Loss at iteration [1157]: 0.21456856499154928
Loss at iteration [1158]: 0.21456851311913255
Loss at iteration [1159]: 0.21456846436499122
Loss at iteration [1160]: 0.21456842066014098
Loss at iteration [1161]: 0.21456837904661724
Loss at iteration [1162]: 0.21456834210613127
Loss at iteration [1163]: 0.21456831770156334
Loss at iteration [1164]: 0.21456829768246083
Loss at iteration [1165]: 0.21456828130747146
Loss at iteration [1166]: 0.2145682621715532
Loss at iteration [1167]: 0.2145682339954455
Loss at iteration [1168]: 0.21456820490596337
Loss at iteration [1169]: 0.21456814485839015
Loss at iteration [1170]: 0.21456809784030317
Loss at iteration [1171]: 0.2145680291816453
Loss at iteration [1172]: 0.21456797688568754
Loss at iteration [1173]: 0.21456791674467243
Loss at iteration [1174]: 0.21456786891261825
Loss at iteration [1175]: 0.2145678254800076
Loss at iteration [1176]: 0.21456778242568617
Loss at iteration [1177]: 0.21456774372487894
Loss at iteration [1178]: 0.21456770847959505
Loss at iteration [1179]: 0.21456767638729007
Loss at iteration [1180]: 0.21456765642151446
Loss at iteration [1181]: 0.2145676433727871
Loss at iteration [1182]: 0.21456764769711473
***** Warning: Loss has increased *****
Loss at iteration [1183]: 0.21456762525179518
Loss at iteration [1184]: 0.2145675941528049
Loss at iteration [1185]: 0.21456751508774455
Loss at iteration [1186]: 0.2145674519285482
Loss at iteration [1187]: 0.2145673943000261
Loss at iteration [1188]: 0.21456735266771668
Loss at iteration [1189]: 0.21456730770688764
Loss at iteration [1190]: 0.21456725624398842
Loss at iteration [1191]: 0.2145672104671449
Loss at iteration [1192]: 0.2145671728206954
Loss at iteration [1193]: 0.21456713428937776
Loss at iteration [1194]: 0.2145671003187918
Loss at iteration [1195]: 0.21456707110335616
Loss at iteration [1196]: 0.21456705234870452
Loss at iteration [1197]: 0.2145670421516299
Loss at iteration [1198]: 0.21456704024123693
Loss at iteration [1199]: 0.21456700147309007
Loss at iteration [1200]: 0.21456696465220004
Loss at iteration [1201]: 0.2145669030983992
Loss at iteration [1202]: 0.21456685104161777
Loss at iteration [1203]: 0.2145667854270545
Loss at iteration [1204]: 0.2145667391764833
Loss at iteration [1205]: 0.21456670060602243
Loss at iteration [1206]: 0.2145666549816232
Loss at iteration [1207]: 0.21456660995291285
Loss at iteration [1208]: 0.21456656837699953
Loss at iteration [1209]: 0.21456653066513937
Loss at iteration [1210]: 0.21456649393554944
Loss at iteration [1211]: 0.21456647021295233
Loss at iteration [1212]: 0.21456646191067627
Loss at iteration [1213]: 0.21456645865899504
Loss at iteration [1214]: 0.21456646638688606
***** Warning: Loss has increased *****
Loss at iteration [1215]: 0.21456641670048718
Loss at iteration [1216]: 0.21456636469741616
Loss at iteration [1217]: 0.21456629085375747
Loss at iteration [1218]: 0.21456623632271937
Loss at iteration [1219]: 0.21456618023556287
Loss at iteration [1220]: 0.21456613270158056
Loss at iteration [1221]: 0.21456609141103278
Loss at iteration [1222]: 0.21456605510504942
Loss at iteration [1223]: 0.21456602030824978
Loss at iteration [1224]: 0.21456598949392824
Loss at iteration [1225]: 0.2145659775893333
Loss at iteration [1226]: 0.21456596471958148
Loss at iteration [1227]: 0.21456595339380688
Loss at iteration [1228]: 0.21456594214263944
Loss at iteration [1229]: 0.21456589502994092
Loss at iteration [1230]: 0.21456584997785974
Loss at iteration [1231]: 0.2145657900872157
Loss at iteration [1232]: 0.21456574441816612
Loss at iteration [1233]: 0.21456569198394862
Loss at iteration [1234]: 0.21456565145033477
Loss at iteration [1235]: 0.21456561106544844
Loss at iteration [1236]: 0.2145655677814355
Loss at iteration [1237]: 0.21456552912754132
Loss at iteration [1238]: 0.21456549842174424
Loss at iteration [1239]: 0.21456546694239778
Loss at iteration [1240]: 0.21456543584393528
Loss at iteration [1241]: 0.2145654210786562
Loss at iteration [1242]: 0.21456541509272714
Loss at iteration [1243]: 0.2145654079384293
Loss at iteration [1244]: 0.2145654196846636
***** Warning: Loss has increased *****
Loss at iteration [1245]: 0.21456538202125955
Loss at iteration [1246]: 0.21456534667169813
Loss at iteration [1247]: 0.21456527295142322
Loss at iteration [1248]: 0.21456522034146186
Loss at iteration [1249]: 0.21456517126601657
Loss at iteration [1250]: 0.21456513554948342
Loss at iteration [1251]: 0.21456509288182124
Loss at iteration [1252]: 0.21456505142742568
Loss at iteration [1253]: 0.2145650134038836
Loss at iteration [1254]: 0.21456497992417656
Loss at iteration [1255]: 0.214564948732851
Loss at iteration [1256]: 0.21456492249001374
Loss at iteration [1257]: 0.21456490145241675
Loss at iteration [1258]: 0.2145649001439314
Loss at iteration [1259]: 0.21456488679325603
Loss at iteration [1260]: 0.21456486024984384
Loss at iteration [1261]: 0.21456482335201463
Loss at iteration [1262]: 0.21456480643678066
Loss at iteration [1263]: 0.21456476087683132
Loss at iteration [1264]: 0.21456471555353257
Loss at iteration [1265]: 0.21456466512697386
Loss at iteration [1266]: 0.21456462384905914
Loss at iteration [1267]: 0.21456457904510898
Loss at iteration [1268]: 0.21456453694700592
Loss at iteration [1269]: 0.2145644968460896
Loss at iteration [1270]: 0.2145644605227584
Loss at iteration [1271]: 0.21456442706422452
Loss at iteration [1272]: 0.21456439624914472
Loss at iteration [1273]: 0.21456437056867778
Loss at iteration [1274]: 0.21456435284006478
Loss at iteration [1275]: 0.21456435146089872
Loss at iteration [1276]: 0.2145643638612296
***** Warning: Loss has increased *****
Loss at iteration [1277]: 0.21456433378756629
Loss at iteration [1278]: 0.21456429706005284
Loss at iteration [1279]: 0.21456424197806145
Loss at iteration [1280]: 0.21456420355582936
Loss at iteration [1281]: 0.21456414949394773
Loss at iteration [1282]: 0.21456410655966776
Loss at iteration [1283]: 0.2145640671560389
Loss at iteration [1284]: 0.21456402430207408
Loss at iteration [1285]: 0.21456398945391927
Loss at iteration [1286]: 0.21456395555747415
Loss at iteration [1287]: 0.2145639238978628
Loss at iteration [1288]: 0.2145638929597952
Loss at iteration [1289]: 0.21456386888406012
Loss at iteration [1290]: 0.21456385792982233
Loss at iteration [1291]: 0.21456385581118637
Loss at iteration [1292]: 0.21456385568434225
