Model name                            : MLP_Multistep
The number of input features          : 20
The number of output features         : 2
Optimizer name                        : SGD
Learning rate                         : 0.2
Max number of iterations              : 3000
Number of samples in training data    : 39
Number of samples in tests data       : 16
Total training time                   : 0.03374624252319336
Total number of parameters            : 205302
Percentage of parameters < 1e-9       : 49.72479566687124%
Percentage of parameters < 1e-7       : 49.725282754186516%
Percentage of parameters < 1e-6       : 49.72576984150179%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5634746643749708
Loss at iteration [2]: 1.4167634123263444
Loss at iteration [3]: 1.2222354702697544
Loss at iteration [4]: 0.9079506515839171
Loss at iteration [5]: 0.5106767692023225
Loss at iteration [6]: 0.4536318976281784
Loss at iteration [7]: 0.5084421351739837
***** Warning: Loss has increased *****
Loss at iteration [8]: 0.45193703572118593
Loss at iteration [9]: 0.42532731895400855
Loss at iteration [10]: 0.3970231567091926
Loss at iteration [11]: 0.40172168574420386
***** Warning: Loss has increased *****
Loss at iteration [12]: 0.39109640672932794
Loss at iteration [13]: 0.39029560972375543
Loss at iteration [14]: 0.39045931638824966
***** Warning: Loss has increased *****
Loss at iteration [15]: 0.38919428310304593
Loss at iteration [16]: 0.3893894134025061
***** Warning: Loss has increased *****
Loss at iteration [17]: 0.38917101067470816
Loss at iteration [18]: 0.3891633899948181
Loss at iteration [19]: 0.3891147526290251
Loss at iteration [20]: 0.38910972555489415
Loss at iteration [21]: 0.389107101133816
Loss at iteration [22]: 0.3890971623364046
Loss at iteration [23]: 0.3891000387981961
***** Warning: Loss has increased *****
Loss at iteration [24]: 0.3890975939509704
Loss at iteration [25]: 0.38909741192833724
Loss at iteration [26]: 0.3890973438015078
Loss at iteration [27]: 0.38909715011794305
Loss at iteration [28]: 0.3890971422234884
Loss at iteration [29]: 0.3890970896046998
Loss at iteration [30]: 0.38909709998321923
***** Warning: Loss has increased *****
Loss at iteration [31]: 0.3890970804012588
Loss at iteration [32]: 0.3890970813220581
***** Warning: Loss has increased *****
