Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : SGD
Learning rate                         : 0.2
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 8.798208475112915
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 51.723250264848865%
Percentage of parameters < 1e-7       : 51.723250264848865%
Percentage of parameters < 1e-6       : 51.72523044326294%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0517870314274385
Loss at iteration [2]: 0.39399626646071034
Loss at iteration [3]: 0.12617987387165902
Loss at iteration [4]: 0.9362816058961144
***** Warning: Loss has increased *****
Loss at iteration [5]: 1.2154831462652143
***** Warning: Loss has increased *****
Loss at iteration [6]: 1.0191918550539414
Loss at iteration [7]: 1.0277620502145781
***** Warning: Loss has increased *****
Loss at iteration [8]: 0.9658199776736363
Loss at iteration [9]: 0.8413792504938955
Loss at iteration [10]: 0.6139712587742202
Loss at iteration [11]: 0.3603868768578796
Loss at iteration [12]: 0.1272541553320675
Loss at iteration [13]: 0.10978957949891395
Loss at iteration [14]: 0.11317324413440928
***** Warning: Loss has increased *****
Loss at iteration [15]: 0.1984482841837933
***** Warning: Loss has increased *****
Loss at iteration [16]: 0.06984213070299894
Loss at iteration [17]: 0.013084253687328622
Loss at iteration [18]: 0.014698186746901654
***** Warning: Loss has increased *****
Loss at iteration [19]: 0.01050336196669452
Loss at iteration [20]: 0.01250201225525446
***** Warning: Loss has increased *****
Loss at iteration [21]: 0.00759133437871426
Loss at iteration [22]: 0.005823752182103554
Loss at iteration [23]: 0.004361220189140811
Loss at iteration [24]: 0.0038397336686789257
Loss at iteration [25]: 0.0035358158911464165
Loss at iteration [26]: 0.0033981466744258896
Loss at iteration [27]: 0.003288746007132527
Loss at iteration [28]: 0.0031780854838513303
Loss at iteration [29]: 0.0030945881171624565
Loss at iteration [30]: 0.0030243229960547307
Loss at iteration [31]: 0.002972158208874252
Loss at iteration [32]: 0.0029304039303695833
Loss at iteration [33]: 0.002896087605983938
Loss at iteration [34]: 0.0028670777444909958
Loss at iteration [35]: 0.002841742865209406
Loss at iteration [36]: 0.0028192170909354807
Loss at iteration [37]: 0.002799026062058303
Loss at iteration [38]: 0.002781121620189255
Loss at iteration [39]: 0.002765351164173559
Loss at iteration [40]: 0.00275121889391714
Loss at iteration [41]: 0.002738536693640421
Loss at iteration [42]: 0.002727230416630621
Loss at iteration [43]: 0.0027170281201834828
Loss at iteration [44]: 0.0027076880567272255
Loss at iteration [45]: 0.0026990901288540337
Loss at iteration [46]: 0.0026911511764965202
Loss at iteration [47]: 0.002683938834546687
Loss at iteration [48]: 0.0026774334023719464
Loss at iteration [49]: 0.00267135709325744
Loss at iteration [50]: 0.0026655733311347156
Loss at iteration [51]: 0.0026603445723460952
Loss at iteration [52]: 0.0026556558751132196
Loss at iteration [53]: 0.0026514026266490688
Loss at iteration [54]: 0.002647427213113056
Loss at iteration [55]: 0.0026437080013967007
Loss at iteration [56]: 0.0026401794001379934
Loss at iteration [57]: 0.002636838282020995
Loss at iteration [58]: 0.0026336587997403058
Loss at iteration [59]: 0.0026306943070520043
Loss at iteration [60]: 0.002627916631885443
Loss at iteration [61]: 0.002625370158741781
Loss at iteration [62]: 0.0026228086067553186
Loss at iteration [63]: 0.0026203597029935974
Loss at iteration [64]: 0.0026180431804830595
Loss at iteration [65]: 0.002615725140076675
Loss at iteration [66]: 0.002613480551715538
Loss at iteration [67]: 0.0026112999581555467
Loss at iteration [68]: 0.002609226427004974
Loss at iteration [69]: 0.002607210887760732
Loss at iteration [70]: 0.0026052587120267644
Loss at iteration [71]: 0.0026034844453005942
Loss at iteration [72]: 0.0026018198363827677
Loss at iteration [73]: 0.002600224583644519
Loss at iteration [74]: 0.0025987046504995026
Loss at iteration [75]: 0.002597232014441783
Loss at iteration [76]: 0.00259588318839414
Loss at iteration [77]: 0.002594622820286488
Loss at iteration [78]: 0.0025933752794473058
Loss at iteration [79]: 0.0025921812813610875
Loss at iteration [80]: 0.0025910255201077557
Loss at iteration [81]: 0.0025899075887891145
Loss at iteration [82]: 0.002588836608482905
Loss at iteration [83]: 0.0025878044124814127
Loss at iteration [84]: 0.0025867868502491573
Loss at iteration [85]: 0.0025857915922513426
Loss at iteration [86]: 0.002584840159618805
Loss at iteration [87]: 0.0025839086484708965
Loss at iteration [88]: 0.002582955972876587
Loss at iteration [89]: 0.0025820369175385727
Loss at iteration [90]: 0.002581138947414725
Loss at iteration [91]: 0.0025802559972626653
Loss at iteration [92]: 0.0025794318244900436
Loss at iteration [93]: 0.0025786265526024066
Loss at iteration [94]: 0.002577869453540113
Loss at iteration [95]: 0.0025771353347109082
Loss at iteration [96]: 0.002576413399435009
Loss at iteration [97]: 0.0025757107568541633
Loss at iteration [98]: 0.0025749855237150675
Loss at iteration [99]: 0.00257426773188743
Loss at iteration [100]: 0.002573565902819907
Loss at iteration [101]: 0.002572881373809421
Loss at iteration [102]: 0.0025721770473102123
Loss at iteration [103]: 0.002571456786674754
Loss at iteration [104]: 0.002570759112089647
Loss at iteration [105]: 0.002570077902342967
Loss at iteration [106]: 0.002569400856636699
Loss at iteration [107]: 0.00256873179698768
Loss at iteration [108]: 0.0025680717015140898
Loss at iteration [109]: 0.0025674470584763198
Loss at iteration [110]: 0.002566833197911847
Loss at iteration [111]: 0.0025662153127713368
Loss at iteration [112]: 0.002565621740915155
Loss at iteration [113]: 0.002565026249159876
Loss at iteration [114]: 0.0025644333227601212
Loss at iteration [115]: 0.002563848719719598
Loss at iteration [116]: 0.0025632724459200506
Loss at iteration [117]: 0.0025627042195783614
Loss at iteration [118]: 0.0025621571253027537
Loss at iteration [119]: 0.0025616006128969257
Loss at iteration [120]: 0.0025610533924236203
Loss at iteration [121]: 0.002560504927566107
Loss at iteration [122]: 0.0025599668566773986
Loss at iteration [123]: 0.0025594485759373484
Loss at iteration [124]: 0.0025589417990251086
Loss at iteration [125]: 0.0025584622101046035
Loss at iteration [126]: 0.0025579896781756984
Loss at iteration [127]: 0.002557522213301871
Loss at iteration [128]: 0.002557055804623774
Loss at iteration [129]: 0.0025565999106169955
Loss at iteration [130]: 0.002556150216304344
Loss at iteration [131]: 0.0025557037509744236
Loss at iteration [132]: 0.0025552629185148787
Loss at iteration [133]: 0.002554832735607982
Loss at iteration [134]: 0.0025544197765186728
Loss at iteration [135]: 0.002554005160348599
Loss at iteration [136]: 0.002553596052063656
Loss at iteration [137]: 0.0025531920825920397
Loss at iteration [138]: 0.0025527934871994555
Loss at iteration [139]: 0.0025523967913800853
Loss at iteration [140]: 0.002552007798217281
Loss at iteration [141]: 0.0025516301053263774
Loss at iteration [142]: 0.0025512534522795292
Loss at iteration [143]: 0.0025508721704497433
Loss at iteration [144]: 0.0025504907114550727
Loss at iteration [145]: 0.0025501035289854403
Loss at iteration [146]: 0.0025497177473582894
Loss at iteration [147]: 0.0025493410933053744
Loss at iteration [148]: 0.002548959094909697
Loss at iteration [149]: 0.0025485798370097528
Loss at iteration [150]: 0.002548198593575319
Loss at iteration [151]: 0.0025478166654027187
Loss at iteration [152]: 0.002547439294278373
Loss at iteration [153]: 0.0025470564485567016
Loss at iteration [154]: 0.0025466857546946123
Loss at iteration [155]: 0.0025463062105948835
Loss at iteration [156]: 0.0025459311078067444
Loss at iteration [157]: 0.002545558771895967
Loss at iteration [158]: 0.0025451881586475073
Loss at iteration [159]: 0.0025448190154411785
Loss at iteration [160]: 0.002544452837624929
Loss at iteration [161]: 0.0025440931345063916
Loss at iteration [162]: 0.002543732382752227
Loss at iteration [163]: 0.002543375067074581
Loss at iteration [164]: 0.002543019091377518
Loss at iteration [165]: 0.0025426623209618953
Loss at iteration [166]: 0.0025423065043537357
Loss at iteration [167]: 0.00254195456458241
Loss at iteration [168]: 0.00254160091053241
Loss at iteration [169]: 0.0025412554275458976
Loss at iteration [170]: 0.0025409061302695385
Loss at iteration [171]: 0.0025405605084901604
Loss at iteration [172]: 0.0025402175319910276
Loss at iteration [173]: 0.0025398743717597234
Loss at iteration [174]: 0.002539522421442389
Loss at iteration [175]: 0.002539175448100101
Loss at iteration [176]: 0.002538833760341008
Loss at iteration [177]: 0.0025384995832951193
Loss at iteration [178]: 0.002538175784876263
Loss at iteration [179]: 0.002537853796633908
Loss at iteration [180]: 0.0025375281340988608
Loss at iteration [181]: 0.00253720019528573
Loss at iteration [182]: 0.0025368695834948308
Loss at iteration [183]: 0.00253653780957396
Loss at iteration [184]: 0.0025362152201472534
Loss at iteration [185]: 0.0025358950737867443
Loss at iteration [186]: 0.002535584469160327
Loss at iteration [187]: 0.002535270858580897
Loss at iteration [188]: 0.0025349614061369896
Loss at iteration [189]: 0.0025346531132130236
Loss at iteration [190]: 0.002534351266952719
Loss at iteration [191]: 0.0025340604868349555
Loss at iteration [192]: 0.00253377743620666
Loss at iteration [193]: 0.002533495502617107
Loss at iteration [194]: 0.002533215926118727
Loss at iteration [195]: 0.0025329393715310834
Loss at iteration [196]: 0.0025326608510846628
Loss at iteration [197]: 0.0025323829843380548
Loss at iteration [198]: 0.002532108703772403
Loss at iteration [199]: 0.002531836734365583
Loss at iteration [200]: 0.0025315716185826654
Loss at iteration [201]: 0.002531313247216506
Loss at iteration [202]: 0.002531056996049829
Loss at iteration [203]: 0.002530801369918302
Loss at iteration [204]: 0.002530555620889179
Loss at iteration [205]: 0.0025303058501633916
Loss at iteration [206]: 0.002530057361764705
Loss at iteration [207]: 0.0025298094348452974
Loss at iteration [208]: 0.002529565246978927
Loss at iteration [209]: 0.0025293214216638134
Loss at iteration [210]: 0.0025290742075818337
Loss at iteration [211]: 0.002528827165244459
Loss at iteration [212]: 0.002528581146986583
Loss at iteration [213]: 0.002528339493209127
Loss at iteration [214]: 0.002528098130464916
Loss at iteration [215]: 0.002527869124606145
Loss at iteration [216]: 0.0025276353775996628
Loss at iteration [217]: 0.002527406918897131
Loss at iteration [218]: 0.002527178410994943
Loss at iteration [219]: 0.00252695402553954
Loss at iteration [220]: 0.0025267302305762482
Loss at iteration [221]: 0.0025265067743484074
Loss at iteration [222]: 0.0025262845118633527
Loss at iteration [223]: 0.0025260704106734207
Loss at iteration [224]: 0.0025258550375763986
Loss at iteration [225]: 0.002525639741027731
Loss at iteration [226]: 0.0025254305809231875
Loss at iteration [227]: 0.0025252175840937492
Loss at iteration [228]: 0.002525007568239138
Loss at iteration [229]: 0.0025247964353771156
Loss at iteration [230]: 0.0025245882384645623
Loss at iteration [231]: 0.002524379220351939
Loss at iteration [232]: 0.002524169382578579
Loss at iteration [233]: 0.0025239538557694286
Loss at iteration [234]: 0.0025237384006857014
Loss at iteration [235]: 0.0025235237545817
Loss at iteration [236]: 0.0025233067750974085
Loss at iteration [237]: 0.0025230920590307597
Loss at iteration [238]: 0.002522884625939466
Loss at iteration [239]: 0.0025226730144904353
Loss at iteration [240]: 0.002522462524634335
Loss at iteration [241]: 0.002522252698631323
Loss at iteration [242]: 0.002522041687496345
Loss at iteration [243]: 0.002521830905882996
Loss at iteration [244]: 0.002521622668674804
Loss at iteration [245]: 0.0025214126711914013
Loss at iteration [246]: 0.0025212024615116056
Loss at iteration [247]: 0.002520995073559931
Loss at iteration [248]: 0.0025207926221956414
Loss at iteration [249]: 0.0025205889248482
Loss at iteration [250]: 0.002520392058498908
Loss at iteration [251]: 0.0025201909204623156
Loss at iteration [252]: 0.0025199949470036784
Loss at iteration [253]: 0.002519795641584277
Loss at iteration [254]: 0.002519597623840732
Loss at iteration [255]: 0.002519401357850324
Loss at iteration [256]: 0.0025192048603726426
Loss at iteration [257]: 0.0025190070285103323
Loss at iteration [258]: 0.0025188135079590437
Loss at iteration [259]: 0.002518618228691562
Loss at iteration [260]: 0.0025184218210760815
Loss at iteration [261]: 0.0025182303076761313
Loss at iteration [262]: 0.002518042023115861
Loss at iteration [263]: 0.002517849413709748
Loss at iteration [264]: 0.0025176628182429954
Loss at iteration [265]: 0.002517480106429048
Loss at iteration [266]: 0.002517295383643415
Loss at iteration [267]: 0.002517115233636482
Loss at iteration [268]: 0.002516934042344615
Loss at iteration [269]: 0.002516756231916975
Loss at iteration [270]: 0.002516579693627031
Loss at iteration [271]: 0.0025164057306745597
Loss at iteration [272]: 0.0025162316334967553
Loss at iteration [273]: 0.0025160590839794317
Loss at iteration [274]: 0.0025158895412394646
Loss at iteration [275]: 0.0025157200853628756
Loss at iteration [276]: 0.0025155514922517116
Loss at iteration [277]: 0.0025153825738259243
Loss at iteration [278]: 0.0025152113287916734
Loss at iteration [279]: 0.002515040406016256
Loss at iteration [280]: 0.0025148684740753375
Loss at iteration [281]: 0.0025146957953780277
Loss at iteration [282]: 0.0025145297226149364
Loss at iteration [283]: 0.002514363365999273
Loss at iteration [284]: 0.0025141968274188585
Loss at iteration [285]: 0.0025140354564390953
Loss at iteration [286]: 0.0025138718556242915
Loss at iteration [287]: 0.002513707746206454
Loss at iteration [288]: 0.0025135451686476205
Loss at iteration [289]: 0.00251338253290666
Loss at iteration [290]: 0.00251322429490187
Loss at iteration [291]: 0.0025130708275217712
Loss at iteration [292]: 0.0025129123254472295
Loss at iteration [293]: 0.0025127551163270923
Loss at iteration [294]: 0.0025125983256226833
Loss at iteration [295]: 0.0025124435324097067
Loss at iteration [296]: 0.0025122864109520127
Loss at iteration [297]: 0.0025121332018095805
Loss at iteration [298]: 0.00251197753459158
Loss at iteration [299]: 0.002511818908708209
Loss at iteration [300]: 0.002511666358474766
Loss at iteration [301]: 0.0025115098645390995
Loss at iteration [302]: 0.0025113551851493987
Loss at iteration [303]: 0.002511203000247027
Loss at iteration [304]: 0.0025110577801415123
Loss at iteration [305]: 0.0025109112778368835
Loss at iteration [306]: 0.002510760579049997
Loss at iteration [307]: 0.0025106142549035887
Loss at iteration [308]: 0.0025104652047852113
Loss at iteration [309]: 0.0025103188154347402
Loss at iteration [310]: 0.00251017180949826
Loss at iteration [311]: 0.002510027780947458
Loss at iteration [312]: 0.0025098839109143614
Loss at iteration [313]: 0.0025097430969252058
Loss at iteration [314]: 0.002509603947762663
Loss at iteration [315]: 0.002509464629906815
Loss at iteration [316]: 0.002509325326981654
Loss at iteration [317]: 0.0025091865569550793
Loss at iteration [318]: 0.002509047970780672
Loss at iteration [319]: 0.002508910884580906
Loss at iteration [320]: 0.002508774485928092
Loss at iteration [321]: 0.002508645619002632
Loss at iteration [322]: 0.002508520957178695
Loss at iteration [323]: 0.002508391003936572
Loss at iteration [324]: 0.002508262502873075
Loss at iteration [325]: 0.002508131755526289
Loss at iteration [326]: 0.002508001433620957
Loss at iteration [327]: 0.0025078698800241107
Loss at iteration [328]: 0.002507737934071827
Loss at iteration [329]: 0.0025076057951357407
Loss at iteration [330]: 0.002507479785089816
Loss at iteration [331]: 0.002507351178943439
Loss at iteration [332]: 0.002507223748440065
Loss at iteration [333]: 0.002507098991580711
Loss at iteration [334]: 0.0025069733741984444
Loss at iteration [335]: 0.002506847956835294
Loss at iteration [336]: 0.0025067249279184293
Loss at iteration [337]: 0.0025065999718204427
Loss at iteration [338]: 0.0025064783838329957
Loss at iteration [339]: 0.0025063530789342795
Loss at iteration [340]: 0.002506231757457642
Loss at iteration [341]: 0.0025061082104741762
Loss at iteration [342]: 0.0025059855362323438
Loss at iteration [343]: 0.0025058638536853547
Loss at iteration [344]: 0.0025057430659290846
Loss at iteration [345]: 0.002505621665661435
Loss at iteration [346]: 0.0025055014557253297
Loss at iteration [347]: 0.002505382936610605
Loss at iteration [348]: 0.0025052636269859044
Loss at iteration [349]: 0.0025051492031703273
Loss at iteration [350]: 0.0025050337943194256
Loss at iteration [351]: 0.0025049198220774053
Loss at iteration [352]: 0.0025048049101732066
Loss at iteration [353]: 0.002504691607582192
Loss at iteration [354]: 0.002504578042469301
Loss at iteration [355]: 0.0025044638076279374
Loss at iteration [356]: 0.002504349438594966
Loss at iteration [357]: 0.002504235343031406
Loss at iteration [358]: 0.002504122036627155
Loss at iteration [359]: 0.0025040119439619546
Loss at iteration [360]: 0.002503898424394522
Loss at iteration [361]: 0.002503785776182848
Loss at iteration [362]: 0.0025036735923670717
Loss at iteration [363]: 0.0025035609355021525
Loss at iteration [364]: 0.002503449860806862
Loss at iteration [365]: 0.0025033378605762995
Loss at iteration [366]: 0.002503224444553106
Loss at iteration [367]: 0.0025031107240451587
Loss at iteration [368]: 0.002502996724095938
Loss at iteration [369]: 0.0025028825279900815
Loss at iteration [370]: 0.002502766942419665
Loss at iteration [371]: 0.0025026566611525886
Loss at iteration [372]: 0.0025025473736399583
Loss at iteration [373]: 0.0025024421523964975
Loss at iteration [374]: 0.0025023354511416016
Loss at iteration [375]: 0.002502227750620924
Loss at iteration [376]: 0.002502125156607603
Loss at iteration [377]: 0.0025020182892646937
Loss at iteration [378]: 0.0025019147591010594
Loss at iteration [379]: 0.00250181058633637
Loss at iteration [380]: 0.002501704072545695
Loss at iteration [381]: 0.0025016020711621455
Loss at iteration [382]: 0.0025014994706840585
Loss at iteration [383]: 0.0025013957949649073
Loss at iteration [384]: 0.0025012964144933807
Loss at iteration [385]: 0.0025011942714263436
Loss at iteration [386]: 0.0025010934755047357
Loss at iteration [387]: 0.002500987945193303
Loss at iteration [388]: 0.0025008821371532096
Loss at iteration [389]: 0.002500773931699549
Loss at iteration [390]: 0.002500665269336508
Loss at iteration [391]: 0.002500557072044442
Loss at iteration [392]: 0.002500450917036297
Loss at iteration [393]: 0.002500340965965017
Loss at iteration [394]: 0.0025002374791122544
Loss at iteration [395]: 0.002500132897468657
Loss at iteration [396]: 0.0025000295188712276
Loss at iteration [397]: 0.002499921195994941
Loss at iteration [398]: 0.0024998117221853968
Loss at iteration [399]: 0.002499703269850222
Loss at iteration [400]: 0.0024995970257140152
Loss at iteration [401]: 0.0024994979984967708
Loss at iteration [402]: 0.0024993929860884236
Loss at iteration [403]: 0.002499288910468156
Loss at iteration [404]: 0.002499184360626818
Loss at iteration [405]: 0.0024990773216985243
Loss at iteration [406]: 0.0024989731121121644
Loss at iteration [407]: 0.002498868836441623
Loss at iteration [408]: 0.0024987680530466536
Loss at iteration [409]: 0.002498670416459556
Loss at iteration [410]: 0.002498573883550953
Loss at iteration [411]: 0.0024984780007206416
Loss at iteration [412]: 0.002498379722488034
Loss at iteration [413]: 0.002498281438235103
Loss at iteration [414]: 0.002498184936644319
Loss at iteration [415]: 0.002498085633247637
Loss at iteration [416]: 0.0024979907521850307
Loss at iteration [417]: 0.0024978964116160077
Loss at iteration [418]: 0.002497803951839365
Loss at iteration [419]: 0.0024977104179068313
Loss at iteration [420]: 0.002497616309554995
Loss at iteration [421]: 0.002497525846337962
Loss at iteration [422]: 0.002497432281635235
Loss at iteration [423]: 0.002497340982207665
Loss at iteration [424]: 0.0024972508600320823
Loss at iteration [425]: 0.002497158717741151
Loss at iteration [426]: 0.0024970676718207977
Loss at iteration [427]: 0.0024969779025288638
Loss at iteration [428]: 0.0024968874611861988
Loss at iteration [429]: 0.0024967961427369767
Loss at iteration [430]: 0.002496704869590089
Loss at iteration [431]: 0.0024966169835305254
Loss at iteration [432]: 0.002496529952637956
Loss at iteration [433]: 0.0024964409153685023
Loss at iteration [434]: 0.002496354374889064
Loss at iteration [435]: 0.002496268671829678
Loss at iteration [436]: 0.0024961855500973563
Loss at iteration [437]: 0.002496098107642591
Loss at iteration [438]: 0.002496011105072842
Loss at iteration [439]: 0.0024959241034728436
Loss at iteration [440]: 0.002495837762464657
Loss at iteration [441]: 0.002495749106935737
Loss at iteration [442]: 0.0024956631053892025
Loss at iteration [443]: 0.0024955763339934733
Loss at iteration [444]: 0.0024954906753711158
Loss at iteration [445]: 0.002495403154544077
Loss at iteration [446]: 0.0024953187059713823
Loss at iteration [447]: 0.002495231850414876
Loss at iteration [448]: 0.0024951494707343346
Loss at iteration [449]: 0.0024950650062746968
Loss at iteration [450]: 0.0024949808503378895
Loss at iteration [451]: 0.0024948968295514557
Loss at iteration [452]: 0.002494814960243218
Loss at iteration [453]: 0.0024947314966334553
Loss at iteration [454]: 0.0024946497976410433
Loss at iteration [455]: 0.002494567589913577
Loss at iteration [456]: 0.0024944849474530964
Loss at iteration [457]: 0.0024944031074975756
Loss at iteration [458]: 0.002494319092870751
Loss at iteration [459]: 0.002494236894564094
Loss at iteration [460]: 0.0024941559609484368
Loss at iteration [461]: 0.002494075198929177
Loss at iteration [462]: 0.0024939937308446204
Loss at iteration [463]: 0.0024939115705284376
Loss at iteration [464]: 0.002493831296777775
Loss at iteration [465]: 0.002493747909380907
Loss at iteration [466]: 0.002493668882014987
Loss at iteration [467]: 0.0024935866133271676
Loss at iteration [468]: 0.002493504968390538
Loss at iteration [469]: 0.0024934246483530576
Loss at iteration [470]: 0.002493341630162165
Loss at iteration [471]: 0.0024932596126325565
Loss at iteration [472]: 0.0024931792139909043
Loss at iteration [473]: 0.002493095684656806
Loss at iteration [474]: 0.002493015359379949
Loss at iteration [475]: 0.002492934449897786
Loss at iteration [476]: 0.002492851945896283
Loss at iteration [477]: 0.0024927715489408797
Loss at iteration [478]: 0.002492691091410709
Loss at iteration [479]: 0.0024926114177548655
Loss at iteration [480]: 0.00249253111994554
Loss at iteration [481]: 0.0024924556403384387
Loss at iteration [482]: 0.0024923769894546854
Loss at iteration [483]: 0.0024922963770245337
Loss at iteration [484]: 0.0024922172427957147
Loss at iteration [485]: 0.002492134778721024
Loss at iteration [486]: 0.0024920514718217985
Loss at iteration [487]: 0.002491971320536264
Loss at iteration [488]: 0.0024918862106926612
Loss at iteration [489]: 0.002491804414655103
Loss at iteration [490]: 0.0024917192621654967
Loss at iteration [491]: 0.0024916363633398194
Loss at iteration [492]: 0.0024915536415101146
Loss at iteration [493]: 0.002491471690000354
Loss at iteration [494]: 0.0024913904604290313
Loss at iteration [495]: 0.002491309993309168
Loss at iteration [496]: 0.002491229897667243
Loss at iteration [497]: 0.0024911497876657076
Loss at iteration [498]: 0.002491069900189262
Loss at iteration [499]: 0.0024909899949095062
Loss at iteration [500]: 0.0024909098300387325
Loss at iteration [501]: 0.0024908291301013346
Loss at iteration [502]: 0.0024907491020551703
Loss at iteration [503]: 0.0024906709985048083
Loss at iteration [504]: 0.00249059132729499
Loss at iteration [505]: 0.0024905120159681706
Loss at iteration [506]: 0.0024904353273967074
Loss at iteration [507]: 0.002490356328124987
Loss at iteration [508]: 0.0024902780171345736
Loss at iteration [509]: 0.0024901988011765777
Loss at iteration [510]: 0.002490121181347841
Loss at iteration [511]: 0.002490042628871482
Loss at iteration [512]: 0.0024899680438896423
Loss at iteration [513]: 0.0024898926441816083
Loss at iteration [514]: 0.002489813688265143
Loss at iteration [515]: 0.002489736671899715
Loss at iteration [516]: 0.0024896606478389256
Loss at iteration [517]: 0.0024895825850260738
Loss at iteration [518]: 0.0024895059455165704
Loss at iteration [519]: 0.0024894312967800565
Loss at iteration [520]: 0.0024893549065225715
Loss at iteration [521]: 0.0024892823201250387
Loss at iteration [522]: 0.0024892050495079854
Loss at iteration [523]: 0.002489127986465351
Loss at iteration [524]: 0.0024890506691300045
Loss at iteration [525]: 0.0024889754444213145
Loss at iteration [526]: 0.0024889007435605326
Loss at iteration [527]: 0.0024888255211529624
Loss at iteration [528]: 0.0024887479866230007
Loss at iteration [529]: 0.002488673016457601
Loss at iteration [530]: 0.0024885986026826797
Loss at iteration [531]: 0.0024885245773427754
Loss at iteration [532]: 0.0024884518570044726
Loss at iteration [533]: 0.002488373160650727
Loss at iteration [534]: 0.0024882976528260896
Loss at iteration [535]: 0.002488223362052706
Loss at iteration [536]: 0.002488150921583566
Loss at iteration [537]: 0.0024880829810850647
Loss at iteration [538]: 0.0024880117345821014
Loss at iteration [539]: 0.002487947064297566
Loss at iteration [540]: 0.0024878779376128943
Loss at iteration [541]: 0.002487805616176607
Loss at iteration [542]: 0.0024877292790179166
Loss at iteration [543]: 0.002487656077827497
Loss at iteration [544]: 0.002487575957023871
Loss at iteration [545]: 0.002487504441559657
Loss at iteration [546]: 0.0024874284938475122
Loss at iteration [547]: 0.002487353758882323
Loss at iteration [548]: 0.0024872790752206776
Loss at iteration [549]: 0.002487206596403653
Loss at iteration [550]: 0.0024871327578328754
Loss at iteration [551]: 0.0024870618348845222
Loss at iteration [552]: 0.0024869880172502697
Loss at iteration [553]: 0.002486913030941856
Loss at iteration [554]: 0.0024868408478476408
Loss at iteration [555]: 0.002486767461226635
Loss at iteration [556]: 0.0024866941481733007
Loss at iteration [557]: 0.002486620187289355
Loss at iteration [558]: 0.0024865468966098823
Loss at iteration [559]: 0.0024864776244096086
Loss at iteration [560]: 0.002486406915220896
Loss at iteration [561]: 0.002486334337466106
Loss at iteration [562]: 0.002486267788020763
Loss at iteration [563]: 0.0024861969420318522
Loss at iteration [564]: 0.002486126830893359
Loss at iteration [565]: 0.00248605742789794
Loss at iteration [566]: 0.0024859837145960865
Loss at iteration [567]: 0.0024859173162489665
Loss at iteration [568]: 0.002485843912318753
Loss at iteration [569]: 0.0024857736932220736
Loss at iteration [570]: 0.0024857031250684215
Loss at iteration [571]: 0.002485633379092597
Loss at iteration [572]: 0.002485563573855068
Loss at iteration [573]: 0.002485491750165368
Loss at iteration [574]: 0.0024854246753319084
Loss at iteration [575]: 0.0024853570309895656
Loss at iteration [576]: 0.0024852888817795825
Loss at iteration [577]: 0.0024852192339608076
Loss at iteration [578]: 0.0024851505358882233
Loss at iteration [579]: 0.0024850849765007616
Loss at iteration [580]: 0.0024850175964339014
Loss at iteration [581]: 0.0024849534236243416
Loss at iteration [582]: 0.0024848885954818468
Loss at iteration [583]: 0.0024848227686994225
Loss at iteration [584]: 0.002484754581531974
Loss at iteration [585]: 0.002484686155799117
Loss at iteration [586]: 0.0024846206472000495
Loss at iteration [587]: 0.00248455602660972
Loss at iteration [588]: 0.0024844891011129334
Loss at iteration [589]: 0.0024844243862888575
Loss at iteration [590]: 0.0024843578579963007
Loss at iteration [591]: 0.002484292276605294
Loss at iteration [592]: 0.0024842265736669737
Loss at iteration [593]: 0.00248416401245701
Loss at iteration [594]: 0.0024840992354758016
Loss at iteration [595]: 0.002484037672498753
Loss at iteration [596]: 0.002483974999929794
Loss at iteration [597]: 0.0024839114317908436
Loss at iteration [598]: 0.002483845772198665
Loss at iteration [599]: 0.0024837802768315665
Loss at iteration [600]: 0.002483716469414444
Loss at iteration [601]: 0.002483652349063563
Loss at iteration [602]: 0.00248358909590594
Loss at iteration [603]: 0.002483528700354378
Loss at iteration [604]: 0.002483463164241893
Loss at iteration [605]: 0.002483398952919872
Loss at iteration [606]: 0.002483337371018427
Loss at iteration [607]: 0.0024832776997623096
Loss at iteration [608]: 0.002483217747018582
Loss at iteration [609]: 0.0024831549305372367
Loss at iteration [610]: 0.0024830910078303577
Loss at iteration [611]: 0.002483026742254763
Loss at iteration [612]: 0.0024829618582922932
Loss at iteration [613]: 0.002482901707496631
Loss at iteration [614]: 0.002482841871825075
Loss at iteration [615]: 0.0024827818821158707
Loss at iteration [616]: 0.0024827209831408503
Loss at iteration [617]: 0.0024826567880015305
Loss at iteration [618]: 0.002482593989542149
Loss at iteration [619]: 0.0024825324598690144
Loss at iteration [620]: 0.0024824679930274193
Loss at iteration [621]: 0.0024824077634044393
Loss at iteration [622]: 0.0024823442235913642
Loss at iteration [623]: 0.0024822839865529475
Loss at iteration [624]: 0.002482222446258232
Loss at iteration [625]: 0.002482159080332494
Loss at iteration [626]: 0.002482093699462448
Loss at iteration [627]: 0.0024820332241986894
Loss at iteration [628]: 0.0024819679264552635
Loss at iteration [629]: 0.0024819095576483915
Loss at iteration [630]: 0.0024818481333977677
Loss at iteration [631]: 0.0024817854014691602
Loss at iteration [632]: 0.002481729316661158
Loss at iteration [633]: 0.002481666790733044
Loss at iteration [634]: 0.002481602024827686
Loss at iteration [635]: 0.002481540931877545
Loss at iteration [636]: 0.002481476514151876
Loss at iteration [637]: 0.002481410286040672
Loss at iteration [638]: 0.0024813432053842955
Loss at iteration [639]: 0.002481284487876231
Loss at iteration [640]: 0.00248122138923113
Loss at iteration [641]: 0.0024811642055159737
Loss at iteration [642]: 0.002481102478744487
Loss at iteration [643]: 0.00248104350738672
Loss at iteration [644]: 0.0024809836407827346
Loss at iteration [645]: 0.002480926344667855
Loss at iteration [646]: 0.002480864366146578
Loss at iteration [647]: 0.0024808027373138105
Loss at iteration [648]: 0.002480743721479358
Loss at iteration [649]: 0.0024806826813463675
Loss at iteration [650]: 0.002480623375207276
Loss at iteration [651]: 0.00248056282206972
Loss at iteration [652]: 0.0024805032833832064
Loss at iteration [653]: 0.0024804398940463787
Loss at iteration [654]: 0.00248038138784166
Loss at iteration [655]: 0.0024803219904191627
Loss at iteration [656]: 0.002480262357983922
Loss at iteration [657]: 0.0024802026520203664
Loss at iteration [658]: 0.0024801451493098184
Loss at iteration [659]: 0.0024800839618622787
Loss at iteration [660]: 0.002480025199661578
Loss at iteration [661]: 0.002479966351398603
Loss at iteration [662]: 0.0024799095857704037
Loss at iteration [663]: 0.0024798525971796723
Loss at iteration [664]: 0.002479793170258193
Loss at iteration [665]: 0.002479737890654519
Loss at iteration [666]: 0.002479681337432971
Loss at iteration [667]: 0.002479624702047433
Loss at iteration [668]: 0.00247956714597528
Loss at iteration [669]: 0.0024795148655479817
Loss at iteration [670]: 0.0024794609764247897
Loss at iteration [671]: 0.0024794105485693896
Loss at iteration [672]: 0.0024793535534615757
Loss at iteration [673]: 0.0024792993971814886
Loss at iteration [674]: 0.0024792417811365123
Loss at iteration [675]: 0.0024791882810061908
Loss at iteration [676]: 0.0024791316078228588
Loss at iteration [677]: 0.002479077615824666
Loss at iteration [678]: 0.002479021656836436
Loss at iteration [679]: 0.002478964558754034
Loss at iteration [680]: 0.0024789106275050258
Loss at iteration [681]: 0.0024788591374746267
Loss at iteration [682]: 0.002478799931699362
Loss at iteration [683]: 0.0024787489690530273
Loss at iteration [684]: 0.00247869186873326
Loss at iteration [685]: 0.0024786383294731424
Loss at iteration [686]: 0.002478583899450043
Loss at iteration [687]: 0.0024785330680445537
Loss at iteration [688]: 0.0024784810926658614
Loss at iteration [689]: 0.002478427863364913
Loss at iteration [690]: 0.002478376829021492
Loss at iteration [691]: 0.002478327628443332
Loss at iteration [692]: 0.002478275024991443
Loss at iteration [693]: 0.002478224143228952
Loss at iteration [694]: 0.0024781708428585495
Loss at iteration [695]: 0.0024781230131889777
Loss at iteration [696]: 0.002478069819406002
Loss at iteration [697]: 0.002478019301013584
Loss at iteration [698]: 0.002477964967506934
Loss at iteration [699]: 0.002477909693349464
Loss at iteration [700]: 0.0024778540925419522
Loss at iteration [701]: 0.0024778017381898274
Loss at iteration [702]: 0.0024777480957697
Loss at iteration [703]: 0.0024776984181934703
Loss at iteration [704]: 0.002477647198957557
Loss at iteration [705]: 0.0024775994940883944
Loss at iteration [706]: 0.002477546790835008
Loss at iteration [707]: 0.002477500861275651
Loss at iteration [708]: 0.0024774492045098314
Loss at iteration [709]: 0.0024773974068995103
Loss at iteration [710]: 0.002477347140350696
Loss at iteration [711]: 0.0024772958987814035
Loss at iteration [712]: 0.002477246472456906
Loss at iteration [713]: 0.0024772005153027984
Loss at iteration [714]: 0.002477150583034502
Loss at iteration [715]: 0.00247710006931758
Loss at iteration [716]: 0.0024770459577398048
Loss at iteration [717]: 0.0024770004190364427
Loss at iteration [718]: 0.002476947980517514
Loss at iteration [719]: 0.002476893805338118
Loss at iteration [720]: 0.0024768424026159735
Loss at iteration [721]: 0.0024767933323281035
Loss at iteration [722]: 0.0024767437748735143
Loss at iteration [723]: 0.002476694350527717
Loss at iteration [724]: 0.00247664600885737
Loss at iteration [725]: 0.0024765976935781893
Loss at iteration [726]: 0.0024765464106537983
Loss at iteration [727]: 0.002476495913177075
Loss at iteration [728]: 0.002476446957640775
Loss at iteration [729]: 0.002476395302157679
Loss at iteration [730]: 0.0024763427178920347
Loss at iteration [731]: 0.0024762924489835957
Loss at iteration [732]: 0.0024762400674196413
Loss at iteration [733]: 0.002476191813999588
Loss at iteration [734]: 0.0024761413511675645
Loss at iteration [735]: 0.0024760917119136198
Loss at iteration [736]: 0.002476041324812225
Loss at iteration [737]: 0.0024759952215305373
Loss at iteration [738]: 0.002475947317571359
Loss at iteration [739]: 0.002475899496607585
Loss at iteration [740]: 0.0024758502465808832
Loss at iteration [741]: 0.0024758066060129764
Loss at iteration [742]: 0.0024757580186946003
Loss at iteration [743]: 0.0024757045196973065
Loss at iteration [744]: 0.0024756578871811623
Loss at iteration [745]: 0.002475607863384561
Loss at iteration [746]: 0.002475560493280644
Loss at iteration [747]: 0.0024755143306967424
Loss at iteration [748]: 0.002475463767598895
Loss at iteration [749]: 0.0024754120782554683
Loss at iteration [750]: 0.0024753614382624117
Loss at iteration [751]: 0.0024753130596274573
Loss at iteration [752]: 0.0024752621217867545
Loss at iteration [753]: 0.0024752168885277823
Loss at iteration [754]: 0.0024751663032544042
Loss at iteration [755]: 0.002475117169697308
Loss at iteration [756]: 0.0024750583330565845
Loss at iteration [757]: 0.002474997370953427
Loss at iteration [758]: 0.002474934144342133
Loss at iteration [759]: 0.0024748646838072003
Loss at iteration [760]: 0.0024747992813111603
Loss at iteration [761]: 0.0024747334741484234
Loss at iteration [762]: 0.0024746759632418612
Loss at iteration [763]: 0.002474617360079496
Loss at iteration [764]: 0.0024745606340057106
Loss at iteration [765]: 0.0024745037849761772
Loss at iteration [766]: 0.0024744485144934806
Loss at iteration [767]: 0.0024743932938195404
Loss at iteration [768]: 0.0024743303585803658
Loss at iteration [769]: 0.0024742540726470858
Loss at iteration [770]: 0.002474163546999423
Loss at iteration [771]: 0.0024740736121339306
Loss at iteration [772]: 0.002473989018514916
Loss at iteration [773]: 0.0024739039566162207
Loss at iteration [774]: 0.0024738218102012804
Loss at iteration [775]: 0.002473733570078155
Loss at iteration [776]: 0.0024736430019022372
Loss at iteration [777]: 0.0024735506571189423
Loss at iteration [778]: 0.002473462155637016
Loss at iteration [779]: 0.002473372112944323
Loss at iteration [780]: 0.0024732867342256402
Loss at iteration [781]: 0.0024731925004660175
Loss at iteration [782]: 0.0024730923598702916
Loss at iteration [783]: 0.002473001288781102
Loss at iteration [784]: 0.0024729176434713923
Loss at iteration [785]: 0.0024728341141642724
Loss at iteration [786]: 0.0024727519562595035
Loss at iteration [787]: 0.0024726775830479506
Loss at iteration [788]: 0.002472606394726136
Loss at iteration [789]: 0.0024725403857096444
Loss at iteration [790]: 0.0024724739075238805
Loss at iteration [791]: 0.0024724098190694456
Loss at iteration [792]: 0.0024723453352876166
Loss at iteration [793]: 0.002472281148569762
Loss at iteration [794]: 0.002472218793917481
Loss at iteration [795]: 0.0024721467528711965
Loss at iteration [796]: 0.002472076921022673
Loss at iteration [797]: 0.0024720012796230345
Loss at iteration [798]: 0.0024719254087404966
Loss at iteration [799]: 0.002471854704100484
Loss at iteration [800]: 0.0024717852549034107
Loss at iteration [801]: 0.0024717164505277047
Loss at iteration [802]: 0.0024716476820304464
Loss at iteration [803]: 0.0024715721109297233
Loss at iteration [804]: 0.002471525129260405
Loss at iteration [805]: 0.0024714762656122976
Loss at iteration [806]: 0.0024714195560864107
Loss at iteration [807]: 0.0024713643420109285
Loss at iteration [808]: 0.0024713201082245
Loss at iteration [809]: 0.00247127037541293
Loss at iteration [810]: 0.002471226881852933
Loss at iteration [811]: 0.0024711898974425837
Loss at iteration [812]: 0.0024711435237988015
Loss at iteration [813]: 0.0024710960048068757
Loss at iteration [814]: 0.002471063623808864
Loss at iteration [815]: 0.0024710118672301726
Loss at iteration [816]: 0.002470964894840631
Loss at iteration [817]: 0.0024709374863533737
Loss at iteration [818]: 0.0024708893527378754
Loss at iteration [819]: 0.002470825484168258
Loss at iteration [820]: 0.0024707702785777353
Loss at iteration [821]: 0.002470725643500712
Loss at iteration [822]: 0.002470672049692977
Loss at iteration [823]: 0.002470612286227687
Loss at iteration [824]: 0.002470551103171699
Loss at iteration [825]: 0.002470488294429486
Loss at iteration [826]: 0.0024704339710651837
Loss at iteration [827]: 0.002470392509189358
Loss at iteration [828]: 0.002470344461358153
Loss at iteration [829]: 0.002470291081186918
Loss at iteration [830]: 0.002470252467709059
Loss at iteration [831]: 0.002470200050444678
Loss at iteration [832]: 0.002470156314443238
Loss at iteration [833]: 0.002470123816602665
Loss at iteration [834]: 0.002470077201263316
Loss at iteration [835]: 0.00247002431324619
Loss at iteration [836]: 0.002469970456795195
Loss at iteration [837]: 0.0024699313307655446
Loss at iteration [838]: 0.0024698827229886846
Loss at iteration [839]: 0.002469829165139294
Loss at iteration [840]: 0.0024697886882817655
Loss at iteration [841]: 0.002469732585707489
Loss at iteration [842]: 0.0024696838241166402
Loss at iteration [843]: 0.002469648189447399
Loss at iteration [844]: 0.002469601162107568
Loss at iteration [845]: 0.0024695422952757
Loss at iteration [846]: 0.0024694900353921098
Loss at iteration [847]: 0.002469448263696378
Loss at iteration [848]: 0.0024693954234459962
Loss at iteration [849]: 0.0024693381482168764
Loss at iteration [850]: 0.002469295699500871
Loss at iteration [851]: 0.002469244482694922
Loss at iteration [852]: 0.002469194541296313
Loss at iteration [853]: 0.002469164129148748
Loss at iteration [854]: 0.0024691134712011404
Loss at iteration [855]: 0.0024690601326228558
Loss at iteration [856]: 0.0024690011933700216
Loss at iteration [857]: 0.0024689707779636904
Loss at iteration [858]: 0.002468921164257242
Loss at iteration [859]: 0.002468859685723636
Loss at iteration [860]: 0.0024688077748256402
Loss at iteration [861]: 0.0024687661427077105
Loss at iteration [862]: 0.0024687157342036473
Loss at iteration [863]: 0.0024686599832141657
Loss at iteration [864]: 0.002468619782505794
Loss at iteration [865]: 0.0024685686023167373
Loss at iteration [866]: 0.0024685230048161967
Loss at iteration [867]: 0.0024684896881445933
Loss at iteration [868]: 0.0024684403396783277
Loss at iteration [869]: 0.002468390335597216
Loss at iteration [870]: 0.0024683310056958004
Loss at iteration [871]: 0.0024682784047036664
Loss at iteration [872]: 0.002468217093840744
Loss at iteration [873]: 0.002468160025627435
Loss at iteration [874]: 0.002468104872822215
Loss at iteration [875]: 0.002468065399192937
Loss at iteration [876]: 0.0024680181007162667
Loss at iteration [877]: 0.002467964703211466
Loss at iteration [878]: 0.0024679173831207617
Loss at iteration [879]: 0.0024678776909390627
Loss at iteration [880]: 0.0024678331151960133
Loss at iteration [881]: 0.002467782432112992
Loss at iteration [882]: 0.00246773857058775
Loss at iteration [883]: 0.0024676887910839423
Loss at iteration [884]: 0.002467639453771266
Loss at iteration [885]: 0.0024675850582710768
Loss at iteration [886]: 0.002467545488371412
Loss at iteration [887]: 0.002467496402984703
Loss at iteration [888]: 0.0024674498330325674
Loss at iteration [889]: 0.002467391903481334
Loss at iteration [890]: 0.0024673521005557373
Loss at iteration [891]: 0.0024672946930004453
Loss at iteration [892]: 0.0024672472691011993
Loss at iteration [893]: 0.0024672086428542
Loss at iteration [894]: 0.0024671551762953012
Loss at iteration [895]: 0.0024670966549921545
Loss at iteration [896]: 0.0024670334947003974
Loss at iteration [897]: 0.002466974765513476
Loss at iteration [898]: 0.0024669098108863127
Loss at iteration [899]: 0.002466846439519262
Loss at iteration [900]: 0.002466790074743175
Loss at iteration [901]: 0.0024667463118027925
Loss at iteration [902]: 0.002466695530276534
Loss at iteration [903]: 0.0024666439688548975
Loss at iteration [904]: 0.0024665899990466158
Loss at iteration [905]: 0.0024665456205087656
Loss at iteration [906]: 0.0024664819634387376
Loss at iteration [907]: 0.0024664271387076836
Loss at iteration [908]: 0.0024663740071354133
Loss at iteration [909]: 0.0024663336234332076
Loss at iteration [910]: 0.0024662839834230536
Loss at iteration [911]: 0.002466233872897828
Loss at iteration [912]: 0.002466182142166209
Loss at iteration [913]: 0.002466141806764833
Loss at iteration [914]: 0.0024660953475738913
Loss at iteration [915]: 0.0024660466388855755
Loss at iteration [916]: 0.0024659961344471724
Loss at iteration [917]: 0.0024659566109128377
Loss at iteration [918]: 0.002465909720690943
Loss at iteration [919]: 0.0024658562680641327
Loss at iteration [920]: 0.002465805560389899
Loss at iteration [921]: 0.002465766788056154
Loss at iteration [922]: 0.0024657191826700387
Loss at iteration [923]: 0.0024656749497466274
Loss at iteration [924]: 0.0024656254807059663
Loss at iteration [925]: 0.002465583948490856
Loss at iteration [926]: 0.0024655345698631893
Loss at iteration [927]: 0.0024654802264991176
Loss at iteration [928]: 0.0024654230525293875
Loss at iteration [929]: 0.0024653786054819496
Loss at iteration [930]: 0.002465324256582668
Loss at iteration [931]: 0.002465266788627907
Loss at iteration [932]: 0.002465210661318177
Loss at iteration [933]: 0.0024651714389872636
Loss at iteration [934]: 0.0024651191535528657
Loss at iteration [935]: 0.002465060908175537
Loss at iteration [936]: 0.0024650040188609803
Loss at iteration [937]: 0.002464969524098616
Loss at iteration [938]: 0.002464918658936231
Loss at iteration [939]: 0.0024648683804103406
Loss at iteration [940]: 0.0024648112477815764
Loss at iteration [941]: 0.002464757106182663
Loss at iteration [942]: 0.0024646930675655703
Loss at iteration [943]: 0.0024646359011839115
Loss at iteration [944]: 0.0024645793496534288
Loss at iteration [945]: 0.0024645271282594584
Loss at iteration [946]: 0.0024644789573258466
Loss at iteration [947]: 0.0024644259050447086
Loss at iteration [948]: 0.0024643760334530523
Loss at iteration [949]: 0.00246432201289121
Loss at iteration [950]: 0.002464281984282969
Loss at iteration [951]: 0.0024642290869830124
Loss at iteration [952]: 0.00246417952424884
Loss at iteration [953]: 0.0024641280162033736
Loss at iteration [954]: 0.0024640861022503092
Loss at iteration [955]: 0.0024640384390057857
Loss at iteration [956]: 0.002463991648916314
Loss at iteration [957]: 0.0024639451267036956
Loss at iteration [958]: 0.0024638957630510917
Loss at iteration [959]: 0.002463854031786607
Loss at iteration [960]: 0.0024638090896036656
Loss at iteration [961]: 0.0024637538425146223
Loss at iteration [962]: 0.002463702238866566
Loss at iteration [963]: 0.0024636676129554695
Loss at iteration [964]: 0.0024636208088824247
Loss at iteration [965]: 0.0024635722706259815
Loss at iteration [966]: 0.002463518116408303
Loss at iteration [967]: 0.002463463553086994
Loss at iteration [968]: 0.0024633972229115586
Loss at iteration [969]: 0.0024633352736587355
Loss at iteration [970]: 0.0024632750879762344
Loss at iteration [971]: 0.0024632208500174678
Loss at iteration [972]: 0.002463179818373891
Loss at iteration [973]: 0.0024631242111424734
Loss at iteration [974]: 0.002463076827242126
Loss at iteration [975]: 0.0024630267519828663
Loss at iteration [976]: 0.0024629859946175077
Loss at iteration [977]: 0.002462944339774691
Loss at iteration [978]: 0.002462897235518697
Loss at iteration [979]: 0.002462846815141231
Loss at iteration [980]: 0.0024627986355121173
Loss at iteration [981]: 0.0024627581884922793
Loss at iteration [982]: 0.0024627127786764932
Loss at iteration [983]: 0.002462659478189515
Loss at iteration [984]: 0.0024626056613389798
Loss at iteration [985]: 0.002462556535656439
Loss at iteration [986]: 0.002462496975410285
Loss at iteration [987]: 0.0024624423783900116
Loss at iteration [988]: 0.002462390297922388
Loss at iteration [989]: 0.002462340385318565
Loss at iteration [990]: 0.0024623039946827345
Loss at iteration [991]: 0.0024622524419547318
Loss at iteration [992]: 0.002462209572851382
Loss at iteration [993]: 0.002462163067471543
Loss at iteration [994]: 0.0024621225291516333
Loss at iteration [995]: 0.002462086098375937
Loss at iteration [996]: 0.002462041890702894
Loss at iteration [997]: 0.0024619936577513738
Loss at iteration [998]: 0.0024619423711846468
Loss at iteration [999]: 0.0024618930591696563
Loss at iteration [1000]: 0.0024618391396362765
Loss at iteration [1001]: 0.0024617891825782817
Loss at iteration [1002]: 0.002461738021771504
Loss at iteration [1003]: 0.0024616888368044
Loss at iteration [1004]: 0.0024616517603471416
Loss at iteration [1005]: 0.0024616091682922387
Loss at iteration [1006]: 0.0024615730985433756
Loss at iteration [1007]: 0.0024615296112814266
Loss at iteration [1008]: 0.0024614853847417387
Loss at iteration [1009]: 0.002461453328728471
Loss at iteration [1010]: 0.0024614133738019087
Loss at iteration [1011]: 0.002461367930433239
Loss at iteration [1012]: 0.002461322289699816
Loss at iteration [1013]: 0.0024612739432728053
Loss at iteration [1014]: 0.002461243046946006
Loss at iteration [1015]: 0.0024611974043450497
Loss at iteration [1016]: 0.002461157042492936
Loss at iteration [1017]: 0.0024611114894180627
Loss at iteration [1018]: 0.002461073212363037
Loss at iteration [1019]: 0.0024610384256609375
Loss at iteration [1020]: 0.002461001616162192
Loss at iteration [1021]: 0.002460957073421034
Loss at iteration [1022]: 0.0024609085728256675
Loss at iteration [1023]: 0.0024608566329116653
Loss at iteration [1024]: 0.0024608259092721682
Loss at iteration [1025]: 0.0024607793478540335
Loss at iteration [1026]: 0.002460736264140255
Loss at iteration [1027]: 0.002460691835926305
Loss at iteration [1028]: 0.002460653398266334
Loss at iteration [1029]: 0.0024606138943096835
Loss at iteration [1030]: 0.0024605753555003106
Loss at iteration [1031]: 0.002460532789347306
Loss at iteration [1032]: 0.002460487218994449
Loss at iteration [1033]: 0.002460439388604175
Loss at iteration [1034]: 0.0024604126958057736
Loss at iteration [1035]: 0.002460362187494639
Loss at iteration [1036]: 0.0024603211167065616
Loss at iteration [1037]: 0.0024602775916273186
Loss at iteration [1038]: 0.0024602385309687716
Loss at iteration [1039]: 0.0024602051868419387
Loss at iteration [1040]: 0.0024601632726085638
Loss at iteration [1041]: 0.002460116010737209
Loss at iteration [1042]: 0.002460068595671698
Loss at iteration [1043]: 0.0024600206458305672
Loss at iteration [1044]: 0.002459991961140257
Loss at iteration [1045]: 0.002459946480255979
Loss at iteration [1046]: 0.002459907020639363
Loss at iteration [1047]: 0.002459859270672628
Loss at iteration [1048]: 0.0024598171698836705
Loss at iteration [1049]: 0.002459788228402907
Loss at iteration [1050]: 0.002459747973052086
Loss at iteration [1051]: 0.0024597203673501398
Loss at iteration [1052]: 0.0024596757881268894
Loss at iteration [1053]: 0.0024596292747989337
Loss at iteration [1054]: 0.002459594250847348
Loss at iteration [1055]: 0.0024595486042291066
Loss at iteration [1056]: 0.002459503633425958
Loss at iteration [1057]: 0.0024594549052946906
Loss at iteration [1058]: 0.0024594067774272015
Loss at iteration [1059]: 0.0024593700358752366
Loss at iteration [1060]: 0.002459327657660479
Loss at iteration [1061]: 0.002459278964843659
Loss at iteration [1062]: 0.0024592330002586567
Loss at iteration [1063]: 0.002459186939517902
Loss at iteration [1064]: 0.002459145524369288
Loss at iteration [1065]: 0.002459107633246846
Loss at iteration [1066]: 0.0024590665134457458
Loss at iteration [1067]: 0.0024590248890731994
Loss at iteration [1068]: 0.0024589765754967747
Loss at iteration [1069]: 0.002458930062211259
Loss at iteration [1070]: 0.0024589012362307883
Loss at iteration [1071]: 0.0024588558128746307
Loss at iteration [1072]: 0.002458816274600288
Loss at iteration [1073]: 0.0024587744435709356
Loss at iteration [1074]: 0.002458729150006341
Loss at iteration [1075]: 0.0024586981326363694
Loss at iteration [1076]: 0.0024586581376986115
Loss at iteration [1077]: 0.002458614351754427
Loss at iteration [1078]: 0.0024585672461025936
Loss at iteration [1079]: 0.00245852225462206
Loss at iteration [1080]: 0.0024584783856099273
Loss at iteration [1081]: 0.0024584426790149566
Loss at iteration [1082]: 0.0024584022913681945
Loss at iteration [1083]: 0.002458358318517382
Loss at iteration [1084]: 0.0024583124788377353
Loss at iteration [1085]: 0.002458264480532239
Loss at iteration [1086]: 0.0024582262114558476
Loss at iteration [1087]: 0.0024581862930497986
Loss at iteration [1088]: 0.0024581445955515376
Loss at iteration [1089]: 0.002458100608632065
Loss at iteration [1090]: 0.002458054320664314
Loss at iteration [1091]: 0.00245800513908658
Loss at iteration [1092]: 0.0024579729479474706
Loss at iteration [1093]: 0.002457929060014234
Loss at iteration [1094]: 0.0024578890149163915
Loss at iteration [1095]: 0.002457846541310808
Loss at iteration [1096]: 0.0024578055693179396
Loss at iteration [1097]: 0.0024577737536114327
Loss at iteration [1098]: 0.002457733418797286
Loss at iteration [1099]: 0.0024576920290233554
Loss at iteration [1100]: 0.00245764254116114
Loss at iteration [1101]: 0.002457596433413158
Loss at iteration [1102]: 0.002457550749695834
Loss at iteration [1103]: 0.0024575186398322676
Loss at iteration [1104]: 0.0024574767080420773
Loss at iteration [1105]: 0.002457436225503552
Loss at iteration [1106]: 0.0024573897815403033
Loss at iteration [1107]: 0.0024573378741554264
Loss at iteration [1108]: 0.002457296418390185
Loss at iteration [1109]: 0.00245726019079348
Loss at iteration [1110]: 0.0024572209454489656
Loss at iteration [1111]: 0.0024571773798936956
Loss at iteration [1112]: 0.0024571347913973003
Loss at iteration [1113]: 0.0024570882233408663
Loss at iteration [1114]: 0.002457051548048872
Loss at iteration [1115]: 0.0024570146894710705
Loss at iteration [1116]: 0.002456975455537494
Loss at iteration [1117]: 0.00245692876315484
Loss at iteration [1118]: 0.002456883969090631
Loss at iteration [1119]: 0.0024568366668171835
Loss at iteration [1120]: 0.002456801375445659
Loss at iteration [1121]: 0.00245676415325827
Loss at iteration [1122]: 0.0024567229342368716
Loss at iteration [1123]: 0.002456676932650866
Loss at iteration [1124]: 0.0024566278133293902
Loss at iteration [1125]: 0.0024565844937920757
Loss at iteration [1126]: 0.002456550952322712
Loss at iteration [1127]: 0.002456506318882534
Loss at iteration [1128]: 0.0024564686940203133
Loss at iteration [1129]: 0.002456427576759729
Loss at iteration [1130]: 0.0024563819931744464
Loss at iteration [1131]: 0.0024563348460791244
Loss at iteration [1132]: 0.0024563005699078182
Loss at iteration [1133]: 0.002456260425493523
Loss at iteration [1134]: 0.002456222611896112
Loss at iteration [1135]: 0.0024561832686210957
Loss at iteration [1136]: 0.002456139612990711
Loss at iteration [1137]: 0.002456091662763166
Loss at iteration [1138]: 0.00245605817797637
Loss at iteration [1139]: 0.0024560174754561944
Loss at iteration [1140]: 0.002455977300226695
Loss at iteration [1141]: 0.002455932817442836
Loss at iteration [1142]: 0.0024558867293280823
Loss at iteration [1143]: 0.0024558492439574076
Loss at iteration [1144]: 0.002455821904993419
Loss at iteration [1145]: 0.0024557894699939937
Loss at iteration [1146]: 0.0024557515978630715
Loss at iteration [1147]: 0.0024557066031522614
Loss at iteration [1148]: 0.00245565853650037
Loss at iteration [1149]: 0.00245560790873559
Loss at iteration [1150]: 0.0024555714884979827
Loss at iteration [1151]: 0.0024555343418922386
Loss at iteration [1152]: 0.002455494765922738
Loss at iteration [1153]: 0.002455465467295382
Loss at iteration [1154]: 0.002455423460969275
Loss at iteration [1155]: 0.002455377938928409
Loss at iteration [1156]: 0.0024553379254258554
Loss at iteration [1157]: 0.002455307035627026
Loss at iteration [1158]: 0.0024552678333486133
Loss at iteration [1159]: 0.002455222614591103
Loss at iteration [1160]: 0.002455177303342034
Loss at iteration [1161]: 0.002455138624722384
Loss at iteration [1162]: 0.00245510424326783
Loss at iteration [1163]: 0.002455079896370453
Loss at iteration [1164]: 0.002455042690785603
Loss at iteration [1165]: 0.002454996898849263
Loss at iteration [1166]: 0.0024549500314310388
Loss at iteration [1167]: 0.002454905375281122
Loss at iteration [1168]: 0.0024548602642241683
Loss at iteration [1169]: 0.0024548451381362787
Loss at iteration [1170]: 0.0024548131313081698
Loss at iteration [1171]: 0.002454779783717276
Loss at iteration [1172]: 0.0024547381757086656
Loss at iteration [1173]: 0.0024546942952244476
Loss at iteration [1174]: 0.0024546453234480046
Loss at iteration [1175]: 0.002454599909457335
Loss at iteration [1176]: 0.002454569504980601
Loss at iteration [1177]: 0.002454531055195918
Loss at iteration [1178]: 0.0024544910234623814
Loss at iteration [1179]: 0.002454453992708233
Loss at iteration [1180]: 0.0024544112726619764
Loss at iteration [1181]: 0.0024543712477005804
Loss at iteration [1182]: 0.0024543304144182903
Loss at iteration [1183]: 0.002454295968935694
Loss at iteration [1184]: 0.002454254535707912
Loss at iteration [1185]: 0.0024542096324099228
Loss at iteration [1186]: 0.0024541623921008108
Loss at iteration [1187]: 0.0024541267991568575
Loss at iteration [1188]: 0.0024540819164414335
Loss at iteration [1189]: 0.0024540380734483828
Loss at iteration [1190]: 0.0024540082260296834
Loss at iteration [1191]: 0.0024539716878179975
Loss at iteration [1192]: 0.002453936418704215
Loss at iteration [1193]: 0.0024538938900344354
Loss at iteration [1194]: 0.0024538519312986913
Loss at iteration [1195]: 0.0024538147328123762
Loss at iteration [1196]: 0.002453774841126724
Loss at iteration [1197]: 0.002453744292504489
Loss at iteration [1198]: 0.0024537073078490135
Loss at iteration [1199]: 0.0024536631927319645
Loss at iteration [1200]: 0.002453617984671172
Loss at iteration [1201]: 0.0024535793634940456
Loss at iteration [1202]: 0.0024535326930604662
Loss at iteration [1203]: 0.0024534897641611524
Loss at iteration [1204]: 0.0024534599048146493
Loss at iteration [1205]: 0.0024534214555390816
Loss at iteration [1206]: 0.002453381578336408
Loss at iteration [1207]: 0.0024533403153327174
Loss at iteration [1208]: 0.002453293110791469
Loss at iteration [1209]: 0.0024532503374006167
Loss at iteration [1210]: 0.002453215337615057
Loss at iteration [1211]: 0.0024531953070009416
Loss at iteration [1212]: 0.0024531570698463276
Loss at iteration [1213]: 0.002453110916564333
Loss at iteration [1214]: 0.0024530600646022757
Loss at iteration [1215]: 0.002453010262906988
Loss at iteration [1216]: 0.0024529664818178777
Loss at iteration [1217]: 0.002452935065753193
Loss at iteration [1218]: 0.002452911197463766
Loss at iteration [1219]: 0.002452867394290317
Loss at iteration [1220]: 0.002452830475872076
Loss at iteration [1221]: 0.002452788807774401
Loss at iteration [1222]: 0.0024527418492148846
Loss at iteration [1223]: 0.002452702172756509
Loss at iteration [1224]: 0.002452662281577781
Loss at iteration [1225]: 0.0024526381581715996
Loss at iteration [1226]: 0.0024525989932428303
Loss at iteration [1227]: 0.0024525563488774407
Loss at iteration [1228]: 0.002452505279855507
Loss at iteration [1229]: 0.0024524580223870933
Loss at iteration [1230]: 0.0024524087559625505
Loss at iteration [1231]: 0.002452367713686759
Loss at iteration [1232]: 0.0024523425331688313
Loss at iteration [1233]: 0.0024523173750313557
Loss at iteration [1234]: 0.002452283103341302
Loss at iteration [1235]: 0.0024522435133054006
Loss at iteration [1236]: 0.0024521956155665065
Loss at iteration [1237]: 0.0024521443876204256
Loss at iteration [1238]: 0.002452095206719608
Loss at iteration [1239]: 0.0024520843664400265
Loss at iteration [1240]: 0.0024520531304012593
Loss at iteration [1241]: 0.0024520133338371266
Loss at iteration [1242]: 0.0024519675417412445
Loss at iteration [1243]: 0.002451915266975973
Loss at iteration [1244]: 0.002451867403200229
Loss at iteration [1245]: 0.0024518271183927463
Loss at iteration [1246]: 0.0024517907027091904
Loss at iteration [1247]: 0.0024517708114348007
Loss at iteration [1248]: 0.0024517372663078745
Loss at iteration [1249]: 0.002451690949388331
Loss at iteration [1250]: 0.0024516445683449074
Loss at iteration [1251]: 0.0024516040148715433
Loss at iteration [1252]: 0.002451559018702383
Loss at iteration [1253]: 0.002451513233127054
Loss at iteration [1254]: 0.0024514780678718995
Loss at iteration [1255]: 0.0024514462796047746
Loss at iteration [1256]: 0.002451414278443558
Loss at iteration [1257]: 0.0024513733256257253
Loss at iteration [1258]: 0.002451327242378653
Loss at iteration [1259]: 0.002451289175082636
Loss at iteration [1260]: 0.0024512424684032443
Loss at iteration [1261]: 0.002451194891126341
Loss at iteration [1262]: 0.002451159991302357
Loss at iteration [1263]: 0.0024511313271364504
Loss at iteration [1264]: 0.0024510998314685476
Loss at iteration [1265]: 0.0024510600562313153
Loss at iteration [1266]: 0.002451018980853843
Loss at iteration [1267]: 0.0024509701087334075
Loss at iteration [1268]: 0.0024509231297400805
Loss at iteration [1269]: 0.0024508829245648085
Loss at iteration [1270]: 0.002450852411333463
Loss at iteration [1271]: 0.0024508292284450704
Loss at iteration [1272]: 0.002450798820168431
Loss at iteration [1273]: 0.0024507547087258617
Loss at iteration [1274]: 0.002450711973392317
Loss at iteration [1275]: 0.002450667423227892
Loss at iteration [1276]: 0.002450621624102708
Loss at iteration [1277]: 0.0024505820224919417
Loss at iteration [1278]: 0.00245054894676662
Loss at iteration [1279]: 0.002450525050621969
Loss at iteration [1280]: 0.0024504929512555226
Loss at iteration [1281]: 0.0024504510883570875
Loss at iteration [1282]: 0.0024504031580399504
Loss at iteration [1283]: 0.00245036290993696
Loss at iteration [1284]: 0.0024503183865643846
Loss at iteration [1285]: 0.002450274836863904
Loss at iteration [1286]: 0.002450238494406394
Loss at iteration [1287]: 0.002450199551446288
Loss at iteration [1288]: 0.002450164309357017
Loss at iteration [1289]: 0.002450129625710291
Loss at iteration [1290]: 0.0024500903603636064
Loss at iteration [1291]: 0.002450054725035691
Loss at iteration [1292]: 0.002450014292418471
Loss at iteration [1293]: 0.0024499765961816956
Loss at iteration [1294]: 0.0024499456137739494
Loss at iteration [1295]: 0.0024499149348639654
Loss at iteration [1296]: 0.002449881026694099
Loss at iteration [1297]: 0.0024498488512762337
Loss at iteration [1298]: 0.002449804421363794
Loss at iteration [1299]: 0.0024497598900015695
Loss at iteration [1300]: 0.0024497138578477512
Loss at iteration [1301]: 0.002449667092137338
Loss at iteration [1302]: 0.0024496280932280365
Loss at iteration [1303]: 0.002449595898900686
Loss at iteration [1304]: 0.0024495585089332602
Loss at iteration [1305]: 0.0024495179250956962
Loss at iteration [1306]: 0.0024494714722082447
Loss at iteration [1307]: 0.00244943229582988
Loss at iteration [1308]: 0.0024493910506976097
Loss at iteration [1309]: 0.002449349268387726
Loss at iteration [1310]: 0.002449311588525398
Loss at iteration [1311]: 0.002449279837749396
Loss at iteration [1312]: 0.00244924237845703
Loss at iteration [1313]: 0.0024492087002231373
Loss at iteration [1314]: 0.002449167885615847
Loss at iteration [1315]: 0.0024491270118275014
Loss at iteration [1316]: 0.0024490832178504646
Loss at iteration [1317]: 0.002449048278144926
Loss at iteration [1318]: 0.0024490083737590713
Loss at iteration [1319]: 0.00244898472566632
Loss at iteration [1320]: 0.002448949006509457
Loss at iteration [1321]: 0.0024489156308509417
Loss at iteration [1322]: 0.0024488721955003567
Loss at iteration [1323]: 0.002448830650726222
Loss at iteration [1324]: 0.0024487861301393144
Loss at iteration [1325]: 0.002448744238205318
Loss at iteration [1326]: 0.0024486993119783736
Loss at iteration [1327]: 0.0024486555393756103
Loss at iteration [1328]: 0.0024486192335041353
Loss at iteration [1329]: 0.002448588117979438
Loss at iteration [1330]: 0.0024485544020296304
Loss at iteration [1331]: 0.002448525043495254
Loss at iteration [1332]: 0.002448485534470931
Loss at iteration [1333]: 0.0024484437072563054
Loss at iteration [1334]: 0.002448402499183994
Loss at iteration [1335]: 0.002448368888221198
Loss at iteration [1336]: 0.0024483287359253867
Loss at iteration [1337]: 0.002448287555606834
Loss at iteration [1338]: 0.002448252189600351
Loss at iteration [1339]: 0.002448235184710783
Loss at iteration [1340]: 0.0024482041466787436
Loss at iteration [1341]: 0.0024481669636737423
Loss at iteration [1342]: 0.002448121765595749
Loss at iteration [1343]: 0.0024480840894771892
Loss at iteration [1344]: 0.00244804510306288
Loss at iteration [1345]: 0.0024480083678220495
Loss at iteration [1346]: 0.002447969648638793
Loss at iteration [1347]: 0.0024479312334731236
Loss at iteration [1348]: 0.0024478939881887336
Loss at iteration [1349]: 0.0024478608675520016
Loss at iteration [1350]: 0.0024478290654410613
Loss at iteration [1351]: 0.0024477943697644943
Loss at iteration [1352]: 0.002447755420199128
Loss at iteration [1353]: 0.0024477217660006014
Loss at iteration [1354]: 0.002447682873763168
Loss at iteration [1355]: 0.0024476467202430474
Loss at iteration [1356]: 0.0024476046595015653
Loss at iteration [1357]: 0.002447566040456151
Loss at iteration [1358]: 0.0024475313261664946
Loss at iteration [1359]: 0.0024475088978538405
Loss at iteration [1360]: 0.002447475087680314
Loss at iteration [1361]: 0.0024474412306460143
Loss at iteration [1362]: 0.0024474056412724346
Loss at iteration [1363]: 0.0024473824703545363
Loss at iteration [1364]: 0.0024473505055919676
Loss at iteration [1365]: 0.0024473116855719887
Loss at iteration [1366]: 0.0024472704007508827
Loss at iteration [1367]: 0.0024472292813358127
Loss at iteration [1368]: 0.002447188775411438
Loss at iteration [1369]: 0.002447161409395715
Loss at iteration [1370]: 0.0024471268414141315
Loss at iteration [1371]: 0.002447095556366851
Loss at iteration [1372]: 0.0024470567699841354
Loss at iteration [1373]: 0.002447025107680261
Loss at iteration [1374]: 0.002446985937529835
Loss at iteration [1375]: 0.0024469560888695395
Loss at iteration [1376]: 0.002446915604740193
Loss at iteration [1377]: 0.0024468730932506815
Loss at iteration [1378]: 0.0024468365156394955
Loss at iteration [1379]: 0.0024468044042468226
Loss at iteration [1380]: 0.002446768817913898
Loss at iteration [1381]: 0.0024467466505251607
Loss at iteration [1382]: 0.0024467157515718854
Loss at iteration [1383]: 0.002446683056283536
Loss at iteration [1384]: 0.0024466425859727757
Loss at iteration [1385]: 0.00244660367099586
Loss at iteration [1386]: 0.0024465631203001007
Loss at iteration [1387]: 0.0024465324118778265
Loss at iteration [1388]: 0.0024464933944065835
Loss at iteration [1389]: 0.0024464584460780333
Loss at iteration [1390]: 0.0024464206644878414
Loss at iteration [1391]: 0.0024463823180369618
Loss at iteration [1392]: 0.00244634162391301
Loss at iteration [1393]: 0.0024463217437046543
Loss at iteration [1394]: 0.0024462897425720554
Loss at iteration [1395]: 0.002446262831724426
Loss at iteration [1396]: 0.002446227455682709
Loss at iteration [1397]: 0.0024461890910228916
Loss at iteration [1398]: 0.002446151456904454
Loss at iteration [1399]: 0.002446111665346258
Loss at iteration [1400]: 0.0024460741903431804
Loss at iteration [1401]: 0.0024460353877749324
Loss at iteration [1402]: 0.002445996252784277
Loss at iteration [1403]: 0.0024459600201707154
Loss at iteration [1404]: 0.002445921698457686
Loss at iteration [1405]: 0.0024458834678321596
Loss at iteration [1406]: 0.0024458475739638024
Loss at iteration [1407]: 0.0024458383225056312
Loss at iteration [1408]: 0.0024458055812668273
Loss at iteration [1409]: 0.0024457672358310445
Loss at iteration [1410]: 0.0024457275905821587
Loss at iteration [1411]: 0.0024456956674792463
Loss at iteration [1412]: 0.0024456601907274795
Loss at iteration [1413]: 0.002445628672761794
Loss at iteration [1414]: 0.0024455935489942383
Loss at iteration [1415]: 0.002445558251257927
Loss at iteration [1416]: 0.002445517431232338
Loss at iteration [1417]: 0.0024454754366701565
Loss at iteration [1418]: 0.0024454379081628227
Loss at iteration [1419]: 0.0024454063472086124
Loss at iteration [1420]: 0.0024453699955715
Loss at iteration [1421]: 0.0024453334917054967
Loss at iteration [1422]: 0.002445296688012143
Loss at iteration [1423]: 0.002445262111267486
Loss at iteration [1424]: 0.0024452299702819426
Loss at iteration [1425]: 0.00244520895000601
Loss at iteration [1426]: 0.0024451809992215123
Loss at iteration [1427]: 0.0024451465921417007
Loss at iteration [1428]: 0.0024451097716473623
Loss at iteration [1429]: 0.0024450859780413992
Loss at iteration [1430]: 0.0024450552575295105
Loss at iteration [1431]: 0.002445019473367647
Loss at iteration [1432]: 0.002444978021993732
Loss at iteration [1433]: 0.0024449348961913296
Loss at iteration [1434]: 0.002444896104967808
Loss at iteration [1435]: 0.002444873442406238
Loss at iteration [1436]: 0.0024448418370939824
Loss at iteration [1437]: 0.00244480653199522
Loss at iteration [1438]: 0.002444768836116862
Loss at iteration [1439]: 0.002444728111836539
Loss at iteration [1440]: 0.0024446886476097076
Loss at iteration [1441]: 0.0024446657267259185
Loss at iteration [1442]: 0.0024446362000042935
Loss at iteration [1443]: 0.0024446052235392673
Loss at iteration [1444]: 0.0024445681295052475
Loss at iteration [1445]: 0.0024445342577331924
Loss at iteration [1446]: 0.002444498312744015
Loss at iteration [1447]: 0.0024444705137602854
Loss at iteration [1448]: 0.002444434097319403
Loss at iteration [1449]: 0.002444395360706525
Loss at iteration [1450]: 0.0024443594915610793
Loss at iteration [1451]: 0.0024443245493153904
Loss at iteration [1452]: 0.002444287504288987
Loss at iteration [1453]: 0.002444252581561223
Loss at iteration [1454]: 0.0024442184977429083
Loss at iteration [1455]: 0.0024441807126163607
Loss at iteration [1456]: 0.002444143781012972
Loss at iteration [1457]: 0.0024441168510302
Loss at iteration [1458]: 0.00244408152807835
Loss at iteration [1459]: 0.0024440605529658438
Loss at iteration [1460]: 0.0024440339720306595
Loss at iteration [1461]: 0.0024440075129978497
Loss at iteration [1462]: 0.002443977119519456
Loss at iteration [1463]: 0.00244394777902593
Loss at iteration [1464]: 0.002443910856367343
Loss at iteration [1465]: 0.0024438711518564565
Loss at iteration [1466]: 0.00244383057436528
Loss at iteration [1467]: 0.002443791383810312
Loss at iteration [1468]: 0.002443754556034323
Loss at iteration [1469]: 0.002443728979906806
Loss at iteration [1470]: 0.002443695722112022
Loss at iteration [1471]: 0.0024436602367968346
Loss at iteration [1472]: 0.002443624800455199
Loss at iteration [1473]: 0.0024435888530240984
Loss at iteration [1474]: 0.0024435491719710096
Loss at iteration [1475]: 0.0024435083752449693
Loss at iteration [1476]: 0.0024434714943933197
Loss at iteration [1477]: 0.0024434463107898337
Loss at iteration [1478]: 0.00244341237935809
Loss at iteration [1479]: 0.0024433786061269294
Loss at iteration [1480]: 0.002443342398456157
Loss at iteration [1481]: 0.0024433200993204222
Loss at iteration [1482]: 0.0024432911822290207
Loss at iteration [1483]: 0.002443261395446469
Loss at iteration [1484]: 0.0024432297924993633
Loss at iteration [1485]: 0.002443200961707436
Loss at iteration [1486]: 0.0024431662559931413
Loss at iteration [1487]: 0.0024431290373941486
Loss at iteration [1488]: 0.0024430876069781037
Loss at iteration [1489]: 0.0024430556804881658
Loss at iteration [1490]: 0.0024430169099468016
Loss at iteration [1491]: 0.002442975906392731
Loss at iteration [1492]: 0.0024429340075672545
Loss at iteration [1493]: 0.002442903613490898
Loss at iteration [1494]: 0.002442867213191618
Loss at iteration [1495]: 0.0024428263673103357
Loss at iteration [1496]: 0.002442789112255403
Loss at iteration [1497]: 0.0024427573318539
Loss at iteration [1498]: 0.0024427227496137812
Loss at iteration [1499]: 0.0024426878552599547
Loss at iteration [1500]: 0.0024426477152669985
Loss at iteration [1501]: 0.002442610049987637
Loss at iteration [1502]: 0.0024425740922838692
Loss at iteration [1503]: 0.0024425424512394566
Loss at iteration [1504]: 0.0024425102690880584
Loss at iteration [1505]: 0.0024424752288264603
Loss at iteration [1506]: 0.002442437074258785
Loss at iteration [1507]: 0.002442405217436068
Loss at iteration [1508]: 0.0024423668874126564
Loss at iteration [1509]: 0.002442344547930291
Loss at iteration [1510]: 0.002442317815941368
Loss at iteration [1511]: 0.0024422957130825207
Loss at iteration [1512]: 0.002442267164644131
Loss at iteration [1513]: 0.002442241951099166
Loss at iteration [1514]: 0.002442208026498837
Loss at iteration [1515]: 0.0024421872255655686
Loss at iteration [1516]: 0.002442155638391761
Loss at iteration [1517]: 0.00244212681665303
Loss at iteration [1518]: 0.0024420907772629294
Loss at iteration [1519]: 0.0024420645511146408
Loss at iteration [1520]: 0.002442031342803121
Loss at iteration [1521]: 0.0024419944391506766
Loss at iteration [1522]: 0.0024419555462585904
Loss at iteration [1523]: 0.0024419153271957902
Loss at iteration [1524]: 0.002441875532214235
Loss at iteration [1525]: 0.002441845555936024
Loss at iteration [1526]: 0.002441811332821706
Loss at iteration [1527]: 0.0024417731922412025
Loss at iteration [1528]: 0.0024417342626247727
Loss at iteration [1529]: 0.0024417045815262983
Loss at iteration [1530]: 0.00244167378826998
Loss at iteration [1531]: 0.0024416449642852807
Loss at iteration [1532]: 0.0024416121519138948
Loss at iteration [1533]: 0.0024415816753086883
Loss at iteration [1534]: 0.002441548245374818
Loss at iteration [1535]: 0.002441514751864983
Loss at iteration [1536]: 0.002441480175539272
Loss at iteration [1537]: 0.0024414562812788997
Loss at iteration [1538]: 0.0024414279495255176
Loss at iteration [1539]: 0.0024413998354546923
Loss at iteration [1540]: 0.002441368323047979
Loss at iteration [1541]: 0.0024413384908952766
Loss at iteration [1542]: 0.0024413048440332535
Loss at iteration [1543]: 0.002441272226572334
Loss at iteration [1544]: 0.0024412380181951794
Loss at iteration [1545]: 0.0024412129305956035
Loss at iteration [1546]: 0.0024411873332417524
Loss at iteration [1547]: 0.002441159642756597
Loss at iteration [1548]: 0.0024411332750037783
Loss at iteration [1549]: 0.0024411023479378804
Loss at iteration [1550]: 0.0024410677683003182
Loss at iteration [1551]: 0.002441043849453798
Loss at iteration [1552]: 0.002441012357761442
Loss at iteration [1553]: 0.002440980832142614
Loss at iteration [1554]: 0.0024409473802092857
Loss at iteration [1555]: 0.002440914810701357
Loss at iteration [1556]: 0.002440882225742029
Loss at iteration [1557]: 0.002440850286702007
Loss at iteration [1558]: 0.0024408131941802036
Loss at iteration [1559]: 0.0024407897485651106
Loss at iteration [1560]: 0.002440759068745341
Loss at iteration [1561]: 0.002440726911953619
Loss at iteration [1562]: 0.0024406921871866533
Loss at iteration [1563]: 0.002440660960778295
Loss at iteration [1564]: 0.0024406279297573656
Loss at iteration [1565]: 0.0024406024465418318
Loss at iteration [1566]: 0.0024405707183937572
Loss at iteration [1567]: 0.002440540505778082
Loss at iteration [1568]: 0.002440509142286554
Loss at iteration [1569]: 0.0024404752087848164
Loss at iteration [1570]: 0.002440441578359183
Loss at iteration [1571]: 0.0024404156726222453
Loss at iteration [1572]: 0.0024403867230489693
Loss at iteration [1573]: 0.002440351865573106
Loss at iteration [1574]: 0.0024403154806369684
Loss at iteration [1575]: 0.002440289761623902
Loss at iteration [1576]: 0.0024402559435088055
Loss at iteration [1577]: 0.0024402305131152684
Loss at iteration [1578]: 0.0024402000631312325
Loss at iteration [1579]: 0.002440163471024485
Loss at iteration [1580]: 0.0024401264953440452
Loss at iteration [1581]: 0.002440093451133642
Loss at iteration [1582]: 0.002440062486281986
Loss at iteration [1583]: 0.002440043720045423
Loss at iteration [1584]: 0.0024400164247626666
Loss at iteration [1585]: 0.0024399878827705
Loss at iteration [1586]: 0.0024399553116067497
Loss at iteration [1587]: 0.0024399196460022183
Loss at iteration [1588]: 0.0024398825866073824
Loss at iteration [1589]: 0.0024398544455416313
Loss at iteration [1590]: 0.0024398219320550143
Loss at iteration [1591]: 0.002439790814504702
Loss at iteration [1592]: 0.0024397557153458464
Loss at iteration [1593]: 0.0024397258890621132
Loss at iteration [1594]: 0.0024396979499132668
Loss at iteration [1595]: 0.0024396660768794755
Loss at iteration [1596]: 0.0024396336288089117
Loss at iteration [1597]: 0.0024396069128302575
Loss at iteration [1598]: 0.002439578665091475
Loss at iteration [1599]: 0.002439564396749303
Loss at iteration [1600]: 0.0024395387592823707
Loss at iteration [1601]: 0.0024395226019286874
Loss at iteration [1602]: 0.0024394982825580416
Loss at iteration [1603]: 0.0024394668516638687
Loss at iteration [1604]: 0.002439427888680204
Loss at iteration [1605]: 0.002439400833219335
Loss at iteration [1606]: 0.0024393641568578525
Loss at iteration [1607]: 0.0024393285579461807
Loss at iteration [1608]: 0.002439290166391993
Loss at iteration [1609]: 0.002439256916984148
Loss at iteration [1610]: 0.0024392239514051187
Loss at iteration [1611]: 0.002439200733381124
Loss at iteration [1612]: 0.002439166173842752
Loss at iteration [1613]: 0.002439130305488139
Loss at iteration [1614]: 0.00243909568313645
Loss at iteration [1615]: 0.002439059603691977
Loss at iteration [1616]: 0.0024390217876831296
Loss at iteration [1617]: 0.002438996730401605
Loss at iteration [1618]: 0.002438968763380466
Loss at iteration [1619]: 0.0024389467680773565
Loss at iteration [1620]: 0.0024389151603406026
Loss at iteration [1621]: 0.002438879491374902
Loss at iteration [1622]: 0.0024388426450948062
Loss at iteration [1623]: 0.0024388106797428232
Loss at iteration [1624]: 0.002438776355422245
Loss at iteration [1625]: 0.0024387610022884474
Loss at iteration [1626]: 0.002438738149146839
Loss at iteration [1627]: 0.002438702658823276
Loss at iteration [1628]: 0.0024386679425237403
Loss at iteration [1629]: 0.0024386318205618196
Loss at iteration [1630]: 0.0024385959514273492
Loss at iteration [1631]: 0.002438578680922152
Loss at iteration [1632]: 0.0024385495969873818
Loss at iteration [1633]: 0.0024385162344419224
Loss at iteration [1634]: 0.0024384843316347968
Loss at iteration [1635]: 0.0024384724419153053
Loss at iteration [1636]: 0.002438448779029996
Loss at iteration [1637]: 0.002438421675030225
Loss at iteration [1638]: 0.0024383872664646066
Loss at iteration [1639]: 0.0024383645097770124
Loss at iteration [1640]: 0.0024383317162148736
Loss at iteration [1641]: 0.0024383076961377458
Loss at iteration [1642]: 0.0024382745272214894
Loss at iteration [1643]: 0.0024382418683095634
Loss at iteration [1644]: 0.0024382022244308042
Loss at iteration [1645]: 0.0024381792957429054
Loss at iteration [1646]: 0.0024381451269126293
Loss at iteration [1647]: 0.0024381163329183957
Loss at iteration [1648]: 0.0024380744263665043
Loss at iteration [1649]: 0.0024380398398882344
Loss at iteration [1650]: 0.0024379959713723993
Loss at iteration [1651]: 0.002437967096170042
Loss at iteration [1652]: 0.0024379339104989538
Loss at iteration [1653]: 0.00243790579252734
Loss at iteration [1654]: 0.002437872734030147
Loss at iteration [1655]: 0.0024378313423153502
Loss at iteration [1656]: 0.00243779949944662
Loss at iteration [1657]: 0.0024377725096337267
Loss at iteration [1658]: 0.00243773986987771
Loss at iteration [1659]: 0.0024377055340970705
Loss at iteration [1660]: 0.0024376705800177624
Loss at iteration [1661]: 0.0024376384199399576
Loss at iteration [1662]: 0.002437609498712003
Loss at iteration [1663]: 0.002437575420023299
Loss at iteration [1664]: 0.0024375456687670215
Loss at iteration [1665]: 0.0024375193546402087
Loss at iteration [1666]: 0.0024374872185102823
Loss at iteration [1667]: 0.002437453135733058
Loss at iteration [1668]: 0.0024374208593583822
Loss at iteration [1669]: 0.00243739217414509
Loss at iteration [1670]: 0.002437363940015627
Loss at iteration [1671]: 0.0024373392080498397
Loss at iteration [1672]: 0.002437310838200781
Loss at iteration [1673]: 0.0024372765592417557
Loss at iteration [1674]: 0.0024372424789828673
Loss at iteration [1675]: 0.002437206792087548
Loss at iteration [1676]: 0.0024371744009069915
Loss at iteration [1677]: 0.002437171233526
Loss at iteration [1678]: 0.0024371542157145105
Loss at iteration [1679]: 0.0024371344460748768
Loss at iteration [1680]: 0.0024370974499112285
Loss at iteration [1681]: 0.002437059335782267
Loss at iteration [1682]: 0.0024370173098534143
Loss at iteration [1683]: 0.002437004418240118
Loss at iteration [1684]: 0.0024369709872643576
Loss at iteration [1685]: 0.0024369380131801783
Loss at iteration [1686]: 0.002436899593276966
Loss at iteration [1687]: 0.002436869392253712
Loss at iteration [1688]: 0.002436829528782585
Loss at iteration [1689]: 0.0024367927255847795
Loss at iteration [1690]: 0.0024367505700824887
Loss at iteration [1691]: 0.0024367162663963067
Loss at iteration [1692]: 0.002436678271800249
Loss at iteration [1693]: 0.0024366518443257297
Loss at iteration [1694]: 0.0024366239823062586
Loss at iteration [1695]: 0.0024365938156808123
Loss at iteration [1696]: 0.0024365579488157363
Loss at iteration [1697]: 0.002436523726591942
Loss at iteration [1698]: 0.0024364893003071013
Loss at iteration [1699]: 0.002436474994763664
Loss at iteration [1700]: 0.0024364485029320783
Loss at iteration [1701]: 0.002436414994415597
Loss at iteration [1702]: 0.002436380686891776
Loss at iteration [1703]: 0.0024363550764281887
Loss at iteration [1704]: 0.0024363219914514587
Loss at iteration [1705]: 0.002436287927657952
Loss at iteration [1706]: 0.0024362500473843075
Loss at iteration [1707]: 0.002436219373436162
Loss at iteration [1708]: 0.002436185932549715
Loss at iteration [1709]: 0.0024361579726388818
Loss at iteration [1710]: 0.002436121688829301
Loss at iteration [1711]: 0.002436088355179826
Loss at iteration [1712]: 0.0024360545551993795
Loss at iteration [1713]: 0.002436028687489078
Loss at iteration [1714]: 0.002435997394612398
Loss at iteration [1715]: 0.002435968832811063
Loss at iteration [1716]: 0.0024359335473516687
Loss at iteration [1717]: 0.002435896469675478
Loss at iteration [1718]: 0.0024358641806006775
Loss at iteration [1719]: 0.002435834951924622
Loss at iteration [1720]: 0.0024358020698321903
Loss at iteration [1721]: 0.0024357899005737556
Loss at iteration [1722]: 0.0024357671402649677
Loss at iteration [1723]: 0.002435748566768761
Loss at iteration [1724]: 0.002435722880713696
Loss at iteration [1725]: 0.0024356948988372208
Loss at iteration [1726]: 0.0024356578551303094
Loss at iteration [1727]: 0.0024356237904537034
Loss at iteration [1728]: 0.0024355829741590164
Loss at iteration [1729]: 0.0024355511490775942
Loss at iteration [1730]: 0.0024355151099636624
Loss at iteration [1731]: 0.002435490721189119
Loss at iteration [1732]: 0.0024354571441365506
Loss at iteration [1733]: 0.0024354234096782986
Loss at iteration [1734]: 0.002435384285932106
Loss at iteration [1735]: 0.002435350013133056
Loss at iteration [1736]: 0.0024353127886402896
Loss at iteration [1737]: 0.002435276597101939
Loss at iteration [1738]: 0.002435236633376505
Loss at iteration [1739]: 0.0024351978910030474
Loss at iteration [1740]: 0.002435162203247738
Loss at iteration [1741]: 0.002435131757581666
Loss at iteration [1742]: 0.0024351025228021386
Loss at iteration [1743]: 0.002435077088340398
Loss at iteration [1744]: 0.0024350451487244592
Loss at iteration [1745]: 0.002435009658093315
Loss at iteration [1746]: 0.0024349782632087113
Loss at iteration [1747]: 0.002434951834904804
Loss at iteration [1748]: 0.0024349249749227382
Loss at iteration [1749]: 0.0024348953905476392
Loss at iteration [1750]: 0.002434860470316401
Loss at iteration [1751]: 0.002434843480112949
Loss at iteration [1752]: 0.00243481555384177
Loss at iteration [1753]: 0.0024347843380335657
Loss at iteration [1754]: 0.0024347514896530293
Loss at iteration [1755]: 0.0024347242701013437
Loss at iteration [1756]: 0.00243469353243171
Loss at iteration [1757]: 0.0024346633752237376
Loss at iteration [1758]: 0.002434629036622184
Loss at iteration [1759]: 0.002434612653552504
Loss at iteration [1760]: 0.0024345875521071363
Loss at iteration [1761]: 0.0024345556156921758
Loss at iteration [1762]: 0.0024345183509485524
Loss at iteration [1763]: 0.002434488185760793
Loss at iteration [1764]: 0.002434455179174043
Loss at iteration [1765]: 0.0024344234454815846
Loss at iteration [1766]: 0.0024343889903239192
Loss at iteration [1767]: 0.002434366405920726
Loss at iteration [1768]: 0.0024343356480426314
Loss at iteration [1769]: 0.002434308817519509
Loss at iteration [1770]: 0.002434276003861421
Loss at iteration [1771]: 0.002434252826372907
Loss at iteration [1772]: 0.0024342248282963807
Loss at iteration [1773]: 0.002434198886801723
Loss at iteration [1774]: 0.002434166361293496
Loss at iteration [1775]: 0.0024341417786918427
Loss at iteration [1776]: 0.0024341037739191653
Loss at iteration [1777]: 0.00243407894952723
Loss at iteration [1778]: 0.0024340451649747384
Loss at iteration [1779]: 0.0024340184258937396
Loss at iteration [1780]: 0.0024339861147953186
Loss at iteration [1781]: 0.00243395339068457
Loss at iteration [1782]: 0.00243391259783511
Loss at iteration [1783]: 0.0024338844581602674
Loss at iteration [1784]: 0.002433852533455474
Loss at iteration [1785]: 0.0024338224479218784
Loss at iteration [1786]: 0.0024337874179043073
Loss at iteration [1787]: 0.002433762246762628
Loss at iteration [1788]: 0.002433727846336369
Loss at iteration [1789]: 0.002433700459722582
Loss at iteration [1790]: 0.002433666559371119
Loss at iteration [1791]: 0.002433642227208897
Loss at iteration [1792]: 0.002433609470402787
Loss at iteration [1793]: 0.002433581327395826
Loss at iteration [1794]: 0.0024335462682487726
Loss at iteration [1795]: 0.002433514648596504
Loss at iteration [1796]: 0.002433482894858955
Loss at iteration [1797]: 0.0024334593133024727
Loss at iteration [1798]: 0.0024334244132515268
Loss at iteration [1799]: 0.002433389242209374
Loss at iteration [1800]: 0.0024333526204598954
Loss at iteration [1801]: 0.0024333312178986804
Loss at iteration [1802]: 0.00243330163669481
Loss at iteration [1803]: 0.002433267353165094
Loss at iteration [1804]: 0.0024332362249091044
Loss at iteration [1805]: 0.0024332111839475016
Loss at iteration [1806]: 0.002433178406525923
Loss at iteration [1807]: 0.0024331502135318318
Loss at iteration [1808]: 0.0024331160608574566
Loss at iteration [1809]: 0.002433091564200436
Loss at iteration [1810]: 0.002433060870532571
Loss at iteration [1811]: 0.002433028764185099
Loss at iteration [1812]: 0.0024329932028155144
Loss at iteration [1813]: 0.0024329684855455627
Loss at iteration [1814]: 0.0024329364230871254
Loss at iteration [1815]: 0.002432938168330999
***** Warning: Loss has increased *****
Loss at iteration [1816]: 0.0024329238489757647
Loss at iteration [1817]: 0.002432903552131143
Loss at iteration [1818]: 0.0024328720407647826
Loss at iteration [1819]: 0.002432836063089449
Loss at iteration [1820]: 0.0024327955899923685
Loss at iteration [1821]: 0.0024327660461195147
Loss at iteration [1822]: 0.002432731264450536
Loss at iteration [1823]: 0.002432704576158009
Loss at iteration [1824]: 0.0024326665008259224
Loss at iteration [1825]: 0.002432626842399995
Loss at iteration [1826]: 0.0024325868957644167
Loss at iteration [1827]: 0.002432560303823967
Loss at iteration [1828]: 0.0024325219921548583
Loss at iteration [1829]: 0.0024324779524956394
Loss at iteration [1830]: 0.0024324410159987943
Loss at iteration [1831]: 0.002432424431117668
Loss at iteration [1832]: 0.0024323948253544667
Loss at iteration [1833]: 0.002432360969454653
Loss at iteration [1834]: 0.002432321716354468
Loss at iteration [1835]: 0.002432287969184265
Loss at iteration [1836]: 0.0024322569029378398
Loss at iteration [1837]: 0.0024322208750335557
Loss at iteration [1838]: 0.0024321864241678894
Loss at iteration [1839]: 0.0024321591872394953
Loss at iteration [1840]: 0.0024321253946003965
Loss at iteration [1841]: 0.0024321028786603045
Loss at iteration [1842]: 0.0024320709403967064
Loss at iteration [1843]: 0.0024320481050986195
Loss at iteration [1844]: 0.002432021117366478
Loss at iteration [1845]: 0.0024320097409219416
Loss at iteration [1846]: 0.00243198561556098
Loss at iteration [1847]: 0.0024319665693770553
Loss at iteration [1848]: 0.002431938018000243
Loss at iteration [1849]: 0.0024319070420713972
Loss at iteration [1850]: 0.0024318686901711795
Loss at iteration [1851]: 0.0024318380756376775
Loss at iteration [1852]: 0.0024318033944096856
Loss at iteration [1853]: 0.0024317798699814495
Loss at iteration [1854]: 0.002431745880061908
Loss at iteration [1855]: 0.002431720586667993
Loss at iteration [1856]: 0.0024316885180666998
Loss at iteration [1857]: 0.0024316529296857453
Loss at iteration [1858]: 0.0024316146814219904
Loss at iteration [1859]: 0.0024315877656400035
Loss at iteration [1860]: 0.0024315547474516477
Loss at iteration [1861]: 0.002431551127946372
Loss at iteration [1862]: 0.0024315311464627987
Loss at iteration [1863]: 0.002431510787822852
Loss at iteration [1864]: 0.0024314817264135804
Loss at iteration [1865]: 0.0024314526014513928
Loss at iteration [1866]: 0.0024314200668413997
Loss at iteration [1867]: 0.0024314076986810193
Loss at iteration [1868]: 0.0024313801354131654
Loss at iteration [1869]: 0.002431344622811372
Loss at iteration [1870]: 0.002431302329262382
Loss at iteration [1871]: 0.0024312727214997865
Loss at iteration [1872]: 0.0024312323226569274
Loss at iteration [1873]: 0.0024311897642300398
Loss at iteration [1874]: 0.0024311515372307706
Loss at iteration [1875]: 0.0024311352220901506
Loss at iteration [1876]: 0.0024311028847666524
Loss at iteration [1877]: 0.002431067146013676
Loss at iteration [1878]: 0.0024310299347359075
Loss at iteration [1879]: 0.00243099844859088
Loss at iteration [1880]: 0.0024309641088877325
Loss at iteration [1881]: 0.00243093047565333
Loss at iteration [1882]: 0.0024308957819070246
Loss at iteration [1883]: 0.0024308848534297794
Loss at iteration [1884]: 0.0024308556843428654
Loss at iteration [1885]: 0.002430829573398136
Loss at iteration [1886]: 0.002430793743208177
Loss at iteration [1887]: 0.0024307646533397735
Loss at iteration [1888]: 0.0024307321333462695
Loss at iteration [1889]: 0.0024307096990659675
Loss at iteration [1890]: 0.0024306756476314036
Loss at iteration [1891]: 0.0024306420271121942
Loss at iteration [1892]: 0.00243060377024223
Loss at iteration [1893]: 0.002430574256535092
Loss at iteration [1894]: 0.0024305407218662015
Loss at iteration [1895]: 0.00243051579493755
Loss at iteration [1896]: 0.0024304871925381957
Loss at iteration [1897]: 0.002430467618871818
Loss at iteration [1898]: 0.0024304337641471944
Loss at iteration [1899]: 0.0024304047475685346
Loss at iteration [1900]: 0.002430374360842799
Loss at iteration [1901]: 0.0024303691279480033
Loss at iteration [1902]: 0.0024303492038293298
Loss at iteration [1903]: 0.002430328903277531
Loss at iteration [1904]: 0.0024302987410870585
Loss at iteration [1905]: 0.0024302798104575882
Loss at iteration [1906]: 0.0024302528598705033
Loss at iteration [1907]: 0.002430227327063843
Loss at iteration [1908]: 0.0024301978492911186
Loss at iteration [1909]: 0.0024301737180916693
Loss at iteration [1910]: 0.0024301406883388203
Loss at iteration [1911]: 0.002430117503161633
Loss at iteration [1912]: 0.002430081931102442
Loss at iteration [1913]: 0.002430046822090466
Loss at iteration [1914]: 0.0024300054663251355
Loss at iteration [1915]: 0.0024299683418374488
Loss at iteration [1916]: 0.0024299324896036967
Loss at iteration [1917]: 0.0024299048405192797
Loss at iteration [1918]: 0.002429870814813787
Loss at iteration [1919]: 0.002429842873703584
Loss at iteration [1920]: 0.0024298137050454537
Loss at iteration [1921]: 0.0024297889239309122
Loss at iteration [1922]: 0.002429755658918719
Loss at iteration [1923]: 0.0024297213185441586
Loss at iteration [1924]: 0.002429684283220231
Loss at iteration [1925]: 0.002429659653425193
Loss at iteration [1926]: 0.002429629357890438
Loss at iteration [1927]: 0.002429605184319678
Loss at iteration [1928]: 0.0024295779824400152
Loss at iteration [1929]: 0.002429558203640991
Loss at iteration [1930]: 0.0024295307265993138
Loss at iteration [1931]: 0.002429512859997187
Loss at iteration [1932]: 0.002429478216122209
Loss at iteration [1933]: 0.0024294585645795053
Loss at iteration [1934]: 0.002429427633186766
Loss at iteration [1935]: 0.0024293926732743454
Loss at iteration [1936]: 0.0024293573951505686
Loss at iteration [1937]: 0.002429328504989181
Loss at iteration [1938]: 0.002429291812451449
Loss at iteration [1939]: 0.0024292569883381226
Loss at iteration [1940]: 0.0024292201856191955
Loss at iteration [1941]: 0.0024292016897693378
Loss at iteration [1942]: 0.002429170495303826
Loss at iteration [1943]: 0.002429139624062161
Loss at iteration [1944]: 0.002429107174818583
Loss at iteration [1945]: 0.0024290899141138967
Loss at iteration [1946]: 0.0024290584841002113
Loss at iteration [1947]: 0.00242904297373927
Loss at iteration [1948]: 0.002429012673955043
Loss at iteration [1949]: 0.0024289860962421633
Loss at iteration [1950]: 0.0024289541227259033
Loss at iteration [1951]: 0.002428957577997734
***** Warning: Loss has increased *****
Loss at iteration [1952]: 0.0024289392168174656
Loss at iteration [1953]: 0.0024289075467045437
Loss at iteration [1954]: 0.002428871888170283
Loss at iteration [1955]: 0.002428862357317592
Loss at iteration [1956]: 0.002428834914108703
Loss at iteration [1957]: 0.0024288143991904117
Loss at iteration [1958]: 0.002428781728111591
Loss at iteration [1959]: 0.002428740144113906
Loss at iteration [1960]: 0.002428695212985865
Loss at iteration [1961]: 0.00242867957986229
Loss at iteration [1962]: 0.002428651115307201
Loss at iteration [1963]: 0.00242863512435918
Loss at iteration [1964]: 0.002428598602212701
Loss at iteration [1965]: 0.00242856546179831
Loss at iteration [1966]: 0.002428533274947134
Loss at iteration [1967]: 0.002428509237399028
Loss at iteration [1968]: 0.002428471890355432
Loss at iteration [1969]: 0.0024284409673075396
Loss at iteration [1970]: 0.002428402403938971
Loss at iteration [1971]: 0.002428376963254534
Loss at iteration [1972]: 0.002428343317816617
Loss at iteration [1973]: 0.0024283105585162557
Loss at iteration [1974]: 0.002428270407489147
Loss at iteration [1975]: 0.0024282476189531545
Loss at iteration [1976]: 0.0024282174723651174
Loss at iteration [1977]: 0.0024281888652634403
Loss at iteration [1978]: 0.002428152785029151
Loss at iteration [1979]: 0.0024281292190371988
Loss at iteration [1980]: 0.0024280948075938087
Loss at iteration [1981]: 0.0024280651881893236
Loss at iteration [1982]: 0.002428032683167576
Loss at iteration [1983]: 0.0024280132272013716
Loss at iteration [1984]: 0.0024279841022891114
Loss at iteration [1985]: 0.00242795369786962
Loss at iteration [1986]: 0.0024279154349406517
Loss at iteration [1987]: 0.0024278895343277313
Loss at iteration [1988]: 0.0024278556917898333
Loss at iteration [1989]: 0.0024278299001662057
Loss at iteration [1990]: 0.0024277951883000908
Loss at iteration [1991]: 0.0024277716816201163
Loss at iteration [1992]: 0.0024277403044287715
Loss at iteration [1993]: 0.0024277128553724502
Loss at iteration [1994]: 0.002427679979924546
Loss at iteration [1995]: 0.0024276584493329876
Loss at iteration [1996]: 0.0024276290573186365
Loss at iteration [1997]: 0.0024276117414226924
Loss at iteration [1998]: 0.002427581251388361
Loss at iteration [1999]: 0.002427549144338939
Loss at iteration [2000]: 0.0024275068285288855
Loss at iteration [2001]: 0.0024274850844331567
Loss at iteration [2002]: 0.0024274522344102023
Loss at iteration [2003]: 0.002427427404048188
Loss at iteration [2004]: 0.002427397062889471
Loss at iteration [2005]: 0.002427371079429066
Loss at iteration [2006]: 0.0024273412710966356
Loss at iteration [2007]: 0.0024273068793876605
Loss at iteration [2008]: 0.0024272703421967215
Loss at iteration [2009]: 0.0024272504539978096
Loss at iteration [2010]: 0.002427218781376645
Loss at iteration [2011]: 0.002427205121734058
Loss at iteration [2012]: 0.002427179972475013
Loss at iteration [2013]: 0.002427185332213984
***** Warning: Loss has increased *****
Loss at iteration [2014]: 0.002427161977229659
Loss at iteration [2015]: 0.002427143945634233
Loss at iteration [2016]: 0.0024271139733212325
Loss at iteration [2017]: 0.0024271055333315384
Loss at iteration [2018]: 0.00242707393691822
Loss at iteration [2019]: 0.0024270536493471726
Loss at iteration [2020]: 0.0024270209671512517
Loss at iteration [2021]: 0.002427012644851985
Loss at iteration [2022]: 0.002426981752314035
Loss at iteration [2023]: 0.0024269685618591038
Loss at iteration [2024]: 0.002426935571207637
Loss at iteration [2025]: 0.002426909526681174
Loss at iteration [2026]: 0.0024268700168600956
Loss at iteration [2027]: 0.0024268265199619645
Loss at iteration [2028]: 0.002426779181653115
Loss at iteration [2029]: 0.002426759359192495
Loss at iteration [2030]: 0.0024267189709240968
Loss at iteration [2031]: 0.002426696065311997
Loss at iteration [2032]: 0.002426660321587909
Loss at iteration [2033]: 0.002426627765647877
Loss at iteration [2034]: 0.002426582408012678
Loss at iteration [2035]: 0.002426535357499146
Loss at iteration [2036]: 0.0024264977238064346
Loss at iteration [2037]: 0.002426480872262798
Loss at iteration [2038]: 0.002426454107191173
Loss at iteration [2039]: 0.0024264323254768647
Loss at iteration [2040]: 0.00242639657086905
Loss at iteration [2041]: 0.0024263668615794346
Loss at iteration [2042]: 0.002426328323256011
Loss at iteration [2043]: 0.0024262980000191053
Loss at iteration [2044]: 0.0024262594403378205
Loss at iteration [2045]: 0.0024262534024180457
Loss at iteration [2046]: 0.002426229207243535
Loss at iteration [2047]: 0.0024262022044660944
Loss at iteration [2048]: 0.0024261649183905817
Loss at iteration [2049]: 0.0024261286249256733
Loss at iteration [2050]: 0.002426089150627696
Loss at iteration [2051]: 0.0024260723117912135
Loss at iteration [2052]: 0.0024260447034393563
Loss at iteration [2053]: 0.002426023707670533
Loss at iteration [2054]: 0.0024259925090703107
Loss at iteration [2055]: 0.002425963193012198
Loss at iteration [2056]: 0.0024259232070278844
Loss at iteration [2057]: 0.0024259197793561602
Loss at iteration [2058]: 0.002425896681517843
Loss at iteration [2059]: 0.0024258837784274625
Loss at iteration [2060]: 0.002425849239023975
Loss at iteration [2061]: 0.0024258108868465216
Loss at iteration [2062]: 0.002425770348496033
Loss at iteration [2063]: 0.0024257465668076463
Loss at iteration [2064]: 0.002425713178794239
Loss at iteration [2065]: 0.002425694160117088
Loss at iteration [2066]: 0.0024256572460532445
Loss at iteration [2067]: 0.0024256364441178828
Loss at iteration [2068]: 0.002425613544016844
Loss at iteration [2069]: 0.0024255872042389195
Loss at iteration [2070]: 0.0024255545865528834
Loss at iteration [2071]: 0.002425535229775514
Loss at iteration [2072]: 0.002425506144496983
Loss at iteration [2073]: 0.002425485736252684
Loss at iteration [2074]: 0.0024254523068129906
Loss at iteration [2075]: 0.0024254353534531826
Loss at iteration [2076]: 0.0024254055319159386
Loss at iteration [2077]: 0.0024253792854846377
Loss at iteration [2078]: 0.002425344805428569
Loss at iteration [2079]: 0.0024253250563626354
Loss at iteration [2080]: 0.002425293053956528
Loss at iteration [2081]: 0.00242527101324266
Loss at iteration [2082]: 0.0024252362987727447
Loss at iteration [2083]: 0.0024252030091351296
Loss at iteration [2084]: 0.0024251682599428295
Loss at iteration [2085]: 0.0024251644277971276
Loss at iteration [2086]: 0.0024251364937488925
Loss at iteration [2087]: 0.0024251014948835693
Loss at iteration [2088]: 0.0024250599375160832
Loss at iteration [2089]: 0.00242503575209664
Loss at iteration [2090]: 0.0024250033744767077
Loss at iteration [2091]: 0.0024249962277886243
Loss at iteration [2092]: 0.0024249687858282415
Loss at iteration [2093]: 0.0024249416785558538
Loss at iteration [2094]: 0.002424907434299861
Loss at iteration [2095]: 0.002424892527767803
Loss at iteration [2096]: 0.0024248596888229276
Loss at iteration [2097]: 0.002424830939135489
Loss at iteration [2098]: 0.0024247968932774363
Loss at iteration [2099]: 0.002424777243496337
Loss at iteration [2100]: 0.0024247438855363704
Loss at iteration [2101]: 0.0024247218212965775
Loss at iteration [2102]: 0.0024246899811201034
Loss at iteration [2103]: 0.0024246574493682047
Loss at iteration [2104]: 0.002424618637538591
Loss at iteration [2105]: 0.0024246010867224168
Loss at iteration [2106]: 0.0024245694554390317
Loss at iteration [2107]: 0.002424552750568309
Loss at iteration [2108]: 0.0024245192399828712
Loss at iteration [2109]: 0.0024244894909139767
Loss at iteration [2110]: 0.0024244487208039726
Loss at iteration [2111]: 0.0024244379776470616
Loss at iteration [2112]: 0.002424415333190718
Loss at iteration [2113]: 0.0024243868882404722
Loss at iteration [2114]: 0.0024243543383578787
Loss at iteration [2115]: 0.0024243289631555487
Loss at iteration [2116]: 0.0024242950397597787
Loss at iteration [2117]: 0.002424272592512899
Loss at iteration [2118]: 0.0024242421244442926
Loss at iteration [2119]: 0.0024242245634105448
Loss at iteration [2120]: 0.002424195795307643
Loss at iteration [2121]: 0.0024241722007867362
Loss at iteration [2122]: 0.002424143062335868
Loss at iteration [2123]: 0.002424118844129144
Loss at iteration [2124]: 0.0024240860358497166
Loss at iteration [2125]: 0.002424060992715596
Loss at iteration [2126]: 0.002424025109394281
Loss at iteration [2127]: 0.0024240029448198046
Loss at iteration [2128]: 0.002423977135624608
Loss at iteration [2129]: 0.0024239583959816944
Loss at iteration [2130]: 0.0024239255731341386
Loss at iteration [2131]: 0.0024239082269223587
Loss at iteration [2132]: 0.0024238818592893087
Loss at iteration [2133]: 0.002423863951809886
Loss at iteration [2134]: 0.0024238336463117574
Loss at iteration [2135]: 0.0024238177180731487
Loss at iteration [2136]: 0.002423783270451282
Loss at iteration [2137]: 0.0024237569755963763
Loss at iteration [2138]: 0.0024237264890213566
Loss at iteration [2139]: 0.0024237040018846843
Loss at iteration [2140]: 0.002423665247040693
Loss at iteration [2141]: 0.002423639921796387
Loss at iteration [2142]: 0.002423604754260431
Loss at iteration [2143]: 0.0024235694699179435
Loss at iteration [2144]: 0.002423534887466961
Loss at iteration [2145]: 0.0024235178793514008
Loss at iteration [2146]: 0.0024234915649308754
Loss at iteration [2147]: 0.0024234724636173098
Loss at iteration [2148]: 0.002423440044503009
Loss at iteration [2149]: 0.0024234100928514067
Loss at iteration [2150]: 0.002423375629073046
Loss at iteration [2151]: 0.002423368060401682
Loss at iteration [2152]: 0.002423343873078831
Loss at iteration [2153]: 0.002423327021785084
Loss at iteration [2154]: 0.002423294327162277
Loss at iteration [2155]: 0.0024232522361734175
Loss at iteration [2156]: 0.002423213236223549
Loss at iteration [2157]: 0.002423202366436159
Loss at iteration [2158]: 0.002423175982418855
Loss at iteration [2159]: 0.0024231565138795345
Loss at iteration [2160]: 0.002423129653708388
Loss at iteration [2161]: 0.0024231006667696474
Loss at iteration [2162]: 0.002423061790575136
Loss at iteration [2163]: 0.002423036905357357
Loss at iteration [2164]: 0.00242299977798887
Loss at iteration [2165]: 0.002422986166168581
Loss at iteration [2166]: 0.0024229604729903514
Loss at iteration [2167]: 0.002422924600327005
Loss at iteration [2168]: 0.0024228917986596375
Loss at iteration [2169]: 0.002422867719379028
Loss at iteration [2170]: 0.0024228394521488375
Loss at iteration [2171]: 0.002422837235710128
Loss at iteration [2172]: 0.002422811889016628
Loss at iteration [2173]: 0.002422777561969659
Loss at iteration [2174]: 0.0024227398300896022
Loss at iteration [2175]: 0.0024227099673431026
Loss at iteration [2176]: 0.0024226767322698306
Loss at iteration [2177]: 0.0024226642704858353
Loss at iteration [2178]: 0.002422643382263501
Loss at iteration [2179]: 0.0024226133119690397
Loss at iteration [2180]: 0.002422582576542367
Loss at iteration [2181]: 0.002422563968913894
Loss at iteration [2182]: 0.0024225313235428005
Loss at iteration [2183]: 0.0024225263549713188
Loss at iteration [2184]: 0.0024225039464651488
Loss at iteration [2185]: 0.002422474108218972
Loss at iteration [2186]: 0.0024224381333100902
Loss at iteration [2187]: 0.002422402901652762
Loss at iteration [2188]: 0.0024223694072482687
Loss at iteration [2189]: 0.002422344907900525
Loss at iteration [2190]: 0.002422312302603571
Loss at iteration [2191]: 0.002422294946399245
Loss at iteration [2192]: 0.0024222667964693017
Loss at iteration [2193]: 0.002422243064782693
Loss at iteration [2194]: 0.0024222145627070153
Loss at iteration [2195]: 0.0024221931587789353
Loss at iteration [2196]: 0.002422163625039911
Loss at iteration [2197]: 0.0024221637897909936
***** Warning: Loss has increased *****
