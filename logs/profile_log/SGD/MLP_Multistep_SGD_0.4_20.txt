Model name                            : MLP_Multistep
The number of input features          : 20
The number of output features         : 2
Optimizer name                        : SGD
Learning rate                         : 0.4
Max number of iterations              : 3000
Number of samples in training data    : 39
Number of samples in tests data       : 16
Total training time                   : 0.0451810359954834
Total number of parameters            : 205302
Percentage of parameters < 1e-9       : 49.823674391871485%
Percentage of parameters < 1e-7       : 49.823674391871485%
Percentage of parameters < 1e-6       : 49.82562274113258%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5138648094354812
Loss at iteration [2]: 1.2789578438641593
Loss at iteration [3]: 0.7906927582107635
Loss at iteration [4]: 0.5315912018536972
Loss at iteration [5]: 0.4486870119675033
Loss at iteration [6]: 0.3934038819030493
Loss at iteration [7]: 0.4228039620840683
***** Warning: Loss has increased *****
Loss at iteration [8]: 0.4279843727210515
***** Warning: Loss has increased *****
Loss at iteration [9]: 0.3898615874982998
Loss at iteration [10]: 0.3980941721784556
***** Warning: Loss has increased *****
Loss at iteration [11]: 0.40152334856817895
***** Warning: Loss has increased *****
Loss at iteration [12]: 0.39307829273543093
Loss at iteration [13]: 0.397969057015389
***** Warning: Loss has increased *****
Loss at iteration [14]: 0.4075228285328714
***** Warning: Loss has increased *****
Loss at iteration [15]: 0.42333878420608245
***** Warning: Loss has increased *****
Loss at iteration [16]: 0.43458696873007835
***** Warning: Loss has increased *****
Loss at iteration [17]: 0.4808021968959642
***** Warning: Loss has increased *****
Loss at iteration [18]: 0.4379285227761838
Loss at iteration [19]: 0.42118862141103197
Loss at iteration [20]: 0.4055336435751158
Loss at iteration [21]: 0.39853520368960954
Loss at iteration [22]: 0.39252068590768885
Loss at iteration [23]: 0.39034222886175374
Loss at iteration [24]: 0.38940620823359284
Loss at iteration [25]: 0.3891454401708912
Loss at iteration [26]: 0.38909947777214554
Loss at iteration [27]: 0.3891088948974383
***** Warning: Loss has increased *****
Loss at iteration [28]: 0.3891185466966024
***** Warning: Loss has increased *****
Loss at iteration [29]: 0.3891169248773336
Loss at iteration [30]: 0.38911073363361437
Loss at iteration [31]: 0.3891048953409943
Loss at iteration [32]: 0.3891008203731631
Loss at iteration [33]: 0.38909851127683587
Loss at iteration [34]: 0.3890975196878274
Loss at iteration [35]: 0.38909717250011183
Loss at iteration [36]: 0.3890970833897294
Loss at iteration [37]: 0.38909708190091324
Loss at iteration [38]: 0.389097091512462
***** Warning: Loss has increased *****
Loss at iteration [39]: 0.38909709293781036
***** Warning: Loss has increased *****
Loss at iteration [40]: 0.3890970897155749
Loss at iteration [41]: 0.38909708546633953
Loss at iteration [42]: 0.3890970818707018
Loss at iteration [43]: 0.3890970796779372
Loss at iteration [44]: 0.38909707863354925
Loss at iteration [45]: 0.38909707820983
