Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : SGD
Learning rate                         : 0.3
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 36.09625267982483
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 56.30707386300035%
Percentage of parameters < 1e-7       : 56.30732261860045%
Percentage of parameters < 1e-6       : 56.308068885400786%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.044947597345361
Loss at iteration [2]: 0.13565653199680328
Loss at iteration [3]: 4.019620024283287
***** Warning: Loss has increased *****
Loss at iteration [4]: 3.675105354863359
Loss at iteration [5]: 1.0926979671776742
Loss at iteration [6]: 0.4976850704541652
Loss at iteration [7]: 0.3097353824816702
Loss at iteration [8]: 2.3170686652766648
***** Warning: Loss has increased *****
Loss at iteration [9]: 1.5716629822414543
Loss at iteration [10]: 1.2910426302658868
Loss at iteration [11]: 0.4686535504319126
Loss at iteration [12]: 2.6590440238297584
***** Warning: Loss has increased *****
Loss at iteration [13]: 1.5224211514075379
Loss at iteration [14]: 1.329473991160108
Loss at iteration [15]: 0.9285514293920974
Loss at iteration [16]: 0.7226953829563307
Loss at iteration [17]: 0.33311567452406166
Loss at iteration [18]: 0.3045862886798761
Loss at iteration [19]: 1.385308399417825
***** Warning: Loss has increased *****
Loss at iteration [20]: 0.4442297661490866
Loss at iteration [21]: 0.7834566837564177
***** Warning: Loss has increased *****
Loss at iteration [22]: 1.2551048649953354
***** Warning: Loss has increased *****
Loss at iteration [23]: 0.8358262732791958
Loss at iteration [24]: 0.8410805770754788
***** Warning: Loss has increased *****
Loss at iteration [25]: 0.6661204219263512
Loss at iteration [26]: 0.24046659506932194
Loss at iteration [27]: 0.4664859391415691
***** Warning: Loss has increased *****
Loss at iteration [28]: 1.220885150303534
***** Warning: Loss has increased *****
Loss at iteration [29]: 0.9082236105938913
Loss at iteration [30]: 0.5542198117437164
Loss at iteration [31]: 0.5340138212818767
Loss at iteration [32]: 0.34377228439256635
Loss at iteration [33]: 0.2272111599489886
Loss at iteration [34]: 0.1417191093533852
Loss at iteration [35]: 0.12451950768435542
Loss at iteration [36]: 0.16635744958918702
***** Warning: Loss has increased *****
Loss at iteration [37]: 0.536123323909692
***** Warning: Loss has increased *****
Loss at iteration [38]: 0.9325614260313903
***** Warning: Loss has increased *****
Loss at iteration [39]: 0.2760858977376362
Loss at iteration [40]: 0.35852254051819404
***** Warning: Loss has increased *****
Loss at iteration [41]: 0.2146243668662516
Loss at iteration [42]: 0.1975733515631853
Loss at iteration [43]: 0.1409452093038995
Loss at iteration [44]: 0.10633997638313233
Loss at iteration [45]: 0.07178533356220317
Loss at iteration [46]: 0.04553443985991356
Loss at iteration [47]: 0.03205853598059483
Loss at iteration [48]: 0.02354524686760787
Loss at iteration [49]: 0.016236737211485372
Loss at iteration [50]: 0.010949401497512366
Loss at iteration [51]: 0.009869738178125276
Loss at iteration [52]: 0.012184669736776657
***** Warning: Loss has increased *****
Loss at iteration [53]: 0.02668688761811434
***** Warning: Loss has increased *****
Loss at iteration [54]: 0.050309936592617795
***** Warning: Loss has increased *****
Loss at iteration [55]: 0.14765798288352996
***** Warning: Loss has increased *****
Loss at iteration [56]: 0.03550040864327392
Loss at iteration [57]: 0.014009372267662615
Loss at iteration [58]: 0.008663295636954508
Loss at iteration [59]: 0.010690625382528598
***** Warning: Loss has increased *****
Loss at iteration [60]: 0.007135923457067347
Loss at iteration [61]: 0.004808313124897926
Loss at iteration [62]: 0.004084139972638642
Loss at iteration [63]: 0.0035024363003060485
Loss at iteration [64]: 0.003472361847177698
Loss at iteration [65]: 0.0034128668093735153
Loss at iteration [66]: 0.0033702597995931647
Loss at iteration [67]: 0.003342694835518307
Loss at iteration [68]: 0.003297996943875467
Loss at iteration [69]: 0.003264773667078946
Loss at iteration [70]: 0.0032349165915456593
Loss at iteration [71]: 0.0032091095593109182
Loss at iteration [72]: 0.003187195684089909
Loss at iteration [73]: 0.0031662075730077527
Loss at iteration [74]: 0.0031460205077925695
Loss at iteration [75]: 0.003126708440263714
Loss at iteration [76]: 0.0031083916348340307
Loss at iteration [77]: 0.0030907000265278936
Loss at iteration [78]: 0.0030735627887271337
Loss at iteration [79]: 0.0030565951497906825
Loss at iteration [80]: 0.0030400472394449827
Loss at iteration [81]: 0.0030237883170493416
Loss at iteration [82]: 0.003007715362151243
Loss at iteration [83]: 0.0029920206409907164
Loss at iteration [84]: 0.0029784706306214977
Loss at iteration [85]: 0.0029656082506733707
Loss at iteration [86]: 0.0029533670542552584
Loss at iteration [87]: 0.0029415998107799133
Loss at iteration [88]: 0.0029301669263081895
Loss at iteration [89]: 0.0029192860769704375
Loss at iteration [90]: 0.002908945974595094
Loss at iteration [91]: 0.0028990684494584725
Loss at iteration [92]: 0.0028896438939275252
Loss at iteration [93]: 0.0028807323296049228
Loss at iteration [94]: 0.0028724196816292957
Loss at iteration [95]: 0.002864570171113968
Loss at iteration [96]: 0.002857137783125821
Loss at iteration [97]: 0.002850108919150883
Loss at iteration [98]: 0.0028433697043705615
Loss at iteration [99]: 0.002836905254985223
Loss at iteration [100]: 0.0028302997561767585
Loss at iteration [101]: 0.0028238773685439205
Loss at iteration [102]: 0.0028177103533038868
Loss at iteration [103]: 0.0028117535067047473
Loss at iteration [104]: 0.0028060291168057632
Loss at iteration [105]: 0.0028009028235081593
Loss at iteration [106]: 0.002795782433551822
Loss at iteration [107]: 0.002790950894769377
Loss at iteration [108]: 0.0027864460430712217
Loss at iteration [109]: 0.0027820894140663356
Loss at iteration [110]: 0.0027778725846811536
Loss at iteration [111]: 0.0027738164694074193
Loss at iteration [112]: 0.002769936905567086
Loss at iteration [113]: 0.0027662112708805474
Loss at iteration [114]: 0.002762568179306836
Loss at iteration [115]: 0.0027590187286345045
Loss at iteration [116]: 0.0027555493383342924
Loss at iteration [117]: 0.0027521999790761018
Loss at iteration [118]: 0.0027489555789684257
Loss at iteration [119]: 0.002745803905194268
Loss at iteration [120]: 0.0027427041253302122
Loss at iteration [121]: 0.002739482095144312
Loss at iteration [122]: 0.0027361779704935412
Loss at iteration [123]: 0.002732868635419278
Loss at iteration [124]: 0.0027295783350299195
Loss at iteration [125]: 0.002726350494500343
Loss at iteration [126]: 0.002723218104552124
Loss at iteration [127]: 0.002720150571940112
Loss at iteration [128]: 0.0027171519697931214
Loss at iteration [129]: 0.002714210482157889
Loss at iteration [130]: 0.002711326460679295
Loss at iteration [131]: 0.002708511419164054
Loss at iteration [132]: 0.0027057796333929177
Loss at iteration [133]: 0.0027031230859643532
Loss at iteration [134]: 0.002700555239576192
Loss at iteration [135]: 0.002698044230039143
Loss at iteration [136]: 0.0026955922084794205
Loss at iteration [137]: 0.0026931896089471227
Loss at iteration [138]: 0.002690817952744181
Loss at iteration [139]: 0.002688504738738087
Loss at iteration [140]: 0.0026862336970539737
Loss at iteration [141]: 0.0026840079825955587
Loss at iteration [142]: 0.0026818255554006587
Loss at iteration [143]: 0.002679687382428634
Loss at iteration [144]: 0.00267757083034744
Loss at iteration [145]: 0.0026754538365254617
Loss at iteration [146]: 0.0026732413031595875
Loss at iteration [147]: 0.0026706588054543866
Loss at iteration [148]: 0.0026681294528901303
Loss at iteration [149]: 0.002665690606861393
Loss at iteration [150]: 0.0026632795456120934
Loss at iteration [151]: 0.0026609059921607007
Loss at iteration [152]: 0.0026586268736911683
Loss at iteration [153]: 0.0026564200246111127
Loss at iteration [154]: 0.002654272639896109
Loss at iteration [155]: 0.002652173508104583
Loss at iteration [156]: 0.002650202606393648
Loss at iteration [157]: 0.0026483563007802568
Loss at iteration [158]: 0.002646668969986576
Loss at iteration [159]: 0.0026450663185967425
Loss at iteration [160]: 0.002643602875582686
Loss at iteration [161]: 0.0026422564422876947
Loss at iteration [162]: 0.0026410478032575665
Loss at iteration [163]: 0.0026400389198162253
Loss at iteration [164]: 0.0026391035578701774
Loss at iteration [165]: 0.002638510235743295
Loss at iteration [166]: 0.002637966935211266
Loss at iteration [167]: 0.0026378300035930858
Loss at iteration [168]: 0.0026377257846656845
Loss at iteration [169]: 0.0026380996380491475
***** Warning: Loss has increased *****
Loss at iteration [170]: 0.0026384816552845964
***** Warning: Loss has increased *****
Loss at iteration [171]: 0.002639804563217449
***** Warning: Loss has increased *****
Loss at iteration [172]: 0.002641079325251853
***** Warning: Loss has increased *****
Loss at iteration [173]: 0.002643498826863962
***** Warning: Loss has increased *****
Loss at iteration [174]: 0.002645747923515429
***** Warning: Loss has increased *****
Loss at iteration [175]: 0.0026495647939720457
***** Warning: Loss has increased *****
Loss at iteration [176]: 0.0026530276252479013
***** Warning: Loss has increased *****
Loss at iteration [177]: 0.0026586157159104954
***** Warning: Loss has increased *****
Loss at iteration [178]: 0.0026625334915032694
***** Warning: Loss has increased *****
Loss at iteration [179]: 0.002669613505720258
***** Warning: Loss has increased *****
Loss at iteration [180]: 0.002675030303684359
***** Warning: Loss has increased *****
Loss at iteration [181]: 0.0026843960915879563
***** Warning: Loss has increased *****
Loss at iteration [182]: 0.002691783477469416
***** Warning: Loss has increased *****
Loss at iteration [183]: 0.0027042835102857633
***** Warning: Loss has increased *****
Loss at iteration [184]: 0.0027141545504730242
***** Warning: Loss has increased *****
Loss at iteration [185]: 0.002730977658291784
***** Warning: Loss has increased *****
Loss at iteration [186]: 0.0027439312624029503
***** Warning: Loss has increased *****
Loss at iteration [187]: 0.002766403460960015
***** Warning: Loss has increased *****
Loss at iteration [188]: 0.002784192967210314
***** Warning: Loss has increased *****
Loss at iteration [189]: 0.002814908435867856
***** Warning: Loss has increased *****
Loss at iteration [190]: 0.0028387420240624252
***** Warning: Loss has increased *****
Loss at iteration [191]: 0.0028809109570045615
***** Warning: Loss has increased *****
Loss at iteration [192]: 0.002911186846852668
***** Warning: Loss has increased *****
Loss at iteration [193]: 0.002968113513081222
***** Warning: Loss has increased *****
Loss at iteration [194]: 0.003005211501807649
***** Warning: Loss has increased *****
Loss at iteration [195]: 0.0030810256612219945
***** Warning: Loss has increased *****
Loss at iteration [196]: 0.003123828114596898
***** Warning: Loss has increased *****
Loss at iteration [197]: 0.0032223896765203887
***** Warning: Loss has increased *****
Loss at iteration [198]: 0.0032686939219949184
***** Warning: Loss has increased *****
Loss at iteration [199]: 0.003395060610214206
***** Warning: Loss has increased *****
Loss at iteration [200]: 0.003444988471206369
***** Warning: Loss has increased *****
Loss at iteration [201]: 0.003604624046440194
***** Warning: Loss has increased *****
Loss at iteration [202]: 0.003650309945109579
***** Warning: Loss has increased *****
Loss at iteration [203]: 0.003845188695629888
***** Warning: Loss has increased *****
Loss at iteration [204]: 0.0038792790558869333
***** Warning: Loss has increased *****
Loss at iteration [205]: 0.004110102116236798
***** Warning: Loss has increased *****
Loss at iteration [206]: 0.004119722304624559
***** Warning: Loss has increased *****
Loss at iteration [207]: 0.0043800149513616
***** Warning: Loss has increased *****
Loss at iteration [208]: 0.004346362882262539
Loss at iteration [209]: 0.0046200138646662345
***** Warning: Loss has increased *****
Loss at iteration [210]: 0.00453160681806441
Loss at iteration [211]: 0.004806160906913063
***** Warning: Loss has increased *****
Loss at iteration [212]: 0.004659857249579909
Loss at iteration [213]: 0.004910007463167236
***** Warning: Loss has increased *****
Loss at iteration [214]: 0.00469966175210496
Loss at iteration [215]: 0.004903705719938664
***** Warning: Loss has increased *****
Loss at iteration [216]: 0.004642226233043799
Loss at iteration [217]: 0.004787948369438763
***** Warning: Loss has increased *****
Loss at iteration [218]: 0.0044995688260942215
Loss at iteration [219]: 0.004583542938759557
***** Warning: Loss has increased *****
Loss at iteration [220]: 0.0042950166728332965
Loss at iteration [221]: 0.00433092239794676
***** Warning: Loss has increased *****
Loss at iteration [222]: 0.004065426926203882
Loss at iteration [223]: 0.0040702128400264374
***** Warning: Loss has increased *****
Loss at iteration [224]: 0.003840329537593217
Loss at iteration [225]: 0.003829025334185892
Loss at iteration [226]: 0.003636550150473522
Loss at iteration [227]: 0.0036123273503051623
Loss at iteration [228]: 0.0034549019589946496
Loss at iteration [229]: 0.0034286527588828024
Loss at iteration [230]: 0.003303026345371685
Loss at iteration [231]: 0.0032792930931403114
Loss at iteration [232]: 0.0031788008658702235
Loss at iteration [233]: 0.003156775235991198
Loss at iteration [234]: 0.003076487169011602
Loss at iteration [235]: 0.0030565530622142785
Loss at iteration [236]: 0.0029932386796685554
Loss at iteration [237]: 0.002978293610858727
Loss at iteration [238]: 0.002928897450629722
Loss at iteration [239]: 0.0029164331491561544
Loss at iteration [240]: 0.0028777345837603338
Loss at iteration [241]: 0.002867619822534468
Loss at iteration [242]: 0.0028365128066088277
Loss at iteration [243]: 0.00282948121274805
Loss at iteration [244]: 0.002804774482661563
Loss at iteration [245]: 0.0028000844070954793
Loss at iteration [246]: 0.002780566151577894
Loss at iteration [247]: 0.0027789532108376683
Loss at iteration [248]: 0.0027635008867658477
Loss at iteration [249]: 0.002763383008801531
Loss at iteration [250]: 0.002750378426187389
Loss at iteration [251]: 0.0027512679541558645
***** Warning: Loss has increased *****
Loss at iteration [252]: 0.002740708114474972
Loss at iteration [253]: 0.002742553669428962
***** Warning: Loss has increased *****
Loss at iteration [254]: 0.0027339266458584277
Loss at iteration [255]: 0.002736707573574681
***** Warning: Loss has increased *****
Loss at iteration [256]: 0.0027294227903902697
Loss at iteration [257]: 0.0027330938277701855
***** Warning: Loss has increased *****
Loss at iteration [258]: 0.0027269646544534993
Loss at iteration [259]: 0.0027327119065552645
***** Warning: Loss has increased *****
Loss at iteration [260]: 0.0027280737011462805
Loss at iteration [261]: 0.0027352822373128776
***** Warning: Loss has increased *****
Loss at iteration [262]: 0.002731593533039228
Loss at iteration [263]: 0.0027399906659495202
***** Warning: Loss has increased *****
Loss at iteration [264]: 0.002737131892350966
Loss at iteration [265]: 0.0027480188215619787
***** Warning: Loss has increased *****
Loss at iteration [266]: 0.0027462089871325753
Loss at iteration [267]: 0.0027582169163051016
***** Warning: Loss has increased *****
Loss at iteration [268]: 0.0027566580087420556
Loss at iteration [269]: 0.002770125737825599
***** Warning: Loss has increased *****
Loss at iteration [270]: 0.002768910716000168
Loss at iteration [271]: 0.0027835611932428717
***** Warning: Loss has increased *****
Loss at iteration [272]: 0.002782632948382659
Loss at iteration [273]: 0.0027990322489358423
***** Warning: Loss has increased *****
Loss at iteration [274]: 0.002798351284979821
Loss at iteration [275]: 0.0028163934541008927
***** Warning: Loss has increased *****
Loss at iteration [276]: 0.002815031516130544
Loss at iteration [277]: 0.0028340380979481545
***** Warning: Loss has increased *****
Loss at iteration [278]: 0.002831983213317622
Loss at iteration [279]: 0.0028531322680431927
***** Warning: Loss has increased *****
Loss at iteration [280]: 0.0028511413429908434
Loss at iteration [281]: 0.002873643229753402
***** Warning: Loss has increased *****
Loss at iteration [282]: 0.0028714202270923384
Loss at iteration [283]: 0.002896583928139297
***** Warning: Loss has increased *****
Loss at iteration [284]: 0.002894141985258256
Loss at iteration [285]: 0.002921145655289597
***** Warning: Loss has increased *****
Loss at iteration [286]: 0.0029170113179962827
Loss at iteration [287]: 0.0029459314998350125
***** Warning: Loss has increased *****
Loss at iteration [288]: 0.0029412212171738353
Loss at iteration [289]: 0.002973047436925522
***** Warning: Loss has increased *****
Loss at iteration [290]: 0.0029676487610829313
Loss at iteration [291]: 0.0030014489862571097
***** Warning: Loss has increased *****
Loss at iteration [292]: 0.002993399757371052
Loss at iteration [293]: 0.0030287131491988826
***** Warning: Loss has increased *****
Loss at iteration [294]: 0.0030194834501758934
Loss at iteration [295]: 0.003058099551099619
***** Warning: Loss has increased *****
Loss at iteration [296]: 0.003047795514149293
Loss at iteration [297]: 0.0030882161123299986
***** Warning: Loss has increased *****
Loss at iteration [298]: 0.0030751593564698016
Loss at iteration [299]: 0.003117248036156565
***** Warning: Loss has increased *****
Loss at iteration [300]: 0.0031018284496043525
Loss at iteration [301]: 0.0031454506411013206
***** Warning: Loss has increased *****
Loss at iteration [302]: 0.003127093086027197
Loss at iteration [303]: 0.0031717178442088683
***** Warning: Loss has increased *****
Loss at iteration [304]: 0.0031492225682619767
Loss at iteration [305]: 0.0031944427443853798
***** Warning: Loss has increased *****
Loss at iteration [306]: 0.003168677301220042
Loss at iteration [307]: 0.0032139731758413274
***** Warning: Loss has increased *****
Loss at iteration [308]: 0.0031850098069355435
Loss at iteration [309]: 0.003230096706931586
***** Warning: Loss has increased *****
Loss at iteration [310]: 0.0031982691217626734
Loss at iteration [311]: 0.003243146892783651
***** Warning: Loss has increased *****
Loss at iteration [312]: 0.003208918043077087
Loss at iteration [313]: 0.003253054415176375
***** Warning: Loss has increased *****
Loss at iteration [314]: 0.003216381754466935
Loss at iteration [315]: 0.003259774220500423
***** Warning: Loss has increased *****
Loss at iteration [316]: 0.0032202205985722584
Loss at iteration [317]: 0.003261529638171884
***** Warning: Loss has increased *****
Loss at iteration [318]: 0.003220044372800214
Loss at iteration [319]: 0.003259849719946525
***** Warning: Loss has increased *****
Loss at iteration [320]: 0.0032168994458183007
Loss at iteration [321]: 0.003255091500708732
***** Warning: Loss has increased *****
Loss at iteration [322]: 0.003209664345127951
Loss at iteration [323]: 0.0032442299351627545
***** Warning: Loss has increased *****
Loss at iteration [324]: 0.0031970976245133155
Loss at iteration [325]: 0.003228388054991547
***** Warning: Loss has increased *****
Loss at iteration [326]: 0.003180507378952981
Loss at iteration [327]: 0.003208695148418845
***** Warning: Loss has increased *****
Loss at iteration [328]: 0.003161230060597153
Loss at iteration [329]: 0.003186963240896885
***** Warning: Loss has increased *****
Loss at iteration [330]: 0.00314045513131044
Loss at iteration [331]: 0.003162842051331935
***** Warning: Loss has increased *****
Loss at iteration [332]: 0.00311741475390164
Loss at iteration [333]: 0.0031384476172422035
***** Warning: Loss has increased *****
Loss at iteration [334]: 0.003095259941035479
Loss at iteration [335]: 0.0031146735969906723
***** Warning: Loss has increased *****
Loss at iteration [336]: 0.003072120834644733
Loss at iteration [337]: 0.0030893775299451532
***** Warning: Loss has increased *****
Loss at iteration [338]: 0.003048801449468505
Loss at iteration [339]: 0.0030649555131801245
***** Warning: Loss has increased *****
Loss at iteration [340]: 0.0030264893509833334
Loss at iteration [341]: 0.003039902606861361
***** Warning: Loss has increased *****
Loss at iteration [342]: 0.0030022580798874533
Loss at iteration [343]: 0.0030156250763667056
***** Warning: Loss has increased *****
Loss at iteration [344]: 0.0029801143182892433
Loss at iteration [345]: 0.002991933959702537
***** Warning: Loss has increased *****
Loss at iteration [346]: 0.002958453906918085
Loss at iteration [347]: 0.0029693617173415523
***** Warning: Loss has increased *****
Loss at iteration [348]: 0.002937937951600734
Loss at iteration [349]: 0.0029485570008682543
***** Warning: Loss has increased *****
Loss at iteration [350]: 0.0029188233236014213
Loss at iteration [351]: 0.0029288849548890494
***** Warning: Loss has increased *****
Loss at iteration [352]: 0.002901568372927223
Loss at iteration [353]: 0.002911332646948943
***** Warning: Loss has increased *****
Loss at iteration [354]: 0.0028854245368968245
Loss at iteration [355]: 0.0028940382637291174
***** Warning: Loss has increased *****
Loss at iteration [356]: 0.002868767405938593
Loss at iteration [357]: 0.0028770571762469005
***** Warning: Loss has increased *****
Loss at iteration [358]: 0.002853492578163851
Loss at iteration [359]: 0.0028614306705229448
***** Warning: Loss has increased *****
Loss at iteration [360]: 0.0028394028765435318
Loss at iteration [361]: 0.002847275642128941
***** Warning: Loss has increased *****
Loss at iteration [362]: 0.0028268494245126943
Loss at iteration [363]: 0.0028348554772019
***** Warning: Loss has increased *****
Loss at iteration [364]: 0.0028156456058673974
Loss at iteration [365]: 0.002823962527005156
***** Warning: Loss has increased *****
Loss at iteration [366]: 0.002806420348392428
Loss at iteration [367]: 0.0028141186145490132
***** Warning: Loss has increased *****
Loss at iteration [368]: 0.002797042997717997
Loss at iteration [369]: 0.00280546471870297
***** Warning: Loss has increased *****
Loss at iteration [370]: 0.002789925584957137
Loss at iteration [371]: 0.0027986996179611666
***** Warning: Loss has increased *****
Loss at iteration [372]: 0.002784282192020558
Loss at iteration [373]: 0.002792849760521278
***** Warning: Loss has increased *****
Loss at iteration [374]: 0.002778706591967697
Loss at iteration [375]: 0.0027882382149140446
***** Warning: Loss has increased *****
Loss at iteration [376]: 0.002775403810029293
Loss at iteration [377]: 0.0027853174014245453
***** Warning: Loss has increased *****
Loss at iteration [378]: 0.002773154095967218
Loss at iteration [379]: 0.002783087270697812
***** Warning: Loss has increased *****
Loss at iteration [380]: 0.0027710018828753786
Loss at iteration [381]: 0.0027812082265485797
***** Warning: Loss has increased *****
Loss at iteration [382]: 0.002769923066923687
Loss at iteration [383]: 0.002781393431175549
***** Warning: Loss has increased *****
Loss at iteration [384]: 0.002771147418478868
Loss at iteration [385]: 0.0027828288746425717
***** Warning: Loss has increased *****
Loss at iteration [386]: 0.0027728816911751016
Loss at iteration [387]: 0.00278571923420094
***** Warning: Loss has increased *****
Loss at iteration [388]: 0.00277639299516998
Loss at iteration [389]: 0.0027900412662938013
***** Warning: Loss has increased *****
Loss at iteration [390]: 0.0027807990715021597
Loss at iteration [391]: 0.0027946207136081832
***** Warning: Loss has increased *****
Loss at iteration [392]: 0.0027850287404587166
Loss at iteration [393]: 0.0027995058653147256
***** Warning: Loss has increased *****
Loss at iteration [394]: 0.002790351787255389
Loss at iteration [395]: 0.002806276831292975
***** Warning: Loss has increased *****
Loss at iteration [396]: 0.0027974294290577235
Loss at iteration [397]: 0.002813336094513553
***** Warning: Loss has increased *****
Loss at iteration [398]: 0.0028038180866594245
Loss at iteration [399]: 0.0028208343731919118
***** Warning: Loss has increased *****
Loss at iteration [400]: 0.0028120591851958014
Loss at iteration [401]: 0.002830428803350832
***** Warning: Loss has increased *****
Loss at iteration [402]: 0.0028211844494460308
Loss at iteration [403]: 0.002839744412678148
***** Warning: Loss has increased *****
Loss at iteration [404]: 0.002829492068194908
Loss at iteration [405]: 0.0028488073016441798
***** Warning: Loss has increased *****
Loss at iteration [406]: 0.0028387459601129416
Loss at iteration [407]: 0.0028583165777411003
***** Warning: Loss has increased *****
Loss at iteration [408]: 0.0028474529863801706
Loss at iteration [409]: 0.00286861513453384
***** Warning: Loss has increased *****
Loss at iteration [410]: 0.0028575947256133318
Loss at iteration [411]: 0.002879337360328783
***** Warning: Loss has increased *****
Loss at iteration [412]: 0.00286761199023174
Loss at iteration [413]: 0.002889591283704933
***** Warning: Loss has increased *****
Loss at iteration [414]: 0.0028767502961777416
Loss at iteration [415]: 0.0028994349187089754
***** Warning: Loss has increased *****
Loss at iteration [416]: 0.0028859528139531794
Loss at iteration [417]: 0.0029084399673413755
***** Warning: Loss has increased *****
Loss at iteration [418]: 0.0028941922211150146
Loss at iteration [419]: 0.0029180704407655556
***** Warning: Loss has increased *****
Loss at iteration [420]: 0.0029033384040993283
Loss at iteration [421]: 0.0029269360460177293
***** Warning: Loss has increased *****
Loss at iteration [422]: 0.0029108209902119755
Loss at iteration [423]: 0.002935213279510678
***** Warning: Loss has increased *****
Loss at iteration [424]: 0.0029185290737709074
Loss at iteration [425]: 0.002942789732311638
***** Warning: Loss has increased *****
Loss at iteration [426]: 0.0029255891150032766
Loss at iteration [427]: 0.002951083389176274
***** Warning: Loss has increased *****
Loss at iteration [428]: 0.0029329858378386985
Loss at iteration [429]: 0.0029574408408127592
***** Warning: Loss has increased *****
Loss at iteration [430]: 0.002937543949949823
Loss at iteration [431]: 0.0029628541716913348
***** Warning: Loss has increased *****
Loss at iteration [432]: 0.0029429320509053456
Loss at iteration [433]: 0.0029675326115732557
***** Warning: Loss has increased *****
Loss at iteration [434]: 0.0029462853697411755
Loss at iteration [435]: 0.002970949162194471
***** Warning: Loss has increased *****
Loss at iteration [436]: 0.002949772976471584
Loss at iteration [437]: 0.0029738275935491283
***** Warning: Loss has increased *****
Loss at iteration [438]: 0.002952019591615929
Loss at iteration [439]: 0.002975900802563511
***** Warning: Loss has increased *****
Loss at iteration [440]: 0.002952793754045796
Loss at iteration [441]: 0.002977277214592662
***** Warning: Loss has increased *****
Loss at iteration [442]: 0.002954177391628282
Loss at iteration [443]: 0.002977264165366524
***** Warning: Loss has increased *****
Loss at iteration [444]: 0.002953264119085654
Loss at iteration [445]: 0.0029765342900834606
***** Warning: Loss has increased *****
Loss at iteration [446]: 0.002953159677888576
Loss at iteration [447]: 0.0029759750028703464
***** Warning: Loss has increased *****
Loss at iteration [448]: 0.0029514608380523458
Loss at iteration [449]: 0.0029729255119513216
***** Warning: Loss has increased *****
Loss at iteration [450]: 0.0029473820687571906
Loss at iteration [451]: 0.0029689912555785334
***** Warning: Loss has increased *****
Loss at iteration [452]: 0.0029449165048214434
Loss at iteration [453]: 0.002965642037637973
***** Warning: Loss has increased *****
Loss at iteration [454]: 0.0029407048285201303
Loss at iteration [455]: 0.0029621786241030333
***** Warning: Loss has increased *****
Loss at iteration [456]: 0.0029381682201853774
Loss at iteration [457]: 0.002957824117127401
***** Warning: Loss has increased *****
Loss at iteration [458]: 0.0029323372305558614
Loss at iteration [459]: 0.002951068464782002
***** Warning: Loss has increased *****
Loss at iteration [460]: 0.002925656103802363
Loss at iteration [461]: 0.00294510761784856
***** Warning: Loss has increased *****
Loss at iteration [462]: 0.0029219222690273867
Loss at iteration [463]: 0.002940513354865137
***** Warning: Loss has increased *****
Loss at iteration [464]: 0.002916337833522053
Loss at iteration [465]: 0.0029349393038632862
***** Warning: Loss has increased *****
Loss at iteration [466]: 0.0029105430718275333
Loss at iteration [467]: 0.002927432313593521
***** Warning: Loss has increased *****
Loss at iteration [468]: 0.0029038845966829698
Loss at iteration [469]: 0.0029208662867070095
***** Warning: Loss has increased *****
Loss at iteration [470]: 0.0028975401251617587
Loss at iteration [471]: 0.002915019205175092
***** Warning: Loss has increased *****
Loss at iteration [472]: 0.0028927732818889392
Loss at iteration [473]: 0.0029091694757103002
***** Warning: Loss has increased *****
Loss at iteration [474]: 0.0028867336967366075
Loss at iteration [475]: 0.0029034975817692796
***** Warning: Loss has increased *****
Loss at iteration [476]: 0.0028818386551896762
Loss at iteration [477]: 0.002897811922684188
***** Warning: Loss has increased *****
Loss at iteration [478]: 0.0028762502516560106
Loss at iteration [479]: 0.0028917706924159537
***** Warning: Loss has increased *****
Loss at iteration [480]: 0.0028703350057568294
Loss at iteration [481]: 0.002886122081695283
***** Warning: Loss has increased *****
Loss at iteration [482]: 0.0028659522368195476
Loss at iteration [483]: 0.002882449415790318
***** Warning: Loss has increased *****
Loss at iteration [484]: 0.002862776722065396
Loss at iteration [485]: 0.0028776560842392975
***** Warning: Loss has increased *****
Loss at iteration [486]: 0.0028571546513392857
Loss at iteration [487]: 0.002871621273203597
***** Warning: Loss has increased *****
Loss at iteration [488]: 0.0028523248869021323
Loss at iteration [489]: 0.002867940639925778
***** Warning: Loss has increased *****
Loss at iteration [490]: 0.002849596666013069
Loss at iteration [491]: 0.0028649134037877044
***** Warning: Loss has increased *****
Loss at iteration [492]: 0.002845832626394559
Loss at iteration [493]: 0.0028607632166074825
***** Warning: Loss has increased *****
Loss at iteration [494]: 0.002842057704310946
Loss at iteration [495]: 0.002855870461157059
***** Warning: Loss has increased *****
Loss at iteration [496]: 0.002837069359941288
Loss at iteration [497]: 0.0028512805214723894
***** Warning: Loss has increased *****
Loss at iteration [498]: 0.0028339355647755194
Loss at iteration [499]: 0.0028485281872876566
***** Warning: Loss has increased *****
Loss at iteration [500]: 0.0028311886089631734
Loss at iteration [501]: 0.0028457169044291642
***** Warning: Loss has increased *****
Loss at iteration [502]: 0.002828583225541019
Loss at iteration [503]: 0.002843099097851363
***** Warning: Loss has increased *****
Loss at iteration [504]: 0.0028257490187649443
Loss at iteration [505]: 0.0028393880143427427
***** Warning: Loss has increased *****
Loss at iteration [506]: 0.002822916569417966
Loss at iteration [507]: 0.0028376712127890316
***** Warning: Loss has increased *****
Loss at iteration [508]: 0.0028216110554557926
Loss at iteration [509]: 0.002835855288278355
***** Warning: Loss has increased *****
Loss at iteration [510]: 0.0028193692358706784
Loss at iteration [511]: 0.0028340076332682433
***** Warning: Loss has increased *****
Loss at iteration [512]: 0.0028180999758200655
Loss at iteration [513]: 0.002832869819754035
***** Warning: Loss has increased *****
Loss at iteration [514]: 0.0028177683389157796
Loss at iteration [515]: 0.002832795119473177
***** Warning: Loss has increased *****
Loss at iteration [516]: 0.0028173181286601802
Loss at iteration [517]: 0.0028320094658090085
***** Warning: Loss has increased *****
Loss at iteration [518]: 0.0028161304567773564
Loss at iteration [519]: 0.0028313829666332655
***** Warning: Loss has increased *****
Loss at iteration [520]: 0.0028161479643302964
Loss at iteration [521]: 0.002830751634923045
***** Warning: Loss has increased *****
Loss at iteration [522]: 0.00281507922859121
Loss at iteration [523]: 0.0028300468749564577
***** Warning: Loss has increased *****
Loss at iteration [524]: 0.0028152694907051493
Loss at iteration [525]: 0.0028303450367834285
***** Warning: Loss has increased *****
Loss at iteration [526]: 0.002815271550170511
Loss at iteration [527]: 0.002830077790425788
***** Warning: Loss has increased *****
Loss at iteration [528]: 0.00281490254507579
Loss at iteration [529]: 0.0028307507027622233
***** Warning: Loss has increased *****
Loss at iteration [530]: 0.002816038691410778
Loss at iteration [531]: 0.0028319709893149503
***** Warning: Loss has increased *****
Loss at iteration [532]: 0.0028173338204233417
Loss at iteration [533]: 0.0028323526084499295
***** Warning: Loss has increased *****
Loss at iteration [534]: 0.0028169153780835936
Loss at iteration [535]: 0.002832355141328976
***** Warning: Loss has increased *****
Loss at iteration [536]: 0.002817446839866366
Loss at iteration [537]: 0.0028332854188102864
***** Warning: Loss has increased *****
Loss at iteration [538]: 0.0028182214195530304
Loss at iteration [539]: 0.002833777741883551
***** Warning: Loss has increased *****
Loss at iteration [540]: 0.002818345664838507
Loss at iteration [541]: 0.0028339737205361347
***** Warning: Loss has increased *****
Loss at iteration [542]: 0.0028193995392102027
Loss at iteration [543]: 0.0028353476147263646
***** Warning: Loss has increased *****
Loss at iteration [544]: 0.0028202142470888755
Loss at iteration [545]: 0.002836472032906065
***** Warning: Loss has increased *****
Loss at iteration [546]: 0.002821073511958857
Loss at iteration [547]: 0.0028367081556511862
***** Warning: Loss has increased *****
Loss at iteration [548]: 0.002821317472812045
Loss at iteration [549]: 0.0028372233890362358
***** Warning: Loss has increased *****
Loss at iteration [550]: 0.0028219342475292184
Loss at iteration [551]: 0.0028380637751834427
***** Warning: Loss has increased *****
Loss at iteration [552]: 0.0028228963277828773
Loss at iteration [553]: 0.0028395019215747535
***** Warning: Loss has increased *****
Loss at iteration [554]: 0.002824197118394464
Loss at iteration [555]: 0.0028398727144644544
***** Warning: Loss has increased *****
Loss at iteration [556]: 0.002824126223794352
Loss at iteration [557]: 0.0028398940899510942
***** Warning: Loss has increased *****
Loss at iteration [558]: 0.0028238706981666526
Loss at iteration [559]: 0.0028404106870118244
***** Warning: Loss has increased *****
Loss at iteration [560]: 0.0028250308915169025
Loss at iteration [561]: 0.002840999479609163
***** Warning: Loss has increased *****
Loss at iteration [562]: 0.002824903056275324
Loss at iteration [563]: 0.0028413133250867035
***** Warning: Loss has increased *****
Loss at iteration [564]: 0.0028256024958539724
Loss at iteration [565]: 0.00284138706446468
***** Warning: Loss has increased *****
Loss at iteration [566]: 0.0028248922710559553
Loss at iteration [567]: 0.002840324795294078
***** Warning: Loss has increased *****
Loss at iteration [568]: 0.0028240983078625936
Loss at iteration [569]: 0.0028402061171925925
***** Warning: Loss has increased *****
Loss at iteration [570]: 0.0028244674895286795
Loss at iteration [571]: 0.0028406239881352954
***** Warning: Loss has increased *****
Loss at iteration [572]: 0.002825067428946403
Loss at iteration [573]: 0.002842065744831324
***** Warning: Loss has increased *****
Loss at iteration [574]: 0.002826221916340797
Loss at iteration [575]: 0.0028421727884062413
***** Warning: Loss has increased *****
Loss at iteration [576]: 0.002825713621178322
Loss at iteration [577]: 0.002842137591017338
***** Warning: Loss has increased *****
Loss at iteration [578]: 0.0028265988948793976
Loss at iteration [579]: 0.0028428019106977436
***** Warning: Loss has increased *****
Loss at iteration [580]: 0.0028265722274518497
Loss at iteration [581]: 0.0028419460068252384
***** Warning: Loss has increased *****
Loss at iteration [582]: 0.002824665132580666
Loss at iteration [583]: 0.00284087502661074
***** Warning: Loss has increased *****
Loss at iteration [584]: 0.0028245054792108443
Loss at iteration [585]: 0.002839828931976997
***** Warning: Loss has increased *****
Loss at iteration [586]: 0.0028236956591784074
Loss at iteration [587]: 0.0028398465932533445
***** Warning: Loss has increased *****
Loss at iteration [588]: 0.0028235025847035853
Loss at iteration [589]: 0.002839071104509377
***** Warning: Loss has increased *****
Loss at iteration [590]: 0.00282252570250423
Loss at iteration [591]: 0.0028381603965851264
***** Warning: Loss has increased *****
Loss at iteration [592]: 0.0028213224129540284
Loss at iteration [593]: 0.0028363684787257397
***** Warning: Loss has increased *****
Loss at iteration [594]: 0.0028199668048904374
Loss at iteration [595]: 0.0028359173073453227
***** Warning: Loss has increased *****
Loss at iteration [596]: 0.0028198490904440855
Loss at iteration [597]: 0.002834932441184738
***** Warning: Loss has increased *****
Loss at iteration [598]: 0.0028183179847343846
Loss at iteration [599]: 0.002833701225972311
***** Warning: Loss has increased *****
Loss at iteration [600]: 0.002816684926689849
Loss at iteration [601]: 0.0028307991839923237
***** Warning: Loss has increased *****
Loss at iteration [602]: 0.0028135127344066594
Loss at iteration [603]: 0.002828299469846262
***** Warning: Loss has increased *****
Loss at iteration [604]: 0.002811980697332495
Loss at iteration [605]: 0.002825027424713613
***** Warning: Loss has increased *****
Loss at iteration [606]: 0.0028078151361368023
Loss at iteration [607]: 0.0028219220702791893
***** Warning: Loss has increased *****
Loss at iteration [608]: 0.0028055091491346264
Loss at iteration [609]: 0.0028194769877229746
***** Warning: Loss has increased *****
Loss at iteration [610]: 0.002804107136866127
Loss at iteration [611]: 0.002817502191414244
***** Warning: Loss has increased *****
Loss at iteration [612]: 0.0028014980194021838
Loss at iteration [613]: 0.00281525627476204
***** Warning: Loss has increased *****
Loss at iteration [614]: 0.002798716911686262
Loss at iteration [615]: 0.0028126701589371514
***** Warning: Loss has increased *****
Loss at iteration [616]: 0.0027970446949230287
Loss at iteration [617]: 0.0028093416018772385
***** Warning: Loss has increased *****
Loss at iteration [618]: 0.0027933035027772174
Loss at iteration [619]: 0.002806768764195274
***** Warning: Loss has increased *****
Loss at iteration [620]: 0.0027919817891485115
Loss at iteration [621]: 0.002805642042140192
***** Warning: Loss has increased *****
Loss at iteration [622]: 0.002790613840394713
Loss at iteration [623]: 0.002804721341845152
***** Warning: Loss has increased *****
Loss at iteration [624]: 0.0027894850042424818
Loss at iteration [625]: 0.0028027894233517677
***** Warning: Loss has increased *****
Loss at iteration [626]: 0.002787651036030578
Loss at iteration [627]: 0.002800191117364824
***** Warning: Loss has increased *****
Loss at iteration [628]: 0.0027850636117098273
Loss at iteration [629]: 0.0027980097134586153
***** Warning: Loss has increased *****
Loss at iteration [630]: 0.002783411058562531
Loss at iteration [631]: 0.002797376080042305
***** Warning: Loss has increased *****
Loss at iteration [632]: 0.002783805726681033
Loss at iteration [633]: 0.0027972710394608452
***** Warning: Loss has increased *****
Loss at iteration [634]: 0.0027824928823807446
Loss at iteration [635]: 0.0027964309767343656
***** Warning: Loss has increased *****
Loss at iteration [636]: 0.0027824110994001242
Loss at iteration [637]: 0.002796065803386605
***** Warning: Loss has increased *****
Loss at iteration [638]: 0.002781872393006205
Loss at iteration [639]: 0.0027958217373212683
***** Warning: Loss has increased *****
Loss at iteration [640]: 0.002782345846503875
Loss at iteration [641]: 0.0027961523633845686
***** Warning: Loss has increased *****
Loss at iteration [642]: 0.002782074226608001
Loss at iteration [643]: 0.002796261660274849
***** Warning: Loss has increased *****
Loss at iteration [644]: 0.0027814495401991424
Loss at iteration [645]: 0.0027947715953871577
***** Warning: Loss has increased *****
Loss at iteration [646]: 0.0027803560641490826
Loss at iteration [647]: 0.0027929443124843946
***** Warning: Loss has increased *****
Loss at iteration [648]: 0.00277815279042461
Loss at iteration [649]: 0.0027906388459777126
***** Warning: Loss has increased *****
Loss at iteration [650]: 0.00277612817084833
Loss at iteration [651]: 0.0027897200480371974
***** Warning: Loss has increased *****
Loss at iteration [652]: 0.002776054001733892
Loss at iteration [653]: 0.0027894964516580506
***** Warning: Loss has increased *****
Loss at iteration [654]: 0.0027756967002905315
Loss at iteration [655]: 0.0027895028621025392
***** Warning: Loss has increased *****
Loss at iteration [656]: 0.002775690283543319
Loss at iteration [657]: 0.0027889865646883076
***** Warning: Loss has increased *****
Loss at iteration [658]: 0.0027748914916536694
Loss at iteration [659]: 0.0027886177781682167
***** Warning: Loss has increased *****
Loss at iteration [660]: 0.0027749255575455043
Loss at iteration [661]: 0.002788501466364497
***** Warning: Loss has increased *****
Loss at iteration [662]: 0.002774621387416622
Loss at iteration [663]: 0.0027883697379393908
***** Warning: Loss has increased *****
Loss at iteration [664]: 0.002774790574898363
Loss at iteration [665]: 0.0027864105412894083
***** Warning: Loss has increased *****
Loss at iteration [666]: 0.002771486991950783
Loss at iteration [667]: 0.0027846434515442367
***** Warning: Loss has increased *****
Loss at iteration [668]: 0.0027702515940054486
Loss at iteration [669]: 0.002782901147095814
***** Warning: Loss has increased *****
Loss at iteration [670]: 0.0027690713619872805
Loss at iteration [671]: 0.0027825801903170787
***** Warning: Loss has increased *****
Loss at iteration [672]: 0.002769571808309825
Loss at iteration [673]: 0.002782731564671283
***** Warning: Loss has increased *****
Loss at iteration [674]: 0.0027692479955323274
Loss at iteration [675]: 0.002782259283560563
***** Warning: Loss has increased *****
Loss at iteration [676]: 0.0027686739263176174
Loss at iteration [677]: 0.0027818470337375124
***** Warning: Loss has increased *****
Loss at iteration [678]: 0.002767599501519015
Loss at iteration [679]: 0.002779981730532543
***** Warning: Loss has increased *****
Loss at iteration [680]: 0.002766392836574922
Loss at iteration [681]: 0.0027788249643842834
***** Warning: Loss has increased *****
Loss at iteration [682]: 0.0027650313368381544
Loss at iteration [683]: 0.0027779760889634817
***** Warning: Loss has increased *****
Loss at iteration [684]: 0.0027645462965515126
Loss at iteration [685]: 0.002777170113553793
***** Warning: Loss has increased *****
Loss at iteration [686]: 0.002763155643947996
Loss at iteration [687]: 0.002775917409159378
***** Warning: Loss has increased *****
Loss at iteration [688]: 0.0027627667602106365
Loss at iteration [689]: 0.0027740152347568194
***** Warning: Loss has increased *****
Loss at iteration [690]: 0.002759947473266277
Loss at iteration [691]: 0.0027717775051611976
***** Warning: Loss has increased *****
Loss at iteration [692]: 0.002758807248296867
Loss at iteration [693]: 0.0027711860207854187
***** Warning: Loss has increased *****
Loss at iteration [694]: 0.0027580209107436102
Loss at iteration [695]: 0.0027705798323246025
***** Warning: Loss has increased *****
Loss at iteration [696]: 0.0027572211758040677
Loss at iteration [697]: 0.0027692865208895487
***** Warning: Loss has increased *****
Loss at iteration [698]: 0.002756122401534771
Loss at iteration [699]: 0.0027686999781596226
***** Warning: Loss has increased *****
Loss at iteration [700]: 0.0027565665959699785
Loss at iteration [701]: 0.0027692241418790166
***** Warning: Loss has increased *****
Loss at iteration [702]: 0.002756465786631811
Loss at iteration [703]: 0.002768790259248246
***** Warning: Loss has increased *****
Loss at iteration [704]: 0.002755413278523635
Loss at iteration [705]: 0.002768265011733546
***** Warning: Loss has increased *****
Loss at iteration [706]: 0.0027555947929125356
Loss at iteration [707]: 0.0027682343232816836
***** Warning: Loss has increased *****
Loss at iteration [708]: 0.002755056618509609
Loss at iteration [709]: 0.0027670776917645733
***** Warning: Loss has increased *****
Loss at iteration [710]: 0.0027540819316309298
Loss at iteration [711]: 0.00276694377815484
***** Warning: Loss has increased *****
Loss at iteration [712]: 0.002754853035387915
Loss at iteration [713]: 0.002767396620296225
***** Warning: Loss has increased *****
Loss at iteration [714]: 0.0027541488293131547
Loss at iteration [715]: 0.00276476378101559
***** Warning: Loss has increased *****
Loss at iteration [716]: 0.0027505514298409623
Loss at iteration [717]: 0.0027627389345692083
***** Warning: Loss has increased *****
Loss at iteration [718]: 0.002750173338491052
Loss at iteration [719]: 0.0027622438048818965
***** Warning: Loss has increased *****
Loss at iteration [720]: 0.0027490331955929764
Loss at iteration [721]: 0.002760632924549041
***** Warning: Loss has increased *****
Loss at iteration [722]: 0.0027480257892383442
Loss at iteration [723]: 0.0027609860275557177
***** Warning: Loss has increased *****
Loss at iteration [724]: 0.002748765643772177
Loss at iteration [725]: 0.002761006395544458
***** Warning: Loss has increased *****
Loss at iteration [726]: 0.0027483047678500015
Loss at iteration [727]: 0.002760906262059918
***** Warning: Loss has increased *****
Loss at iteration [728]: 0.0027482003943717333
Loss at iteration [729]: 0.0027589167120902524
***** Warning: Loss has increased *****
Loss at iteration [730]: 0.002745241935764193
Loss at iteration [731]: 0.0027564481815868544
***** Warning: Loss has increased *****
Loss at iteration [732]: 0.002743608242346782
Loss at iteration [733]: 0.002755820003277346
***** Warning: Loss has increased *****
Loss at iteration [734]: 0.0027443626980771653
Loss at iteration [735]: 0.002756758020128088
***** Warning: Loss has increased *****
Loss at iteration [736]: 0.002744092094277969
Loss at iteration [737]: 0.0027560661126656864
***** Warning: Loss has increased *****
Loss at iteration [738]: 0.0027433972554902616
Loss at iteration [739]: 0.0027560156464674006
***** Warning: Loss has increased *****
Loss at iteration [740]: 0.00274414818871754
Loss at iteration [741]: 0.0027550145649253314
***** Warning: Loss has increased *****
Loss at iteration [742]: 0.0027422132410569017
Loss at iteration [743]: 0.0027544906839914673
***** Warning: Loss has increased *****
Loss at iteration [744]: 0.002742097336664389
Loss at iteration [745]: 0.0027539293884180334
***** Warning: Loss has increased *****
Loss at iteration [746]: 0.0027416827407427623
Loss at iteration [747]: 0.002753563003987339
***** Warning: Loss has increased *****
Loss at iteration [748]: 0.002741362795103288
Loss at iteration [749]: 0.002753703705024349
***** Warning: Loss has increased *****
Loss at iteration [750]: 0.002740877219422011
Loss at iteration [751]: 0.0027515481091882977
***** Warning: Loss has increased *****
Loss at iteration [752]: 0.002738703903122362
Loss at iteration [753]: 0.002749899768487408
***** Warning: Loss has increased *****
Loss at iteration [754]: 0.002737800508662334
Loss at iteration [755]: 0.0027497910109257513
***** Warning: Loss has increased *****
Loss at iteration [756]: 0.002737268337511442
Loss at iteration [757]: 0.0027486316998912874
***** Warning: Loss has increased *****
Loss at iteration [758]: 0.0027362315452884754
Loss at iteration [759]: 0.002747090230460975
***** Warning: Loss has increased *****
Loss at iteration [760]: 0.0027347412284541794
Loss at iteration [761]: 0.002745912794087221
***** Warning: Loss has increased *****
Loss at iteration [762]: 0.0027333428490566468
Loss at iteration [763]: 0.0027441059492669196
***** Warning: Loss has increased *****
Loss at iteration [764]: 0.0027311865016621745
Loss at iteration [765]: 0.0027424814840012026
***** Warning: Loss has increased *****
Loss at iteration [766]: 0.00273047460634036
Loss at iteration [767]: 0.0027404792241811477
***** Warning: Loss has increased *****
Loss at iteration [768]: 0.002728265736353331
Loss at iteration [769]: 0.0027386749872177056
***** Warning: Loss has increased *****
Loss at iteration [770]: 0.0027261897774612134
Loss at iteration [771]: 0.0027375368998393376
***** Warning: Loss has increased *****
Loss at iteration [772]: 0.0027265216283556388
Loss at iteration [773]: 0.0027378880853038525
***** Warning: Loss has increased *****
Loss at iteration [774]: 0.002726368287936751
Loss at iteration [775]: 0.002738078374792952
***** Warning: Loss has increased *****
Loss at iteration [776]: 0.0027270615489125394
Loss at iteration [777]: 0.002738497310641554
***** Warning: Loss has increased *****
Loss at iteration [778]: 0.002726825051571038
Loss at iteration [779]: 0.002737974717411696
***** Warning: Loss has increased *****
Loss at iteration [780]: 0.0027269497931310706
Loss at iteration [781]: 0.002738608188766738
***** Warning: Loss has increased *****
Loss at iteration [782]: 0.00272731964208346
Loss at iteration [783]: 0.002738767632022119
***** Warning: Loss has increased *****
Loss at iteration [784]: 0.0027273252998123313
Loss at iteration [785]: 0.002738885433390733
***** Warning: Loss has increased *****
Loss at iteration [786]: 0.0027267063283690223
Loss at iteration [787]: 0.002736253333977131
***** Warning: Loss has increased *****
Loss at iteration [788]: 0.002724026040056649
Loss at iteration [789]: 0.0027343214931112615
***** Warning: Loss has increased *****
Loss at iteration [790]: 0.0027226502973508064
Loss at iteration [791]: 0.0027340561140894943
***** Warning: Loss has increased *****
Loss at iteration [792]: 0.00272222651595038
Loss at iteration [793]: 0.0027325571620489203
***** Warning: Loss has increased *****
Loss at iteration [794]: 0.0027207451776188035
Loss at iteration [795]: 0.002730807940676946
***** Warning: Loss has increased *****
Loss at iteration [796]: 0.002719873460504666
Loss at iteration [797]: 0.002730374167059201
***** Warning: Loss has increased *****
Loss at iteration [798]: 0.002718900732195506
Loss at iteration [799]: 0.0027293014565294214
***** Warning: Loss has increased *****
Loss at iteration [800]: 0.002718073679656105
Loss at iteration [801]: 0.002728744518311394
***** Warning: Loss has increased *****
Loss at iteration [802]: 0.0027172751646630773
Loss at iteration [803]: 0.0027277423291036593
***** Warning: Loss has increased *****
Loss at iteration [804]: 0.0027161363218290665
Loss at iteration [805]: 0.0027270525444364462
***** Warning: Loss has increased *****
Loss at iteration [806]: 0.0027159168976796386
Loss at iteration [807]: 0.0027264130210560692
***** Warning: Loss has increased *****
Loss at iteration [808]: 0.0027153787190935578
Loss at iteration [809]: 0.0027262295341199585
***** Warning: Loss has increased *****
Loss at iteration [810]: 0.0027152610191408413
Loss at iteration [811]: 0.0027257412034347645
***** Warning: Loss has increased *****
Loss at iteration [812]: 0.002714709317693688
Loss at iteration [813]: 0.002725345104985389
***** Warning: Loss has increased *****
Loss at iteration [814]: 0.00271389929141014
Loss at iteration [815]: 0.0027250431584462703
***** Warning: Loss has increased *****
Loss at iteration [816]: 0.0027136347163535506
Loss at iteration [817]: 0.002722306802931847
***** Warning: Loss has increased *****
Loss at iteration [818]: 0.0027103286257471487
Loss at iteration [819]: 0.0027204306033953374
***** Warning: Loss has increased *****
Loss at iteration [820]: 0.0027089144030207134
Loss at iteration [821]: 0.002718908577258346
***** Warning: Loss has increased *****
Loss at iteration [822]: 0.002708015686282859
Loss at iteration [823]: 0.002717886351317463
***** Warning: Loss has increased *****
Loss at iteration [824]: 0.0027075934193607158
Loss at iteration [825]: 0.0027186500973591745
***** Warning: Loss has increased *****
Loss at iteration [826]: 0.002708335670224219
Loss at iteration [827]: 0.002719102329818529
***** Warning: Loss has increased *****
Loss at iteration [828]: 0.0027079503963054544
Loss at iteration [829]: 0.0027186648771902557
***** Warning: Loss has increased *****
Loss at iteration [830]: 0.0027077927289086345
Loss at iteration [831]: 0.0027168069589750135
***** Warning: Loss has increased *****
Loss at iteration [832]: 0.002705656168609665
Loss at iteration [833]: 0.002716308968875394
***** Warning: Loss has increased *****
Loss at iteration [834]: 0.0027060592326094896
Loss at iteration [835]: 0.0027162759569943976
***** Warning: Loss has increased *****
Loss at iteration [836]: 0.002705451367274026
Loss at iteration [837]: 0.002715484955848789
***** Warning: Loss has increased *****
Loss at iteration [838]: 0.002705030419716561
Loss at iteration [839]: 0.002715655695815251
***** Warning: Loss has increased *****
Loss at iteration [840]: 0.002705647717640198
Loss at iteration [841]: 0.0027162651823356807
***** Warning: Loss has increased *****
Loss at iteration [842]: 0.0027055585728032643
Loss at iteration [843]: 0.0027167459174547086
***** Warning: Loss has increased *****
Loss at iteration [844]: 0.0027063828994406164
Loss at iteration [845]: 0.0027166961237854058
***** Warning: Loss has increased *****
Loss at iteration [846]: 0.002706203835153637
Loss at iteration [847]: 0.002717198648624892
***** Warning: Loss has increased *****
Loss at iteration [848]: 0.002706108723443919
Loss at iteration [849]: 0.002716607321351667
***** Warning: Loss has increased *****
Loss at iteration [850]: 0.002705870119633111
Loss at iteration [851]: 0.0027162011383185837
***** Warning: Loss has increased *****
Loss at iteration [852]: 0.0027055620291407327
Loss at iteration [853]: 0.002716381220629642
***** Warning: Loss has increased *****
Loss at iteration [854]: 0.002706105804603706
Loss at iteration [855]: 0.002716638073732722
***** Warning: Loss has increased *****
Loss at iteration [856]: 0.002706036456977491
Loss at iteration [857]: 0.0027171868041031442
***** Warning: Loss has increased *****
Loss at iteration [858]: 0.0027065168768521516
Loss at iteration [859]: 0.002716662301483595
***** Warning: Loss has increased *****
Loss at iteration [860]: 0.0027055435139746002
Loss at iteration [861]: 0.0027157336307231986
***** Warning: Loss has increased *****
Loss at iteration [862]: 0.00270514650955737
Loss at iteration [863]: 0.0027161148564164095
***** Warning: Loss has increased *****
Loss at iteration [864]: 0.0027056538669953767
Loss at iteration [865]: 0.002716644817479497
***** Warning: Loss has increased *****
Loss at iteration [866]: 0.0027063490738829165
Loss at iteration [867]: 0.0027167189659117875
***** Warning: Loss has increased *****
Loss at iteration [868]: 0.0027053768704966784
Loss at iteration [869]: 0.0027154492712552466
***** Warning: Loss has increased *****
Loss at iteration [870]: 0.0027041083592845142
Loss at iteration [871]: 0.0027145837653242168
***** Warning: Loss has increased *****
Loss at iteration [872]: 0.0027034568633774727
Loss at iteration [873]: 0.0027132891485277155
***** Warning: Loss has increased *****
Loss at iteration [874]: 0.002701896996463824
Loss at iteration [875]: 0.002711187956439148
***** Warning: Loss has increased *****
Loss at iteration [876]: 0.0026996081503204354
Loss at iteration [877]: 0.0027093837999335913
***** Warning: Loss has increased *****
Loss at iteration [878]: 0.002698119567613526
Loss at iteration [879]: 0.0027073077255195975
***** Warning: Loss has increased *****
Loss at iteration [880]: 0.0026955632456765553
Loss at iteration [881]: 0.0027037929452725794
***** Warning: Loss has increased *****
Loss at iteration [882]: 0.0026923920152259985
Loss at iteration [883]: 0.0027011329476211055
***** Warning: Loss has increased *****
Loss at iteration [884]: 0.0026899534275547678
Loss at iteration [885]: 0.0026993568751375835
***** Warning: Loss has increased *****
Loss at iteration [886]: 0.0026883725227869965
Loss at iteration [887]: 0.002697135073800893
***** Warning: Loss has increased *****
Loss at iteration [888]: 0.0026867583535112303
Loss at iteration [889]: 0.0026955353736038408
***** Warning: Loss has increased *****
Loss at iteration [890]: 0.002684475617150013
Loss at iteration [891]: 0.002693351748401255
***** Warning: Loss has increased *****
Loss at iteration [892]: 0.002682382309417512
Loss at iteration [893]: 0.0026897433147334434
***** Warning: Loss has increased *****
Loss at iteration [894]: 0.0026783098310812475
Loss at iteration [895]: 0.002686341785517884
***** Warning: Loss has increased *****
Loss at iteration [896]: 0.0026760312821945825
Loss at iteration [897]: 0.002684186805559011
***** Warning: Loss has increased *****
Loss at iteration [898]: 0.002674070148316765
Loss at iteration [899]: 0.0026819965571701966
***** Warning: Loss has increased *****
Loss at iteration [900]: 0.002670885428282087
Loss at iteration [901]: 0.0026788909053986976
***** Warning: Loss has increased *****
Loss at iteration [902]: 0.002669050326873644
Loss at iteration [903]: 0.002676234123836813
***** Warning: Loss has increased *****
Loss at iteration [904]: 0.0026668370989618535
Loss at iteration [905]: 0.0026753748925599217
***** Warning: Loss has increased *****
Loss at iteration [906]: 0.002665845824127222
Loss at iteration [907]: 0.002673992303096481
***** Warning: Loss has increased *****
Loss at iteration [908]: 0.0026638520531862206
Loss at iteration [909]: 0.0026719069322216136
***** Warning: Loss has increased *****
Loss at iteration [910]: 0.002662155725814387
Loss at iteration [911]: 0.002669905798158749
***** Warning: Loss has increased *****
Loss at iteration [912]: 0.0026607194983965473
Loss at iteration [913]: 0.0026685917589486046
***** Warning: Loss has increased *****
Loss at iteration [914]: 0.002659253928493572
Loss at iteration [915]: 0.002667240842223761
***** Warning: Loss has increased *****
Loss at iteration [916]: 0.002657942487297583
Loss at iteration [917]: 0.002665716454206264
***** Warning: Loss has increased *****
Loss at iteration [918]: 0.0026569716874198742
Loss at iteration [919]: 0.0026655336905167764
***** Warning: Loss has increased *****
Loss at iteration [920]: 0.002656456868134498
Loss at iteration [921]: 0.0026642228784185726
***** Warning: Loss has increased *****
Loss at iteration [922]: 0.002654642953868099
Loss at iteration [923]: 0.0026620448893987635
***** Warning: Loss has increased *****
Loss at iteration [924]: 0.002653285363433838
Loss at iteration [925]: 0.0026615922213295367
***** Warning: Loss has increased *****
Loss at iteration [926]: 0.0026534578370912155
Loss at iteration [927]: 0.0026619034734048494
***** Warning: Loss has increased *****
Loss at iteration [928]: 0.002652643524020876
Loss at iteration [929]: 0.0026601934381629837
***** Warning: Loss has increased *****
Loss at iteration [930]: 0.002650853641299573
Loss at iteration [931]: 0.0026579448573410863
***** Warning: Loss has increased *****
Loss at iteration [932]: 0.0026486248667269394
Loss at iteration [933]: 0.002655254111019542
***** Warning: Loss has increased *****
Loss at iteration [934]: 0.002645939713499607
Loss at iteration [935]: 0.0026528229438141982
***** Warning: Loss has increased *****
Loss at iteration [936]: 0.002643743372257576
Loss at iteration [937]: 0.0026504939637685293
***** Warning: Loss has increased *****
Loss at iteration [938]: 0.0026412169907727157
Loss at iteration [939]: 0.0026483598331808076
***** Warning: Loss has increased *****
Loss at iteration [940]: 0.0026400696169960177
Loss at iteration [941]: 0.0026472689449304598
***** Warning: Loss has increased *****
Loss at iteration [942]: 0.002638459313966758
Loss at iteration [943]: 0.0026444945162429267
***** Warning: Loss has increased *****
Loss at iteration [944]: 0.002635391272141678
Loss at iteration [945]: 0.0026408581869431795
***** Warning: Loss has increased *****
Loss at iteration [946]: 0.0026318735286102357
Loss at iteration [947]: 0.0026371721808015175
***** Warning: Loss has increased *****
Loss at iteration [948]: 0.0026285699876076917
Loss at iteration [949]: 0.002634335749338669
***** Warning: Loss has increased *****
Loss at iteration [950]: 0.0026263864045425246
Loss at iteration [951]: 0.0026333906832936575
***** Warning: Loss has increased *****
Loss at iteration [952]: 0.0026258239291920738
Loss at iteration [953]: 0.002632510593381884
***** Warning: Loss has increased *****
Loss at iteration [954]: 0.002624870265234645
Loss at iteration [955]: 0.0026304908075298326
***** Warning: Loss has increased *****
Loss at iteration [956]: 0.0026221681606019066
Loss at iteration [957]: 0.002628998501455655
***** Warning: Loss has increased *****
Loss at iteration [958]: 0.002622094073878078
Loss at iteration [959]: 0.002629041612701229
***** Warning: Loss has increased *****
Loss at iteration [960]: 0.002622041645803676
Loss at iteration [961]: 0.00262956499074664
***** Warning: Loss has increased *****
Loss at iteration [962]: 0.002622595472707203
Loss at iteration [963]: 0.0026297314071365707
***** Warning: Loss has increased *****
Loss at iteration [964]: 0.002623032801469655
Loss at iteration [965]: 0.0026302572973747
***** Warning: Loss has increased *****
Loss at iteration [966]: 0.0026231768402790318
Loss at iteration [967]: 0.0026307272997691945
***** Warning: Loss has increased *****
Loss at iteration [968]: 0.002624010118314671
Loss at iteration [969]: 0.0026313136226040544
***** Warning: Loss has increased *****
Loss at iteration [970]: 0.002623906941468656
Loss at iteration [971]: 0.002631268216576788
***** Warning: Loss has increased *****
Loss at iteration [972]: 0.0026245707518737196
Loss at iteration [973]: 0.00263212164052626
***** Warning: Loss has increased *****
Loss at iteration [974]: 0.0026247642843302177
Loss at iteration [975]: 0.0026326421847093382
***** Warning: Loss has increased *****
Loss at iteration [976]: 0.0026256781025280297
Loss at iteration [977]: 0.002633464078248116
***** Warning: Loss has increased *****
Loss at iteration [978]: 0.002626295131415089
Loss at iteration [979]: 0.0026338452657287416
***** Warning: Loss has increased *****
Loss at iteration [980]: 0.0026267866759241447
Loss at iteration [981]: 0.002634673569070336
***** Warning: Loss has increased *****
Loss at iteration [982]: 0.0026277651114379784
Loss at iteration [983]: 0.0026360173128875466
***** Warning: Loss has increased *****
Loss at iteration [984]: 0.002629248125113473
Loss at iteration [985]: 0.002637284108328702
***** Warning: Loss has increased *****
Loss at iteration [986]: 0.002629706520229088
Loss at iteration [987]: 0.002637181427099374
***** Warning: Loss has increased *****
Loss at iteration [988]: 0.0026292314694530247
Loss at iteration [989]: 0.0026360939061220653
***** Warning: Loss has increased *****
Loss at iteration [990]: 0.0026282947377360743
Loss at iteration [991]: 0.0026354872911987276
***** Warning: Loss has increased *****
Loss at iteration [992]: 0.0026276599425662964
Loss at iteration [993]: 0.002634912385409722
***** Warning: Loss has increased *****
Loss at iteration [994]: 0.002627103621947821
Loss at iteration [995]: 0.002634827755088785
***** Warning: Loss has increased *****
Loss at iteration [996]: 0.00262776653874993
Loss at iteration [997]: 0.002635534213157931
***** Warning: Loss has increased *****
Loss at iteration [998]: 0.002627964806210846
Loss at iteration [999]: 0.002635658368907542
***** Warning: Loss has increased *****
Loss at iteration [1000]: 0.0026274895500997156
Loss at iteration [1001]: 0.002633775554812029
***** Warning: Loss has increased *****
Loss at iteration [1002]: 0.0026258924914385334
Loss at iteration [1003]: 0.0026331338085396248
***** Warning: Loss has increased *****
Loss at iteration [1004]: 0.0026254320952866393
Loss at iteration [1005]: 0.0026328032070390968
***** Warning: Loss has increased *****
Loss at iteration [1006]: 0.002624827252865798
Loss at iteration [1007]: 0.002631128063437437
***** Warning: Loss has increased *****
Loss at iteration [1008]: 0.002622948281487346
Loss at iteration [1009]: 0.002629509771393967
***** Warning: Loss has increased *****
Loss at iteration [1010]: 0.0026215456913251667
Loss at iteration [1011]: 0.0026274195078001073
***** Warning: Loss has increased *****
Loss at iteration [1012]: 0.0026192216117961282
Loss at iteration [1013]: 0.0026259890325242563
***** Warning: Loss has increased *****
Loss at iteration [1014]: 0.002618980992302689
Loss at iteration [1015]: 0.0026269140759370337
***** Warning: Loss has increased *****
Loss at iteration [1016]: 0.0026195141939167567
Loss at iteration [1017]: 0.0026247675186378745
***** Warning: Loss has increased *****
Loss at iteration [1018]: 0.00261644043478868
Loss at iteration [1019]: 0.002622490920897111
***** Warning: Loss has increased *****
Loss at iteration [1020]: 0.0026152092824112856
Loss at iteration [1021]: 0.0026228303370098823
***** Warning: Loss has increased *****
Loss at iteration [1022]: 0.0026158578036232856
Loss at iteration [1023]: 0.002622118648064874
***** Warning: Loss has increased *****
Loss at iteration [1024]: 0.0026142443517840385
Loss at iteration [1025]: 0.002620606495520515
***** Warning: Loss has increased *****
Loss at iteration [1026]: 0.002613598890178595
Loss at iteration [1027]: 0.002620465298266877
***** Warning: Loss has increased *****
Loss at iteration [1028]: 0.0026133312021486064
Loss at iteration [1029]: 0.002620236573582493
***** Warning: Loss has increased *****
Loss at iteration [1030]: 0.0026132889591256327
Loss at iteration [1031]: 0.002620021736776914
***** Warning: Loss has increased *****
Loss at iteration [1032]: 0.002613371710195738
Loss at iteration [1033]: 0.0026207336977076257
***** Warning: Loss has increased *****
Loss at iteration [1034]: 0.0026137389564086376
Loss at iteration [1035]: 0.0026206478270262177
***** Warning: Loss has increased *****
Loss at iteration [1036]: 0.002613552627369539
Loss at iteration [1037]: 0.0026212360678991857
***** Warning: Loss has increased *****
Loss at iteration [1038]: 0.0026150800795325395
Loss at iteration [1039]: 0.0026225686444535986
***** Warning: Loss has increased *****
Loss at iteration [1040]: 0.0026150103429814216
Loss at iteration [1041]: 0.0026226554142170966
***** Warning: Loss has increased *****
Loss at iteration [1042]: 0.0026158374285844906
Loss at iteration [1043]: 0.002622096481084199
***** Warning: Loss has increased *****
Loss at iteration [1044]: 0.0026146738073849576
Loss at iteration [1045]: 0.0026207319224453986
***** Warning: Loss has increased *****
Loss at iteration [1046]: 0.002612760705733337
Loss at iteration [1047]: 0.0026199882116249174
***** Warning: Loss has increased *****
Loss at iteration [1048]: 0.0026129903081566317
Loss at iteration [1049]: 0.002620030840672538
***** Warning: Loss has increased *****
Loss at iteration [1050]: 0.0026130213986349723
Loss at iteration [1051]: 0.002620181516040993
***** Warning: Loss has increased *****
Loss at iteration [1052]: 0.0026130263986380515
Loss at iteration [1053]: 0.002620597584533937
***** Warning: Loss has increased *****
Loss at iteration [1054]: 0.002613354930867109
Loss at iteration [1055]: 0.0026192738941723886
***** Warning: Loss has increased *****
Loss at iteration [1056]: 0.0026115531952431476
Loss at iteration [1057]: 0.002618152070705167
***** Warning: Loss has increased *****
Loss at iteration [1058]: 0.002610988375387613
Loss at iteration [1059]: 0.0026184039209790418
***** Warning: Loss has increased *****
Loss at iteration [1060]: 0.0026112708723285876
Loss at iteration [1061]: 0.0026170728588836077
***** Warning: Loss has increased *****
Loss at iteration [1062]: 0.002609083205936428
Loss at iteration [1063]: 0.0026146875734152477
***** Warning: Loss has increased *****
Loss at iteration [1064]: 0.002607703147835659
Loss at iteration [1065]: 0.002614981765221533
***** Warning: Loss has increased *****
Loss at iteration [1066]: 0.0026082457585241766
Loss at iteration [1067]: 0.0026152712453594495
***** Warning: Loss has increased *****
Loss at iteration [1068]: 0.002607930307423595
Loss at iteration [1069]: 0.002614252422656502
***** Warning: Loss has increased *****
Loss at iteration [1070]: 0.002607305365143671
Loss at iteration [1071]: 0.002613675426290237
***** Warning: Loss has increased *****
Loss at iteration [1072]: 0.0026065763120498666
Loss at iteration [1073]: 0.002612315232859879
***** Warning: Loss has increased *****
Loss at iteration [1074]: 0.0026045101165191333
Loss at iteration [1075]: 0.0026107988517025046
***** Warning: Loss has increased *****
Loss at iteration [1076]: 0.002603744552615443
Loss at iteration [1077]: 0.0026099002833631477
***** Warning: Loss has increased *****
Loss at iteration [1078]: 0.002603182760524718
Loss at iteration [1079]: 0.002609833308357124
***** Warning: Loss has increased *****
Loss at iteration [1080]: 0.002602335014217036
Loss at iteration [1081]: 0.002608015351763537
***** Warning: Loss has increased *****
Loss at iteration [1082]: 0.0026010016333789876
Loss at iteration [1083]: 0.0026071168161963905
***** Warning: Loss has increased *****
Loss at iteration [1084]: 0.0026009381548854284
Loss at iteration [1085]: 0.002607685754475344
***** Warning: Loss has increased *****
Loss at iteration [1086]: 0.002600541868184175
Loss at iteration [1087]: 0.0026071542438195665
***** Warning: Loss has increased *****
Loss at iteration [1088]: 0.002600970833819042
Loss at iteration [1089]: 0.0026060034658573856
***** Warning: Loss has increased *****
Loss at iteration [1090]: 0.0025986426145187345
Loss at iteration [1091]: 0.002604919936970795
***** Warning: Loss has increased *****
Loss at iteration [1092]: 0.0025989047878231444
Loss at iteration [1093]: 0.002606229797880108
***** Warning: Loss has increased *****
Loss at iteration [1094]: 0.002599711164183312
Loss at iteration [1095]: 0.002606503981197759
***** Warning: Loss has increased *****
Loss at iteration [1096]: 0.0025999028948715365
Loss at iteration [1097]: 0.0026070536603528785
***** Warning: Loss has increased *****
Loss at iteration [1098]: 0.0026011551970005265
Loss at iteration [1099]: 0.0026082425871240627
***** Warning: Loss has increased *****
Loss at iteration [1100]: 0.0026015847535272437
Loss at iteration [1101]: 0.002608442692734339
***** Warning: Loss has increased *****
Loss at iteration [1102]: 0.002601706097974262
Loss at iteration [1103]: 0.0026089768820764074
***** Warning: Loss has increased *****
Loss at iteration [1104]: 0.0026024407518524927
Loss at iteration [1105]: 0.0026092225501945033
***** Warning: Loss has increased *****
Loss at iteration [1106]: 0.0026030782295639074
Loss at iteration [1107]: 0.002610324051643879
***** Warning: Loss has increased *****
Loss at iteration [1108]: 0.0026034694149311734
Loss at iteration [1109]: 0.002609800145521676
***** Warning: Loss has increased *****
Loss at iteration [1110]: 0.0026033087355205775
Loss at iteration [1111]: 0.0026100046318011903
***** Warning: Loss has increased *****
Loss at iteration [1112]: 0.0026032222497950398
Loss at iteration [1113]: 0.002610425798610056
***** Warning: Loss has increased *****
Loss at iteration [1114]: 0.0026041700581628715
Loss at iteration [1115]: 0.0026113509845782793
***** Warning: Loss has increased *****
Loss at iteration [1116]: 0.0026045382219934076
Loss at iteration [1117]: 0.0026113267926240707
***** Warning: Loss has increased *****
Loss at iteration [1118]: 0.0026043918116663857
Loss at iteration [1119]: 0.002611036513018541
***** Warning: Loss has increased *****
Loss at iteration [1120]: 0.002604262728436243
Loss at iteration [1121]: 0.002611401223596479
***** Warning: Loss has increased *****
Loss at iteration [1122]: 0.0026047675672314606
Loss at iteration [1123]: 0.0026115307926677404
***** Warning: Loss has increased *****
Loss at iteration [1124]: 0.0026049284459591726
Loss at iteration [1125]: 0.0026121992210656076
***** Warning: Loss has increased *****
Loss at iteration [1126]: 0.00260521640312105
Loss at iteration [1127]: 0.0026118898927000064
***** Warning: Loss has increased *****
Loss at iteration [1128]: 0.0026050723729198317
Loss at iteration [1129]: 0.0026120993047690573
***** Warning: Loss has increased *****
Loss at iteration [1130]: 0.002605131583514323
Loss at iteration [1131]: 0.002610873795585274
***** Warning: Loss has increased *****
Loss at iteration [1132]: 0.0026032871769651744
Loss at iteration [1133]: 0.0026099594736745235
***** Warning: Loss has increased *****
Loss at iteration [1134]: 0.00260267472676165
Loss at iteration [1135]: 0.0026089416945641393
***** Warning: Loss has increased *****
Loss at iteration [1136]: 0.0026022011730014697
Loss at iteration [1137]: 0.0026072516188362983
***** Warning: Loss has increased *****
Loss at iteration [1138]: 0.0025991728831290585
Loss at iteration [1139]: 0.002604825543426621
***** Warning: Loss has increased *****
Loss at iteration [1140]: 0.002597697166803353
Loss at iteration [1141]: 0.002604439146375274
***** Warning: Loss has increased *****
Loss at iteration [1142]: 0.002597665616495169
Loss at iteration [1143]: 0.0026021319134011006
***** Warning: Loss has increased *****
Loss at iteration [1144]: 0.002594928845700355
Loss at iteration [1145]: 0.0026010970831645787
***** Warning: Loss has increased *****
Loss at iteration [1146]: 0.002593836803285519
Loss at iteration [1147]: 0.0025988802137956427
***** Warning: Loss has increased *****
Loss at iteration [1148]: 0.0025916882061716314
Loss at iteration [1149]: 0.002598061934092515
***** Warning: Loss has increased *****
Loss at iteration [1150]: 0.0025921007423897295
Loss at iteration [1151]: 0.002597745116917345
***** Warning: Loss has increased *****
Loss at iteration [1152]: 0.0025910038148350524
Loss at iteration [1153]: 0.002596470334621585
***** Warning: Loss has increased *****
Loss at iteration [1154]: 0.0025893792998195157
Loss at iteration [1155]: 0.0025949492679752475
***** Warning: Loss has increased *****
Loss at iteration [1156]: 0.0025883592024876366
Loss at iteration [1157]: 0.0025947369676841063
***** Warning: Loss has increased *****
Loss at iteration [1158]: 0.0025888713224824817
Loss at iteration [1159]: 0.002593028153646038
***** Warning: Loss has increased *****
Loss at iteration [1160]: 0.0025857307665159077
Loss at iteration [1161]: 0.002591539187969554
***** Warning: Loss has increased *****
Loss at iteration [1162]: 0.0025849979664081114
Loss at iteration [1163]: 0.0025906960007947275
***** Warning: Loss has increased *****
Loss at iteration [1164]: 0.0025845805530793566
Loss at iteration [1165]: 0.0025901112257436694
***** Warning: Loss has increased *****
Loss at iteration [1166]: 0.002584483673434776
Loss at iteration [1167]: 0.0025906668579894704
***** Warning: Loss has increased *****
Loss at iteration [1168]: 0.002584432157034377
Loss at iteration [1169]: 0.0025908433325234634
***** Warning: Loss has increased *****
Loss at iteration [1170]: 0.0025848344007735327
Loss at iteration [1171]: 0.0025908724893580325
***** Warning: Loss has increased *****
Loss at iteration [1172]: 0.002584737398368893
Loss at iteration [1173]: 0.0025911824095833954
***** Warning: Loss has increased *****
Loss at iteration [1174]: 0.0025854871614686315
Loss at iteration [1175]: 0.002591793363939594
***** Warning: Loss has increased *****
Loss at iteration [1176]: 0.0025858660395710774
Loss at iteration [1177]: 0.002592542274534949
***** Warning: Loss has increased *****
Loss at iteration [1178]: 0.0025864981134143743
Loss at iteration [1179]: 0.0025924288703185992
***** Warning: Loss has increased *****
Loss at iteration [1180]: 0.0025861042145049295
Loss at iteration [1181]: 0.0025926194670221237
***** Warning: Loss has increased *****
Loss at iteration [1182]: 0.0025866435511232157
Loss at iteration [1183]: 0.0025928590306878484
***** Warning: Loss has increased *****
Loss at iteration [1184]: 0.002586763511326812
Loss at iteration [1185]: 0.002593348683750303
***** Warning: Loss has increased *****
Loss at iteration [1186]: 0.002587109492774126
Loss at iteration [1187]: 0.0025923718199282546
***** Warning: Loss has increased *****
Loss at iteration [1188]: 0.0025855630512903764
Loss at iteration [1189]: 0.002591507863970954
***** Warning: Loss has increased *****
Loss at iteration [1190]: 0.002585375429483071
Loss at iteration [1191]: 0.0025918366582666943
***** Warning: Loss has increased *****
Loss at iteration [1192]: 0.002585520949268929
Loss at iteration [1193]: 0.002590558983047666
***** Warning: Loss has increased *****
Loss at iteration [1194]: 0.0025836023639188225
Loss at iteration [1195]: 0.0025887636476342566
***** Warning: Loss has increased *****
Loss at iteration [1196]: 0.00258253702985254
Loss at iteration [1197]: 0.0025886701107102096
***** Warning: Loss has increased *****
Loss at iteration [1198]: 0.0025824589414143065
Loss at iteration [1199]: 0.002588293867830421
***** Warning: Loss has increased *****
Loss at iteration [1200]: 0.002581835096872046
Loss at iteration [1201]: 0.002588018949034299
***** Warning: Loss has increased *****
Loss at iteration [1202]: 0.002581768714813592
Loss at iteration [1203]: 0.0025867511593912307
***** Warning: Loss has increased *****
Loss at iteration [1204]: 0.0025803320221608342
Loss at iteration [1205]: 0.0025858620422072884
***** Warning: Loss has increased *****
Loss at iteration [1206]: 0.00257953205292825
Loss at iteration [1207]: 0.002584735581576942
***** Warning: Loss has increased *****
Loss at iteration [1208]: 0.0025778182529716124
Loss at iteration [1209]: 0.0025836148614471026
***** Warning: Loss has increased *****
Loss at iteration [1210]: 0.002577680787500658
Loss at iteration [1211]: 0.0025827577154554095
***** Warning: Loss has increased *****
Loss at iteration [1212]: 0.0025769198610527955
Loss at iteration [1213]: 0.0025828552177725545
***** Warning: Loss has increased *****
Loss at iteration [1214]: 0.002576412042559591
Loss at iteration [1215]: 0.0025820220308522533
***** Warning: Loss has increased *****
Loss at iteration [1216]: 0.002576053306851058
Loss at iteration [1217]: 0.0025817651339639794
***** Warning: Loss has increased *****
Loss at iteration [1218]: 0.002575924455669744
Loss at iteration [1219]: 0.002580536892649278
***** Warning: Loss has increased *****
Loss at iteration [1220]: 0.002573969382371076
Loss at iteration [1221]: 0.0025796150015918663
***** Warning: Loss has increased *****
Loss at iteration [1222]: 0.00257386745975727
Loss at iteration [1223]: 0.0025795364882512635
***** Warning: Loss has increased *****
Loss at iteration [1224]: 0.0025737099554316994
Loss at iteration [1225]: 0.002579595414976754
***** Warning: Loss has increased *****
Loss at iteration [1226]: 0.0025734618395327536
Loss at iteration [1227]: 0.0025789599171088225
***** Warning: Loss has increased *****
Loss at iteration [1228]: 0.0025727985796278638
Loss at iteration [1229]: 0.002577563219387973
***** Warning: Loss has increased *****
Loss at iteration [1230]: 0.002571958860720878
Loss at iteration [1231]: 0.002577657041345902
***** Warning: Loss has increased *****
Loss at iteration [1232]: 0.002571241363334338
Loss at iteration [1233]: 0.0025767989988495776
***** Warning: Loss has increased *****
Loss at iteration [1234]: 0.002570871488240739
Loss at iteration [1235]: 0.0025764098471557297
***** Warning: Loss has increased *****
Loss at iteration [1236]: 0.0025707512739171225
Loss at iteration [1237]: 0.0025755248528084107
***** Warning: Loss has increased *****
Loss at iteration [1238]: 0.0025695443426205127
Loss at iteration [1239]: 0.002574570248250261
***** Warning: Loss has increased *****
Loss at iteration [1240]: 0.002568398431326753
Loss at iteration [1241]: 0.0025734499551724518
***** Warning: Loss has increased *****
Loss at iteration [1242]: 0.002567325516872784
Loss at iteration [1243]: 0.0025726349223970913
***** Warning: Loss has increased *****
Loss at iteration [1244]: 0.002567151223340491
Loss at iteration [1245]: 0.0025731978256633443
***** Warning: Loss has increased *****
Loss at iteration [1246]: 0.0025679202844912027
Loss at iteration [1247]: 0.002573829377022728
***** Warning: Loss has increased *****
Loss at iteration [1248]: 0.0025686062254248604
Loss at iteration [1249]: 0.002574895905550478
***** Warning: Loss has increased *****
Loss at iteration [1250]: 0.0025693199777664997
Loss at iteration [1251]: 0.002574443517068799
***** Warning: Loss has increased *****
Loss at iteration [1252]: 0.0025684875669796047
Loss at iteration [1253]: 0.0025738348859345467
***** Warning: Loss has increased *****
Loss at iteration [1254]: 0.0025678458972999713
Loss at iteration [1255]: 0.0025737059811571763
***** Warning: Loss has increased *****
Loss at iteration [1256]: 0.0025684246210285044
Loss at iteration [1257]: 0.002574070271906717
***** Warning: Loss has increased *****
Loss at iteration [1258]: 0.0025690728209811302
Loss at iteration [1259]: 0.002575552262374953
***** Warning: Loss has increased *****
Loss at iteration [1260]: 0.0025699501461848174
Loss at iteration [1261]: 0.002575986842895703
***** Warning: Loss has increased *****
Loss at iteration [1262]: 0.00257057564571737
Loss at iteration [1263]: 0.002576354966073022
***** Warning: Loss has increased *****
Loss at iteration [1264]: 0.0025706779950008334
Loss at iteration [1265]: 0.002576652003695483
***** Warning: Loss has increased *****
Loss at iteration [1266]: 0.002570743863421152
Loss at iteration [1267]: 0.0025767455025538203
***** Warning: Loss has increased *****
Loss at iteration [1268]: 0.0025711912233153823
Loss at iteration [1269]: 0.002577262685381228
***** Warning: Loss has increased *****
Loss at iteration [1270]: 0.002571718755137431
Loss at iteration [1271]: 0.0025778121681492407
***** Warning: Loss has increased *****
Loss at iteration [1272]: 0.0025715363850654332
Loss at iteration [1273]: 0.0025758278627456836
***** Warning: Loss has increased *****
Loss at iteration [1274]: 0.0025695811355391112
Loss at iteration [1275]: 0.002575487458491404
***** Warning: Loss has increased *****
Loss at iteration [1276]: 0.002569752276543311
Loss at iteration [1277]: 0.002575669052617141
***** Warning: Loss has increased *****
Loss at iteration [1278]: 0.0025697005336859915
Loss at iteration [1279]: 0.0025751142273548458
***** Warning: Loss has increased *****
Loss at iteration [1280]: 0.00256918368368379
Loss at iteration [1281]: 0.0025749780870502497
***** Warning: Loss has increased *****
Loss at iteration [1282]: 0.0025686684633063244
Loss at iteration [1283]: 0.002572247717448268
***** Warning: Loss has increased *****
Loss at iteration [1284]: 0.0025657131950995167
Loss at iteration [1285]: 0.0025706449343265523
***** Warning: Loss has increased *****
Loss at iteration [1286]: 0.00256423539805457
Loss at iteration [1287]: 0.002569250260986499
***** Warning: Loss has increased *****
Loss at iteration [1288]: 0.002563678711336038
Loss at iteration [1289]: 0.0025683780545890447
***** Warning: Loss has increased *****
Loss at iteration [1290]: 0.002562068121826138
Loss at iteration [1291]: 0.002566609587447547
***** Warning: Loss has increased *****
Loss at iteration [1292]: 0.0025610198308694924
Loss at iteration [1293]: 0.0025663477797445225
***** Warning: Loss has increased *****
Loss at iteration [1294]: 0.0025603773028860296
Loss at iteration [1295]: 0.002564214398917671
***** Warning: Loss has increased *****
Loss at iteration [1296]: 0.0025577910007547238
Loss at iteration [1297]: 0.0025627580400538036
***** Warning: Loss has increased *****
Loss at iteration [1298]: 0.0025574276594065907
Loss at iteration [1299]: 0.002561286426727952
***** Warning: Loss has increased *****
Loss at iteration [1300]: 0.0025554163977882377
Loss at iteration [1301]: 0.0025600104145053976
***** Warning: Loss has increased *****
Loss at iteration [1302]: 0.0025542785427911004
Loss at iteration [1303]: 0.0025593961114517066
***** Warning: Loss has increased *****
Loss at iteration [1304]: 0.002554233214090294
Loss at iteration [1305]: 0.0025576488161978754
***** Warning: Loss has increased *****
Loss at iteration [1306]: 0.0025515067113050984
Loss at iteration [1307]: 0.0025561438549709502
***** Warning: Loss has increased *****
Loss at iteration [1308]: 0.0025508174186894095
Loss at iteration [1309]: 0.0025555991387128133
***** Warning: Loss has increased *****
Loss at iteration [1310]: 0.0025501016774163617
Loss at iteration [1311]: 0.002554391785356893
***** Warning: Loss has increased *****
Loss at iteration [1312]: 0.00254934702511839
Loss at iteration [1313]: 0.0025531982846815652
***** Warning: Loss has increased *****
Loss at iteration [1314]: 0.0025475710876337065
Loss at iteration [1315]: 0.002552454144882606
***** Warning: Loss has increased *****
Loss at iteration [1316]: 0.0025470528664826527
Loss at iteration [1317]: 0.002551241526414876
***** Warning: Loss has increased *****
Loss at iteration [1318]: 0.0025462443419541282
Loss at iteration [1319]: 0.002551230298438351
***** Warning: Loss has increased *****
Loss at iteration [1320]: 0.002546572150340927
Loss at iteration [1321]: 0.0025511991052406344
***** Warning: Loss has increased *****
Loss at iteration [1322]: 0.002545714116101127
Loss at iteration [1323]: 0.002550573275962581
***** Warning: Loss has increased *****
Loss at iteration [1324]: 0.002546185112626671
Loss at iteration [1325]: 0.0025519059133469807
***** Warning: Loss has increased *****
Loss at iteration [1326]: 0.0025468878852648305
Loss at iteration [1327]: 0.002551314994142148
***** Warning: Loss has increased *****
Loss at iteration [1328]: 0.0025456774775551396
Loss at iteration [1329]: 0.0025502974999758424
***** Warning: Loss has increased *****
Loss at iteration [1330]: 0.0025456681592928806
Loss at iteration [1331]: 0.002549542686371333
***** Warning: Loss has increased *****
Loss at iteration [1332]: 0.002544591995738083
Loss at iteration [1333]: 0.0025495735005845684
***** Warning: Loss has increased *****
Loss at iteration [1334]: 0.002544701195866189
Loss at iteration [1335]: 0.002549572081145301
***** Warning: Loss has increased *****
Loss at iteration [1336]: 0.0025447040516974545
Loss at iteration [1337]: 0.002549882260947711
***** Warning: Loss has increased *****
Loss at iteration [1338]: 0.0025450809697959414
Loss at iteration [1339]: 0.0025500620715321193
***** Warning: Loss has increased *****
Loss at iteration [1340]: 0.002545458383457738
Loss at iteration [1341]: 0.0025505784893877013
***** Warning: Loss has increased *****
Loss at iteration [1342]: 0.0025454532751148745
Loss at iteration [1343]: 0.0025504185897721465
***** Warning: Loss has increased *****
Loss at iteration [1344]: 0.0025457267199791643
Loss at iteration [1345]: 0.002550996767668055
***** Warning: Loss has increased *****
Loss at iteration [1346]: 0.0025462030093007723
Loss at iteration [1347]: 0.002551356401810368
***** Warning: Loss has increased *****
Loss at iteration [1348]: 0.0025466961798880005
Loss at iteration [1349]: 0.002551808726239781
***** Warning: Loss has increased *****
Loss at iteration [1350]: 0.0025467839716677805
Loss at iteration [1351]: 0.0025521009562957586
***** Warning: Loss has increased *****
Loss at iteration [1352]: 0.002547463980589456
Loss at iteration [1353]: 0.0025529671715271594
***** Warning: Loss has increased *****
Loss at iteration [1354]: 0.002547783228753037
Loss at iteration [1355]: 0.0025522231859262125
***** Warning: Loss has increased *****
Loss at iteration [1356]: 0.0025469448332276954
Loss at iteration [1357]: 0.0025515365403263007
***** Warning: Loss has increased *****
Loss at iteration [1358]: 0.002546193270566788
Loss at iteration [1359]: 0.0025507636562859806
***** Warning: Loss has increased *****
Loss at iteration [1360]: 0.0025454801554068806
Loss at iteration [1361]: 0.0025502450838433837
***** Warning: Loss has increased *****
Loss at iteration [1362]: 0.0025453612120846336
Loss at iteration [1363]: 0.002550736557080011
***** Warning: Loss has increased *****
Loss at iteration [1364]: 0.0025460392530885934
Loss at iteration [1365]: 0.0025512037721386353
***** Warning: Loss has increased *****
Loss at iteration [1366]: 0.0025457016876566465
Loss at iteration [1367]: 0.0025507065181585107
***** Warning: Loss has increased *****
Loss at iteration [1368]: 0.002545801038276961
Loss at iteration [1369]: 0.0025496506644901674
***** Warning: Loss has increased *****
Loss at iteration [1370]: 0.002544163080848001
Loss at iteration [1371]: 0.002549113276847974
***** Warning: Loss has increased *****
Loss at iteration [1372]: 0.002544191134962591
Loss at iteration [1373]: 0.002549553232316622
***** Warning: Loss has increased *****
Loss at iteration [1374]: 0.002544364105234033
Loss at iteration [1375]: 0.0025487846417990914
***** Warning: Loss has increased *****
Loss at iteration [1376]: 0.0025435567068268607
Loss at iteration [1377]: 0.002548439567663268
***** Warning: Loss has increased *****
Loss at iteration [1378]: 0.0025435037935133847
Loss at iteration [1379]: 0.0025475109185240786
***** Warning: Loss has increased *****
Loss at iteration [1380]: 0.0025422228538735862
Loss at iteration [1381]: 0.0025471190856145307
***** Warning: Loss has increased *****
Loss at iteration [1382]: 0.0025418442976038766
Loss at iteration [1383]: 0.0025463543200203837
***** Warning: Loss has increased *****
Loss at iteration [1384]: 0.002541447676889766
Loss at iteration [1385]: 0.0025460783763410146
***** Warning: Loss has increased *****
Loss at iteration [1386]: 0.0025411104920867607
Loss at iteration [1387]: 0.0025463390870586397
***** Warning: Loss has increased *****
Loss at iteration [1388]: 0.002541712696114371
Loss at iteration [1389]: 0.0025466736127866055
***** Warning: Loss has increased *****
Loss at iteration [1390]: 0.0025414788475635213
Loss at iteration [1391]: 0.002545639170758374
***** Warning: Loss has increased *****
Loss at iteration [1392]: 0.002539971206526132
Loss at iteration [1393]: 0.0025438776542295875
***** Warning: Loss has increased *****
Loss at iteration [1394]: 0.002538779936149425
Loss at iteration [1395]: 0.0025435444188930934
***** Warning: Loss has increased *****
Loss at iteration [1396]: 0.002538988343118977
Loss at iteration [1397]: 0.002544190666885942
***** Warning: Loss has increased *****
Loss at iteration [1398]: 0.002539218592828509
Loss at iteration [1399]: 0.0025429027535247973
***** Warning: Loss has increased *****
Loss at iteration [1400]: 0.0025373146715813996
Loss at iteration [1401]: 0.0025407994302362137
***** Warning: Loss has increased *****
Loss at iteration [1402]: 0.002535529792847618
Loss at iteration [1403]: 0.0025400706157569742
***** Warning: Loss has increased *****
Loss at iteration [1404]: 0.0025349829754569655
Loss at iteration [1405]: 0.002538368046617837
***** Warning: Loss has increased *****
Loss at iteration [1406]: 0.0025330945003703143
Loss at iteration [1407]: 0.002537696781494029
***** Warning: Loss has increased *****
Loss at iteration [1408]: 0.0025329739230360356
Loss at iteration [1409]: 0.0025366723096924477
***** Warning: Loss has increased *****
Loss at iteration [1410]: 0.002531760135347739
Loss at iteration [1411]: 0.002535022343253645
***** Warning: Loss has increased *****
Loss at iteration [1412]: 0.002530043729860771
Loss at iteration [1413]: 0.002534637175990829
***** Warning: Loss has increased *****
Loss at iteration [1414]: 0.002530016341784384
Loss at iteration [1415]: 0.002534915003252803
***** Warning: Loss has increased *****
Loss at iteration [1416]: 0.002530845010834566
Loss at iteration [1417]: 0.002535511342704034
***** Warning: Loss has increased *****
Loss at iteration [1418]: 0.002530958526181256
Loss at iteration [1419]: 0.0025356061724355478
***** Warning: Loss has increased *****
Loss at iteration [1420]: 0.0025309341720407856
Loss at iteration [1421]: 0.0025354890007556366
***** Warning: Loss has increased *****
Loss at iteration [1422]: 0.0025310949123587643
Loss at iteration [1423]: 0.0025362772852293674
***** Warning: Loss has increased *****
Loss at iteration [1424]: 0.0025321409620292505
Loss at iteration [1425]: 0.0025373740832880186
***** Warning: Loss has increased *****
Loss at iteration [1426]: 0.0025328933129262414
Loss at iteration [1427]: 0.002537616447139774
***** Warning: Loss has increased *****
Loss at iteration [1428]: 0.0025331073629042697
Loss at iteration [1429]: 0.0025369795622919183
***** Warning: Loss has increased *****
Loss at iteration [1430]: 0.002532239120444405
Loss at iteration [1431]: 0.0025372301729257207
***** Warning: Loss has increased *****
Loss at iteration [1432]: 0.00253269116773067
Loss at iteration [1433]: 0.0025371862212840014
***** Warning: Loss has increased *****
Loss at iteration [1434]: 0.0025324151453378304
Loss at iteration [1435]: 0.002537358319628501
***** Warning: Loss has increased *****
Loss at iteration [1436]: 0.002532864150802639
Loss at iteration [1437]: 0.002538080181197701
***** Warning: Loss has increased *****
Loss at iteration [1438]: 0.002533674084116556
Loss at iteration [1439]: 0.0025381610857395944
***** Warning: Loss has increased *****
Loss at iteration [1440]: 0.002533148374714219
Loss at iteration [1441]: 0.0025378525233199336
***** Warning: Loss has increased *****
Loss at iteration [1442]: 0.002533427000866925
Loss at iteration [1443]: 0.0025386775477979203
***** Warning: Loss has increased *****
Loss at iteration [1444]: 0.0025341710089663043
Loss at iteration [1445]: 0.002538745674005784
***** Warning: Loss has increased *****
Loss at iteration [1446]: 0.0025340572775007057
Loss at iteration [1447]: 0.002538849450060929
***** Warning: Loss has increased *****
Loss at iteration [1448]: 0.0025338958847187752
Loss at iteration [1449]: 0.002538796032612411
***** Warning: Loss has increased *****
Loss at iteration [1450]: 0.0025343341559652052
Loss at iteration [1451]: 0.002539079150777795
***** Warning: Loss has increased *****
Loss at iteration [1452]: 0.0025340260779414944
Loss at iteration [1453]: 0.002538933504637397
***** Warning: Loss has increased *****
Loss at iteration [1454]: 0.0025344533011803774
Loss at iteration [1455]: 0.002539646003835269
***** Warning: Loss has increased *****
Loss at iteration [1456]: 0.0025346974402200395
Loss at iteration [1457]: 0.002538085348443574
***** Warning: Loss has increased *****
Loss at iteration [1458]: 0.002532642376284081
Loss at iteration [1459]: 0.002537240845787485
***** Warning: Loss has increased *****
Loss at iteration [1460]: 0.002532196034067413
Loss at iteration [1461]: 0.0025369656386239905
***** Warning: Loss has increased *****
Loss at iteration [1462]: 0.0025327074355087523
Loss at iteration [1463]: 0.0025372503650953017
***** Warning: Loss has increased *****
Loss at iteration [1464]: 0.002532213865720991
Loss at iteration [1465]: 0.002536989163522933
***** Warning: Loss has increased *****
Loss at iteration [1466]: 0.002532274685059396
Loss at iteration [1467]: 0.0025370801957646768
***** Warning: Loss has increased *****
Loss at iteration [1468]: 0.0025320635779948614
Loss at iteration [1469]: 0.002535625576747491
***** Warning: Loss has increased *****
Loss at iteration [1470]: 0.0025303476894272823
Loss at iteration [1471]: 0.0025348051775871965
***** Warning: Loss has increased *****
Loss at iteration [1472]: 0.0025298102697803994
Loss at iteration [1473]: 0.0025335212572895334
***** Warning: Loss has increased *****
Loss at iteration [1474]: 0.0025281603822529053
Loss at iteration [1475]: 0.0025313658615280852
***** Warning: Loss has increased *****
Loss at iteration [1476]: 0.0025262200324881797
Loss at iteration [1477]: 0.002530513050301051
***** Warning: Loss has increased *****
Loss at iteration [1478]: 0.0025256932828100534
Loss at iteration [1479]: 0.002529336084641178
***** Warning: Loss has increased *****
Loss at iteration [1480]: 0.0025246190039094024
Loss at iteration [1481]: 0.002528677072662058
***** Warning: Loss has increased *****
Loss at iteration [1482]: 0.0025237255845489324
Loss at iteration [1483]: 0.002527490563321621
***** Warning: Loss has increased *****
Loss at iteration [1484]: 0.002523126010059664
Loss at iteration [1485]: 0.0025278710341589627
***** Warning: Loss has increased *****
Loss at iteration [1486]: 0.0025236481694702563
Loss at iteration [1487]: 0.0025275099114333826
***** Warning: Loss has increased *****
Loss at iteration [1488]: 0.002523029928823132
Loss at iteration [1489]: 0.002527716675380141
***** Warning: Loss has increased *****
Loss at iteration [1490]: 0.002523316219427584
Loss at iteration [1491]: 0.0025280368686159587
***** Warning: Loss has increased *****
Loss at iteration [1492]: 0.00252369325080658
Loss at iteration [1493]: 0.0025277809874682824
***** Warning: Loss has increased *****
Loss at iteration [1494]: 0.002523167011458942
Loss at iteration [1495]: 0.002527796298582339
***** Warning: Loss has increased *****
Loss at iteration [1496]: 0.0025236694650909637
Loss at iteration [1497]: 0.002527890121402947
***** Warning: Loss has increased *****
Loss at iteration [1498]: 0.002523063821697145
Loss at iteration [1499]: 0.002526724099073371
***** Warning: Loss has increased *****
Loss at iteration [1500]: 0.0025221416808572407
Loss at iteration [1501]: 0.002526757017497626
***** Warning: Loss has increased *****
Loss at iteration [1502]: 0.0025221478804507086
Loss at iteration [1503]: 0.0025259880138801174
***** Warning: Loss has increased *****
Loss at iteration [1504]: 0.0025216765063154206
Loss at iteration [1505]: 0.002525504488981846
***** Warning: Loss has increased *****
Loss at iteration [1506]: 0.0025206413533041096
Loss at iteration [1507]: 0.0025249905074643526
***** Warning: Loss has increased *****
Loss at iteration [1508]: 0.0025208848114185592
Loss at iteration [1509]: 0.0025249603484445832
***** Warning: Loss has increased *****
Loss at iteration [1510]: 0.0025206083804787806
Loss at iteration [1511]: 0.002524362066265057
***** Warning: Loss has increased *****
Loss at iteration [1512]: 0.0025198903765009406
Loss at iteration [1513]: 0.002524544346251509
***** Warning: Loss has increased *****
Loss at iteration [1514]: 0.0025204172121707264
Loss at iteration [1515]: 0.0025250615325716087
***** Warning: Loss has increased *****
Loss at iteration [1516]: 0.0025209685769022115
Loss at iteration [1517]: 0.002525224668903646
***** Warning: Loss has increased *****
Loss at iteration [1518]: 0.00252064142817187
Loss at iteration [1519]: 0.002524960451037506
***** Warning: Loss has increased *****
Loss at iteration [1520]: 0.0025207996583760653
Loss at iteration [1521]: 0.0025256033012444855
***** Warning: Loss has increased *****
Loss at iteration [1522]: 0.002521329234835668
Loss at iteration [1523]: 0.002525401687430027
***** Warning: Loss has increased *****
Loss at iteration [1524]: 0.002520943838273153
Loss at iteration [1525]: 0.0025252825874820983
***** Warning: Loss has increased *****
Loss at iteration [1526]: 0.00252077580590098
Loss at iteration [1527]: 0.0025253417073423084
***** Warning: Loss has increased *****
Loss at iteration [1528]: 0.002521143888315454
Loss at iteration [1529]: 0.002525409008578485
***** Warning: Loss has increased *****
Loss at iteration [1530]: 0.002520649075990259
Loss at iteration [1531]: 0.002524002683245386
***** Warning: Loss has increased *****
Loss at iteration [1532]: 0.0025195966808024216
Loss at iteration [1533]: 0.002524020873931252
***** Warning: Loss has increased *****
Loss at iteration [1534]: 0.002519533134988982
Loss at iteration [1535]: 0.0025236276913183373
***** Warning: Loss has increased *****
Loss at iteration [1536]: 0.002519301918805605
Loss at iteration [1537]: 0.002523749376545612
***** Warning: Loss has increased *****
Loss at iteration [1538]: 0.0025194262620998444
Loss at iteration [1539]: 0.0025232388839575275
***** Warning: Loss has increased *****
Loss at iteration [1540]: 0.002518825669031409
Loss at iteration [1541]: 0.0025227943534104295
***** Warning: Loss has increased *****
Loss at iteration [1542]: 0.0025180149666511136
Loss at iteration [1543]: 0.0025213765782775522
***** Warning: Loss has increased *****
Loss at iteration [1544]: 0.002516412000700454
Loss at iteration [1545]: 0.0025203604046809282
***** Warning: Loss has increased *****
Loss at iteration [1546]: 0.0025162193284184
Loss at iteration [1547]: 0.002520212616466572
***** Warning: Loss has increased *****
Loss at iteration [1548]: 0.0025154685698828244
Loss at iteration [1549]: 0.0025196419623132425
***** Warning: Loss has increased *****
Loss at iteration [1550]: 0.002515734458109538
Loss at iteration [1551]: 0.002519447064269846
***** Warning: Loss has increased *****
Loss at iteration [1552]: 0.0025147499130613814
Loss at iteration [1553]: 0.002517733029747371
***** Warning: Loss has increased *****
Loss at iteration [1554]: 0.0025131513466265013
Loss at iteration [1555]: 0.0025174364803564
***** Warning: Loss has increased *****
Loss at iteration [1556]: 0.0025133005897377022
Loss at iteration [1557]: 0.002516735929228341
***** Warning: Loss has increased *****
Loss at iteration [1558]: 0.002512537610359453
Loss at iteration [1559]: 0.002516463163030481
***** Warning: Loss has increased *****
Loss at iteration [1560]: 0.0025121664544701674
Loss at iteration [1561]: 0.0025163931739140095
***** Warning: Loss has increased *****
Loss at iteration [1562]: 0.00251241592724782
Loss at iteration [1563]: 0.0025162420461637396
***** Warning: Loss has increased *****
Loss at iteration [1564]: 0.002512151242933507
Loss at iteration [1565]: 0.002515365164639934
***** Warning: Loss has increased *****
Loss at iteration [1566]: 0.0025106622237681263
Loss at iteration [1567]: 0.002514303176730762
***** Warning: Loss has increased *****
Loss at iteration [1568]: 0.002510256233690599
Loss at iteration [1569]: 0.0025145592714318292
***** Warning: Loss has increased *****
Loss at iteration [1570]: 0.0025107418887751623
Loss at iteration [1571]: 0.0025149658159730735
***** Warning: Loss has increased *****
Loss at iteration [1572]: 0.002511191544543015
Loss at iteration [1573]: 0.0025153802349419906
***** Warning: Loss has increased *****
Loss at iteration [1574]: 0.002511125650660038
Loss at iteration [1575]: 0.002515436310338766
***** Warning: Loss has increased *****
Loss at iteration [1576]: 0.002511861924293546
Loss at iteration [1577]: 0.0025165589943419906
***** Warning: Loss has increased *****
Loss at iteration [1578]: 0.0025126809710034797
Loss at iteration [1579]: 0.0025165670206270807
***** Warning: Loss has increased *****
Loss at iteration [1580]: 0.002512554115856149
Loss at iteration [1581]: 0.002517193808471738
***** Warning: Loss has increased *****
Loss at iteration [1582]: 0.00251311902435212
Loss at iteration [1583]: 0.0025176653564074145
***** Warning: Loss has increased *****
Loss at iteration [1584]: 0.002513767138530521
Loss at iteration [1585]: 0.002518144000165465
***** Warning: Loss has increased *****
Loss at iteration [1586]: 0.0025139544974796695
Loss at iteration [1587]: 0.00251728439957924
***** Warning: Loss has increased *****
Loss at iteration [1588]: 0.0025128705664475256
Loss at iteration [1589]: 0.0025171471981793167
***** Warning: Loss has increased *****
Loss at iteration [1590]: 0.002512752492917458
Loss at iteration [1591]: 0.0025171692649138587
***** Warning: Loss has increased *****
Loss at iteration [1592]: 0.002513272317435474
Loss at iteration [1593]: 0.0025170463407307693
***** Warning: Loss has increased *****
Loss at iteration [1594]: 0.0025126574697556655
Loss at iteration [1595]: 0.002516957591359375
***** Warning: Loss has increased *****
Loss at iteration [1596]: 0.002512977895909761
Loss at iteration [1597]: 0.0025175151274415045
***** Warning: Loss has increased *****
Loss at iteration [1598]: 0.002513143170626493
Loss at iteration [1599]: 0.0025162784170269003
***** Warning: Loss has increased *****
Loss at iteration [1600]: 0.002511859205829204
Loss at iteration [1601]: 0.0025156619437293744
***** Warning: Loss has increased *****
Loss at iteration [1602]: 0.0025111643851270116
Loss at iteration [1603]: 0.0025154270492273247
***** Warning: Loss has increased *****
Loss at iteration [1604]: 0.0025115574367005674
Loss at iteration [1605]: 0.0025156439074785777
***** Warning: Loss has increased *****
Loss at iteration [1606]: 0.002511353269130843
Loss at iteration [1607]: 0.0025153814445797525
***** Warning: Loss has increased *****
Loss at iteration [1608]: 0.0025115501067524944
Loss at iteration [1609]: 0.002515418574675897
***** Warning: Loss has increased *****
Loss at iteration [1610]: 0.002511067709958812
Loss at iteration [1611]: 0.0025142124009204796
***** Warning: Loss has increased *****
Loss at iteration [1612]: 0.00250972587482641
Loss at iteration [1613]: 0.0025138874023943212
***** Warning: Loss has increased *****
Loss at iteration [1614]: 0.0025095504459997694
Loss at iteration [1615]: 0.0025134630687821016
***** Warning: Loss has increased *****
Loss at iteration [1616]: 0.0025097650269281413
Loss at iteration [1617]: 0.002513161266748811
***** Warning: Loss has increased *****
Loss at iteration [1618]: 0.0025086502602644435
Loss at iteration [1619]: 0.0025127433404382223
***** Warning: Loss has increased *****
Loss at iteration [1620]: 0.0025089863679989112
Loss at iteration [1621]: 0.0025134102049023917
***** Warning: Loss has increased *****
Loss at iteration [1622]: 0.0025094106876639524
Loss at iteration [1623]: 0.002513706469771225
***** Warning: Loss has increased *****
Loss at iteration [1624]: 0.002509666294882221
Loss at iteration [1625]: 0.002513425690506982
***** Warning: Loss has increased *****
Loss at iteration [1626]: 0.0025091379989407923
Loss at iteration [1627]: 0.0025135303197817305
***** Warning: Loss has increased *****
Loss at iteration [1628]: 0.0025100067040213815
Loss at iteration [1629]: 0.0025137597819151537
***** Warning: Loss has increased *****
Loss at iteration [1630]: 0.002509779317683817
Loss at iteration [1631]: 0.0025139805686591327
***** Warning: Loss has increased *****
Loss at iteration [1632]: 0.002509641155708361
Loss at iteration [1633]: 0.002513933700461399
***** Warning: Loss has increased *****
Loss at iteration [1634]: 0.0025100366925462684
Loss at iteration [1635]: 0.002513307380333759
***** Warning: Loss has increased *****
Loss at iteration [1636]: 0.0025085927364558555
Loss at iteration [1637]: 0.002512353278348814
***** Warning: Loss has increased *****
Loss at iteration [1638]: 0.002508652969701915
Loss at iteration [1639]: 0.0025131624107541045
***** Warning: Loss has increased *****
Loss at iteration [1640]: 0.002509134701975586
Loss at iteration [1641]: 0.0025127570890403506
***** Warning: Loss has increased *****
Loss at iteration [1642]: 0.0025086542994871433
Loss at iteration [1643]: 0.0025124297986354777
***** Warning: Loss has increased *****
Loss at iteration [1644]: 0.0025079498694379034
Loss at iteration [1645]: 0.0025120479158734434
***** Warning: Loss has increased *****
Loss at iteration [1646]: 0.0025083319946907086
Loss at iteration [1647]: 0.0025126406430134103
***** Warning: Loss has increased *****
Loss at iteration [1648]: 0.0025084795561759288
Loss at iteration [1649]: 0.0025117201792262357
***** Warning: Loss has increased *****
Loss at iteration [1650]: 0.0025074326135525755
Loss at iteration [1651]: 0.0025115156240989246
***** Warning: Loss has increased *****
Loss at iteration [1652]: 0.0025072630979319282
Loss at iteration [1653]: 0.002510074264189037
***** Warning: Loss has increased *****
Loss at iteration [1654]: 0.0025056028886859664
Loss at iteration [1655]: 0.0025096215490416946
***** Warning: Loss has increased *****
Loss at iteration [1656]: 0.0025054888644217327
Loss at iteration [1657]: 0.0025087103560974106
***** Warning: Loss has increased *****
Loss at iteration [1658]: 0.002504688722180346
Loss at iteration [1659]: 0.0025082664961737135
***** Warning: Loss has increased *****
Loss at iteration [1660]: 0.002504043206401972
Loss at iteration [1661]: 0.002507223242647137
***** Warning: Loss has increased *****
Loss at iteration [1662]: 0.0025028865782032146
Loss at iteration [1663]: 0.002506464987946644
***** Warning: Loss has increased *****
Loss at iteration [1664]: 0.0025023244868184313
Loss at iteration [1665]: 0.0025058032072653724
***** Warning: Loss has increased *****
Loss at iteration [1666]: 0.00250214945930981
Loss at iteration [1667]: 0.0025063679122974804
***** Warning: Loss has increased *****
Loss at iteration [1668]: 0.002502591060359645
Loss at iteration [1669]: 0.002506253140856118
***** Warning: Loss has increased *****
Loss at iteration [1670]: 0.002502378013771096
Loss at iteration [1671]: 0.002505812403057405
***** Warning: Loss has increased *****
Loss at iteration [1672]: 0.002501510115057095
Loss at iteration [1673]: 0.0025049211280917605
***** Warning: Loss has increased *****
Loss at iteration [1674]: 0.0025010002444066474
Loss at iteration [1675]: 0.0025048648092717255
***** Warning: Loss has increased *****
Loss at iteration [1676]: 0.002501468250442076
Loss at iteration [1677]: 0.002505698763509757
***** Warning: Loss has increased *****
Loss at iteration [1678]: 0.0025019736512524424
Loss at iteration [1679]: 0.0025061935141000957
***** Warning: Loss has increased *****
Loss at iteration [1680]: 0.002502690499851573
Loss at iteration [1681]: 0.002506954285490419
***** Warning: Loss has increased *****
Loss at iteration [1682]: 0.0025031175613318365
Loss at iteration [1683]: 0.0025065056858067387
***** Warning: Loss has increased *****
Loss at iteration [1684]: 0.0025022740844027223
Loss at iteration [1685]: 0.0025062585455851777
***** Warning: Loss has increased *****
Loss at iteration [1686]: 0.002502267121601288
Loss at iteration [1687]: 0.00250558551120543
***** Warning: Loss has increased *****
Loss at iteration [1688]: 0.002501504354541088
Loss at iteration [1689]: 0.0025054505126956427
***** Warning: Loss has increased *****
Loss at iteration [1690]: 0.002501952593502444
Loss at iteration [1691]: 0.002506245558509952
***** Warning: Loss has increased *****
Loss at iteration [1692]: 0.0025024254762155904
Loss at iteration [1693]: 0.0025065186458482656
***** Warning: Loss has increased *****
Loss at iteration [1694]: 0.002502791050822345
Loss at iteration [1695]: 0.002507039686602024
***** Warning: Loss has increased *****
Loss at iteration [1696]: 0.0025030203781498876
Loss at iteration [1697]: 0.002506119414703194
***** Warning: Loss has increased *****
Loss at iteration [1698]: 0.002501755469756267
Loss at iteration [1699]: 0.002505645594733373
***** Warning: Loss has increased *****
Loss at iteration [1700]: 0.0025020089440406083
Loss at iteration [1701]: 0.002505127998701548
***** Warning: Loss has increased *****
Loss at iteration [1702]: 0.0025005529754565966
Loss at iteration [1703]: 0.002504251379208715
***** Warning: Loss has increased *****
Loss at iteration [1704]: 0.002500476705729722
Loss at iteration [1705]: 0.0025037377749234447
***** Warning: Loss has increased *****
Loss at iteration [1706]: 0.002499535810186615
Loss at iteration [1707]: 0.0025032833163859433
***** Warning: Loss has increased *****
Loss at iteration [1708]: 0.00249941101979982
Loss at iteration [1709]: 0.002503057198604725
***** Warning: Loss has increased *****
Loss at iteration [1710]: 0.0024988955538823867
Loss at iteration [1711]: 0.0025027374051736946
***** Warning: Loss has increased *****
Loss at iteration [1712]: 0.0024989187010892424
Loss at iteration [1713]: 0.0025015076520913866
***** Warning: Loss has increased *****
Loss at iteration [1714]: 0.0024971278260071377
Loss at iteration [1715]: 0.0025002571789592126
***** Warning: Loss has increased *****
Loss at iteration [1716]: 0.0024961587933606216
Loss at iteration [1717]: 0.0024999873942628007
***** Warning: Loss has increased *****
Loss at iteration [1718]: 0.002496295940882563
Loss at iteration [1719]: 0.002499133006784937
***** Warning: Loss has increased *****
Loss at iteration [1720]: 0.0024953203837653933
Loss at iteration [1721]: 0.0024990680271266847
***** Warning: Loss has increased *****
Loss at iteration [1722]: 0.0024952425571033226
Loss at iteration [1723]: 0.0024983547404880562
***** Warning: Loss has increased *****
Loss at iteration [1724]: 0.002494475838247593
Loss at iteration [1725]: 0.002498101018282319
***** Warning: Loss has increased *****
Loss at iteration [1726]: 0.0024942910021023153
Loss at iteration [1727]: 0.002497297473170929
***** Warning: Loss has increased *****
Loss at iteration [1728]: 0.0024934950984513158
Loss at iteration [1729]: 0.0024969424589553935
***** Warning: Loss has increased *****
Loss at iteration [1730]: 0.002493039013468613
Loss at iteration [1731]: 0.002496729351556713
***** Warning: Loss has increased *****
Loss at iteration [1732]: 0.0024933102046796954
Loss at iteration [1733]: 0.002497255310951854
***** Warning: Loss has increased *****
Loss at iteration [1734]: 0.0024940417970910514
Loss at iteration [1735]: 0.0024977962644660394
***** Warning: Loss has increased *****
Loss at iteration [1736]: 0.002494152329139592
Loss at iteration [1737]: 0.0024980272629476297
***** Warning: Loss has increased *****
Loss at iteration [1738]: 0.0024943464272818086
Loss at iteration [1739]: 0.0024975413856107693
***** Warning: Loss has increased *****
Loss at iteration [1740]: 0.0024936747092733245
Loss at iteration [1741]: 0.0024972814302239464
***** Warning: Loss has increased *****
Loss at iteration [1742]: 0.002493772517643967
Loss at iteration [1743]: 0.0024973109866602058
***** Warning: Loss has increased *****
Loss at iteration [1744]: 0.002493386694262567
Loss at iteration [1745]: 0.0024962335736260738
***** Warning: Loss has increased *****
Loss at iteration [1746]: 0.002492363401018653
Loss at iteration [1747]: 0.0024961126790039277
***** Warning: Loss has increased *****
Loss at iteration [1748]: 0.0024924565604858197
Loss at iteration [1749]: 0.0024954019216201395
***** Warning: Loss has increased *****
Loss at iteration [1750]: 0.002491674554571367
Loss at iteration [1751]: 0.002495425450521569
***** Warning: Loss has increased *****
Loss at iteration [1752]: 0.002491759975206648
Loss at iteration [1753]: 0.0024950356980859402
***** Warning: Loss has increased *****
Loss at iteration [1754]: 0.0024913281723773642
Loss at iteration [1755]: 0.0024947000079697736
***** Warning: Loss has increased *****
Loss at iteration [1756]: 0.0024913375029338223
Loss at iteration [1757]: 0.00249531586746719
***** Warning: Loss has increased *****
Loss at iteration [1758]: 0.002492084081424804
Loss at iteration [1759]: 0.0024959789727545738
***** Warning: Loss has increased *****
Loss at iteration [1760]: 0.0024925504551952834
Loss at iteration [1761]: 0.0024964160903757004
***** Warning: Loss has increased *****
Loss at iteration [1762]: 0.002492738229489322
Loss at iteration [1763]: 0.0024957287074144903
***** Warning: Loss has increased *****
Loss at iteration [1764]: 0.002491756063673083
Loss at iteration [1765]: 0.0024953609931801565
***** Warning: Loss has increased *****
Loss at iteration [1766]: 0.002491556577638239
Loss at iteration [1767]: 0.0024945553556820635
***** Warning: Loss has increased *****
Loss at iteration [1768]: 0.00249077262551641
Loss at iteration [1769]: 0.002494038243799216
***** Warning: Loss has increased *****
Loss at iteration [1770]: 0.0024900200336509183
Loss at iteration [1771]: 0.002492354993282759
***** Warning: Loss has increased *****
Loss at iteration [1772]: 0.0024884539082276396
Loss at iteration [1773]: 0.0024918333971336295
***** Warning: Loss has increased *****
Loss at iteration [1774]: 0.0024885865253286474
Loss at iteration [1775]: 0.0024923839937663218
***** Warning: Loss has increased *****
Loss at iteration [1776]: 0.002488765997470821
Loss at iteration [1777]: 0.002492367415941478
***** Warning: Loss has increased *****
Loss at iteration [1778]: 0.0024889179737277874
Loss at iteration [1779]: 0.002492728249502182
***** Warning: Loss has increased *****
Loss at iteration [1780]: 0.002489220733401019
Loss at iteration [1781]: 0.0024916793380078176
***** Warning: Loss has increased *****
Loss at iteration [1782]: 0.0024876782880486475
Loss at iteration [1783]: 0.002490660588934235
***** Warning: Loss has increased *****
Loss at iteration [1784]: 0.0024869957939071628
Loss at iteration [1785]: 0.0024904475066763257
***** Warning: Loss has increased *****
Loss at iteration [1786]: 0.002486920760071015
Loss at iteration [1787]: 0.00249059503525609
***** Warning: Loss has increased *****
Loss at iteration [1788]: 0.002487327506296265
Loss at iteration [1789]: 0.0024904271395498013
***** Warning: Loss has increased *****
Loss at iteration [1790]: 0.0024866431189032724
Loss at iteration [1791]: 0.002489712019882627
***** Warning: Loss has increased *****
Loss at iteration [1792]: 0.0024862927671995857
Loss at iteration [1793]: 0.0024899483946710624
***** Warning: Loss has increased *****
Loss at iteration [1794]: 0.0024864731530333532
Loss at iteration [1795]: 0.002490021529134171
***** Warning: Loss has increased *****
Loss at iteration [1796]: 0.0024868863156555982
Loss at iteration [1797]: 0.002490173789834321
***** Warning: Loss has increased *****
Loss at iteration [1798]: 0.0024868457775280997
Loss at iteration [1799]: 0.0024905414993196106
***** Warning: Loss has increased *****
Loss at iteration [1800]: 0.002487201180478051
Loss at iteration [1801]: 0.0024903408334289723
***** Warning: Loss has increased *****
Loss at iteration [1802]: 0.0024870451017216313
Loss at iteration [1803]: 0.0024907958242591415
***** Warning: Loss has increased *****
Loss at iteration [1804]: 0.002487307269679707
Loss at iteration [1805]: 0.002490870021542059
***** Warning: Loss has increased *****
Loss at iteration [1806]: 0.0024876001870847643
Loss at iteration [1807]: 0.002491376576426946
***** Warning: Loss has increased *****
Loss at iteration [1808]: 0.0024878134308328596
Loss at iteration [1809]: 0.002490833122255266
***** Warning: Loss has increased *****
Loss at iteration [1810]: 0.0024873819796627237
Loss at iteration [1811]: 0.002491096432268131
***** Warning: Loss has increased *****
Loss at iteration [1812]: 0.0024874621050184767
Loss at iteration [1813]: 0.002490945547586413
***** Warning: Loss has increased *****
Loss at iteration [1814]: 0.002487635918388126
Loss at iteration [1815]: 0.0024912225538988933
***** Warning: Loss has increased *****
Loss at iteration [1816]: 0.0024877338639645143
Loss at iteration [1817]: 0.0024913559921499674
***** Warning: Loss has increased *****
Loss at iteration [1818]: 0.002487928814356638
Loss at iteration [1819]: 0.002491345869553399
***** Warning: Loss has increased *****
Loss at iteration [1820]: 0.0024879729957395687
Loss at iteration [1821]: 0.0024912146654050477
***** Warning: Loss has increased *****
Loss at iteration [1822]: 0.0024872235650706515
Loss at iteration [1823]: 0.0024904950897846343
***** Warning: Loss has increased *****
Loss at iteration [1824]: 0.0024871105078334843
Loss at iteration [1825]: 0.002490614662777382
***** Warning: Loss has increased *****
Loss at iteration [1826]: 0.002487100583133791
Loss at iteration [1827]: 0.002490424269271875
***** Warning: Loss has increased *****
Loss at iteration [1828]: 0.0024870400731877053
Loss at iteration [1829]: 0.0024905837816803326
***** Warning: Loss has increased *****
Loss at iteration [1830]: 0.0024870820463526438
Loss at iteration [1831]: 0.0024906637738460226
***** Warning: Loss has increased *****
Loss at iteration [1832]: 0.0024871298963113193
Loss at iteration [1833]: 0.0024899014331221025
***** Warning: Loss has increased *****
Loss at iteration [1834]: 0.002486236427797684
Loss at iteration [1835]: 0.0024896470536434983
***** Warning: Loss has increased *****
Loss at iteration [1836]: 0.002485712140356489
Loss at iteration [1837]: 0.0024890415810604073
***** Warning: Loss has increased *****
Loss at iteration [1838]: 0.0024853718339969996
Loss at iteration [1839]: 0.002488237902234037
***** Warning: Loss has increased *****
Loss at iteration [1840]: 0.002484789828271912
Loss at iteration [1841]: 0.0024879815607299805
***** Warning: Loss has increased *****
Loss at iteration [1842]: 0.0024842016421183903
Loss at iteration [1843]: 0.0024875633885576122
***** Warning: Loss has increased *****
Loss at iteration [1844]: 0.0024844481536896073
Loss at iteration [1845]: 0.0024872511075597443
***** Warning: Loss has increased *****
Loss at iteration [1846]: 0.0024836774330529165
Loss at iteration [1847]: 0.0024870273257195023
***** Warning: Loss has increased *****
Loss at iteration [1848]: 0.0024834357403431535
Loss at iteration [1849]: 0.002486726405332985
***** Warning: Loss has increased *****
Loss at iteration [1850]: 0.0024835252344118422
Loss at iteration [1851]: 0.0024870369316407357
***** Warning: Loss has increased *****
Loss at iteration [1852]: 0.0024837771604227267
Loss at iteration [1853]: 0.002486705640902066
***** Warning: Loss has increased *****
Loss at iteration [1854]: 0.002482994643665473
Loss at iteration [1855]: 0.0024862799810724564
***** Warning: Loss has increased *****
Loss at iteration [1856]: 0.0024826110699663275
Loss at iteration [1857]: 0.0024852904653529873
***** Warning: Loss has increased *****
Loss at iteration [1858]: 0.0024819089827120616
Loss at iteration [1859]: 0.0024850698752471694
***** Warning: Loss has increased *****
Loss at iteration [1860]: 0.002481868119390832
Loss at iteration [1861]: 0.002485304574478107
***** Warning: Loss has increased *****
Loss at iteration [1862]: 0.0024818948371019382
Loss at iteration [1863]: 0.002485233906520087
***** Warning: Loss has increased *****
Loss at iteration [1864]: 0.0024822340248206237
Loss at iteration [1865]: 0.002485792656304335
***** Warning: Loss has increased *****
Loss at iteration [1866]: 0.002482464024263486
Loss at iteration [1867]: 0.002485901002320617
***** Warning: Loss has increased *****
Loss at iteration [1868]: 0.0024826059388757273
Loss at iteration [1869]: 0.0024859975497135067
***** Warning: Loss has increased *****
Loss at iteration [1870]: 0.0024823962293330716
Loss at iteration [1871]: 0.0024855751836666065
***** Warning: Loss has increased *****
Loss at iteration [1872]: 0.0024822658058971663
Loss at iteration [1873]: 0.0024857737525993753
***** Warning: Loss has increased *****
Loss at iteration [1874]: 0.002482762792195559
Loss at iteration [1875]: 0.002485920928802029
***** Warning: Loss has increased *****
Loss at iteration [1876]: 0.0024819321942643833
Loss at iteration [1877]: 0.00248495555521371
***** Warning: Loss has increased *****
Loss at iteration [1878]: 0.0024816563182252114
Loss at iteration [1879]: 0.0024850660536759506
***** Warning: Loss has increased *****
Loss at iteration [1880]: 0.0024822043913280472
Loss at iteration [1881]: 0.0024858653222713622
***** Warning: Loss has increased *****
Loss at iteration [1882]: 0.0024822630161828937
Loss at iteration [1883]: 0.0024853789473003743
***** Warning: Loss has increased *****
Loss at iteration [1884]: 0.0024819584596987754
Loss at iteration [1885]: 0.002485303028546641
***** Warning: Loss has increased *****
Loss at iteration [1886]: 0.002481900755590843
Loss at iteration [1887]: 0.0024852212193966195
***** Warning: Loss has increased *****
Loss at iteration [1888]: 0.0024819550815030314
Loss at iteration [1889]: 0.0024854473538855086
***** Warning: Loss has increased *****
Loss at iteration [1890]: 0.0024815745659429005
Loss at iteration [1891]: 0.002483649101081845
***** Warning: Loss has increased *****
Loss at iteration [1892]: 0.0024799149775624227
Loss at iteration [1893]: 0.0024828892413241445
***** Warning: Loss has increased *****
Loss at iteration [1894]: 0.002479695502126007
Loss at iteration [1895]: 0.002483148902470668
***** Warning: Loss has increased *****
Loss at iteration [1896]: 0.002479845946959647
Loss at iteration [1897]: 0.0024831370215074454
***** Warning: Loss has increased *****
Loss at iteration [1898]: 0.0024800021009034577
Loss at iteration [1899]: 0.0024833833845004575
***** Warning: Loss has increased *****
Loss at iteration [1900]: 0.00247990913442977
Loss at iteration [1901]: 0.0024830614211815744
***** Warning: Loss has increased *****
Loss at iteration [1902]: 0.0024799685824461225
Loss at iteration [1903]: 0.0024833642682120126
***** Warning: Loss has increased *****
Loss at iteration [1904]: 0.0024795975102507016
Loss at iteration [1905]: 0.002482526601239376
***** Warning: Loss has increased *****
Loss at iteration [1906]: 0.0024790543451199934
Loss at iteration [1907]: 0.002481938167933204
***** Warning: Loss has increased *****
Loss at iteration [1908]: 0.002478680433001121
Loss at iteration [1909]: 0.0024821320893165196
***** Warning: Loss has increased *****
Loss at iteration [1910]: 0.002478882795175886
Loss at iteration [1911]: 0.0024820964570526124
***** Warning: Loss has increased *****
Loss at iteration [1912]: 0.002479023232082339
Loss at iteration [1913]: 0.002482396004647956
***** Warning: Loss has increased *****
Loss at iteration [1914]: 0.0024790246248932893
Loss at iteration [1915]: 0.002482269733625499
***** Warning: Loss has increased *****
Loss at iteration [1916]: 0.002479010134599328
Loss at iteration [1917]: 0.0024823634726116096
***** Warning: Loss has increased *****
Loss at iteration [1918]: 0.0024787738126578656
Loss at iteration [1919]: 0.0024816883516940616
***** Warning: Loss has increased *****
Loss at iteration [1920]: 0.0024782855608364972
Loss at iteration [1921]: 0.0024816310635646164
***** Warning: Loss has increased *****
Loss at iteration [1922]: 0.0024786100802714406
Loss at iteration [1923]: 0.0024814764535366633
***** Warning: Loss has increased *****
Loss at iteration [1924]: 0.00247777926384098
Loss at iteration [1925]: 0.0024808508622604732
***** Warning: Loss has increased *****
Loss at iteration [1926]: 0.002477534063171925
Loss at iteration [1927]: 0.0024801192024835163
***** Warning: Loss has increased *****
Loss at iteration [1928]: 0.00247658195272972
Loss at iteration [1929]: 0.002479541242263105
***** Warning: Loss has increased *****
Loss at iteration [1930]: 0.0024762380824488533
Loss at iteration [1931]: 0.0024794225136268435
***** Warning: Loss has increased *****
Loss at iteration [1932]: 0.002476096090389071
Loss at iteration [1933]: 0.002479128072606729
***** Warning: Loss has increased *****
Loss at iteration [1934]: 0.0024759111325119328
Loss at iteration [1935]: 0.002479165385135645
***** Warning: Loss has increased *****
Loss at iteration [1936]: 0.002475584107570816
Loss at iteration [1937]: 0.002478223849554533
***** Warning: Loss has increased *****
Loss at iteration [1938]: 0.0024749273484143152
Loss at iteration [1939]: 0.002477945955799583
***** Warning: Loss has increased *****
Loss at iteration [1940]: 0.0024749607573735233
Loss at iteration [1941]: 0.0024782934966271477
***** Warning: Loss has increased *****
Loss at iteration [1942]: 0.0024751591852655774
Loss at iteration [1943]: 0.0024782475139388986
***** Warning: Loss has increased *****
Loss at iteration [1944]: 0.0024750284190801996
Loss at iteration [1945]: 0.002478167898731297
***** Warning: Loss has increased *****
Loss at iteration [1946]: 0.0024748267361972675
Loss at iteration [1947]: 0.002477318714325933
***** Warning: Loss has increased *****
Loss at iteration [1948]: 0.002473996836661137
Loss at iteration [1949]: 0.0024771463128210654
***** Warning: Loss has increased *****
Loss at iteration [1950]: 0.0024740405454684597
Loss at iteration [1951]: 0.0024767919439875156
***** Warning: Loss has increased *****
Loss at iteration [1952]: 0.0024731833247086553
Loss at iteration [1953]: 0.0024761042668733518
***** Warning: Loss has increased *****
Loss at iteration [1954]: 0.002473247110772338
Loss at iteration [1955]: 0.002476443068785126
***** Warning: Loss has increased *****
Loss at iteration [1956]: 0.0024732460446703613
Loss at iteration [1957]: 0.0024763209458057885
***** Warning: Loss has increased *****
Loss at iteration [1958]: 0.00247307802817522
Loss at iteration [1959]: 0.002476093447103537
***** Warning: Loss has increased *****
Loss at iteration [1960]: 0.0024728406492598672
Loss at iteration [1961]: 0.0024759085556829846
***** Warning: Loss has increased *****
Loss at iteration [1962]: 0.0024727186929745853
Loss at iteration [1963]: 0.0024757800205474027
***** Warning: Loss has increased *****
Loss at iteration [1964]: 0.002472711792434124
Loss at iteration [1965]: 0.0024755105005924167
***** Warning: Loss has increased *****
Loss at iteration [1966]: 0.0024721231715426903
Loss at iteration [1967]: 0.0024750933376935075
***** Warning: Loss has increased *****
Loss at iteration [1968]: 0.0024720761949246456
Loss at iteration [1969]: 0.002475189008068722
***** Warning: Loss has increased *****
Loss at iteration [1970]: 0.0024720763594689014
Loss at iteration [1971]: 0.0024750405055406557
***** Warning: Loss has increased *****
Loss at iteration [1972]: 0.0024718783903377086
Loss at iteration [1973]: 0.002474962064271894
***** Warning: Loss has increased *****
Loss at iteration [1974]: 0.0024716541431528253
Loss at iteration [1975]: 0.0024746436098618675
***** Warning: Loss has increased *****
Loss at iteration [1976]: 0.002471603680697904
Loss at iteration [1977]: 0.0024748295430296586
***** Warning: Loss has increased *****
Loss at iteration [1978]: 0.002471462021097813
Loss at iteration [1979]: 0.0024740033459745695
***** Warning: Loss has increased *****
Loss at iteration [1980]: 0.0024708188470660922
Loss at iteration [1981]: 0.0024736675482972237
***** Warning: Loss has increased *****
Loss at iteration [1982]: 0.002470841128871777
Loss at iteration [1983]: 0.0024740310305346666
***** Warning: Loss has increased *****
Loss at iteration [1984]: 0.0024706853633381033
Loss at iteration [1985]: 0.0024737782048970675
***** Warning: Loss has increased *****
Loss at iteration [1986]: 0.0024708424465802376
Loss at iteration [1987]: 0.0024739114159159576
***** Warning: Loss has increased *****
Loss at iteration [1988]: 0.002470597424054088
Loss at iteration [1989]: 0.0024735608871182505
***** Warning: Loss has increased *****
Loss at iteration [1990]: 0.002470457219003021
Loss at iteration [1991]: 0.0024732541218850167
***** Warning: Loss has increased *****
Loss at iteration [1992]: 0.002470232595326024
Loss at iteration [1993]: 0.002473235000390019
***** Warning: Loss has increased *****
Loss at iteration [1994]: 0.002470082854054646
Loss at iteration [1995]: 0.0024732248241901783
***** Warning: Loss has increased *****
Loss at iteration [1996]: 0.0024706052888015437
Loss at iteration [1997]: 0.0024739238315061836
***** Warning: Loss has increased *****
Loss at iteration [1998]: 0.002470877777443442
Loss at iteration [1999]: 0.002473972091647205
***** Warning: Loss has increased *****
Loss at iteration [2000]: 0.002470830545409813
Loss at iteration [2001]: 0.0024740189150055333
***** Warning: Loss has increased *****
Loss at iteration [2002]: 0.0024710343621169536
Loss at iteration [2003]: 0.0024741234221693875
***** Warning: Loss has increased *****
Loss at iteration [2004]: 0.0024706533471715686
Loss at iteration [2005]: 0.002473145967503789
***** Warning: Loss has increased *****
Loss at iteration [2006]: 0.0024700567224368955
Loss at iteration [2007]: 0.0024730610532570523
***** Warning: Loss has increased *****
Loss at iteration [2008]: 0.0024702776667757014
Loss at iteration [2009]: 0.002473475186454288
***** Warning: Loss has increased *****
Loss at iteration [2010]: 0.0024701886182450484
Loss at iteration [2011]: 0.0024731234374664865
***** Warning: Loss has increased *****
Loss at iteration [2012]: 0.002470123854723159
Loss at iteration [2013]: 0.0024733331223812625
***** Warning: Loss has increased *****
Loss at iteration [2014]: 0.0024702163561912834
Loss at iteration [2015]: 0.0024725765888355565
***** Warning: Loss has increased *****
Loss at iteration [2016]: 0.002469031406484089
Loss at iteration [2017]: 0.0024714144035790074
***** Warning: Loss has increased *****
Loss at iteration [2018]: 0.0024679922973768376
Loss at iteration [2019]: 0.002470713072552776
***** Warning: Loss has increased *****
Loss at iteration [2020]: 0.002467830167427763
Loss at iteration [2021]: 0.002470951071576769
***** Warning: Loss has increased *****
Loss at iteration [2022]: 0.0024677065944570522
Loss at iteration [2023]: 0.002470299585785536
***** Warning: Loss has increased *****
Loss at iteration [2024]: 0.002467512309973804
Loss at iteration [2025]: 0.002470683653563751
***** Warning: Loss has increased *****
Loss at iteration [2026]: 0.0024679243308772703
Loss at iteration [2027]: 0.002471165135574079
***** Warning: Loss has increased *****
Loss at iteration [2028]: 0.0024680818717245363
Loss at iteration [2029]: 0.0024711027993207456
***** Warning: Loss has increased *****
Loss at iteration [2030]: 0.0024681539008799013
Loss at iteration [2031]: 0.0024709951485672936
***** Warning: Loss has increased *****
Loss at iteration [2032]: 0.002467578282654549
Loss at iteration [2033]: 0.0024702416720217353
***** Warning: Loss has increased *****
Loss at iteration [2034]: 0.002467308874050146
Loss at iteration [2035]: 0.002470349039304016
***** Warning: Loss has increased *****
Loss at iteration [2036]: 0.00246782193077721
Loss at iteration [2037]: 0.002471155672542803
***** Warning: Loss has increased *****
Loss at iteration [2038]: 0.002467896975125507
Loss at iteration [2039]: 0.0024706733901653658
***** Warning: Loss has increased *****
Loss at iteration [2040]: 0.002467727308589011
Loss at iteration [2041]: 0.0024708701099099522
***** Warning: Loss has increased *****
Loss at iteration [2042]: 0.002467862234593557
Loss at iteration [2043]: 0.0024704478881251227
***** Warning: Loss has increased *****
Loss at iteration [2044]: 0.0024672266546962246
Loss at iteration [2045]: 0.002469961398092209
***** Warning: Loss has increased *****
Loss at iteration [2046]: 0.0024671131212358746
Loss at iteration [2047]: 0.0024702095800907217
***** Warning: Loss has increased *****
Loss at iteration [2048]: 0.0024672788415304294
Loss at iteration [2049]: 0.0024702574677957978
***** Warning: Loss has increased *****
Loss at iteration [2050]: 0.002467387280509004
Loss at iteration [2051]: 0.0024704813891529074
***** Warning: Loss has increased *****
Loss at iteration [2052]: 0.002467418443267232
Loss at iteration [2053]: 0.0024705532913414895
***** Warning: Loss has increased *****
Loss at iteration [2054]: 0.002467293498656982
Loss at iteration [2055]: 0.002469783723284127
***** Warning: Loss has increased *****
Loss at iteration [2056]: 0.002466736361311721
Loss at iteration [2057]: 0.002469699964978162
***** Warning: Loss has increased *****
Loss at iteration [2058]: 0.0024667350019386336
Loss at iteration [2059]: 0.002469158018498311
***** Warning: Loss has increased *****
Loss at iteration [2060]: 0.002466247055941653
Loss at iteration [2061]: 0.002469049723700072
***** Warning: Loss has increased *****
Loss at iteration [2062]: 0.002465762382279248
Loss at iteration [2063]: 0.0024685365295277987
***** Warning: Loss has increased *****
Loss at iteration [2064]: 0.0024654601398718854
Loss at iteration [2065]: 0.002468514894003712
***** Warning: Loss has increased *****
Loss at iteration [2066]: 0.002465751964039779
Loss at iteration [2067]: 0.002468409281720513
***** Warning: Loss has increased *****
Loss at iteration [2068]: 0.0024652397666840697
Loss at iteration [2069]: 0.002468071416934818
***** Warning: Loss has increased *****
Loss at iteration [2070]: 0.002465357993252807
Loss at iteration [2071]: 0.002468412475057615
***** Warning: Loss has increased *****
Loss at iteration [2072]: 0.002465975149400423
Loss at iteration [2073]: 0.00246926825401618
***** Warning: Loss has increased *****
Loss at iteration [2074]: 0.002466197488464818
Loss at iteration [2075]: 0.002469125360757085
***** Warning: Loss has increased *****
Loss at iteration [2076]: 0.0024659864394539544
Loss at iteration [2077]: 0.00246900626144532
***** Warning: Loss has increased *****
Loss at iteration [2078]: 0.00246588349839255
Loss at iteration [2079]: 0.0024682765173616368
***** Warning: Loss has increased *****
Loss at iteration [2080]: 0.002465250149455759
Loss at iteration [2081]: 0.0024680676667419347
***** Warning: Loss has increased *****
Loss at iteration [2082]: 0.002465331347708027
Loss at iteration [2083]: 0.0024684616206301735
***** Warning: Loss has increased *****
Loss at iteration [2084]: 0.002465594683906018
Loss at iteration [2085]: 0.002468582869563538
***** Warning: Loss has increased *****
Loss at iteration [2086]: 0.002465615405451232
Loss at iteration [2087]: 0.002468676076471484
***** Warning: Loss has increased *****
Loss at iteration [2088]: 0.0024656937634182294
Loss at iteration [2089]: 0.002468617722601237
***** Warning: Loss has increased *****
Loss at iteration [2090]: 0.0024653887641672896
Loss at iteration [2091]: 0.002467950353837446
***** Warning: Loss has increased *****
Loss at iteration [2092]: 0.0024650221411057394
Loss at iteration [2093]: 0.002468036390719876
***** Warning: Loss has increased *****
Loss at iteration [2094]: 0.0024650178513720552
Loss at iteration [2095]: 0.002467331322439882
***** Warning: Loss has increased *****
Loss at iteration [2096]: 0.002464126879518807
Loss at iteration [2097]: 0.0024669956568458785
***** Warning: Loss has increased *****
Loss at iteration [2098]: 0.0024637347012586184
Loss at iteration [2099]: 0.0024664780738291904
***** Warning: Loss has increased *****
Loss at iteration [2100]: 0.0024636159199447594
Loss at iteration [2101]: 0.0024666742611080764
***** Warning: Loss has increased *****
Loss at iteration [2102]: 0.0024640534914059107
Loss at iteration [2103]: 0.002466890928343179
***** Warning: Loss has increased *****
Loss at iteration [2104]: 0.0024635808153693557
Loss at iteration [2105]: 0.0024662443720560256
***** Warning: Loss has increased *****
Loss at iteration [2106]: 0.0024632499880455705
Loss at iteration [2107]: 0.002466177013170588
***** Warning: Loss has increased *****
Loss at iteration [2108]: 0.0024636235531669028
Loss at iteration [2109]: 0.0024668097852106365
***** Warning: Loss has increased *****
Loss at iteration [2110]: 0.002463908224654885
Loss at iteration [2111]: 0.0024668566402146486
***** Warning: Loss has increased *****
Loss at iteration [2112]: 0.002464014843713218
Loss at iteration [2113]: 0.0024671701064438727
***** Warning: Loss has increased *****
Loss at iteration [2114]: 0.0024640341691114583
Loss at iteration [2115]: 0.0024665425730012553
***** Warning: Loss has increased *****
Loss at iteration [2116]: 0.002463452195116134
Loss at iteration [2117]: 0.002466216085137068
***** Warning: Loss has increased *****
Loss at iteration [2118]: 0.0024633393082083726
Loss at iteration [2119]: 0.0024656532415499055
***** Warning: Loss has increased *****
Loss at iteration [2120]: 0.0024621456288649794
Loss at iteration [2121]: 0.0024648678072950294
***** Warning: Loss has increased *****
Loss at iteration [2122]: 0.0024619900178434726
Loss at iteration [2123]: 0.0024649291906010205
***** Warning: Loss has increased *****
Loss at iteration [2124]: 0.002461897217443886
Loss at iteration [2125]: 0.0024644621815309393
***** Warning: Loss has increased *****
Loss at iteration [2126]: 0.002461677118623889
Loss at iteration [2127]: 0.002464676466120831
***** Warning: Loss has increased *****
Loss at iteration [2128]: 0.002462219520434713
Loss at iteration [2129]: 0.0024653858548734104
***** Warning: Loss has increased *****
Loss at iteration [2130]: 0.002462625037229406
Loss at iteration [2131]: 0.0024656857315158306
***** Warning: Loss has increased *****
Loss at iteration [2132]: 0.002462899304051106
Loss at iteration [2133]: 0.0024657354905208064
***** Warning: Loss has increased *****
Loss at iteration [2134]: 0.0024622498970812067
Loss at iteration [2135]: 0.002464887120084523
***** Warning: Loss has increased *****
Loss at iteration [2136]: 0.0024620714127012717
Loss at iteration [2137]: 0.002464775102460992
***** Warning: Loss has increased *****
Loss at iteration [2138]: 0.0024620717426150312
Loss at iteration [2139]: 0.0024651345750026617
***** Warning: Loss has increased *****
Loss at iteration [2140]: 0.0024622448246829297
Loss at iteration [2141]: 0.002465238606806022
***** Warning: Loss has increased *****
Loss at iteration [2142]: 0.002462378531913499
Loss at iteration [2143]: 0.0024653552279539784
***** Warning: Loss has increased *****
Loss at iteration [2144]: 0.0024622518875028845
Loss at iteration [2145]: 0.0024645712634929528
***** Warning: Loss has increased *****
Loss at iteration [2146]: 0.002461367528150367
Loss at iteration [2147]: 0.0024638157359936877
***** Warning: Loss has increased *****
Loss at iteration [2148]: 0.002460874467854052
Loss at iteration [2149]: 0.0024637717712885465
***** Warning: Loss has increased *****
Loss at iteration [2150]: 0.0024607937575066336
Loss at iteration [2151]: 0.0024634657274293843
***** Warning: Loss has increased *****
Loss at iteration [2152]: 0.0024606893807156806
Loss at iteration [2153]: 0.0024636704757161855
***** Warning: Loss has increased *****
Loss at iteration [2154]: 0.002460840883882887
Loss at iteration [2155]: 0.0024637602488014536
***** Warning: Loss has increased *****
Loss at iteration [2156]: 0.002461087778854741
Loss at iteration [2157]: 0.0024641431347426504
***** Warning: Loss has increased *****
Loss at iteration [2158]: 0.002461018516157373
Loss at iteration [2159]: 0.0024635776725364803
***** Warning: Loss has increased *****
Loss at iteration [2160]: 0.002460368591634535
Loss at iteration [2161]: 0.002463188925947529
***** Warning: Loss has increased *****
Loss at iteration [2162]: 0.0024605986937295274
Loss at iteration [2163]: 0.0024637282052837163
***** Warning: Loss has increased *****
Loss at iteration [2164]: 0.002460928105248957
Loss at iteration [2165]: 0.0024639457108358216
***** Warning: Loss has increased *****
Loss at iteration [2166]: 0.002461253558573493
Loss at iteration [2167]: 0.002464388181269631
***** Warning: Loss has increased *****
Loss at iteration [2168]: 0.0024614138568779475
Loss at iteration [2169]: 0.0024638077026001995
***** Warning: Loss has increased *****
Loss at iteration [2170]: 0.002460490726933118
Loss at iteration [2171]: 0.002462621099680181
***** Warning: Loss has increased *****
Loss at iteration [2172]: 0.00245981572420672
Loss at iteration [2173]: 0.0024627167356006353
***** Warning: Loss has increased *****
Loss at iteration [2174]: 0.0024598106360004318
Loss at iteration [2175]: 0.002462686473119812
***** Warning: Loss has increased *****
Loss at iteration [2176]: 0.0024596460265091237
Loss at iteration [2177]: 0.002462533831828598
***** Warning: Loss has increased *****
Loss at iteration [2178]: 0.002459903852319516
Loss at iteration [2179]: 0.0024625490277941017
***** Warning: Loss has increased *****
Loss at iteration [2180]: 0.00245932841224529
Loss at iteration [2181]: 0.002462089207671322
***** Warning: Loss has increased *****
Loss at iteration [2182]: 0.0024595018922948034
Loss at iteration [2183]: 0.002462649260505838
***** Warning: Loss has increased *****
Loss at iteration [2184]: 0.0024597163293732602
Loss at iteration [2185]: 0.0024623965543135676
***** Warning: Loss has increased *****
Loss at iteration [2186]: 0.0024593746966443484
Loss at iteration [2187]: 0.0024623237731329147
***** Warning: Loss has increased *****
Loss at iteration [2188]: 0.0024597319340276813
Loss at iteration [2189]: 0.0024624212278171326
***** Warning: Loss has increased *****
Loss at iteration [2190]: 0.0024592644068789538
Loss at iteration [2191]: 0.002461355905146126
***** Warning: Loss has increased *****
Loss at iteration [2192]: 0.0024581521727043898
Loss at iteration [2193]: 0.0024608197642344287
***** Warning: Loss has increased *****
Loss at iteration [2194]: 0.0024576898916940327
Loss at iteration [2195]: 0.002460519627641473
***** Warning: Loss has increased *****
Loss at iteration [2196]: 0.0024579561057046038
Loss at iteration [2197]: 0.0024611443332444747
***** Warning: Loss has increased *****
Loss at iteration [2198]: 0.0024587709247556187
Loss at iteration [2199]: 0.002462022808028992
***** Warning: Loss has increased *****
Loss at iteration [2200]: 0.0024590855815673236
Loss at iteration [2201]: 0.002461594726148297
***** Warning: Loss has increased *****
Loss at iteration [2202]: 0.002458584926819823
Loss at iteration [2203]: 0.0024613528033228384
***** Warning: Loss has increased *****
Loss at iteration [2204]: 0.0024585821386119322
Loss at iteration [2205]: 0.002461429914707893
***** Warning: Loss has increased *****
Loss at iteration [2206]: 0.00245829537234101
Loss at iteration [2207]: 0.0024610706124059234
***** Warning: Loss has increased *****
Loss at iteration [2208]: 0.0024582035177178095
Loss at iteration [2209]: 0.0024606450382422223
***** Warning: Loss has increased *****
Loss at iteration [2210]: 0.00245768404910454
Loss at iteration [2211]: 0.002460191385896982
***** Warning: Loss has increased *****
Loss at iteration [2212]: 0.0024570809283228565
Loss at iteration [2213]: 0.00245968424024304
***** Warning: Loss has increased *****
Loss at iteration [2214]: 0.002456947608934017
Loss at iteration [2215]: 0.0024598712651291966
***** Warning: Loss has increased *****
Loss at iteration [2216]: 0.0024572435554428003
Loss at iteration [2217]: 0.0024603085971552988
***** Warning: Loss has increased *****
Loss at iteration [2218]: 0.0024574878680498013
Loss at iteration [2219]: 0.0024602724861935072
***** Warning: Loss has increased *****
Loss at iteration [2220]: 0.002457626528871369
Loss at iteration [2221]: 0.002460236454329669
***** Warning: Loss has increased *****
Loss at iteration [2222]: 0.0024567765801934206
Loss at iteration [2223]: 0.002459281829639693
***** Warning: Loss has increased *****
Loss at iteration [2224]: 0.0024564960200918608
Loss at iteration [2225]: 0.002459484078663192
***** Warning: Loss has increased *****
Loss at iteration [2226]: 0.0024569271697568846
Loss at iteration [2227]: 0.0024598567749535834
***** Warning: Loss has increased *****
Loss at iteration [2228]: 0.0024568672256426553
Loss at iteration [2229]: 0.0024590407876512116
***** Warning: Loss has increased *****
Loss at iteration [2230]: 0.0024559890257162733
Loss at iteration [2231]: 0.002458381695150483
***** Warning: Loss has increased *****
Loss at iteration [2232]: 0.002455217952711222
Loss at iteration [2233]: 0.0024578922665305825
***** Warning: Loss has increased *****
Loss at iteration [2234]: 0.0024553424577794646
Loss at iteration [2235]: 0.002458004265382778
***** Warning: Loss has increased *****
Loss at iteration [2236]: 0.0024554892935625817
Loss at iteration [2237]: 0.0024585087069145996
***** Warning: Loss has increased *****
Loss at iteration [2238]: 0.0024555508721767665
Loss at iteration [2239]: 0.00245842700854014
***** Warning: Loss has increased *****
Loss at iteration [2240]: 0.0024554801237881927
Loss at iteration [2241]: 0.0024578539270656745
***** Warning: Loss has increased *****
Loss at iteration [2242]: 0.0024547174583263516
Loss at iteration [2243]: 0.002456763922327137
***** Warning: Loss has increased *****
Loss at iteration [2244]: 0.002453933638203574
Loss at iteration [2245]: 0.0024566790236926744
***** Warning: Loss has increased *****
Loss at iteration [2246]: 0.002454051610365643
Loss at iteration [2247]: 0.0024569611169143373
***** Warning: Loss has increased *****
Loss at iteration [2248]: 0.0024541636003160634
Loss at iteration [2249]: 0.0024570562676907866
***** Warning: Loss has increased *****
Loss at iteration [2250]: 0.002454267668940579
Loss at iteration [2251]: 0.002456649471458367
***** Warning: Loss has increased *****
Loss at iteration [2252]: 0.00245378300947438
Loss at iteration [2253]: 0.0024564121181289483
***** Warning: Loss has increased *****
Loss at iteration [2254]: 0.002453603521817863
Loss at iteration [2255]: 0.0024564255441968494
***** Warning: Loss has increased *****
Loss at iteration [2256]: 0.0024541000747064796
Loss at iteration [2257]: 0.002457144642095855
***** Warning: Loss has increased *****
Loss at iteration [2258]: 0.0024540801293611377
Loss at iteration [2259]: 0.002456873427521165
***** Warning: Loss has increased *****
Loss at iteration [2260]: 0.0024540528397742267
Loss at iteration [2261]: 0.002456522772916811
***** Warning: Loss has increased *****
Loss at iteration [2262]: 0.0024537395597350213
Loss at iteration [2263]: 0.0024565480746264893
***** Warning: Loss has increased *****
Loss at iteration [2264]: 0.002453827237446706
Loss at iteration [2265]: 0.0024561906768803612
***** Warning: Loss has increased *****
Loss at iteration [2266]: 0.002453362748294096
Loss at iteration [2267]: 0.002456198328381867
***** Warning: Loss has increased *****
Loss at iteration [2268]: 0.0024536059004028506
Loss at iteration [2269]: 0.002456488588025428
***** Warning: Loss has increased *****
Loss at iteration [2270]: 0.0024535264944309902
Loss at iteration [2271]: 0.002455948995574936
***** Warning: Loss has increased *****
Loss at iteration [2272]: 0.0024533131076290935
Loss at iteration [2273]: 0.002456139838871408
***** Warning: Loss has increased *****
Loss at iteration [2274]: 0.0024534074773487785
Loss at iteration [2275]: 0.0024562641193000916
***** Warning: Loss has increased *****
Loss at iteration [2276]: 0.002453407276899785
Loss at iteration [2277]: 0.0024561101316214183
***** Warning: Loss has increased *****
Loss at iteration [2278]: 0.002453456309953126
Loss at iteration [2279]: 0.0024565259096159184
***** Warning: Loss has increased *****
Loss at iteration [2280]: 0.0024537362271627697
Loss at iteration [2281]: 0.002456179575711735
***** Warning: Loss has increased *****
Loss at iteration [2282]: 0.002453359098401352
Loss at iteration [2283]: 0.002455977223851976
***** Warning: Loss has increased *****
Loss at iteration [2284]: 0.002453343980391104
Loss at iteration [2285]: 0.002456349191196897
***** Warning: Loss has increased *****
Loss at iteration [2286]: 0.002453611800533192
Loss at iteration [2287]: 0.002455918744401916
***** Warning: Loss has increased *****
Loss at iteration [2288]: 0.0024532425866742513
Loss at iteration [2289]: 0.002456369696372453
***** Warning: Loss has increased *****
Loss at iteration [2290]: 0.0024538781413243277
Loss at iteration [2291]: 0.002456465955486562
***** Warning: Loss has increased *****
Loss at iteration [2292]: 0.0024532148290596478
Loss at iteration [2293]: 0.002455982239569158
***** Warning: Loss has increased *****
Loss at iteration [2294]: 0.002453391257337192
Loss at iteration [2295]: 0.002456310828600507
***** Warning: Loss has increased *****
Loss at iteration [2296]: 0.002453455045134331
Loss at iteration [2297]: 0.0024564746198271477
***** Warning: Loss has increased *****
Loss at iteration [2298]: 0.0024537167459970683
Loss at iteration [2299]: 0.0024562757061218346
***** Warning: Loss has increased *****
Loss at iteration [2300]: 0.002453630966697453
Loss at iteration [2301]: 0.002456598553737504
***** Warning: Loss has increased *****
Loss at iteration [2302]: 0.002453934032334584
Loss at iteration [2303]: 0.00245674524177486
***** Warning: Loss has increased *****
Loss at iteration [2304]: 0.002454058457381852
Loss at iteration [2305]: 0.002457183102906669
***** Warning: Loss has increased *****
Loss at iteration [2306]: 0.002454680475265075
Loss at iteration [2307]: 0.0024578398861256714
***** Warning: Loss has increased *****
Loss at iteration [2308]: 0.0024548398881562832
Loss at iteration [2309]: 0.0024572148919760305
***** Warning: Loss has increased *****
Loss at iteration [2310]: 0.0024539851743152516
Loss at iteration [2311]: 0.0024565581954678092
***** Warning: Loss has increased *****
Loss at iteration [2312]: 0.0024538326251703777
Loss at iteration [2313]: 0.002456309070633737
***** Warning: Loss has increased *****
Loss at iteration [2314]: 0.002453313988939838
Loss at iteration [2315]: 0.0024561548284195555
***** Warning: Loss has increased *****
Loss at iteration [2316]: 0.0024534648681722756
Loss at iteration [2317]: 0.0024564377145389863
***** Warning: Loss has increased *****
Loss at iteration [2318]: 0.0024539340197159373
Loss at iteration [2319]: 0.002456718781010124
***** Warning: Loss has increased *****
Loss at iteration [2320]: 0.0024535911774856514
Loss at iteration [2321]: 0.002456246709688869
***** Warning: Loss has increased *****
Loss at iteration [2322]: 0.00245368792143636
Loss at iteration [2323]: 0.002456761295506047
***** Warning: Loss has increased *****
Loss at iteration [2324]: 0.0024537882451460893
Loss at iteration [2325]: 0.0024566178839779784
***** Warning: Loss has increased *****
Loss at iteration [2326]: 0.002453724150876313
Loss at iteration [2327]: 0.0024563121424364563
***** Warning: Loss has increased *****
Loss at iteration [2328]: 0.0024533091102260734
Loss at iteration [2329]: 0.0024560556015920778
***** Warning: Loss has increased *****
Loss at iteration [2330]: 0.0024531897155620074
Loss at iteration [2331]: 0.002455393082951837
***** Warning: Loss has increased *****
Loss at iteration [2332]: 0.0024524088928008373
Loss at iteration [2333]: 0.002455180413309347
***** Warning: Loss has increased *****
Loss at iteration [2334]: 0.0024522955083135472
Loss at iteration [2335]: 0.002455034992416371
***** Warning: Loss has increased *****
Loss at iteration [2336]: 0.002451940506645048
Loss at iteration [2337]: 0.0024544406334029868
***** Warning: Loss has increased *****
Loss at iteration [2338]: 0.002451726668193881
Loss at iteration [2339]: 0.002454510557566959
***** Warning: Loss has increased *****
Loss at iteration [2340]: 0.002451825038073583
Loss at iteration [2341]: 0.002454783301027339
***** Warning: Loss has increased *****
Loss at iteration [2342]: 0.002451926997728573
Loss at iteration [2343]: 0.00245427562684367
***** Warning: Loss has increased *****
Loss at iteration [2344]: 0.002451154109245279
Loss at iteration [2345]: 0.002453271753422552
***** Warning: Loss has increased *****
Loss at iteration [2346]: 0.0024499670551824753
Loss at iteration [2347]: 0.0024524012130337507
***** Warning: Loss has increased *****
Loss at iteration [2348]: 0.0024496539193138876
Loss at iteration [2349]: 0.0024519502967899655
***** Warning: Loss has increased *****
Loss at iteration [2350]: 0.002449264513155562
Loss at iteration [2351]: 0.002452126982790541
***** Warning: Loss has increased *****
Loss at iteration [2352]: 0.002449279114518795
Loss at iteration [2353]: 0.002452017354504027
***** Warning: Loss has increased *****
Loss at iteration [2354]: 0.0024494812416971314
Loss at iteration [2355]: 0.0024522241826356143
***** Warning: Loss has increased *****
Loss at iteration [2356]: 0.0024494054054513075
Loss at iteration [2357]: 0.0024521430718962736
***** Warning: Loss has increased *****
Loss at iteration [2358]: 0.002449055608186817
Loss at iteration [2359]: 0.002451449122268571
***** Warning: Loss has increased *****
Loss at iteration [2360]: 0.0024487041779008233
Loss at iteration [2361]: 0.002451524812041066
***** Warning: Loss has increased *****
Loss at iteration [2362]: 0.0024490065476831446
Loss at iteration [2363]: 0.0024514733917469435
***** Warning: Loss has increased *****
Loss at iteration [2364]: 0.0024484804084691164
Loss at iteration [2365]: 0.0024511508030484346
***** Warning: Loss has increased *****
Loss at iteration [2366]: 0.002448460047811744
Loss at iteration [2367]: 0.002450641135392308
***** Warning: Loss has increased *****
Loss at iteration [2368]: 0.0024475171348502787
Loss at iteration [2369]: 0.002450178702435654
***** Warning: Loss has increased *****
Loss at iteration [2370]: 0.0024477346014229475
Loss at iteration [2371]: 0.0024507789102862724
***** Warning: Loss has increased *****
Loss at iteration [2372]: 0.0024480871113956304
Loss at iteration [2373]: 0.0024504159899598624
***** Warning: Loss has increased *****
Loss at iteration [2374]: 0.002447193838574638
Loss at iteration [2375]: 0.002449575197164534
***** Warning: Loss has increased *****
Loss at iteration [2376]: 0.002446936342886548
Loss at iteration [2377]: 0.002449116634616665
***** Warning: Loss has increased *****
Loss at iteration [2378]: 0.0024464091993788278
Loss at iteration [2379]: 0.002449258904198863
***** Warning: Loss has increased *****
Loss at iteration [2380]: 0.002446238630633427
Loss at iteration [2381]: 0.002448486366660502
***** Warning: Loss has increased *****
Loss at iteration [2382]: 0.002445756552568216
Loss at iteration [2383]: 0.0024483973667450147
***** Warning: Loss has increased *****
Loss at iteration [2384]: 0.0024457122011338514
Loss at iteration [2385]: 0.002448573008858935
***** Warning: Loss has increased *****
Loss at iteration [2386]: 0.00244626718105375
Loss at iteration [2387]: 0.0024491186428750667
***** Warning: Loss has increased *****
Loss at iteration [2388]: 0.002446306921229807
Loss at iteration [2389]: 0.002449117749210047
***** Warning: Loss has increased *****
Loss at iteration [2390]: 0.002446117262879495
Loss at iteration [2391]: 0.002447845166500486
***** Warning: Loss has increased *****
Loss at iteration [2392]: 0.002444791912519387
Loss at iteration [2393]: 0.002447300131693412
***** Warning: Loss has increased *****
Loss at iteration [2394]: 0.002444913189363014
Loss at iteration [2395]: 0.002447722546850615
***** Warning: Loss has increased *****
Loss at iteration [2396]: 0.0024447235894904428
Loss at iteration [2397]: 0.002447217200318791
***** Warning: Loss has increased *****
Loss at iteration [2398]: 0.0024446713604203596
Loss at iteration [2399]: 0.0024472275266359117
***** Warning: Loss has increased *****
Loss at iteration [2400]: 0.0024447196305578183
Loss at iteration [2401]: 0.0024476468662951575
***** Warning: Loss has increased *****
Loss at iteration [2402]: 0.002445160736003828
Loss at iteration [2403]: 0.0024478141114371837
***** Warning: Loss has increased *****
Loss at iteration [2404]: 0.0024449604745143054
Loss at iteration [2405]: 0.002447741462172177
***** Warning: Loss has increased *****
Loss at iteration [2406]: 0.002444837865824924
Loss at iteration [2407]: 0.0024468635294277214
***** Warning: Loss has increased *****
Loss at iteration [2408]: 0.0024439909093588452
Loss at iteration [2409]: 0.002446323118030234
***** Warning: Loss has increased *****
Loss at iteration [2410]: 0.002443582871225884
Loss at iteration [2411]: 0.0024462565120622433
***** Warning: Loss has increased *****
Loss at iteration [2412]: 0.0024433002321415176
Loss at iteration [2413]: 0.002445963627187607
***** Warning: Loss has increased *****
Loss at iteration [2414]: 0.002443592802882961
Loss at iteration [2415]: 0.0024465571145805
***** Warning: Loss has increased *****
Loss at iteration [2416]: 0.0024443043953757512
Loss at iteration [2417]: 0.002446879158864825
***** Warning: Loss has increased *****
Loss at iteration [2418]: 0.0024440307261332293
Loss at iteration [2419]: 0.0024467755932127753
***** Warning: Loss has increased *****
Loss at iteration [2420]: 0.002444018496083934
Loss at iteration [2421]: 0.0024462143490424433
***** Warning: Loss has increased *****
Loss at iteration [2422]: 0.0024435759464364686
Loss at iteration [2423]: 0.0024462394048702265
***** Warning: Loss has increased *****
Loss at iteration [2424]: 0.0024433616126167445
Loss at iteration [2425]: 0.002445680372339469
***** Warning: Loss has increased *****
Loss at iteration [2426]: 0.002442979016573395
Loss at iteration [2427]: 0.002445627123886594
***** Warning: Loss has increased *****
Loss at iteration [2428]: 0.00244279511176722
Loss at iteration [2429]: 0.0024455371892258366
***** Warning: Loss has increased *****
Loss at iteration [2430]: 0.0024432345637186415
Loss at iteration [2431]: 0.002446307013572322
***** Warning: Loss has increased *****
Loss at iteration [2432]: 0.002443888774210493
Loss at iteration [2433]: 0.0024467739740041865
***** Warning: Loss has increased *****
Loss at iteration [2434]: 0.0024439438103700173
Loss at iteration [2435]: 0.0024464385549207855
***** Warning: Loss has increased *****
Loss at iteration [2436]: 0.0024435872817447062
Loss at iteration [2437]: 0.0024461281639370906
***** Warning: Loss has increased *****
Loss at iteration [2438]: 0.0024435973945420857
Loss at iteration [2439]: 0.002445838539583695
***** Warning: Loss has increased *****
Loss at iteration [2440]: 0.002442700487690925
Loss at iteration [2441]: 0.0024453789903990616
***** Warning: Loss has increased *****
Loss at iteration [2442]: 0.0024426191691334477
Loss at iteration [2443]: 0.002444980249382854
***** Warning: Loss has increased *****
Loss at iteration [2444]: 0.002442007472763132
Loss at iteration [2445]: 0.0024445925511794244
***** Warning: Loss has increased *****
Loss at iteration [2446]: 0.002442207686598224
Loss at iteration [2447]: 0.002445238071098081
***** Warning: Loss has increased *****
Loss at iteration [2448]: 0.002442998349273957
Loss at iteration [2449]: 0.002446032643642342
***** Warning: Loss has increased *****
Loss at iteration [2450]: 0.00244320314814704
Loss at iteration [2451]: 0.002445572736938679
***** Warning: Loss has increased *****
Loss at iteration [2452]: 0.0024427543540769
Loss at iteration [2453]: 0.0024447429461416396
***** Warning: Loss has increased *****
Loss at iteration [2454]: 0.0024417702074741986
Loss at iteration [2455]: 0.0024443536108491094
***** Warning: Loss has increased *****
Loss at iteration [2456]: 0.0024414672118996785
Loss at iteration [2457]: 0.0024435186200141993
***** Warning: Loss has increased *****
Loss at iteration [2458]: 0.002440633424779109
Loss at iteration [2459]: 0.0024433752438628876
***** Warning: Loss has increased *****
Loss at iteration [2460]: 0.0024410264815922537
Loss at iteration [2461]: 0.002443013509476876
***** Warning: Loss has increased *****
Loss at iteration [2462]: 0.0024396942057618425
Loss at iteration [2463]: 0.0024419744122696676
***** Warning: Loss has increased *****
Loss at iteration [2464]: 0.0024394885305204306
Loss at iteration [2465]: 0.0024422753497093702
***** Warning: Loss has increased *****
Loss at iteration [2466]: 0.00243951064871569
Loss at iteration [2467]: 0.0024417202344156155
***** Warning: Loss has increased *****
Loss at iteration [2468]: 0.002438900980540013
Loss at iteration [2469]: 0.002440393101052248
***** Warning: Loss has increased *****
Loss at iteration [2470]: 0.0024372839384815756
Loss at iteration [2471]: 0.002439568087282807
***** Warning: Loss has increased *****
Loss at iteration [2472]: 0.0024368914488119282
Loss at iteration [2473]: 0.0024395277919716488
***** Warning: Loss has increased *****
Loss at iteration [2474]: 0.0024373697966063182
Loss at iteration [2475]: 0.0024397834768671925
***** Warning: Loss has increased *****
Loss at iteration [2476]: 0.002437064543239434
Loss at iteration [2477]: 0.0024393519908472426
***** Warning: Loss has increased *****
Loss at iteration [2478]: 0.002436875054315167
Loss at iteration [2479]: 0.002439583708356291
***** Warning: Loss has increased *****
Loss at iteration [2480]: 0.002437319408375926
Loss at iteration [2481]: 0.0024401571321210186
***** Warning: Loss has increased *****
Loss at iteration [2482]: 0.002437350731107149
Loss at iteration [2483]: 0.0024398526141628787
***** Warning: Loss has increased *****
Loss at iteration [2484]: 0.0024373436138553596
Loss at iteration [2485]: 0.002440213510642165
***** Warning: Loss has increased *****
Loss at iteration [2486]: 0.002438164564619555
Loss at iteration [2487]: 0.002440914770777608
***** Warning: Loss has increased *****
Loss at iteration [2488]: 0.0024383246532429558
Loss at iteration [2489]: 0.002441002455810475
***** Warning: Loss has increased *****
Loss at iteration [2490]: 0.002438494057746501
Loss at iteration [2491]: 0.0024414524002042684
***** Warning: Loss has increased *****
Loss at iteration [2492]: 0.002439233680815316
Loss at iteration [2493]: 0.002442312167260631
***** Warning: Loss has increased *****
Loss at iteration [2494]: 0.0024395516274701395
Loss at iteration [2495]: 0.0024418302764862548
***** Warning: Loss has increased *****
Loss at iteration [2496]: 0.002439266717792818
Loss at iteration [2497]: 0.002442110211125238
***** Warning: Loss has increased *****
Loss at iteration [2498]: 0.0024393661599705387
Loss at iteration [2499]: 0.0024416345682367426
***** Warning: Loss has increased *****
Loss at iteration [2500]: 0.00243882032936938
Loss at iteration [2501]: 0.002441484805518488
***** Warning: Loss has increased *****
Loss at iteration [2502]: 0.0024389738056355766
Loss at iteration [2503]: 0.002441912762805147
***** Warning: Loss has increased *****
Loss at iteration [2504]: 0.0024391871855155524
Loss at iteration [2505]: 0.0024414572304314475
***** Warning: Loss has increased *****
Loss at iteration [2506]: 0.002438806875973077
Loss at iteration [2507]: 0.0024415615481396193
***** Warning: Loss has increased *****
Loss at iteration [2508]: 0.002439091758304041
Loss at iteration [2509]: 0.0024420168412876623
***** Warning: Loss has increased *****
Loss at iteration [2510]: 0.0024393736518190353
Loss at iteration [2511]: 0.0024419253357477674
***** Warning: Loss has increased *****
Loss at iteration [2512]: 0.0024393704251514554
Loss at iteration [2513]: 0.0024424087891110917
***** Warning: Loss has increased *****
Loss at iteration [2514]: 0.0024401103755009824
Loss at iteration [2515]: 0.002442836040250578
***** Warning: Loss has increased *****
Loss at iteration [2516]: 0.0024396798293071223
Loss at iteration [2517]: 0.002441566822276757
***** Warning: Loss has increased *****
Loss at iteration [2518]: 0.002438731119915748
Loss at iteration [2519]: 0.002440959559422593
***** Warning: Loss has increased *****
Loss at iteration [2520]: 0.002437870206141391
Loss at iteration [2521]: 0.002440522168752566
***** Warning: Loss has increased *****
Loss at iteration [2522]: 0.002437996928437856
Loss at iteration [2523]: 0.0024398781785364863
***** Warning: Loss has increased *****
Loss at iteration [2524]: 0.002437030237710746
Loss at iteration [2525]: 0.002439731408820982
***** Warning: Loss has increased *****
Loss at iteration [2526]: 0.0024371327251375588
Loss at iteration [2527]: 0.0024393813259506506
***** Warning: Loss has increased *****
Loss at iteration [2528]: 0.002436759267408815
Loss at iteration [2529]: 0.0024394371715628317
***** Warning: Loss has increased *****
Loss at iteration [2530]: 0.0024367148571629127
Loss at iteration [2531]: 0.002439600685612879
***** Warning: Loss has increased *****
Loss at iteration [2532]: 0.0024370049737155227
Loss at iteration [2533]: 0.0024389218193551724
***** Warning: Loss has increased *****
Loss at iteration [2534]: 0.0024362005694649183
Loss at iteration [2535]: 0.0024387314729120892
***** Warning: Loss has increased *****
Loss at iteration [2536]: 0.0024359432354052737
Loss at iteration [2537]: 0.0024386838048387157
***** Warning: Loss has increased *****
Loss at iteration [2538]: 0.0024362726034524245
Loss at iteration [2539]: 0.0024386406050944417
***** Warning: Loss has increased *****
Loss at iteration [2540]: 0.0024360858109125417
Loss at iteration [2541]: 0.0024390353352673902
***** Warning: Loss has increased *****
Loss at iteration [2542]: 0.002436505411330736
Loss at iteration [2543]: 0.0024390835654488486
***** Warning: Loss has increased *****
Loss at iteration [2544]: 0.002436609335703756
Loss at iteration [2545]: 0.0024394086911153343
***** Warning: Loss has increased *****
Loss at iteration [2546]: 0.002437201210906449
Loss at iteration [2547]: 0.0024402461613745622
***** Warning: Loss has increased *****
Loss at iteration [2548]: 0.0024373641432902104
Loss at iteration [2549]: 0.002440089013494507
***** Warning: Loss has increased *****
Loss at iteration [2550]: 0.0024375888506329843
Loss at iteration [2551]: 0.002440028817326943
***** Warning: Loss has increased *****
Loss at iteration [2552]: 0.0024369334484473336
Loss at iteration [2553]: 0.002438512600537269
***** Warning: Loss has increased *****
Loss at iteration [2554]: 0.0024355625296229844
Loss at iteration [2555]: 0.002438262112737687
***** Warning: Loss has increased *****
Loss at iteration [2556]: 0.0024358281704699464
Loss at iteration [2557]: 0.0024387667083343436
***** Warning: Loss has increased *****
Loss at iteration [2558]: 0.002436129291355379
Loss at iteration [2559]: 0.0024380743782310642
***** Warning: Loss has increased *****
Loss at iteration [2560]: 0.002435322414977132
Loss at iteration [2561]: 0.0024375908061076445
***** Warning: Loss has increased *****
Loss at iteration [2562]: 0.0024351491160112516
Loss at iteration [2563]: 0.0024378258790787215
***** Warning: Loss has increased *****
Loss at iteration [2564]: 0.0024348392554685043
Loss at iteration [2565]: 0.0024370408324125153
***** Warning: Loss has increased *****
Loss at iteration [2566]: 0.0024344627738882717
Loss at iteration [2567]: 0.0024365205983387407
***** Warning: Loss has increased *****
Loss at iteration [2568]: 0.002433846502416779
Loss at iteration [2569]: 0.002436205829833632
***** Warning: Loss has increased *****
Loss at iteration [2570]: 0.002433633749365643
Loss at iteration [2571]: 0.0024364097552937165
***** Warning: Loss has increased *****
Loss at iteration [2572]: 0.0024343885215930153
Loss at iteration [2573]: 0.002437179356375498
***** Warning: Loss has increased *****
Loss at iteration [2574]: 0.002434594194270622
Loss at iteration [2575]: 0.0024370755046658164
***** Warning: Loss has increased *****
Loss at iteration [2576]: 0.0024347244569201155
Loss at iteration [2577]: 0.002437687697326871
***** Warning: Loss has increased *****
Loss at iteration [2578]: 0.0024355398248851654
Loss at iteration [2579]: 0.0024385554363801705
***** Warning: Loss has increased *****
Loss at iteration [2580]: 0.0024361126350501768
Loss at iteration [2581]: 0.0024387322753702976
***** Warning: Loss has increased *****
Loss at iteration [2582]: 0.0024362161835827447
Loss at iteration [2583]: 0.0024393558493091766
***** Warning: Loss has increased *****
Loss at iteration [2584]: 0.00243725143915923
Loss at iteration [2585]: 0.0024405064284858807
***** Warning: Loss has increased *****
Loss at iteration [2586]: 0.0024378162509923313
Loss at iteration [2587]: 0.002440005760183351
***** Warning: Loss has increased *****
Loss at iteration [2588]: 0.0024373413033252447
Loss at iteration [2589]: 0.00243986803829287
***** Warning: Loss has increased *****
Loss at iteration [2590]: 0.0024371737841558613
Loss at iteration [2591]: 0.002440189820903117
***** Warning: Loss has increased *****
Loss at iteration [2592]: 0.002437884966912355
Loss at iteration [2593]: 0.0024411203238156325
***** Warning: Loss has increased *****
Loss at iteration [2594]: 0.0024390221080225725
Loss at iteration [2595]: 0.0024420545989793305
***** Warning: Loss has increased *****
Loss at iteration [2596]: 0.002439446246120122
Loss at iteration [2597]: 0.0024422857682876026
***** Warning: Loss has increased *****
Loss at iteration [2598]: 0.002439726423930617
Loss at iteration [2599]: 0.00244214921539093
***** Warning: Loss has increased *****
Loss at iteration [2600]: 0.002439132141150779
Loss at iteration [2601]: 0.0024415747083606346
***** Warning: Loss has increased *****
Loss at iteration [2602]: 0.0024385636380217807
Loss at iteration [2603]: 0.002440419834433788
***** Warning: Loss has increased *****
Loss at iteration [2604]: 0.0024376576280070276
Loss at iteration [2605]: 0.0024406630341977653
***** Warning: Loss has increased *****
Loss at iteration [2606]: 0.0024383672094956924
Loss at iteration [2607]: 0.0024419118686577986
***** Warning: Loss has increased *****
Loss at iteration [2608]: 0.0024392970934563277
Loss at iteration [2609]: 0.002441026879770175
***** Warning: Loss has increased *****
Loss at iteration [2610]: 0.002438080608077812
Loss at iteration [2611]: 0.002440915091854123
***** Warning: Loss has increased *****
Loss at iteration [2612]: 0.002438555837076124
Loss at iteration [2613]: 0.002441946252759537
***** Warning: Loss has increased *****
Loss at iteration [2614]: 0.0024395816504870394
Loss at iteration [2615]: 0.002442585434923411
***** Warning: Loss has increased *****
Loss at iteration [2616]: 0.0024403626824129475
Loss at iteration [2617]: 0.002443396025900862
***** Warning: Loss has increased *****
Loss at iteration [2618]: 0.002440820182233741
Loss at iteration [2619]: 0.0024439168811895497
***** Warning: Loss has increased *****
Loss at iteration [2620]: 0.0024410186756396775
Loss at iteration [2621]: 0.0024435379619869363
***** Warning: Loss has increased *****
Loss at iteration [2622]: 0.0024409351900732323
Loss at iteration [2623]: 0.002443766010609682
***** Warning: Loss has increased *****
Loss at iteration [2624]: 0.0024409749192760327
Loss at iteration [2625]: 0.0024438643182842497
***** Warning: Loss has increased *****
Loss at iteration [2626]: 0.0024412934283365847
Loss at iteration [2627]: 0.002443739937489467
***** Warning: Loss has increased *****
Loss at iteration [2628]: 0.0024409218056823325
Loss at iteration [2629]: 0.002444033024277838
***** Warning: Loss has increased *****
Loss at iteration [2630]: 0.0024412560290321245
Loss at iteration [2631]: 0.0024447727862284088
***** Warning: Loss has increased *****
Loss at iteration [2632]: 0.0024425943320402566
Loss at iteration [2633]: 0.0024452572999627623
***** Warning: Loss has increased *****
Loss at iteration [2634]: 0.002442748825697414
Loss at iteration [2635]: 0.0024465748657106576
***** Warning: Loss has increased *****
Loss at iteration [2636]: 0.0024439833014303735
Loss at iteration [2637]: 0.0024470566228389024
***** Warning: Loss has increased *****
Loss at iteration [2638]: 0.0024442936798310107
Loss at iteration [2639]: 0.00244735417557932
***** Warning: Loss has increased *****
Loss at iteration [2640]: 0.0024450200741014737
Loss at iteration [2641]: 0.0024482604370174095
***** Warning: Loss has increased *****
Loss at iteration [2642]: 0.0024451644250437607
Loss at iteration [2643]: 0.0024478673675100105
***** Warning: Loss has increased *****
Loss at iteration [2644]: 0.0024450441938028964
Loss at iteration [2645]: 0.0024476746574755566
***** Warning: Loss has increased *****
Loss at iteration [2646]: 0.0024447243855205444
Loss at iteration [2647]: 0.002447812871241819
***** Warning: Loss has increased *****
Loss at iteration [2648]: 0.002444714206163403
Loss at iteration [2649]: 0.002447802676064892
***** Warning: Loss has increased *****
Loss at iteration [2650]: 0.00244529040503218
Loss at iteration [2651]: 0.0024485466215740702
***** Warning: Loss has increased *****
Loss at iteration [2652]: 0.002446105451476536
Loss at iteration [2653]: 0.0024494005464856055
***** Warning: Loss has increased *****
Loss at iteration [2654]: 0.0024464724424650744
Loss at iteration [2655]: 0.0024497342458009936
***** Warning: Loss has increased *****
Loss at iteration [2656]: 0.002446960671766427
Loss at iteration [2657]: 0.002449759230077621
***** Warning: Loss has increased *****
Loss at iteration [2658]: 0.0024466519845378356
Loss at iteration [2659]: 0.0024498177216849206
***** Warning: Loss has increased *****
Loss at iteration [2660]: 0.002447266539723239
Loss at iteration [2661]: 0.0024505560047088266
***** Warning: Loss has increased *****
Loss at iteration [2662]: 0.0024478129359251145
Loss at iteration [2663]: 0.002451318618818743
***** Warning: Loss has increased *****
Loss at iteration [2664]: 0.0024483622898405126
Loss at iteration [2665]: 0.002451962131060789
***** Warning: Loss has increased *****
Loss at iteration [2666]: 0.0024493631038359663
Loss at iteration [2667]: 0.0024520780181523294
***** Warning: Loss has increased *****
Loss at iteration [2668]: 0.0024489590425872875
Loss at iteration [2669]: 0.0024529793796991637
***** Warning: Loss has increased *****
Loss at iteration [2670]: 0.0024507570187991567
Loss at iteration [2671]: 0.002455223654025513
***** Warning: Loss has increased *****
Loss at iteration [2672]: 0.0024529774016554353
Loss at iteration [2673]: 0.0024572731556720163
***** Warning: Loss has increased *****
Loss at iteration [2674]: 0.002454499691911979
Loss at iteration [2675]: 0.0024578515682146746
***** Warning: Loss has increased *****
Loss at iteration [2676]: 0.0024551148713954506
Loss at iteration [2677]: 0.0024594737321911365
***** Warning: Loss has increased *****
Loss at iteration [2678]: 0.0024571595644710393
Loss at iteration [2679]: 0.0024619501778598025
***** Warning: Loss has increased *****
Loss at iteration [2680]: 0.002459400579101035
Loss at iteration [2681]: 0.002463485441619665
***** Warning: Loss has increased *****
Loss at iteration [2682]: 0.002460346302277755
Loss at iteration [2683]: 0.002464239828080752
***** Warning: Loss has increased *****
Loss at iteration [2684]: 0.0024611528374783474
Loss at iteration [2685]: 0.002465641981586756
***** Warning: Loss has increased *****
Loss at iteration [2686]: 0.00246264730769118
Loss at iteration [2687]: 0.0024658105183033665
***** Warning: Loss has increased *****
Loss at iteration [2688]: 0.002461956879816644
Loss at iteration [2689]: 0.0024655118342730934
***** Warning: Loss has increased *****
Loss at iteration [2690]: 0.0024616688609278414
Loss at iteration [2691]: 0.002464025646104586
***** Warning: Loss has increased *****
Loss at iteration [2692]: 0.0024600638098860312
Loss at iteration [2693]: 0.002463369470033376
***** Warning: Loss has increased *****
Loss at iteration [2694]: 0.0024601530567453763
Loss at iteration [2695]: 0.002463746923500461
***** Warning: Loss has increased *****
Loss at iteration [2696]: 0.0024601454211709054
Loss at iteration [2697]: 0.0024630435831782594
***** Warning: Loss has increased *****
Loss at iteration [2698]: 0.0024592711508403925
Loss at iteration [2699]: 0.002461667098723272
***** Warning: Loss has increased *****
Loss at iteration [2700]: 0.002457733673051084
Loss at iteration [2701]: 0.0024606812604157065
***** Warning: Loss has increased *****
Loss at iteration [2702]: 0.0024571772399684024
Loss at iteration [2703]: 0.0024606970297769183
***** Warning: Loss has increased *****
Loss at iteration [2704]: 0.0024578405800090245
Loss at iteration [2705]: 0.0024611044113806244
***** Warning: Loss has increased *****
Loss at iteration [2706]: 0.0024572867038891405
Loss at iteration [2707]: 0.002459893368684504
***** Warning: Loss has increased *****
Loss at iteration [2708]: 0.002456127894630781
Loss at iteration [2709]: 0.002458796836522861
***** Warning: Loss has increased *****
Loss at iteration [2710]: 0.0024551851653290446
Loss at iteration [2711]: 0.002458017234477893
***** Warning: Loss has increased *****
Loss at iteration [2712]: 0.0024547182449017667
Loss at iteration [2713]: 0.0024588440916725202
***** Warning: Loss has increased *****
Loss at iteration [2714]: 0.002456081696268653
Loss at iteration [2715]: 0.002459423655662455
***** Warning: Loss has increased *****
Loss at iteration [2716]: 0.0024561860444421023
Loss at iteration [2717]: 0.002459572304741412
***** Warning: Loss has increased *****
Loss at iteration [2718]: 0.0024565011937733275
Loss at iteration [2719]: 0.0024603048969439765
***** Warning: Loss has increased *****
Loss at iteration [2720]: 0.002457156141307109
Loss at iteration [2721]: 0.002460989026103151
***** Warning: Loss has increased *****
Loss at iteration [2722]: 0.0024579405798715757
Loss at iteration [2723]: 0.0024617744478999132
***** Warning: Loss has increased *****
Loss at iteration [2724]: 0.0024587383000001096
Loss at iteration [2725]: 0.0024625514673575965
***** Warning: Loss has increased *****
Loss at iteration [2726]: 0.002459172857536198
Loss at iteration [2727]: 0.002462311478516011
***** Warning: Loss has increased *****
Loss at iteration [2728]: 0.0024588409884639907
Loss at iteration [2729]: 0.002463139719558519
***** Warning: Loss has increased *****
Loss at iteration [2730]: 0.002460236211529519
Loss at iteration [2731]: 0.0024640458205590393
***** Warning: Loss has increased *****
Loss at iteration [2732]: 0.002460364783418104
Loss at iteration [2733]: 0.002463600523031847
***** Warning: Loss has increased *****
Loss at iteration [2734]: 0.002459811229723548
Loss at iteration [2735]: 0.0024626604900728675
***** Warning: Loss has increased *****
Loss at iteration [2736]: 0.0024585115395917394
Loss at iteration [2737]: 0.002461854673983391
***** Warning: Loss has increased *****
Loss at iteration [2738]: 0.002458461917118482
Loss at iteration [2739]: 0.002461469134465888
***** Warning: Loss has increased *****
Loss at iteration [2740]: 0.0024576561343993405
Loss at iteration [2741]: 0.0024604176753696477
***** Warning: Loss has increased *****
Loss at iteration [2742]: 0.002456304737733874
Loss at iteration [2743]: 0.002459530022399731
***** Warning: Loss has increased *****
Loss at iteration [2744]: 0.0024561910432358224
Loss at iteration [2745]: 0.0024591754287073765
***** Warning: Loss has increased *****
Loss at iteration [2746]: 0.002455163102081351
Loss at iteration [2747]: 0.002457928982982233
***** Warning: Loss has increased *****
Loss at iteration [2748]: 0.002454145928262671
Loss at iteration [2749]: 0.0024562472538678893
***** Warning: Loss has increased *****
Loss at iteration [2750]: 0.0024520331167919413
Loss at iteration [2751]: 0.0024549363116703107
***** Warning: Loss has increased *****
Loss at iteration [2752]: 0.002451714505868762
Loss at iteration [2753]: 0.002454614242497271
***** Warning: Loss has increased *****
Loss at iteration [2754]: 0.0024512156109323365
Loss at iteration [2755]: 0.0024539500987190745
***** Warning: Loss has increased *****
Loss at iteration [2756]: 0.002450106823146207
Loss at iteration [2757]: 0.00245278365557882
***** Warning: Loss has increased *****
Loss at iteration [2758]: 0.002449035353832801
Loss at iteration [2759]: 0.0024520212715653726
***** Warning: Loss has increased *****
Loss at iteration [2760]: 0.002448931133278037
Loss at iteration [2761]: 0.0024519059085522585
***** Warning: Loss has increased *****
Loss at iteration [2762]: 0.002448717727689978
Loss at iteration [2763]: 0.0024517018838238107
***** Warning: Loss has increased *****
Loss at iteration [2764]: 0.002448167983799608
Loss at iteration [2765]: 0.0024507967932225305
***** Warning: Loss has increased *****
Loss at iteration [2766]: 0.002447278059371879
Loss at iteration [2767]: 0.002450522302064558
***** Warning: Loss has increased *****
Loss at iteration [2768]: 0.002447461458698455
Loss at iteration [2769]: 0.0024507492465891075
***** Warning: Loss has increased *****
Loss at iteration [2770]: 0.0024478765610035243
Loss at iteration [2771]: 0.0024511653969550573
***** Warning: Loss has increased *****
Loss at iteration [2772]: 0.0024481808005325087
Loss at iteration [2773]: 0.0024519287205478036
***** Warning: Loss has increased *****
Loss at iteration [2774]: 0.0024486858999712363
Loss at iteration [2775]: 0.002452678981473397
***** Warning: Loss has increased *****
Loss at iteration [2776]: 0.0024503097203887906
Loss at iteration [2777]: 0.002454355075601491
***** Warning: Loss has increased *****
Loss at iteration [2778]: 0.0024511853319558108
Loss at iteration [2779]: 0.0024547621233930983
***** Warning: Loss has increased *****
Loss at iteration [2780]: 0.0024514783282045096
Loss at iteration [2781]: 0.0024553362297458606
***** Warning: Loss has increased *****
Loss at iteration [2782]: 0.002452580255016766
Loss at iteration [2783]: 0.0024565221701796317
***** Warning: Loss has increased *****
Loss at iteration [2784]: 0.0024532540731643194
Loss at iteration [2785]: 0.0024577784334434065
***** Warning: Loss has increased *****
Loss at iteration [2786]: 0.0024550786538403836
Loss at iteration [2787]: 0.0024590648604407798
***** Warning: Loss has increased *****
Loss at iteration [2788]: 0.002455321983662152
Loss at iteration [2789]: 0.0024584160093889637
***** Warning: Loss has increased *****
Loss at iteration [2790]: 0.002454908115423889
Loss at iteration [2791]: 0.0024591315444467484
***** Warning: Loss has increased *****
Loss at iteration [2792]: 0.002456313633109315
Loss at iteration [2793]: 0.002460288280650134
***** Warning: Loss has increased *****
Loss at iteration [2794]: 0.0024566115287625386
Loss at iteration [2795]: 0.0024602699502050654
***** Warning: Loss has increased *****
Loss at iteration [2796]: 0.0024565764685488306
Loss at iteration [2797]: 0.0024596094778584167
***** Warning: Loss has increased *****
Loss at iteration [2798]: 0.002455582520240112
Loss at iteration [2799]: 0.002458350008936257
***** Warning: Loss has increased *****
Loss at iteration [2800]: 0.002454647397160393
Loss at iteration [2801]: 0.002458204009505281
***** Warning: Loss has increased *****
Loss at iteration [2802]: 0.0024546515211010516
Loss at iteration [2803]: 0.0024581022441268357
***** Warning: Loss has increased *****
Loss at iteration [2804]: 0.002454290361589547
Loss at iteration [2805]: 0.0024564584833369706
***** Warning: Loss has increased *****
Loss at iteration [2806]: 0.002452573181506151
Loss at iteration [2807]: 0.002455133479738027
***** Warning: Loss has increased *****
Loss at iteration [2808]: 0.0024509784049454937
Loss at iteration [2809]: 0.002453847882069187
***** Warning: Loss has increased *****
Loss at iteration [2810]: 0.002450321999952886
Loss at iteration [2811]: 0.0024538287045497782
***** Warning: Loss has increased *****
Loss at iteration [2812]: 0.0024502890072360993
Loss at iteration [2813]: 0.0024526799271022996
***** Warning: Loss has increased *****
Loss at iteration [2814]: 0.00244910237715759
Loss at iteration [2815]: 0.0024522231277144474
***** Warning: Loss has increased *****
Loss at iteration [2816]: 0.0024490218255061595
Loss at iteration [2817]: 0.002452310779333235
***** Warning: Loss has increased *****
Loss at iteration [2818]: 0.0024486266630925927
Loss at iteration [2819]: 0.0024515514378093918
***** Warning: Loss has increased *****
Loss at iteration [2820]: 0.0024480877108339315
Loss at iteration [2821]: 0.002450927181388652
***** Warning: Loss has increased *****
Loss at iteration [2822]: 0.0024471229897066174
Loss at iteration [2823]: 0.0024497986955530782
***** Warning: Loss has increased *****
Loss at iteration [2824]: 0.002446664379953752
Loss at iteration [2825]: 0.002450034641756969
***** Warning: Loss has increased *****
Loss at iteration [2826]: 0.002446737830284033
Loss at iteration [2827]: 0.0024499856530967416
***** Warning: Loss has increased *****
Loss at iteration [2828]: 0.0024467159310338957
Loss at iteration [2829]: 0.0024498152962841796
***** Warning: Loss has increased *****
Loss at iteration [2830]: 0.002446348106269233
Loss at iteration [2831]: 0.0024496456465252945
***** Warning: Loss has increased *****
Loss at iteration [2832]: 0.0024464857609779894
Loss at iteration [2833]: 0.0024494739360074315
***** Warning: Loss has increased *****
Loss at iteration [2834]: 0.002446289180417704
Loss at iteration [2835]: 0.002449529020447353
***** Warning: Loss has increased *****
Loss at iteration [2836]: 0.002445950134267078
Loss at iteration [2837]: 0.0024493322707124783
***** Warning: Loss has increased *****
Loss at iteration [2838]: 0.002446215161415657
Loss at iteration [2839]: 0.002449124359140285
***** Warning: Loss has increased *****
Loss at iteration [2840]: 0.0024460496736255946
Loss at iteration [2841]: 0.002449958709308592
***** Warning: Loss has increased *****
Loss at iteration [2842]: 0.0024469102755401984
Loss at iteration [2843]: 0.0024506370618386487
***** Warning: Loss has increased *****
Loss at iteration [2844]: 0.0024475182243984994
Loss at iteration [2845]: 0.0024507587903625522
***** Warning: Loss has increased *****
Loss at iteration [2846]: 0.0024471904950986076
Loss at iteration [2847]: 0.002450694235153805
***** Warning: Loss has increased *****
Loss at iteration [2848]: 0.0024478970594425933
Loss at iteration [2849]: 0.002451477953473395
***** Warning: Loss has increased *****
Loss at iteration [2850]: 0.0024480302937896423
Loss at iteration [2851]: 0.0024517570878027127
***** Warning: Loss has increased *****
Loss at iteration [2852]: 0.0024484599034107876
Loss at iteration [2853]: 0.0024508201859091815
***** Warning: Loss has increased *****
Loss at iteration [2854]: 0.0024470840790720436
Loss at iteration [2855]: 0.0024509097084955277
***** Warning: Loss has increased *****
Loss at iteration [2856]: 0.0024477850030507563
Loss at iteration [2857]: 0.0024508621932144608
***** Warning: Loss has increased *****
Loss at iteration [2858]: 0.0024476048785486166
Loss at iteration [2859]: 0.0024515148699097715
***** Warning: Loss has increased *****
Loss at iteration [2860]: 0.0024480770777172726
Loss at iteration [2861]: 0.0024509188966013525
***** Warning: Loss has increased *****
Loss at iteration [2862]: 0.0024471116671400165
Loss at iteration [2863]: 0.0024498859173632454
***** Warning: Loss has increased *****
Loss at iteration [2864]: 0.0024468350182864167
Loss at iteration [2865]: 0.0024503041198846017
***** Warning: Loss has increased *****
Loss at iteration [2866]: 0.0024468785140014065
Loss at iteration [2867]: 0.0024505403183207545
***** Warning: Loss has increased *****
Loss at iteration [2868]: 0.0024472879109479237
Loss at iteration [2869]: 0.0024500519370097037
***** Warning: Loss has increased *****
Loss at iteration [2870]: 0.002446205870484983
Loss at iteration [2871]: 0.0024493275448097696
***** Warning: Loss has increased *****
Loss at iteration [2872]: 0.0024461080967342497
Loss at iteration [2873]: 0.002449118883295671
***** Warning: Loss has increased *****
Loss at iteration [2874]: 0.00244542980226504
Loss at iteration [2875]: 0.002448128816161429
***** Warning: Loss has increased *****
Loss at iteration [2876]: 0.0024445884090308306
Loss at iteration [2877]: 0.0024479673092100726
***** Warning: Loss has increased *****
Loss at iteration [2878]: 0.0024445283428515493
Loss at iteration [2879]: 0.002447531522946043
***** Warning: Loss has increased *****
Loss at iteration [2880]: 0.002444356082001939
Loss at iteration [2881]: 0.0024472134126712285
***** Warning: Loss has increased *****
Loss at iteration [2882]: 0.002443873799584603
Loss at iteration [2883]: 0.002447211513304898
***** Warning: Loss has increased *****
Loss at iteration [2884]: 0.002443756597971177
Loss at iteration [2885]: 0.002447024890131744
***** Warning: Loss has increased *****
Loss at iteration [2886]: 0.0024439222551558473
Loss at iteration [2887]: 0.0024473117265334356
***** Warning: Loss has increased *****
Loss at iteration [2888]: 0.002444151529405512
Loss at iteration [2889]: 0.0024472362357185534
***** Warning: Loss has increased *****
Loss at iteration [2890]: 0.0024442089469466645
Loss at iteration [2891]: 0.002448345213128645
***** Warning: Loss has increased *****
Loss at iteration [2892]: 0.0024452825418958273
Loss at iteration [2893]: 0.0024486772919480497
***** Warning: Loss has increased *****
Loss at iteration [2894]: 0.002445143361309239
Loss at iteration [2895]: 0.0024479779615384756
***** Warning: Loss has increased *****
Loss at iteration [2896]: 0.0024448432997246487
Loss at iteration [2897]: 0.002448233482922875
***** Warning: Loss has increased *****
Loss at iteration [2898]: 0.0024446029526026527
Loss at iteration [2899]: 0.0024481158338535143
***** Warning: Loss has increased *****
Loss at iteration [2900]: 0.0024449609384409906
Loss at iteration [2901]: 0.0024478616463680324
***** Warning: Loss has increased *****
Loss at iteration [2902]: 0.002444105833600319
Loss at iteration [2903]: 0.0024466198678429617
***** Warning: Loss has increased *****
Loss at iteration [2904]: 0.0024430922637939923
Loss at iteration [2905]: 0.0024457498068016785
***** Warning: Loss has increased *****
Loss at iteration [2906]: 0.0024419549380513927
Loss at iteration [2907]: 0.002445465586771968
***** Warning: Loss has increased *****
Loss at iteration [2908]: 0.0024423985055696995
Loss at iteration [2909]: 0.0024455061867561557
***** Warning: Loss has increased *****
Loss at iteration [2910]: 0.0024423085112051644
Loss at iteration [2911]: 0.0024460623287601555
***** Warning: Loss has increased *****
Loss at iteration [2912]: 0.002442728570266007
Loss at iteration [2913]: 0.0024446127721035327
***** Warning: Loss has increased *****
Loss at iteration [2914]: 0.0024406092222309553
Loss at iteration [2915]: 0.002443615984898109
***** Warning: Loss has increased *****
Loss at iteration [2916]: 0.002440194730142934
Loss at iteration [2917]: 0.002442865602144394
***** Warning: Loss has increased *****
Loss at iteration [2918]: 0.002439935390100165
Loss at iteration [2919]: 0.002443402507429624
***** Warning: Loss has increased *****
Loss at iteration [2920]: 0.002440060523049604
Loss at iteration [2921]: 0.0024426830324304
***** Warning: Loss has increased *****
Loss at iteration [2922]: 0.0024394621253764253
Loss at iteration [2923]: 0.0024427399602780166
***** Warning: Loss has increased *****
Loss at iteration [2924]: 0.0024398231234223342
Loss at iteration [2925]: 0.002443291162673332
***** Warning: Loss has increased *****
Loss at iteration [2926]: 0.002440198591161202
Loss at iteration [2927]: 0.002444429366292895
***** Warning: Loss has increased *****
Loss at iteration [2928]: 0.002441203805329204
Loss at iteration [2929]: 0.0024441545674834773
***** Warning: Loss has increased *****
Loss at iteration [2930]: 0.0024407503797509197
Loss at iteration [2931]: 0.002444007599707887
***** Warning: Loss has increased *****
Loss at iteration [2932]: 0.002440911488817014
Loss at iteration [2933]: 0.002444634789403641
***** Warning: Loss has increased *****
Loss at iteration [2934]: 0.002441649425272446
Loss at iteration [2935]: 0.002445119850440955
***** Warning: Loss has increased *****
Loss at iteration [2936]: 0.002441611194571543
Loss at iteration [2937]: 0.0024445232637791098
***** Warning: Loss has increased *****
Loss at iteration [2938]: 0.0024410686081823723
Loss at iteration [2939]: 0.002443571957493693
***** Warning: Loss has increased *****
Loss at iteration [2940]: 0.002440302952808553
Loss at iteration [2941]: 0.002444586967768961
***** Warning: Loss has increased *****
Loss at iteration [2942]: 0.0024418259806659464
Loss at iteration [2943]: 0.002445198131270949
***** Warning: Loss has increased *****
Loss at iteration [2944]: 0.0024419470695920655
Loss at iteration [2945]: 0.0024448743598483795
***** Warning: Loss has increased *****
Loss at iteration [2946]: 0.0024415130053479423
Loss at iteration [2947]: 0.0024451248449954754
***** Warning: Loss has increased *****
Loss at iteration [2948]: 0.0024418293010479886
Loss at iteration [2949]: 0.0024455796108334145
***** Warning: Loss has increased *****
Loss at iteration [2950]: 0.0024424756362159817
Loss at iteration [2951]: 0.002445870687660927
***** Warning: Loss has increased *****
Loss at iteration [2952]: 0.0024429172175719836
Loss at iteration [2953]: 0.0024466872343280976
***** Warning: Loss has increased *****
Loss at iteration [2954]: 0.002443149591553455
Loss at iteration [2955]: 0.0024463070155811674
***** Warning: Loss has increased *****
Loss at iteration [2956]: 0.0024428557571244004
Loss at iteration [2957]: 0.002446663538426131
***** Warning: Loss has increased *****
Loss at iteration [2958]: 0.00244379775859547
Loss at iteration [2959]: 0.002446555843327672
***** Warning: Loss has increased *****
Loss at iteration [2960]: 0.002442642685288925
Loss at iteration [2961]: 0.0024467866534496935
***** Warning: Loss has increased *****
Loss at iteration [2962]: 0.002444197034655432
Loss at iteration [2963]: 0.0024481998273222843
***** Warning: Loss has increased *****
Loss at iteration [2964]: 0.002445077535234834
Loss at iteration [2965]: 0.002447973353432684
***** Warning: Loss has increased *****
Loss at iteration [2966]: 0.0024437998396332605
Loss at iteration [2967]: 0.0024469233397645365
***** Warning: Loss has increased *****
Loss at iteration [2968]: 0.0024437410293527105
Loss at iteration [2969]: 0.0024474998097303974
***** Warning: Loss has increased *****
Loss at iteration [2970]: 0.002444236985776963
Loss at iteration [2971]: 0.002447629124860283
***** Warning: Loss has increased *****
Loss at iteration [2972]: 0.002443987459742665
Loss at iteration [2973]: 0.002446911851798555
***** Warning: Loss has increased *****
Loss at iteration [2974]: 0.002443443892124179
Loss at iteration [2975]: 0.0024464916348876163
***** Warning: Loss has increased *****
Loss at iteration [2976]: 0.0024427172816676814
Loss at iteration [2977]: 0.002445897462726926
***** Warning: Loss has increased *****
Loss at iteration [2978]: 0.0024425418031595357
Loss at iteration [2979]: 0.0024454609150326833
***** Warning: Loss has increased *****
Loss at iteration [2980]: 0.0024419507352586357
Loss at iteration [2981]: 0.0024453593116081527
***** Warning: Loss has increased *****
Loss at iteration [2982]: 0.0024423159493029622
Loss at iteration [2983]: 0.002445439530868388
***** Warning: Loss has increased *****
Loss at iteration [2984]: 0.0024421838392928827
Loss at iteration [2985]: 0.002444925214467195
***** Warning: Loss has increased *****
Loss at iteration [2986]: 0.0024409092035509892
Loss at iteration [2987]: 0.0024441125046074288
***** Warning: Loss has increased *****
Loss at iteration [2988]: 0.002441362780740644
Loss at iteration [2989]: 0.0024456112067558263
***** Warning: Loss has increased *****
Loss at iteration [2990]: 0.0024427072918668464
Loss at iteration [2991]: 0.0024465002829614745
***** Warning: Loss has increased *****
Loss at iteration [2992]: 0.002443229246586913
Loss at iteration [2993]: 0.0024453692203763333
***** Warning: Loss has increased *****
Loss at iteration [2994]: 0.0024419358447566627
Loss at iteration [2995]: 0.0024459858452679055
***** Warning: Loss has increased *****
Loss at iteration [2996]: 0.0024429484623545022
Loss at iteration [2997]: 0.002447442810881073
***** Warning: Loss has increased *****
Loss at iteration [2998]: 0.002444871047008137
Loss at iteration [2999]: 0.0024492598226597707
***** Warning: Loss has increased *****
Loss at iteration [3000]: 0.002446117055247459
