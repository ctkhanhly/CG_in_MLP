Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.1
Beta type                             :FR_PR
Total number of function evaluations  : 2055
Total number of iterations            : 1172
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 9.699234962463379
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.943069870595345%
Percentage of parameters < 1e-7       : 49.943069870595345%
Percentage of parameters < 1e-6       : 49.94405995980238%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.002633042911880435
Loss at iteration [2]: 0.002627056426971501
Loss at iteration [3]: 0.0026237319482220103
Loss at iteration [4]: 0.0025971120585357756
Loss at iteration [5]: 0.00259216639844778
Loss at iteration [6]: 0.00258085544239975
Loss at iteration [7]: 0.002577321597612332
Loss at iteration [8]: 0.002574542986431465
Loss at iteration [9]: 0.002545297440274376
Loss at iteration [10]: 0.0025412809937653217
Loss at iteration [11]: 0.0025398060747038485
Loss at iteration [12]: 0.0025377282111128606
Loss at iteration [13]: 0.002532212875016856
Loss at iteration [14]: 0.002531473509134537
Loss at iteration [15]: 0.002531473509134537
Loss at iteration [16]: 0.002530902869911772
Loss at iteration [17]: 0.0025293060062628256
Loss at iteration [18]: 0.0025279554015151424
Loss at iteration [19]: 0.002527778636255019
Loss at iteration [20]: 0.0025271761289962884
Loss at iteration [21]: 0.002526468739266282
Loss at iteration [22]: 0.0025259487054122777
Loss at iteration [23]: 0.0025241951162484006
Loss at iteration [24]: 0.002517151644216128
Loss at iteration [25]: 0.0025156913101725444
Loss at iteration [26]: 0.0025090819158128837
Loss at iteration [27]: 0.002502470730766328
Loss at iteration [28]: 0.002500092107457924
Loss at iteration [29]: 0.002500092107457924
Loss at iteration [30]: 0.002499515044418566
Loss at iteration [31]: 0.002497828441361361
Loss at iteration [32]: 0.002496693462561696
Loss at iteration [33]: 0.0024965864985764716
Loss at iteration [34]: 0.0024963440283217674
Loss at iteration [35]: 0.002495382469174792
Loss at iteration [36]: 0.0024944583614511175
Loss at iteration [37]: 0.002493891901430652
Loss at iteration [38]: 0.00249328046861293
Loss at iteration [39]: 0.002492854569867925
Loss at iteration [40]: 0.0024927762347719163
Loss at iteration [41]: 0.0024917527233092912
Loss at iteration [42]: 0.002491143704690314
Loss at iteration [43]: 0.0024899127197719867
Loss at iteration [44]: 0.0024899127197719867
Loss at iteration [45]: 0.0024896705795564417
Loss at iteration [46]: 0.0024893828233493774
Loss at iteration [47]: 0.002488812429292328
Loss at iteration [48]: 0.002488763753073172
Loss at iteration [49]: 0.0024887232533905134
Loss at iteration [50]: 0.0024885842044008216
Loss at iteration [51]: 0.0024879184306174844
Loss at iteration [52]: 0.0024866610409005545
Loss at iteration [53]: 0.002485209831888399
Loss at iteration [54]: 0.0024841203850391423
Loss at iteration [55]: 0.0024812993879398866
Loss at iteration [56]: 0.0024793466994680238
Loss at iteration [57]: 0.0024790585132057866
Loss at iteration [58]: 0.0024781440328624903
Loss at iteration [59]: 0.0024781440328624903
Loss at iteration [60]: 0.0024770971994538334
Loss at iteration [61]: 0.0024769745036826427
Loss at iteration [62]: 0.002475060842070027
Loss at iteration [63]: 0.002475012274001788
Loss at iteration [64]: 0.002474843321159006
Loss at iteration [65]: 0.00247426959689864
Loss at iteration [66]: 0.0024741431059409763
Loss at iteration [67]: 0.0024728532274709623
Loss at iteration [68]: 0.0024720538750953335
Loss at iteration [69]: 0.002471125429945459
Loss at iteration [70]: 0.0024707212749241875
Loss at iteration [71]: 0.0024703823547527824
Loss at iteration [72]: 0.0024695806647167745
Loss at iteration [73]: 0.0024695806647167745
Loss at iteration [74]: 0.0024693925905153596
Loss at iteration [75]: 0.002468976777420717
Loss at iteration [76]: 0.0024688994250852006
Loss at iteration [77]: 0.0024682122977575374
Loss at iteration [78]: 0.002467908449916191
Loss at iteration [79]: 0.002467836253634481
Loss at iteration [80]: 0.002467709636920537
Loss at iteration [81]: 0.002467005452791038
Loss at iteration [82]: 0.0024664779272847532
Loss at iteration [83]: 0.0024662870777440223
Loss at iteration [84]: 0.002466035677716171
Loss at iteration [85]: 0.00246290407098212
Loss at iteration [86]: 0.00246290407098212
Loss at iteration [87]: 0.0024619041195193944
Loss at iteration [88]: 0.0024617542928139302
Loss at iteration [89]: 0.0024612050069753753
Loss at iteration [90]: 0.0024601418296131554
Loss at iteration [91]: 0.0024599924130698342
Loss at iteration [92]: 0.0024595635722250153
Loss at iteration [93]: 0.002459169507919285
Loss at iteration [94]: 0.0024589923865554796
Loss at iteration [95]: 0.0024579768299181942
Loss at iteration [96]: 0.002457866735047152
Loss at iteration [97]: 0.002457428342007625
Loss at iteration [98]: 0.0024567032588720785
Loss at iteration [99]: 0.0024565303864027047
Loss at iteration [100]: 0.0024565303864027047
Loss at iteration [101]: 0.0024563458191707794
Loss at iteration [102]: 0.0024562996860643138
Loss at iteration [103]: 0.0024561683180729667
Loss at iteration [104]: 0.0024559615250668286
Loss at iteration [105]: 0.0024555591819592853
Loss at iteration [106]: 0.0024551746816998915
Loss at iteration [107]: 0.002455015281756579
Loss at iteration [108]: 0.00245490998249834
Loss at iteration [109]: 0.0024545926341006867
Loss at iteration [110]: 0.002454545181068677
Loss at iteration [111]: 0.00245448345777713
Loss at iteration [112]: 0.002454428877438386
Loss at iteration [113]: 0.002454383370636139
Loss at iteration [114]: 0.002454383370636139
Loss at iteration [115]: 0.0024543531860137693
Loss at iteration [116]: 0.0024542950137776596
Loss at iteration [117]: 0.002454252074263582
Loss at iteration [118]: 0.0024542396370364596
Loss at iteration [119]: 0.002454137429896076
Loss at iteration [120]: 0.0024539947549058333
Loss at iteration [121]: 0.0024535012423855876
Loss at iteration [122]: 0.002453438560371921
Loss at iteration [123]: 0.0024532748252071262
Loss at iteration [124]: 0.0024524802545646354
Loss at iteration [125]: 0.002452276537054156
Loss at iteration [126]: 0.0024494416342185047
Loss at iteration [127]: 0.002447671126348344
Loss at iteration [128]: 0.0024474364396200187
Loss at iteration [129]: 0.002446430729468765
Loss at iteration [130]: 0.002446087956693775
Loss at iteration [131]: 0.002446087956693775
Loss at iteration [132]: 0.002445881052617599
Loss at iteration [133]: 0.002445207019283887
Loss at iteration [134]: 0.002445034616892337
Loss at iteration [135]: 0.002444975281392167
Loss at iteration [136]: 0.0024444097398808144
Loss at iteration [137]: 0.0024437908889915565
Loss at iteration [138]: 0.0024436637365767575
Loss at iteration [139]: 0.002443320360243514
Loss at iteration [140]: 0.0024430165953550063
Loss at iteration [141]: 0.002442838656385177
Loss at iteration [142]: 0.0024427719041074537
Loss at iteration [143]: 0.0024425655322663216
Loss at iteration [144]: 0.002442255624796247
Loss at iteration [145]: 0.0024421196880098635
Loss at iteration [146]: 0.0024420459590767368
Loss at iteration [147]: 0.00244163652988572
Loss at iteration [148]: 0.00244163652988572
Loss at iteration [149]: 0.0024415433534978057
Loss at iteration [150]: 0.0024415168647921254
Loss at iteration [151]: 0.0024411091634516163
Loss at iteration [152]: 0.002441063092607964
Loss at iteration [153]: 0.0024408816635936186
Loss at iteration [154]: 0.0024397294915780563
Loss at iteration [155]: 0.0024385209250088764
Loss at iteration [156]: 0.002438220562074798
Loss at iteration [157]: 0.002437971198633998
Loss at iteration [158]: 0.0024375180938009835
Loss at iteration [159]: 0.0024373433704788757
Loss at iteration [160]: 0.002436794105926933
Loss at iteration [161]: 0.0024359224420548746
Loss at iteration [162]: 0.0024359224420548746
Loss at iteration [163]: 0.0024356594292942505
Loss at iteration [164]: 0.0024353082095136293
Loss at iteration [165]: 0.0024347189012727706
Loss at iteration [166]: 0.002434341738098043
Loss at iteration [167]: 0.0024337025631682345
Loss at iteration [168]: 0.002433605704157717
Loss at iteration [169]: 0.0024334510691376887
Loss at iteration [170]: 0.002433097472855229
Loss at iteration [171]: 0.0024330052352317345
Loss at iteration [172]: 0.002432680843459828
Loss at iteration [173]: 0.0024323006273598187
Loss at iteration [174]: 0.002432147965750387
Loss at iteration [175]: 0.002431952963875924
Loss at iteration [176]: 0.002431952963875924
Loss at iteration [177]: 0.002431914056657082
Loss at iteration [178]: 0.00243181719148323
Loss at iteration [179]: 0.0024317006821778136
Loss at iteration [180]: 0.002431341466149332
Loss at iteration [181]: 0.002431080790354806
Loss at iteration [182]: 0.0024307989443309867
Loss at iteration [183]: 0.002430593665946149
Loss at iteration [184]: 0.002430404954186087
Loss at iteration [185]: 0.002430350447550777
Loss at iteration [186]: 0.0024303077818309438
Loss at iteration [187]: 0.0024302183879713426
Loss at iteration [188]: 0.0024301906348648465
Loss at iteration [189]: 0.002430106923358376
Loss at iteration [190]: 0.0024299638902873654
Loss at iteration [191]: 0.0024299638902873654
Loss at iteration [192]: 0.0024299361948960005
Loss at iteration [193]: 0.0024299308590970325
Loss at iteration [194]: 0.002429896628794097
Loss at iteration [195]: 0.002429828227318815
Loss at iteration [196]: 0.0024296460196228473
Loss at iteration [197]: 0.0024294790686697665
Loss at iteration [198]: 0.002429273672229448
Loss at iteration [199]: 0.0024288385302609593
Loss at iteration [200]: 0.0024286060389417884
Loss at iteration [201]: 0.002428100069696385
Loss at iteration [202]: 0.002427606452380863
Loss at iteration [203]: 0.002427385081235311
Loss at iteration [204]: 0.002427144803197289
Loss at iteration [205]: 0.002425811690640576
Loss at iteration [206]: 0.002425443233157611
Loss at iteration [207]: 0.0024252265015900555
Loss at iteration [208]: 0.0024252265015900555
Loss at iteration [209]: 0.0024250502305607448
Loss at iteration [210]: 0.0024249698582612415
Loss at iteration [211]: 0.0024248657476127785
Loss at iteration [212]: 0.002424713059768986
Loss at iteration [213]: 0.0024245915292503907
Loss at iteration [214]: 0.0024245182414369213
Loss at iteration [215]: 0.0024244292695151377
Loss at iteration [216]: 0.0024241687494040622
Loss at iteration [217]: 0.002423782640802615
Loss at iteration [218]: 0.002423564091961795
Loss at iteration [219]: 0.002423357736248216
Loss at iteration [220]: 0.0024232738933929686
Loss at iteration [221]: 0.0024232037693315616
Loss at iteration [222]: 0.002423111730137213
Loss at iteration [223]: 0.002423006186806189
Loss at iteration [224]: 0.002423006186806189
Loss at iteration [225]: 0.0024229451639409087
Loss at iteration [226]: 0.0024228955349196338
Loss at iteration [227]: 0.002422822222061562
Loss at iteration [228]: 0.0024227230075853858
Loss at iteration [229]: 0.0024226888080789194
Loss at iteration [230]: 0.00242265419103004
Loss at iteration [231]: 0.0024226174032176634
Loss at iteration [232]: 0.0024225572926494057
Loss at iteration [233]: 0.0024224616799057895
Loss at iteration [234]: 0.002422360238436743
Loss at iteration [235]: 0.0024221336818720175
Loss at iteration [236]: 0.0024221081024463705
Loss at iteration [237]: 0.002422003588316344
Loss at iteration [238]: 0.002421862277041031
Loss at iteration [239]: 0.0024218247172233193
Loss at iteration [240]: 0.0024217182601695
Loss at iteration [241]: 0.0024217182601695
Loss at iteration [242]: 0.002421651090964889
Loss at iteration [243]: 0.002421628056945651
Loss at iteration [244]: 0.00242146618193371
Loss at iteration [245]: 0.0024214124779168376
Loss at iteration [246]: 0.0024210575276978038
Loss at iteration [247]: 0.0024209041064797185
Loss at iteration [248]: 0.002420594323382046
Loss at iteration [249]: 0.0024204086126883837
Loss at iteration [250]: 0.0024198772250404958
Loss at iteration [251]: 0.0024193635071554117
Loss at iteration [252]: 0.002419243693435225
Loss at iteration [253]: 0.0024190496635213086
Loss at iteration [254]: 0.0024189097823453004
Loss at iteration [255]: 0.0024186836747395993
Loss at iteration [256]: 0.0024176230343541543
Loss at iteration [257]: 0.0024176230343541543
Loss at iteration [258]: 0.00241729375751364
Loss at iteration [259]: 0.002417201797321671
Loss at iteration [260]: 0.0024171393904877697
Loss at iteration [261]: 0.0024171018918522475
Loss at iteration [262]: 0.002417049057318497
Loss at iteration [263]: 0.0024169853497643095
Loss at iteration [264]: 0.0024169624419482366
Loss at iteration [265]: 0.0024169219910949924
Loss at iteration [266]: 0.0024167148639219747
Loss at iteration [267]: 0.002416315489891873
Loss at iteration [268]: 0.002416229830310796
Loss at iteration [269]: 0.002415973219146666
Loss at iteration [270]: 0.0024158983821667597
Loss at iteration [271]: 0.002415824803811947
Loss at iteration [272]: 0.0024155395461794966
Loss at iteration [273]: 0.0024154815485999907
Loss at iteration [274]: 0.0024154815485999907
Loss at iteration [275]: 0.00241543007332788
Loss at iteration [276]: 0.002415404139760975
Loss at iteration [277]: 0.0024153590892461163
Loss at iteration [278]: 0.0024152847194382917
Loss at iteration [279]: 0.0024151308098825422
Loss at iteration [280]: 0.0024145714640950782
Loss at iteration [281]: 0.002414315002541485
Loss at iteration [282]: 0.0024141837462718775
Loss at iteration [283]: 0.0024139586859790205
Loss at iteration [284]: 0.002413513136426857
Loss at iteration [285]: 0.0024131147781934867
Loss at iteration [286]: 0.002412895112885073
Loss at iteration [287]: 0.0024124826205275383
Loss at iteration [288]: 0.0024124826205275383
Loss at iteration [289]: 0.0024123405245899863
Loss at iteration [290]: 0.002412315211777043
Loss at iteration [291]: 0.0024119549385192255
Loss at iteration [292]: 0.0024116485870286034
Loss at iteration [293]: 0.0024114919169114625
Loss at iteration [294]: 0.002411167281613152
Loss at iteration [295]: 0.002410984220856286
Loss at iteration [296]: 0.002410888224578664
Loss at iteration [297]: 0.0024108869803927207
Loss at iteration [298]: 0.0024108754779212835
Loss at iteration [299]: 0.00241074014929459
Loss at iteration [300]: 0.002410645718236059
Loss at iteration [301]: 0.002410559898550935
Loss at iteration [302]: 0.002410559898550935
Loss at iteration [303]: 0.0024104659943701098
Loss at iteration [304]: 0.002410340730791437
Loss at iteration [305]: 0.002410298629902516
Loss at iteration [306]: 0.002410282072113846
Loss at iteration [307]: 0.00241020023739857
Loss at iteration [308]: 0.0024100089115812336
Loss at iteration [309]: 0.002409862887688433
Loss at iteration [310]: 0.0024093659036672323
Loss at iteration [311]: 0.0024090592092741525
Loss at iteration [312]: 0.00240884330495224
Loss at iteration [313]: 0.0024085353038518927
Loss at iteration [314]: 0.002407923307169208
Loss at iteration [315]: 0.002407678370967847
Loss at iteration [316]: 0.002407325590158705
Loss at iteration [317]: 0.002407325590158705
Loss at iteration [318]: 0.002407159169567069
Loss at iteration [319]: 0.002406848144459951
Loss at iteration [320]: 0.002406693594686533
Loss at iteration [321]: 0.0024065915505946892
Loss at iteration [322]: 0.0024061908586279408
Loss at iteration [323]: 0.0024061234200672787
Loss at iteration [324]: 0.002405894409410971
Loss at iteration [325]: 0.002405382947923522
Loss at iteration [326]: 0.0024052227231683423
Loss at iteration [327]: 0.002404710601020715
Loss at iteration [328]: 0.002404638579797707
Loss at iteration [329]: 0.0024046036435514404
Loss at iteration [330]: 0.0024043425887158632
Loss at iteration [331]: 0.0024043425887158632
Loss at iteration [332]: 0.0024042915789526434
Loss at iteration [333]: 0.002404276034969978
Loss at iteration [334]: 0.0024036596957146953
Loss at iteration [335]: 0.0024035967289601115
Loss at iteration [336]: 0.0024034796647519683
Loss at iteration [337]: 0.0024033461650541667
Loss at iteration [338]: 0.00240329396231448
Loss at iteration [339]: 0.002403202333848125
Loss at iteration [340]: 0.002403188933588412
Loss at iteration [341]: 0.0024031399096968724
Loss at iteration [342]: 0.0024029499876471625
Loss at iteration [343]: 0.0024027835118775433
Loss at iteration [344]: 0.002402659590531483
Loss at iteration [345]: 0.0024025969225276497
Loss at iteration [346]: 0.0024025969225276497
Loss at iteration [347]: 0.002402544393596095
Loss at iteration [348]: 0.002402476880898416
Loss at iteration [349]: 0.002402302128324428
Loss at iteration [350]: 0.0024022683106552077
Loss at iteration [351]: 0.002402215283802763
Loss at iteration [352]: 0.0024019604058859107
Loss at iteration [353]: 0.0024016854841248957
Loss at iteration [354]: 0.002401621763642091
Loss at iteration [355]: 0.0024015338453588927
Loss at iteration [356]: 0.002401431604719245
Loss at iteration [357]: 0.002401306977517528
Loss at iteration [358]: 0.002401187877391127
Loss at iteration [359]: 0.0024011346898126264
Loss at iteration [360]: 0.002401040031101398
Loss at iteration [361]: 0.0024009825757509485
Loss at iteration [362]: 0.0024009825757509485
Loss at iteration [363]: 0.002400963517044293
Loss at iteration [364]: 0.0024009098216495406
Loss at iteration [365]: 0.0024008157228779816
Loss at iteration [366]: 0.002400756276316759
Loss at iteration [367]: 0.002400686642492963
Loss at iteration [368]: 0.0024002139348765403
Loss at iteration [369]: 0.0024001268076059962
Loss at iteration [370]: 0.0024000458421253035
Loss at iteration [371]: 0.00239923281342882
Loss at iteration [372]: 0.0023991482808613266
Loss at iteration [373]: 0.002399034249627353
Loss at iteration [374]: 0.002398045391346328
Loss at iteration [375]: 0.0023977866385503374
Loss at iteration [376]: 0.0023977866385503374
Loss at iteration [377]: 0.0023976143341575855
Loss at iteration [378]: 0.002397483683210459
Loss at iteration [379]: 0.002397247363294187
Loss at iteration [380]: 0.002396753111981302
Loss at iteration [381]: 0.0023965407740502006
Loss at iteration [382]: 0.0023964331366088105
Loss at iteration [383]: 0.0023962872116380985
Loss at iteration [384]: 0.0023961343095540142
Loss at iteration [385]: 0.0023960964365010635
Loss at iteration [386]: 0.0023960731022107384
Loss at iteration [387]: 0.002395979052754448
Loss at iteration [388]: 0.0023956558732692416
Loss at iteration [389]: 0.0023956558732692416
Loss at iteration [390]: 0.002395580651546655
Loss at iteration [391]: 0.0023954848286258947
Loss at iteration [392]: 0.002395399754259229
Loss at iteration [393]: 0.0023953629422702877
Loss at iteration [394]: 0.0023953387309874523
Loss at iteration [395]: 0.002395217995184422
Loss at iteration [396]: 0.002395089162005128
Loss at iteration [397]: 0.0023950393779818024
Loss at iteration [398]: 0.0023949774284099854
Loss at iteration [399]: 0.0023947275373709864
Loss at iteration [400]: 0.0023945703826835033
Loss at iteration [401]: 0.0023943556838105914
Loss at iteration [402]: 0.002393436508479882
Loss at iteration [403]: 0.002393193829361676
Loss at iteration [404]: 0.002393104482490046
Loss at iteration [405]: 0.0023928973602016655
Loss at iteration [406]: 0.0023927802516218646
Loss at iteration [407]: 0.0023927802516218646
Loss at iteration [408]: 0.0023927120400737177
Loss at iteration [409]: 0.0023926645251770365
Loss at iteration [410]: 0.002392412596307502
Loss at iteration [411]: 0.0023922938441359957
Loss at iteration [412]: 0.002392027057630895
Loss at iteration [413]: 0.002391865309610468
Loss at iteration [414]: 0.0023915686949955925
Loss at iteration [415]: 0.0023914128971827316
Loss at iteration [416]: 0.0023912426541078903
Loss at iteration [417]: 0.002391192575905358
Loss at iteration [418]: 0.0023911447291977313
Loss at iteration [419]: 0.002391071939078307
Loss at iteration [420]: 0.0023908810430201956
Loss at iteration [421]: 0.002390836709538855
Loss at iteration [422]: 0.0023907964935290862
Loss at iteration [423]: 0.0023907964935290862
Loss at iteration [424]: 0.002390764245204959
Loss at iteration [425]: 0.0023907142234513984
Loss at iteration [426]: 0.0023906168595293907
Loss at iteration [427]: 0.0023905658844816336
Loss at iteration [428]: 0.0023904964832909817
Loss at iteration [429]: 0.002390406605621765
Loss at iteration [430]: 0.0023903670160394753
Loss at iteration [431]: 0.0023901435672397846
Loss at iteration [432]: 0.002389846136968533
Loss at iteration [433]: 0.0023893819868411126
Loss at iteration [434]: 0.0023889486463913112
Loss at iteration [435]: 0.0023884005940652347
Loss at iteration [436]: 0.0023876062737132188
Loss at iteration [437]: 0.002387529254887035
Loss at iteration [438]: 0.002387529254887035
Loss at iteration [439]: 0.0023874862966326805
Loss at iteration [440]: 0.002387229239917975
Loss at iteration [441]: 0.002387047903956254
Loss at iteration [442]: 0.0023869572039786338
Loss at iteration [443]: 0.0023868874598302454
Loss at iteration [444]: 0.0023868179825405063
Loss at iteration [445]: 0.00238667604026706
Loss at iteration [446]: 0.002386661291945253
Loss at iteration [447]: 0.002386305776015471
Loss at iteration [448]: 0.0023862379092564902
Loss at iteration [449]: 0.0023860247484691092
Loss at iteration [450]: 0.002385440892461071
Loss at iteration [451]: 0.002385314078799954
Loss at iteration [452]: 0.0023852196414465656
Loss at iteration [453]: 0.0023852196414465656
Loss at iteration [454]: 0.0023851346043978084
Loss at iteration [455]: 0.0023850665529345345
Loss at iteration [456]: 0.0023850367021285863
Loss at iteration [457]: 0.0023847916854728654
Loss at iteration [458]: 0.002384754877898399
Loss at iteration [459]: 0.002384593585990092
Loss at iteration [460]: 0.0023844211010636746
Loss at iteration [461]: 0.002384401225516222
Loss at iteration [462]: 0.0023843440447632873
Loss at iteration [463]: 0.0023841992793046454
Loss at iteration [464]: 0.002384151870789003
Loss at iteration [465]: 0.0023840976490784754
Loss at iteration [466]: 0.002384011918955448
Loss at iteration [467]: 0.0023839299458698173
Loss at iteration [468]: 0.002383887176944806
Loss at iteration [469]: 0.002383887176944806
Loss at iteration [470]: 0.0023838655784480254
Loss at iteration [471]: 0.0023838194789775145
Loss at iteration [472]: 0.002383761702221705
Loss at iteration [473]: 0.0023837389257225245
Loss at iteration [474]: 0.002383715496292284
Loss at iteration [475]: 0.0023836867089090156
Loss at iteration [476]: 0.0023836609221719795
Loss at iteration [477]: 0.0023835721179728125
Loss at iteration [478]: 0.0023834496156477083
Loss at iteration [479]: 0.002383226722171876
Loss at iteration [480]: 0.002382755610813688
Loss at iteration [481]: 0.0023818533293608777
Loss at iteration [482]: 0.0023817079774969513
Loss at iteration [483]: 0.0023815618320528367
Loss at iteration [484]: 0.0023813536487907717
Loss at iteration [485]: 0.0023813536487907717
Loss at iteration [486]: 0.00238125096296781
Loss at iteration [487]: 0.002381221414036249
Loss at iteration [488]: 0.002381017548414889
Loss at iteration [489]: 0.0023808375363617773
Loss at iteration [490]: 0.002380753143348256
Loss at iteration [491]: 0.0023806341032319013
Loss at iteration [492]: 0.0023805619050035315
Loss at iteration [493]: 0.0023805050454491993
Loss at iteration [494]: 0.0023803641119789186
Loss at iteration [495]: 0.0023802081647587643
Loss at iteration [496]: 0.0023798821367283706
Loss at iteration [497]: 0.002379814170854668
Loss at iteration [498]: 0.0023797225291415846
Loss at iteration [499]: 0.002379542056820629
Loss at iteration [500]: 0.002379400573909118
Loss at iteration [501]: 0.002378993978696883
Loss at iteration [502]: 0.002378993978696883
Loss at iteration [503]: 0.002378916970980258
Loss at iteration [504]: 0.0023787989580030286
Loss at iteration [505]: 0.002378671628363281
Loss at iteration [506]: 0.0023786523975808456
Loss at iteration [507]: 0.0023785246902609278
Loss at iteration [508]: 0.002378417799372546
Loss at iteration [509]: 0.0023783665667144018
Loss at iteration [510]: 0.002378332795802291
Loss at iteration [511]: 0.0023782294811011367
Loss at iteration [512]: 0.002378178237797323
Loss at iteration [513]: 0.002378147296464723
Loss at iteration [514]: 0.0023777703597390905
Loss at iteration [515]: 0.0023776380559642147
Loss at iteration [516]: 0.0023776380559642147
Loss at iteration [517]: 0.0023776053499877173
Loss at iteration [518]: 0.002377500180216867
Loss at iteration [519]: 0.0023774423672988823
Loss at iteration [520]: 0.0023774262960590724
Loss at iteration [521]: 0.0023774022274402674
Loss at iteration [522]: 0.002377248825696542
Loss at iteration [523]: 0.0023770915225683562
Loss at iteration [524]: 0.002376937955426017
Loss at iteration [525]: 0.00237684582346369
Loss at iteration [526]: 0.0023764744237770386
Loss at iteration [527]: 0.0023761888017087905
Loss at iteration [528]: 0.002375549694715558
Loss at iteration [529]: 0.002375162018725258
Loss at iteration [530]: 0.002374684068039458
Loss at iteration [531]: 0.0023730770378681634
Loss at iteration [532]: 0.0023730770378681634
Loss at iteration [533]: 0.0023726126498873016
Loss at iteration [534]: 0.0023722422733201557
Loss at iteration [535]: 0.0023721181455841436
Loss at iteration [536]: 0.0023719954983446633
Loss at iteration [537]: 0.002371828218948045
Loss at iteration [538]: 0.002371663528735205
Loss at iteration [539]: 0.0023715157782744105
Loss at iteration [540]: 0.0023713916160030156
Loss at iteration [541]: 0.002371236566025416
Loss at iteration [542]: 0.002371120077350485
Loss at iteration [543]: 0.002370872300209574
Loss at iteration [544]: 0.0023707273095225387
Loss at iteration [545]: 0.002370590540649033
Loss at iteration [546]: 0.0023704283305058626
Loss at iteration [547]: 0.0023701917393861346
Loss at iteration [548]: 0.0023701019495288803
Loss at iteration [549]: 0.0023701019495288803
Loss at iteration [550]: 0.002370024790014883
Loss at iteration [551]: 0.002369844165044561
Loss at iteration [552]: 0.00236969535429681
Loss at iteration [553]: 0.002369673176657841
Loss at iteration [554]: 0.0023696475184798085
Loss at iteration [555]: 0.0023694938400873815
Loss at iteration [556]: 0.0023694828116181516
Loss at iteration [557]: 0.0023694445309488
Loss at iteration [558]: 0.0023694307066020152
Loss at iteration [559]: 0.002369305217736232
Loss at iteration [560]: 0.0023692636307054552
Loss at iteration [561]: 0.002369172771263342
Loss at iteration [562]: 0.0023690247128168944
Loss at iteration [563]: 0.002368866631177405
Loss at iteration [564]: 0.002368718656900528
Loss at iteration [565]: 0.002368718656900528
Loss at iteration [566]: 0.0023686386167968976
Loss at iteration [567]: 0.00236861751720402
Loss at iteration [568]: 0.0023685989441447415
Loss at iteration [569]: 0.0023685247771981573
Loss at iteration [570]: 0.00236845079378419
Loss at iteration [571]: 0.0023684009802835233
Loss at iteration [572]: 0.0023683286153322095
Loss at iteration [573]: 0.0023681756174125114
Loss at iteration [574]: 0.002368126820243855
Loss at iteration [575]: 0.0023680914383979314
Loss at iteration [576]: 0.0023680386934757736
Loss at iteration [577]: 0.0023678522411076214
Loss at iteration [578]: 0.0023677471867571175
Loss at iteration [579]: 0.0023676641443774104
Loss at iteration [580]: 0.0023674536747763843
Loss at iteration [581]: 0.0023674536747763843
Loss at iteration [582]: 0.0023673631284333863
Loss at iteration [583]: 0.0023673315217095994
Loss at iteration [584]: 0.002367249882611453
Loss at iteration [585]: 0.0023671970161375628
Loss at iteration [586]: 0.00236705999542665
Loss at iteration [587]: 0.0023670115711184893
Loss at iteration [588]: 0.0023669569765977197
Loss at iteration [589]: 0.0023668516804128087
Loss at iteration [590]: 0.0023666984189116163
Loss at iteration [591]: 0.0023666603819981966
Loss at iteration [592]: 0.0023665560229906048
Loss at iteration [593]: 0.0023665137925886616
Loss at iteration [594]: 0.002366477585825337
Loss at iteration [595]: 0.002366410822355122
Loss at iteration [596]: 0.002366410822355122
Loss at iteration [597]: 0.0023664011059329865
Loss at iteration [598]: 0.0023663651215854666
Loss at iteration [599]: 0.002366306776913227
Loss at iteration [600]: 0.0023662951455896497
Loss at iteration [601]: 0.0023662745321137613
Loss at iteration [602]: 0.00236622081163531
Loss at iteration [603]: 0.0023661741238846937
Loss at iteration [604]: 0.0023661063199274563
Loss at iteration [605]: 0.0023660344001308216
Loss at iteration [606]: 0.0023660124616578597
Loss at iteration [607]: 0.0023659347685272103
Loss at iteration [608]: 0.0023657870937571442
Loss at iteration [609]: 0.002365680372596085
Loss at iteration [610]: 0.002365506795049697
Loss at iteration [611]: 0.002365292135723563
Loss at iteration [612]: 0.0023651244388270386
Loss at iteration [613]: 0.0023651244388270386
Loss at iteration [614]: 0.002365094576633822
Loss at iteration [615]: 0.0023650513700989363
Loss at iteration [616]: 0.002364863359566223
Loss at iteration [617]: 0.0023648432921265267
Loss at iteration [618]: 0.0023648213796448724
Loss at iteration [619]: 0.0023647608210762136
Loss at iteration [620]: 0.0023647459469337616
Loss at iteration [621]: 0.0023647434271410754
Loss at iteration [622]: 0.002364719081107369
Loss at iteration [623]: 0.0023646630558328556
Loss at iteration [624]: 0.002364648386344424
Loss at iteration [625]: 0.002364623484961626
Loss at iteration [626]: 0.002364613992729708
Loss at iteration [627]: 0.002364571603046631
Loss at iteration [628]: 0.002364571603046631
Loss at iteration [629]: 0.002364535865323914
Loss at iteration [630]: 0.0023645013710161485
Loss at iteration [631]: 0.0023644878911632393
Loss at iteration [632]: 0.002364471910208156
Loss at iteration [633]: 0.0023644497627300096
Loss at iteration [634]: 0.002364421601664181
Loss at iteration [635]: 0.0023643669277880735
Loss at iteration [636]: 0.0023643094513406014
Loss at iteration [637]: 0.0023642727781827233
Loss at iteration [638]: 0.00236418346982975
Loss at iteration [639]: 0.0023640730925987017
Loss at iteration [640]: 0.0023639484048110344
Loss at iteration [641]: 0.002363669016897554
Loss at iteration [642]: 0.0023633877698640902
Loss at iteration [643]: 0.002363242731560371
Loss at iteration [644]: 0.0023631138399012814
Loss at iteration [645]: 0.0023620936680022156
Loss at iteration [646]: 0.0023620936680022156
Loss at iteration [647]: 0.002361720479296283
Loss at iteration [648]: 0.0023616828902331306
Loss at iteration [649]: 0.0023616625797087468
Loss at iteration [650]: 0.0023616271721740786
Loss at iteration [651]: 0.0023614543351510443
Loss at iteration [652]: 0.0023614047917406983
Loss at iteration [653]: 0.0023613744676657497
Loss at iteration [654]: 0.0023613161297717936
Loss at iteration [655]: 0.002361059967773738
Loss at iteration [656]: 0.0023609990610274833
Loss at iteration [657]: 0.0023608646423324987
Loss at iteration [658]: 0.0023606416668553055
Loss at iteration [659]: 0.0023603109216464736
Loss at iteration [660]: 0.0023601697699780007
Loss at iteration [661]: 0.0023601697699780007
Loss at iteration [662]: 0.0023600778756992804
Loss at iteration [663]: 0.0023599583198984763
Loss at iteration [664]: 0.002359858404224756
Loss at iteration [665]: 0.0023598340077264227
Loss at iteration [666]: 0.0023595758990568524
Loss at iteration [667]: 0.002359523692443946
Loss at iteration [668]: 0.002359475790213607
Loss at iteration [669]: 0.0023594207829884493
Loss at iteration [670]: 0.0023593654569807206
Loss at iteration [671]: 0.002359333054540836
Loss at iteration [672]: 0.002359291265455326
Loss at iteration [673]: 0.002359201691906529
Loss at iteration [674]: 0.0023590454820168645
Loss at iteration [675]: 0.002358997836250805
Loss at iteration [676]: 0.0023589276998239503
Loss at iteration [677]: 0.002358791215066939
Loss at iteration [678]: 0.002358791215066939
Loss at iteration [679]: 0.0023587599149922354
Loss at iteration [680]: 0.0023586089029445797
Loss at iteration [681]: 0.0023585887392487286
Loss at iteration [682]: 0.002358565489059763
Loss at iteration [683]: 0.002358356727357907
Loss at iteration [684]: 0.0023583382549670047
Loss at iteration [685]: 0.0023582715175033673
Loss at iteration [686]: 0.0023582077861108706
Loss at iteration [687]: 0.00235813233754042
Loss at iteration [688]: 0.0023580847684859145
Loss at iteration [689]: 0.002358039797236984
Loss at iteration [690]: 0.0023579560552820828
Loss at iteration [691]: 0.0023578871257999615
Loss at iteration [692]: 0.0023577458380066997
Loss at iteration [693]: 0.0023577458380066997
Loss at iteration [694]: 0.0023577232347974306
Loss at iteration [695]: 0.002357708445882886
Loss at iteration [696]: 0.0023576305173845452
Loss at iteration [697]: 0.0023576101019308665
Loss at iteration [698]: 0.002357596266167574
Loss at iteration [699]: 0.0023575440802757973
Loss at iteration [700]: 0.002357451297909821
Loss at iteration [701]: 0.0023573873199319723
Loss at iteration [702]: 0.002357279201164903
Loss at iteration [703]: 0.002357072268227179
Loss at iteration [704]: 0.002356910088615393
Loss at iteration [705]: 0.0023568248092382346
Loss at iteration [706]: 0.0023567874055545703
Loss at iteration [707]: 0.0023566628722751157
Loss at iteration [708]: 0.0023563613861160722
Loss at iteration [709]: 0.00235626492715385
Loss at iteration [710]: 0.00235626492715385
Loss at iteration [711]: 0.0023562217358887114
Loss at iteration [712]: 0.002356199827952564
Loss at iteration [713]: 0.002356151237198941
Loss at iteration [714]: 0.002356038243734966
Loss at iteration [715]: 0.0023559383900431355
Loss at iteration [716]: 0.0023558703486582103
Loss at iteration [717]: 0.0023558177283723516
Loss at iteration [718]: 0.002355796962896989
Loss at iteration [719]: 0.0023557513195998176
Loss at iteration [720]: 0.0023556556358218014
Loss at iteration [721]: 0.0023555521604908163
Loss at iteration [722]: 0.0023554763774684493
Loss at iteration [723]: 0.002355440615088449
Loss at iteration [724]: 0.0023553793824612167
Loss at iteration [725]: 0.0023553065313617577
Loss at iteration [726]: 0.0023551986370249334
Loss at iteration [727]: 0.0023551986370249334
Loss at iteration [728]: 0.0023551314898006908
Loss at iteration [729]: 0.00235510773733942
Loss at iteration [730]: 0.0023550633129471254
Loss at iteration [731]: 0.00235504937580278
Loss at iteration [732]: 0.002354983582684591
Loss at iteration [733]: 0.002354977558857982
Loss at iteration [734]: 0.0023549557594680784
Loss at iteration [735]: 0.0023549083818232143
Loss at iteration [736]: 0.0023548249301321546
Loss at iteration [737]: 0.002354800095349729
Loss at iteration [738]: 0.002354785671371018
Loss at iteration [739]: 0.0023547551414966287
Loss at iteration [740]: 0.0023546691139876797
Loss at iteration [741]: 0.0023546418307261083
Loss at iteration [742]: 0.0023546214193649728
Loss at iteration [743]: 0.0023546214193649728
Loss at iteration [744]: 0.00235460610648561
Loss at iteration [745]: 0.002354589556010398
Loss at iteration [746]: 0.0023545529971421147
Loss at iteration [747]: 0.0023545358530956306
Loss at iteration [748]: 0.002354535289406177
Loss at iteration [749]: 0.002354535289406177
Loss at iteration [750]: 0.002354533132384212
Loss at iteration [751]: 0.002354524073472567
Loss at iteration [752]: 0.002354405351638436
Loss at iteration [753]: 0.0023543194476265557
Loss at iteration [754]: 0.002354261392603961
Loss at iteration [755]: 0.0023541045544854962
Loss at iteration [756]: 0.002352963377409397
Loss at iteration [757]: 0.0023517783755847973
Loss at iteration [758]: 0.0023516065088065967
Loss at iteration [759]: 0.002350686468162407
Loss at iteration [760]: 0.002348169861101386
Loss at iteration [761]: 0.002347733661519094
Loss at iteration [762]: 0.002346728482679835
Loss at iteration [763]: 0.002345083856267125
Loss at iteration [764]: 0.002345083856267125
Loss at iteration [765]: 0.0023450258169124033
Loss at iteration [766]: 0.002344222217816247
Loss at iteration [767]: 0.0023439842579440636
Loss at iteration [768]: 0.002343675909273232
Loss at iteration [769]: 0.002343413434230497
Loss at iteration [770]: 0.0023431966310788577
Loss at iteration [771]: 0.0023431016268000487
Loss at iteration [772]: 0.0023427060354271802
Loss at iteration [773]: 0.002342079125314458
Loss at iteration [774]: 0.002341927229760646
Loss at iteration [775]: 0.0023417603403186357
Loss at iteration [776]: 0.0023415666728502493
Loss at iteration [777]: 0.002341540269534003
Loss at iteration [778]: 0.0023414338789448734
Loss at iteration [779]: 0.0023414338789448734
Loss at iteration [780]: 0.002341348594735852
Loss at iteration [781]: 0.0023413189116810317
Loss at iteration [782]: 0.002341172254640932
Loss at iteration [783]: 0.002341130741793509
Loss at iteration [784]: 0.002341119839581191
Loss at iteration [785]: 0.00234102330543562
Loss at iteration [786]: 0.0023408404397273212
Loss at iteration [787]: 0.0023407751966209177
Loss at iteration [788]: 0.0023407110468387166
Loss at iteration [789]: 0.0023406078097376877
Loss at iteration [790]: 0.0023405562102924785
Loss at iteration [791]: 0.002340355838113001
Loss at iteration [792]: 0.0023403161450498587
Loss at iteration [793]: 0.002340124553129843
Loss at iteration [794]: 0.0023397508347902125
Loss at iteration [795]: 0.002339685656916187
Loss at iteration [796]: 0.002339685656916187
Loss at iteration [797]: 0.002339638534934372
Loss at iteration [798]: 0.0023394879684881602
Loss at iteration [799]: 0.0023394024643518013
Loss at iteration [800]: 0.0023393175337851745
Loss at iteration [801]: 0.0023392655706229062
Loss at iteration [802]: 0.002339188147540139
Loss at iteration [803]: 0.002339165539198563
Loss at iteration [804]: 0.002339127455027506
Loss at iteration [805]: 0.002339064642893866
Loss at iteration [806]: 0.0023390449425860953
Loss at iteration [807]: 0.002338989965457028
Loss at iteration [808]: 0.00233895679759672
Loss at iteration [809]: 0.0023389392438786165
Loss at iteration [810]: 0.002338602755841731
Loss at iteration [811]: 0.002338602755841731
Loss at iteration [812]: 0.0023384608633684953
Loss at iteration [813]: 0.0023384269922798713
Loss at iteration [814]: 0.0023384049185953223
Loss at iteration [815]: 0.002338381635640906
Loss at iteration [816]: 0.0023383588565125827
Loss at iteration [817]: 0.0023383135270906574
Loss at iteration [818]: 0.002338273735837189
Loss at iteration [819]: 0.0023382440951258215
Loss at iteration [820]: 0.0023381941761449563
Loss at iteration [821]: 0.0023380877624269484
Loss at iteration [822]: 0.002337930790219373
Loss at iteration [823]: 0.002337726204873752
Loss at iteration [824]: 0.0023376012826058306
Loss at iteration [825]: 0.0023374505863523807
Loss at iteration [826]: 0.0023371695878989756
Loss at iteration [827]: 0.002337030107900774
Loss at iteration [828]: 0.002337030107900774
Loss at iteration [829]: 0.0023369758759776336
Loss at iteration [830]: 0.0023366062042190884
Loss at iteration [831]: 0.002336539202699698
Loss at iteration [832]: 0.002336444845148333
Loss at iteration [833]: 0.00233629023590078
Loss at iteration [834]: 0.00233624828680444
Loss at iteration [835]: 0.002336217556317491
Loss at iteration [836]: 0.0023361328395019774
Loss at iteration [837]: 0.0023361014918499693
Loss at iteration [838]: 0.0023360496681877774
Loss at iteration [839]: 0.0023358794791149655
Loss at iteration [840]: 0.0023357234831104514
Loss at iteration [841]: 0.0023356604430350243
Loss at iteration [842]: 0.0023356604430350243
Loss at iteration [843]: 0.0023356262456039572
Loss at iteration [844]: 0.0023354844843170184
Loss at iteration [845]: 0.002335418884018966
Loss at iteration [846]: 0.002335332108431037
Loss at iteration [847]: 0.0023352754642604516
Loss at iteration [848]: 0.002335171382055485
Loss at iteration [849]: 0.002335137427270744
Loss at iteration [850]: 0.002335087496267653
Loss at iteration [851]: 0.0023349406862043605
Loss at iteration [852]: 0.0023348940731706255
Loss at iteration [853]: 0.002334799162862121
Loss at iteration [854]: 0.0023341437537950332
Loss at iteration [855]: 0.00233402599712929
Loss at iteration [856]: 0.00233402599712929
Loss at iteration [857]: 0.0023339572237186597
Loss at iteration [858]: 0.0023337370952517147
Loss at iteration [859]: 0.0023336129627831302
Loss at iteration [860]: 0.0023335578998088117
Loss at iteration [861]: 0.0023334796508182104
Loss at iteration [862]: 0.0023333943626694855
Loss at iteration [863]: 0.0023333668180858112
Loss at iteration [864]: 0.0023332854471078365
Loss at iteration [865]: 0.002333212076659913
Loss at iteration [866]: 0.002333190503794575
Loss at iteration [867]: 0.0023329909173912832
Loss at iteration [868]: 0.002332967989436208
Loss at iteration [869]: 0.0023328779497058857
Loss at iteration [870]: 0.0023328779497058857
Loss at iteration [871]: 0.002332785762245562
Loss at iteration [872]: 0.0023327740890133892
Loss at iteration [873]: 0.002332675575922059
Loss at iteration [874]: 0.0023326594545800224
Loss at iteration [875]: 0.002332597944699286
Loss at iteration [876]: 0.002332488052679353
Loss at iteration [877]: 0.0023324635866675517
Loss at iteration [878]: 0.002332446966124226
Loss at iteration [879]: 0.0023323803049210847
Loss at iteration [880]: 0.0023323581693608375
Loss at iteration [881]: 0.0023323450792767657
Loss at iteration [882]: 0.0023322996451178336
Loss at iteration [883]: 0.0023322504782224336
Loss at iteration [884]: 0.0023322144711691462
Loss at iteration [885]: 0.002332127441726738
Loss at iteration [886]: 0.002332127441726738
Loss at iteration [887]: 0.0023321128450637914
Loss at iteration [888]: 0.002332067574370679
Loss at iteration [889]: 0.002332038152821533
Loss at iteration [890]: 0.0023320229704565437
Loss at iteration [891]: 0.0023319797929006337
Loss at iteration [892]: 0.0023319573316002828
Loss at iteration [893]: 0.002331823344933009
Loss at iteration [894]: 0.0023317840364259004
Loss at iteration [895]: 0.0023317331823526034
Loss at iteration [896]: 0.002331628492722095
Loss at iteration [897]: 0.002331531588333032
Loss at iteration [898]: 0.0023309940582848035
Loss at iteration [899]: 0.0023308779166670997
Loss at iteration [900]: 0.0023305217019095647
Loss at iteration [901]: 0.0023305217019095647
Loss at iteration [902]: 0.0023302547391837757
Loss at iteration [903]: 0.0023302046776179754
Loss at iteration [904]: 0.0023300354233384063
Loss at iteration [905]: 0.0023299721982635016
Loss at iteration [906]: 0.0023299186359244414
Loss at iteration [907]: 0.002329833449644452
Loss at iteration [908]: 0.0023297413652982124
Loss at iteration [909]: 0.0023296844469911696
Loss at iteration [910]: 0.002329598915478109
Loss at iteration [911]: 0.0023294980810345802
Loss at iteration [912]: 0.002329455979879284
Loss at iteration [913]: 0.0023293287751328657
Loss at iteration [914]: 0.0023292906392025038
Loss at iteration [915]: 0.0023292552989251495
Loss at iteration [916]: 0.0023292061894291563
Loss at iteration [917]: 0.002329166092279385
Loss at iteration [918]: 0.0023291235605745328
Loss at iteration [919]: 0.0023289675801197196
Loss at iteration [920]: 0.0023289675801197196
Loss at iteration [921]: 0.0023289300715371862
Loss at iteration [922]: 0.002328918859741815
Loss at iteration [923]: 0.0023288710504215638
Loss at iteration [924]: 0.00232882061450838
Loss at iteration [925]: 0.0023288111941720765
Loss at iteration [926]: 0.0023287309495523324
Loss at iteration [927]: 0.002328693004993778
Loss at iteration [928]: 0.0023285028131487494
Loss at iteration [929]: 0.0023283866671646564
Loss at iteration [930]: 0.0023283113536456553
Loss at iteration [931]: 0.0023281475554471614
Loss at iteration [932]: 0.002327858388311614
Loss at iteration [933]: 0.002327534192836411
Loss at iteration [934]: 0.0023264946774758945
Loss at iteration [935]: 0.0023260619796201367
Loss at iteration [936]: 0.0023260619796201367
Loss at iteration [937]: 0.0023259653650427926
Loss at iteration [938]: 0.0023254567257469955
Loss at iteration [939]: 0.0023252648492894574
Loss at iteration [940]: 0.0023250832099384745
Loss at iteration [941]: 0.002325041293433819
Loss at iteration [942]: 0.0023248540774936936
Loss at iteration [943]: 0.0023247624635720646
Loss at iteration [944]: 0.002324707322311145
Loss at iteration [945]: 0.002324569610049375
Loss at iteration [946]: 0.002324428239005947
Loss at iteration [947]: 0.002324375150125628
Loss at iteration [948]: 0.0023242672188369213
Loss at iteration [949]: 0.0023242672188369213
Loss at iteration [950]: 0.002324246331520725
Loss at iteration [951]: 0.0023242256579837912
Loss at iteration [952]: 0.002324152856214172
Loss at iteration [953]: 0.002324110843652373
Loss at iteration [954]: 0.002324067368984926
Loss at iteration [955]: 0.0023240336829048053
Loss at iteration [956]: 0.0023239625740238334
Loss at iteration [957]: 0.0023238950136469797
Loss at iteration [958]: 0.0023237873810094643
Loss at iteration [959]: 0.0023236643022687287
Loss at iteration [960]: 0.0023235652166170967
Loss at iteration [961]: 0.002323360244470549
Loss at iteration [962]: 0.002322958286124624
Loss at iteration [963]: 0.0023228796329363027
Loss at iteration [964]: 0.0023227202616567517
Loss at iteration [965]: 0.002322694141707337
Loss at iteration [966]: 0.002322668778855486
Loss at iteration [967]: 0.0023225102146972364
Loss at iteration [968]: 0.0023225102146972364
Loss at iteration [969]: 0.002322489512311953
Loss at iteration [970]: 0.002322464248463489
Loss at iteration [971]: 0.0023223086251260884
Loss at iteration [972]: 0.00232226723059466
Loss at iteration [973]: 0.002322236775573126
Loss at iteration [974]: 0.0023221311422529783
Loss at iteration [975]: 0.002322010732976402
Loss at iteration [976]: 0.0023219588807392184
Loss at iteration [977]: 0.002321941165013993
Loss at iteration [978]: 0.002321884244276343
Loss at iteration [979]: 0.0023218557746309976
Loss at iteration [980]: 0.0023216551506276347
Loss at iteration [981]: 0.0023215609547120306
Loss at iteration [982]: 0.0023215277645624102
Loss at iteration [983]: 0.002321436890480716
Loss at iteration [984]: 0.002321377467798565
Loss at iteration [985]: 0.002321377467798565
Loss at iteration [986]: 0.0023213521840521533
Loss at iteration [987]: 0.002321308756959717
Loss at iteration [988]: 0.0023212763530361395
Loss at iteration [989]: 0.0023212121351848594
Loss at iteration [990]: 0.002321190644198783
Loss at iteration [991]: 0.002321153691108902
Loss at iteration [992]: 0.002321098918716949
Loss at iteration [993]: 0.0023210631764505932
Loss at iteration [994]: 0.0023209446487022054
Loss at iteration [995]: 0.0023209002475176976
Loss at iteration [996]: 0.0023208682123872014
Loss at iteration [997]: 0.0023206860254370875
Loss at iteration [998]: 0.0023205796416757643
Loss at iteration [999]: 0.0023205006440176297
Loss at iteration [1000]: 0.0023203163584438985
Loss at iteration [1001]: 0.0023202210165564124
Loss at iteration [1002]: 0.0023183456967885018
Loss at iteration [1003]: 0.0023183456967885018
Loss at iteration [1004]: 0.002317125783502696
Loss at iteration [1005]: 0.0023170151019989804
Loss at iteration [1006]: 0.002316641057982456
Loss at iteration [1007]: 0.0023166056507341443
Loss at iteration [1008]: 0.0023165157541405568
Loss at iteration [1009]: 0.002316444489309615
Loss at iteration [1010]: 0.0023162354773380235
Loss at iteration [1011]: 0.0023160778709146205
Loss at iteration [1012]: 0.0023159062127251807
Loss at iteration [1013]: 0.0023158021221086574
Loss at iteration [1014]: 0.002315750163446488
Loss at iteration [1015]: 0.0023156597458306467
Loss at iteration [1016]: 0.0023156124704471702
Loss at iteration [1017]: 0.0023155015389080234
Loss at iteration [1018]: 0.002315443853478884
Loss at iteration [1019]: 0.0023153842226071248
Loss at iteration [1020]: 0.0023153842226071248
Loss at iteration [1021]: 0.0023153598077511714
Loss at iteration [1022]: 0.002315331562849787
Loss at iteration [1023]: 0.0023152523442224486
Loss at iteration [1024]: 0.002315229405872566
Loss at iteration [1025]: 0.0023152161610989572
Loss at iteration [1026]: 0.002315174468522295
Loss at iteration [1027]: 0.0023149217758720816
Loss at iteration [1028]: 0.0023148880120127647
Loss at iteration [1029]: 0.0023146733699046652
Loss at iteration [1030]: 0.0023145001755337158
Loss at iteration [1031]: 0.0023143983390212725
Loss at iteration [1032]: 0.0023143105239805373
Loss at iteration [1033]: 0.0023143105239805373
Loss at iteration [1034]: 0.002314270672677628
Loss at iteration [1035]: 0.0023142501439280304
Loss at iteration [1036]: 0.002314107980607516
Loss at iteration [1037]: 0.002314073108501387
Loss at iteration [1038]: 0.0023139602257805443
Loss at iteration [1039]: 0.0023139365763765424
Loss at iteration [1040]: 0.0023139252398153083
Loss at iteration [1041]: 0.00231390627610156
Loss at iteration [1042]: 0.002313861781103555
Loss at iteration [1043]: 0.0023137978145890857
Loss at iteration [1044]: 0.002313778029410688
Loss at iteration [1045]: 0.0023137269509007348
Loss at iteration [1046]: 0.0023137269509007348
Loss at iteration [1047]: 0.002313705600311096
Loss at iteration [1048]: 0.002313702894486354
Loss at iteration [1049]: 0.0023136812088058105
Loss at iteration [1050]: 0.0023136647112192536
Loss at iteration [1051]: 0.002313606093774971
Loss at iteration [1052]: 0.002313586546488735
Loss at iteration [1053]: 0.0023135163457007196
Loss at iteration [1054]: 0.002313454592789822
Loss at iteration [1055]: 0.002313413848821987
Loss at iteration [1056]: 0.0023132959074352788
Loss at iteration [1057]: 0.002313153511247912
Loss at iteration [1058]: 0.0023128800431638148
Loss at iteration [1059]: 0.002312633296290074
Loss at iteration [1060]: 0.0023122420237018004
Loss at iteration [1061]: 0.0023118482073079213
Loss at iteration [1062]: 0.002311635774759422
Loss at iteration [1063]: 0.002311635774759422
Loss at iteration [1064]: 0.002311555197708779
Loss at iteration [1065]: 0.0023115004580770014
Loss at iteration [1066]: 0.0023113242374074414
Loss at iteration [1067]: 0.0023112803594127354
Loss at iteration [1068]: 0.0023112251055979536
Loss at iteration [1069]: 0.002311154269224966
Loss at iteration [1070]: 0.0023111297292797046
Loss at iteration [1071]: 0.0023110756396800274
Loss at iteration [1072]: 0.0023110169671779817
Loss at iteration [1073]: 0.002310965310827484
Loss at iteration [1074]: 0.0023109001899482054
Loss at iteration [1075]: 0.0023108182342710774
Loss at iteration [1076]: 0.002310760273967182
Loss at iteration [1077]: 0.002310665825377868
Loss at iteration [1078]: 0.0023105550080677055
Loss at iteration [1079]: 0.0023105382793309755
Loss at iteration [1080]: 0.0023104263722629444
Loss at iteration [1081]: 0.0023102967573017242
Loss at iteration [1082]: 0.0023102967573017242
Loss at iteration [1083]: 0.0023102659451264018
Loss at iteration [1084]: 0.0023102117208807804
Loss at iteration [1085]: 0.0023102033878183996
Loss at iteration [1086]: 0.0023101701027005636
Loss at iteration [1087]: 0.0023101626891798613
Loss at iteration [1088]: 0.002310135505964309
Loss at iteration [1089]: 0.002310112497470592
Loss at iteration [1090]: 0.0023100225468051768
Loss at iteration [1091]: 0.0023099391722167404
Loss at iteration [1092]: 0.002309861448366622
Loss at iteration [1093]: 0.0023097534339267542
Loss at iteration [1094]: 0.0023096705678481056
Loss at iteration [1095]: 0.0023095575738081827
Loss at iteration [1096]: 0.0023092360705602
Loss at iteration [1097]: 0.0023092360705602
Loss at iteration [1098]: 0.0023091033131067817
Loss at iteration [1099]: 0.0023090714277566402
Loss at iteration [1100]: 0.002308876209790285
Loss at iteration [1101]: 0.0023088626242170254
Loss at iteration [1102]: 0.0023088080391433044
Loss at iteration [1103]: 0.0023087379871470953
Loss at iteration [1104]: 0.002308705954292619
Loss at iteration [1105]: 0.0023085770727139334
Loss at iteration [1106]: 0.0023085405938742787
Loss at iteration [1107]: 0.002308228136345463
Loss at iteration [1108]: 0.002308177189198169
Loss at iteration [1109]: 0.0023080995347935993
Loss at iteration [1110]: 0.0023077945854849038
Loss at iteration [1111]: 0.0023077945854849038
Loss at iteration [1112]: 0.0023077162390223735
Loss at iteration [1113]: 0.002307589189255701
Loss at iteration [1114]: 0.0023075543451599673
Loss at iteration [1115]: 0.002307525197282083
Loss at iteration [1116]: 0.0023073953312340405
Loss at iteration [1117]: 0.0023073334251512445
Loss at iteration [1118]: 0.002307306485534122
Loss at iteration [1119]: 0.002307189949256413
Loss at iteration [1120]: 0.0023071674364651272
Loss at iteration [1121]: 0.0023071534326516603
Loss at iteration [1122]: 0.002307134442792202
Loss at iteration [1123]: 0.002307086987190391
Loss at iteration [1124]: 0.0023070378986435407
Loss at iteration [1125]: 0.0023070378986435407
Loss at iteration [1126]: 0.0023070243621022574
Loss at iteration [1127]: 0.0023070121903933237
Loss at iteration [1128]: 0.0023069822572997054
Loss at iteration [1129]: 0.002306951400881075
Loss at iteration [1130]: 0.0023068891420332575
Loss at iteration [1131]: 0.002306814297193785
Loss at iteration [1132]: 0.0023067524976599376
Loss at iteration [1133]: 0.002306528750056959
Loss at iteration [1134]: 0.0023062115185614423
Loss at iteration [1135]: 0.0023057097110112815
Loss at iteration [1136]: 0.002305393684120799
Loss at iteration [1137]: 0.0023047408035631396
Loss at iteration [1138]: 0.0023047408035631396
Loss at iteration [1139]: 0.0023046626168927308
Loss at iteration [1140]: 0.0023046129691239145
Loss at iteration [1141]: 0.002304459821482263
Loss at iteration [1142]: 0.0023043100755376317
Loss at iteration [1143]: 0.002304218374792367
Loss at iteration [1144]: 0.002304130632006599
Loss at iteration [1145]: 0.0023040413886207
Loss at iteration [1146]: 0.002303929007145694
Loss at iteration [1147]: 0.0023038844197401953
Loss at iteration [1148]: 0.002303837634279245
Loss at iteration [1149]: 0.0023037641496630136
Loss at iteration [1150]: 0.0023037456489593874
Loss at iteration [1151]: 0.002303627587169502
Loss at iteration [1152]: 0.0023034184847644853
Loss at iteration [1153]: 0.0023032506652289086
Loss at iteration [1154]: 0.0023032506652289086
Loss at iteration [1155]: 0.0023031997234697495
Loss at iteration [1156]: 0.0023031200481056064
Loss at iteration [1157]: 0.0023030650160703143
Loss at iteration [1158]: 0.0023030140535037066
Loss at iteration [1159]: 0.002302997022706284
Loss at iteration [1160]: 0.002302971312519525
Loss at iteration [1161]: 0.0023029339446208907
Loss at iteration [1162]: 0.002302925866814248
Loss at iteration [1163]: 0.002302897829227377
Loss at iteration [1164]: 0.00230287405454204
Loss at iteration [1165]: 0.0023028441863119607
Loss at iteration [1166]: 0.0023027958855749587
Loss at iteration [1167]: 0.002302646096979386
Loss at iteration [1168]: 0.0023025884846264197
Loss at iteration [1169]: 0.0023025226090985047
Loss at iteration [1170]: 0.002302213617644269
Loss at iteration [1171]: 0.002302213617644269
Loss at iteration [1172]: 0.0023021721614815364
Loss at iteration [1173]: 0.0023020148989348504
Loss at iteration [1174]: 0.0023019605248123377
Loss at iteration [1175]: 0.0023019495087992802
Loss at iteration [1176]: 0.0023019123409348556
Loss at iteration [1177]: 0.002301880809628632
Loss at iteration [1178]: 0.0023018527823026624
Loss at iteration [1179]: 0.002301748061662637
Loss at iteration [1180]: 0.0023017222235764342
Loss at iteration [1181]: 0.0023016757313997515
Loss at iteration [1182]: 0.002301592557762886
Loss at iteration [1183]: 0.002301526820782636
Loss at iteration [1184]: 0.002301457469592726
Loss at iteration [1185]: 0.0023014359516654805
Loss at iteration [1186]: 0.00230140561660686
Loss at iteration [1187]: 0.0023013920840740435
Loss at iteration [1188]: 0.0023013920840740435
Loss at iteration [1189]: 0.0023013548084445893
Loss at iteration [1190]: 0.0023013442947551546
Loss at iteration [1191]: 0.0023013231288995283
Loss at iteration [1192]: 0.002301259172812337
Loss at iteration [1193]: 0.0023012205968851937
Loss at iteration [1194]: 0.002301145175945725
Loss at iteration [1195]: 0.002301111819676618
Loss at iteration [1196]: 0.002300995427888905
Loss at iteration [1197]: 0.002300825036077039
Loss at iteration [1198]: 0.002300679761969525
Loss at iteration [1199]: 0.0023004318465474727
Loss at iteration [1200]: 0.002300306462085652
Loss at iteration [1201]: 0.002300117999534795
Loss at iteration [1202]: 0.0022992449145847194
Loss at iteration [1203]: 0.002297654845695418
Loss at iteration [1204]: 0.002297654845695418
Loss at iteration [1205]: 0.002296705328451992
Loss at iteration [1206]: 0.00229652546423863
Loss at iteration [1207]: 0.0022960216152371035
Loss at iteration [1208]: 0.002295887035713762
Loss at iteration [1209]: 0.002295738347207204
Loss at iteration [1210]: 0.0022956300387482623
Loss at iteration [1211]: 0.00229551018443039
Loss at iteration [1212]: 0.0022954074182771904
Loss at iteration [1213]: 0.0022952390804197157
Loss at iteration [1214]: 0.0022951827651582334
Loss at iteration [1215]: 0.002295139999362773
Loss at iteration [1216]: 0.002294968053180142
Loss at iteration [1217]: 0.002294916761542482
Loss at iteration [1218]: 0.002294916761542482
Loss at iteration [1219]: 0.0022948937487980645
Loss at iteration [1220]: 0.0022948037223837954
Loss at iteration [1221]: 0.002294749474951216
Loss at iteration [1222]: 0.0022947194998535346
Loss at iteration [1223]: 0.0022946844151597997
Loss at iteration [1224]: 0.002294655368647441
Loss at iteration [1225]: 0.002294593959703995
Loss at iteration [1226]: 0.002294549125545286
Loss at iteration [1227]: 0.002294508439226817
Loss at iteration [1228]: 0.0022944620810587775
Loss at iteration [1229]: 0.002294446932039842
Loss at iteration [1230]: 0.0022944193648019263
Loss at iteration [1231]: 0.0022943451881490594
Loss at iteration [1232]: 0.0022942558047011306
Loss at iteration [1233]: 0.002294237668111464
Loss at iteration [1234]: 0.0022941861480331406
Loss at iteration [1235]: 0.0022941651633444776
Loss at iteration [1236]: 0.002294148649802289
Loss at iteration [1237]: 0.002294148649802289
Loss at iteration [1238]: 0.0022941479598528078
