Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :FR_PR
Total number of function evaluations  : 3040
Total number of iterations            : 1012
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 13.256848812103271
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.405451431173944%
Percentage of parameters < 1e-7       : 49.405451431173944%
Percentage of parameters < 1e-6       : 49.405451431173944%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.0032476721819003486
Loss at iteration [2]: 0.0030680439598966477
Loss at iteration [3]: 0.0028688206312705274
Loss at iteration [4]: 0.002740265174490089
Loss at iteration [5]: 0.0027331106715100003
Loss at iteration [6]: 0.0027272964859305117
Loss at iteration [7]: 0.0027050165985243327
Loss at iteration [8]: 0.0026882248468106363
Loss at iteration [9]: 0.0026770076283276515
Loss at iteration [10]: 0.0026709279632160183
Loss at iteration [11]: 0.002650382053037718
Loss at iteration [12]: 0.0026363362685905656
Loss at iteration [13]: 0.0026363362685905656
Loss at iteration [14]: 0.002631159156113197
Loss at iteration [15]: 0.002628191931662003
Loss at iteration [16]: 0.0026146475223451393
Loss at iteration [17]: 0.002611106688612532
Loss at iteration [18]: 0.0025995936328154228
Loss at iteration [19]: 0.002577585804092517
Loss at iteration [20]: 0.0025572208923863196
Loss at iteration [21]: 0.002553995264726066
Loss at iteration [22]: 0.0025529944409409134
Loss at iteration [23]: 0.0025499766780470085
Loss at iteration [24]: 0.0025499766780470085
Loss at iteration [25]: 0.002548393700737531
Loss at iteration [26]: 0.002547899803130632
Loss at iteration [27]: 0.0025389701200538004
Loss at iteration [28]: 0.0025387756203529553
Loss at iteration [29]: 0.002535635056843571
Loss at iteration [30]: 0.0025282066469571028
Loss at iteration [31]: 0.0025205458352816163
Loss at iteration [32]: 0.002518835247849823
Loss at iteration [33]: 0.002518835247849823
Loss at iteration [34]: 0.002518195930158384
Loss at iteration [35]: 0.0025180235013384272
Loss at iteration [36]: 0.0025137558775759025
Loss at iteration [37]: 0.00251352737428569
Loss at iteration [38]: 0.0025122614762769302
Loss at iteration [39]: 0.00250888090464666
Loss at iteration [40]: 0.0025066163685158152
Loss at iteration [41]: 0.002504915362245633
Loss at iteration [42]: 0.002504915362245633
Loss at iteration [43]: 0.0025042973589611815
Loss at iteration [44]: 0.0025037463713961255
Loss at iteration [45]: 0.0025024141096690188
Loss at iteration [46]: 0.0025016960590181405
Loss at iteration [47]: 0.0025010177987583983
Loss at iteration [48]: 0.0025006710396524605
Loss at iteration [49]: 0.0024999567965490205
Loss at iteration [50]: 0.0024991352057093024
Loss at iteration [51]: 0.0024979853430955345
Loss at iteration [52]: 0.0024979853430955345
Loss at iteration [53]: 0.0024978061602433724
Loss at iteration [54]: 0.002497607643565898
Loss at iteration [55]: 0.0024967052696500762
Loss at iteration [56]: 0.0024959208040289463
Loss at iteration [57]: 0.0024957611317831204
Loss at iteration [58]: 0.002495648103407065
Loss at iteration [59]: 0.002495154230320825
Loss at iteration [60]: 0.0024947797452419974
Loss at iteration [61]: 0.0024945806577818656
Loss at iteration [62]: 0.0024941658569867524
Loss at iteration [63]: 0.0024941658569867524
Loss at iteration [64]: 0.002494046162833276
Loss at iteration [65]: 0.0024939806528542626
Loss at iteration [66]: 0.0024930074407872064
Loss at iteration [67]: 0.002491679718416842
Loss at iteration [68]: 0.002490376638713965
Loss at iteration [69]: 0.0024895076425280284
Loss at iteration [70]: 0.002487190401418903
Loss at iteration [71]: 0.0024839699307898405
Loss at iteration [72]: 0.0024839699307898405
Loss at iteration [73]: 0.002483817387427592
Loss at iteration [74]: 0.002483218488586707
Loss at iteration [75]: 0.002482059912389449
Loss at iteration [76]: 0.0024818375287811918
Loss at iteration [77]: 0.0024814094330258838
Loss at iteration [78]: 0.0024803479901729726
Loss at iteration [79]: 0.002480253675357711
Loss at iteration [80]: 0.0024795319847867397
Loss at iteration [81]: 0.002479239239387797
Loss at iteration [82]: 0.002479239239387797
Loss at iteration [83]: 0.0024791824408105143
Loss at iteration [84]: 0.00247893307987283
Loss at iteration [85]: 0.0024784226277992623
Loss at iteration [86]: 0.0024782426171346176
Loss at iteration [87]: 0.0024780497610095826
Loss at iteration [88]: 0.0024768610500401364
Loss at iteration [89]: 0.0024763939437005235
Loss at iteration [90]: 0.0024759621055164675
Loss at iteration [91]: 0.002475702133103805
Loss at iteration [92]: 0.002475702133103805
Loss at iteration [93]: 0.0024755510415560376
Loss at iteration [94]: 0.0024745211487404717
Loss at iteration [95]: 0.0024742635864771836
Loss at iteration [96]: 0.0024739216013782493
Loss at iteration [97]: 0.002473521450166446
Loss at iteration [98]: 0.0024735038133695455
Loss at iteration [99]: 0.0024734354220419708
Loss at iteration [100]: 0.002473194224776662
Loss at iteration [101]: 0.002472568296795797
Loss at iteration [102]: 0.002472568296795797
Loss at iteration [103]: 0.002472488537217033
Loss at iteration [104]: 0.002471846453526258
Loss at iteration [105]: 0.0024717361503792702
Loss at iteration [106]: 0.002471629200892556
Loss at iteration [107]: 0.0024712417721849344
Loss at iteration [108]: 0.0024711854279773474
Loss at iteration [109]: 0.002471134626079574
Loss at iteration [110]: 0.0024709950820099255
Loss at iteration [111]: 0.0024709504779540942
Loss at iteration [112]: 0.0024709504779540942
Loss at iteration [113]: 0.0024709146740176094
Loss at iteration [114]: 0.002470517937668865
Loss at iteration [115]: 0.002470290672962314
Loss at iteration [116]: 0.002470183081868503
Loss at iteration [117]: 0.0024690715177511126
Loss at iteration [118]: 0.0024684975059543982
Loss at iteration [119]: 0.0024677459359919427
Loss at iteration [120]: 0.002467433445820139
Loss at iteration [121]: 0.00246727539817057
Loss at iteration [122]: 0.00246727539817057
Loss at iteration [123]: 0.0024671890513587305
Loss at iteration [124]: 0.002467085068816548
Loss at iteration [125]: 0.002466531229716633
Loss at iteration [126]: 0.002466263522860307
Loss at iteration [127]: 0.0024659666457912924
Loss at iteration [128]: 0.0024655742311664704
Loss at iteration [129]: 0.0024648222635522137
Loss at iteration [130]: 0.00246462385534739
Loss at iteration [131]: 0.002464339506259749
Loss at iteration [132]: 0.002464339506259749
Loss at iteration [133]: 0.002464205343656767
Loss at iteration [134]: 0.0024641485531994795
Loss at iteration [135]: 0.0024638916113592694
Loss at iteration [136]: 0.0024638179365834754
Loss at iteration [137]: 0.002463617853555086
Loss at iteration [138]: 0.0024634519005679906
Loss at iteration [139]: 0.0024630891036563386
Loss at iteration [140]: 0.002462994583970474
Loss at iteration [141]: 0.0024629238943772915
Loss at iteration [142]: 0.0024625947694605034
Loss at iteration [143]: 0.0024625947694605034
Loss at iteration [144]: 0.002462524260751148
Loss at iteration [145]: 0.0024624383813203203
Loss at iteration [146]: 0.002462016966448603
Loss at iteration [147]: 0.0024619343729830475
Loss at iteration [148]: 0.0024611643268437474
Loss at iteration [149]: 0.0024605527862168034
Loss at iteration [150]: 0.002459924431159663
Loss at iteration [151]: 0.002459139615780208
Loss at iteration [152]: 0.002459139615780208
Loss at iteration [153]: 0.0024588597342838883
Loss at iteration [154]: 0.0024588030292767996
Loss at iteration [155]: 0.002458571190577705
Loss at iteration [156]: 0.0024583143203053843
Loss at iteration [157]: 0.0024575202786183284
Loss at iteration [158]: 0.0024570864395920384
Loss at iteration [159]: 0.0024570023545831404
Loss at iteration [160]: 0.002456601542739566
Loss at iteration [161]: 0.002456601542739566
Loss at iteration [162]: 0.002456452374875091
Loss at iteration [163]: 0.00245638335159679
Loss at iteration [164]: 0.0024557694763346987
Loss at iteration [165]: 0.0024553660485324333
Loss at iteration [166]: 0.0024552840539017033
Loss at iteration [167]: 0.0024550579005673256
Loss at iteration [168]: 0.0024542909122709897
Loss at iteration [169]: 0.0024542909122709897
Loss at iteration [170]: 0.002454097673168178
Loss at iteration [171]: 0.002454013301407508
Loss at iteration [172]: 0.0024536158750268476
Loss at iteration [173]: 0.002453487061494011
Loss at iteration [174]: 0.0024532092290540297
Loss at iteration [175]: 0.002452937973964394
Loss at iteration [176]: 0.002452839169108357
Loss at iteration [177]: 0.002452438753107233
Loss at iteration [178]: 0.0024522413322096153
Loss at iteration [179]: 0.0024522413322096153
Loss at iteration [180]: 0.002452167016656423
Loss at iteration [181]: 0.0024520666956889003
Loss at iteration [182]: 0.0024518368896870716
Loss at iteration [183]: 0.0024516878668722495
Loss at iteration [184]: 0.0024516305665403535
Loss at iteration [185]: 0.00245150435589236
Loss at iteration [186]: 0.0024511730793348487
Loss at iteration [187]: 0.002450949535389092
Loss at iteration [188]: 0.002450949535389092
Loss at iteration [189]: 0.0024509028832125856
Loss at iteration [190]: 0.002450695288387211
Loss at iteration [191]: 0.002450446427137883
Loss at iteration [192]: 0.0024504181218756803
Loss at iteration [193]: 0.0024503399620068194
Loss at iteration [194]: 0.002450238701062105
Loss at iteration [195]: 0.0024500473125125028
Loss at iteration [196]: 0.002449989526134195
Loss at iteration [197]: 0.0024497951702755517
Loss at iteration [198]: 0.0024497951702755517
Loss at iteration [199]: 0.002449746717851766
Loss at iteration [200]: 0.002449710816984796
Loss at iteration [201]: 0.0024496451084680816
Loss at iteration [202]: 0.002449559263272128
Loss at iteration [203]: 0.002449438308168839
Loss at iteration [204]: 0.002449336725538965
Loss at iteration [205]: 0.002449262004318066
Loss at iteration [206]: 0.0024490742741785526
Loss at iteration [207]: 0.002448976114254842
Loss at iteration [208]: 0.002448734793768499
Loss at iteration [209]: 0.002448734793768499
Loss at iteration [210]: 0.002448654307321175
Loss at iteration [211]: 0.00244855603566388
Loss at iteration [212]: 0.002448498899121652
Loss at iteration [213]: 0.0024484405291577017
Loss at iteration [214]: 0.002448219123756998
Loss at iteration [215]: 0.0024480509308766577
Loss at iteration [216]: 0.0024477824835916085
Loss at iteration [217]: 0.002447169738398817
Loss at iteration [218]: 0.002447169738398817
Loss at iteration [219]: 0.0024468072322552568
Loss at iteration [220]: 0.0024467168845018626
Loss at iteration [221]: 0.002446453914502407
Loss at iteration [222]: 0.002446406339423518
Loss at iteration [223]: 0.002446210128353172
Loss at iteration [224]: 0.002446037169036016
Loss at iteration [225]: 0.0024459421611554783
Loss at iteration [226]: 0.002445721775964619
Loss at iteration [227]: 0.002445288591215975
Loss at iteration [228]: 0.0024451326260812407
Loss at iteration [229]: 0.0024451326260812407
Loss at iteration [230]: 0.0024450786229937493
Loss at iteration [231]: 0.0024448834626284487
Loss at iteration [232]: 0.002444791260208423
Loss at iteration [233]: 0.002444707285353318
Loss at iteration [234]: 0.002444530496728294
Loss at iteration [235]: 0.002444476590362075
Loss at iteration [236]: 0.0024443913948603057
Loss at iteration [237]: 0.0024442544231171137
Loss at iteration [238]: 0.0024441973918349776
Loss at iteration [239]: 0.0024441973918349776
Loss at iteration [240]: 0.0024441648915470436
Loss at iteration [241]: 0.0024440519255434696
Loss at iteration [242]: 0.0024439981195245124
Loss at iteration [243]: 0.0024439743321597505
Loss at iteration [244]: 0.002443942363396331
Loss at iteration [245]: 0.002443897669209423
Loss at iteration [246]: 0.0024438390015885337
Loss at iteration [247]: 0.0024437081607841036
Loss at iteration [248]: 0.0024436114848580904
Loss at iteration [249]: 0.0024436114848580904
Loss at iteration [250]: 0.0024434044610573856
Loss at iteration [251]: 0.0024430918937430007
Loss at iteration [252]: 0.0024430516236616877
Loss at iteration [253]: 0.0024428497137502837
Loss at iteration [254]: 0.0024428020907946353
Loss at iteration [255]: 0.002442733117649959
Loss at iteration [256]: 0.0024426949604442866
Loss at iteration [257]: 0.002442624508591148
Loss at iteration [258]: 0.002442420313539861
Loss at iteration [259]: 0.002442420313539861
Loss at iteration [260]: 0.0024423622145144713
Loss at iteration [261]: 0.0024422689695056932
Loss at iteration [262]: 0.0024421981667281482
Loss at iteration [263]: 0.0024421743716322632
Loss at iteration [264]: 0.0024420669120830967
Loss at iteration [265]: 0.0024419453335526643
Loss at iteration [266]: 0.0024418882500838665
Loss at iteration [267]: 0.002441758275566715
Loss at iteration [268]: 0.0024416301317659395
Loss at iteration [269]: 0.002441598275013718
Loss at iteration [270]: 0.002441598275013718
Loss at iteration [271]: 0.0024415717796983376
Loss at iteration [272]: 0.0024415455578571566
Loss at iteration [273]: 0.002441407502084973
Loss at iteration [274]: 0.002441194697487571
Loss at iteration [275]: 0.002441119531328194
Loss at iteration [276]: 0.0024402376389250175
Loss at iteration [277]: 0.0024390702990177955
Loss at iteration [278]: 0.002438031271425898
Loss at iteration [279]: 0.002438031271425898
Loss at iteration [280]: 0.002437844173567042
Loss at iteration [281]: 0.002436820973002332
Loss at iteration [282]: 0.002436515420891905
Loss at iteration [283]: 0.002436431019999603
Loss at iteration [284]: 0.002436143271436643
Loss at iteration [285]: 0.0024360374589322076
Loss at iteration [286]: 0.0024359813736958363
Loss at iteration [287]: 0.002435695897293835
Loss at iteration [288]: 0.0024354688080564114
Loss at iteration [289]: 0.0024354688080564114
Loss at iteration [290]: 0.0024353124752928956
Loss at iteration [291]: 0.002435191349724483
Loss at iteration [292]: 0.00243508994262385
Loss at iteration [293]: 0.0024350603870016527
Loss at iteration [294]: 0.0024350377827896293
Loss at iteration [295]: 0.0024349512154931007
Loss at iteration [296]: 0.002434846296559892
Loss at iteration [297]: 0.002434669315226822
Loss at iteration [298]: 0.0024345669054812217
Loss at iteration [299]: 0.0024345669054812217
Loss at iteration [300]: 0.0024344942726487284
Loss at iteration [301]: 0.0024344602714201865
Loss at iteration [302]: 0.0024343573090762996
Loss at iteration [303]: 0.0024343192804853852
Loss at iteration [304]: 0.002434270341245779
Loss at iteration [305]: 0.002434181045073427
Loss at iteration [306]: 0.0024341540574302958
Loss at iteration [307]: 0.002433962576696283
Loss at iteration [308]: 0.0024339002116111836
Loss at iteration [309]: 0.0024338638227452197
Loss at iteration [310]: 0.0024338638227452197
Loss at iteration [311]: 0.002433838284834313
Loss at iteration [312]: 0.002433807017992839
Loss at iteration [313]: 0.002433746248508978
Loss at iteration [314]: 0.0024336156655275242
Loss at iteration [315]: 0.002433576581535018
Loss at iteration [316]: 0.002433537295722755
Loss at iteration [317]: 0.002433520085237483
Loss at iteration [318]: 0.002433500351917773
Loss at iteration [319]: 0.002433455360866389
Loss at iteration [320]: 0.002433455360866389
Loss at iteration [321]: 0.002433428723150177
Loss at iteration [322]: 0.002433408167772793
Loss at iteration [323]: 0.0024333728073925856
Loss at iteration [324]: 0.0024331369640981206
Loss at iteration [325]: 0.0024330603098146916
Loss at iteration [326]: 0.0024329707521800496
Loss at iteration [327]: 0.002432493739834461
Loss at iteration [328]: 0.0024321797214835804
Loss at iteration [329]: 0.0024320746473493683
Loss at iteration [330]: 0.0024320746473493683
Loss at iteration [331]: 0.0024319840744375042
Loss at iteration [332]: 0.0024318519125455084
Loss at iteration [333]: 0.002431658177282535
Loss at iteration [334]: 0.002431608098560306
Loss at iteration [335]: 0.002431526526482139
Loss at iteration [336]: 0.0024313956973265054
Loss at iteration [337]: 0.0024313927008509334
Loss at iteration [338]: 0.0024313796208400356
Loss at iteration [339]: 0.002431280874350947
Loss at iteration [340]: 0.002431280874350947
Loss at iteration [341]: 0.002431246787482747
Loss at iteration [342]: 0.0024311995051149228
Loss at iteration [343]: 0.0024311450714491634
Loss at iteration [344]: 0.0024310906752339112
Loss at iteration [345]: 0.0024309526798849106
Loss at iteration [346]: 0.002430879002605797
Loss at iteration [347]: 0.0024308058822669182
Loss at iteration [348]: 0.0024305804113635126
Loss at iteration [349]: 0.0024305804113635126
Loss at iteration [350]: 0.0024305426167968006
Loss at iteration [351]: 0.002430493257246614
Loss at iteration [352]: 0.0024304280016056255
Loss at iteration [353]: 0.0024303979693167392
Loss at iteration [354]: 0.0024303087792690676
Loss at iteration [355]: 0.0024302306210773074
Loss at iteration [356]: 0.0024302143310270933
Loss at iteration [357]: 0.0024301931898652743
Loss at iteration [358]: 0.0024301931898652743
Loss at iteration [359]: 0.0024301583660937425
Loss at iteration [360]: 0.0024301005464663307
Loss at iteration [361]: 0.0024300407091391307
Loss at iteration [362]: 0.002429828000624275
Loss at iteration [363]: 0.002429649222939572
Loss at iteration [364]: 0.002429453208864248
Loss at iteration [365]: 0.0024292587662148074
Loss at iteration [366]: 0.0024289700995034002
Loss at iteration [367]: 0.0024289700995034002
Loss at iteration [368]: 0.002428888726509659
Loss at iteration [369]: 0.002428806684807709
Loss at iteration [370]: 0.0024287426537612233
Loss at iteration [371]: 0.0024287024998556252
Loss at iteration [372]: 0.002428619863841254
Loss at iteration [373]: 0.0024285106499563197
Loss at iteration [374]: 0.0024284783271121407
Loss at iteration [375]: 0.002428433282258151
Loss at iteration [376]: 0.0024284189050008493
Loss at iteration [377]: 0.0024283651644849535
Loss at iteration [378]: 0.0024283651644849535
Loss at iteration [379]: 0.00242831879221066
Loss at iteration [380]: 0.0024282625776257368
Loss at iteration [381]: 0.0024282272235615996
Loss at iteration [382]: 0.002428174173771356
Loss at iteration [383]: 0.0024281052193139752
Loss at iteration [384]: 0.002427942910959814
Loss at iteration [385]: 0.0024272853981198044
Loss at iteration [386]: 0.002426344793288605
Loss at iteration [387]: 0.002426344793288605
Loss at iteration [388]: 0.0024262248074789308
Loss at iteration [389]: 0.0024258375543756987
Loss at iteration [390]: 0.0024256125997388564
Loss at iteration [391]: 0.002425575391601317
Loss at iteration [392]: 0.0024254016219569393
Loss at iteration [393]: 0.002425289431248499
Loss at iteration [394]: 0.0024252310753913545
Loss at iteration [395]: 0.0024251384492822168
Loss at iteration [396]: 0.0024251384492822168
Loss at iteration [397]: 0.0024251045774604514
Loss at iteration [398]: 0.0024250727313315313
Loss at iteration [399]: 0.0024249406819876846
Loss at iteration [400]: 0.002424869235705224
Loss at iteration [401]: 0.002424794956445409
Loss at iteration [402]: 0.002424619413809683
Loss at iteration [403]: 0.002424557243627466
Loss at iteration [404]: 0.002424428595442128
Loss at iteration [405]: 0.002424428595442128
Loss at iteration [406]: 0.0024243668779852664
Loss at iteration [407]: 0.0024243485862972818
Loss at iteration [408]: 0.0024243018881070387
Loss at iteration [409]: 0.002424184099961295
Loss at iteration [410]: 0.0024240636856659005
Loss at iteration [411]: 0.0024237549701748317
Loss at iteration [412]: 0.0024236098245085497
Loss at iteration [413]: 0.0024235342741267918
Loss at iteration [414]: 0.0024235342741267918
Loss at iteration [415]: 0.0024234447817559745
Loss at iteration [416]: 0.002423409241896711
Loss at iteration [417]: 0.0024229059113508164
Loss at iteration [418]: 0.002422852928218974
Loss at iteration [419]: 0.002422735160936014
Loss at iteration [420]: 0.0024226293863228603
Loss at iteration [421]: 0.002422512973192007
Loss at iteration [422]: 0.002422333525169904
Loss at iteration [423]: 0.0024222096387226584
Loss at iteration [424]: 0.0024222096387226584
Loss at iteration [425]: 0.0024221712212786706
Loss at iteration [426]: 0.0024221273278346943
Loss at iteration [427]: 0.0024219891206801905
Loss at iteration [428]: 0.0024219029104841235
Loss at iteration [429]: 0.0024217435479998196
Loss at iteration [430]: 0.0024216789676860242
Loss at iteration [431]: 0.002421621663665563
Loss at iteration [432]: 0.0024214817999197685
Loss at iteration [433]: 0.0024214817999197685
Loss at iteration [434]: 0.00242142748697465
Loss at iteration [435]: 0.0024213951995578973
Loss at iteration [436]: 0.0024213701074150833
Loss at iteration [437]: 0.002421307778480033
Loss at iteration [438]: 0.0024212721171970064
Loss at iteration [439]: 0.0024212470310831574
Loss at iteration [440]: 0.0024212020463795347
Loss at iteration [441]: 0.002421184265629434
Loss at iteration [442]: 0.002421101737302809
Loss at iteration [443]: 0.002420870942268153
Loss at iteration [444]: 0.002420870942268153
Loss at iteration [445]: 0.0024207461893466934
Loss at iteration [446]: 0.0024207016106584612
Loss at iteration [447]: 0.002420572199408059
Loss at iteration [448]: 0.00242051964163249
Loss at iteration [449]: 0.002420470809454922
Loss at iteration [450]: 0.0024204323891111705
Loss at iteration [451]: 0.002420379464234282
Loss at iteration [452]: 0.0024203300827573947
Loss at iteration [453]: 0.00242027914513893
Loss at iteration [454]: 0.00242027914513893
Loss at iteration [455]: 0.0024202220578912292
Loss at iteration [456]: 0.0024201956181421217
Loss at iteration [457]: 0.0024201640888286643
Loss at iteration [458]: 0.002420089408573684
Loss at iteration [459]: 0.0024199323368169086
Loss at iteration [460]: 0.00241981098537143
Loss at iteration [461]: 0.0024194171892932006
Loss at iteration [462]: 0.002419078842235273
Loss at iteration [463]: 0.002418889926648553
Loss at iteration [464]: 0.002418889926648553
Loss at iteration [465]: 0.0024187829413127456
Loss at iteration [466]: 0.0024187425520411504
Loss at iteration [467]: 0.002418624507432367
Loss at iteration [468]: 0.002418556193597517
Loss at iteration [469]: 0.0024183830596377514
Loss at iteration [470]: 0.002418360406125216
Loss at iteration [471]: 0.002418324926673919
Loss at iteration [472]: 0.0024182450858095802
Loss at iteration [473]: 0.0024181848122612694
Loss at iteration [474]: 0.0024181848122612694
Loss at iteration [475]: 0.002418156917323625
Loss at iteration [476]: 0.0024181183428615667
Loss at iteration [477]: 0.002418088636239135
Loss at iteration [478]: 0.0024180666167329355
Loss at iteration [479]: 0.002418037138246255
Loss at iteration [480]: 0.0024180078791578986
Loss at iteration [481]: 0.0024179848929293266
Loss at iteration [482]: 0.0024178670053253155
Loss at iteration [483]: 0.0024178670053253155
Loss at iteration [484]: 0.0024178072645218113
Loss at iteration [485]: 0.0024177468259336506
Loss at iteration [486]: 0.0024177114815475734
Loss at iteration [487]: 0.0024176703252075517
Loss at iteration [488]: 0.0024174345231079476
Loss at iteration [489]: 0.002417337423848954
Loss at iteration [490]: 0.0024172510852191425
Loss at iteration [491]: 0.002416994674608315
Loss at iteration [492]: 0.0024165745646395264
Loss at iteration [493]: 0.0024165745646395264
Loss at iteration [494]: 0.0024161522195873747
Loss at iteration [495]: 0.0024160551841234694
Loss at iteration [496]: 0.002415963444262354
Loss at iteration [497]: 0.0024158297991615065
Loss at iteration [498]: 0.002415763159456013
Loss at iteration [499]: 0.002415671564023949
Loss at iteration [500]: 0.00241547599100082
Loss at iteration [501]: 0.0024152859257024323
Loss at iteration [502]: 0.0024152859257024323
Loss at iteration [503]: 0.002415252679144647
Loss at iteration [504]: 0.0024152356226582947
Loss at iteration [505]: 0.0024151931383177105
Loss at iteration [506]: 0.002415124922373042
Loss at iteration [507]: 0.0024149216798582305
Loss at iteration [508]: 0.0024148571962074755
Loss at iteration [509]: 0.002414806284123429
Loss at iteration [510]: 0.002414640857704669
Loss at iteration [511]: 0.002414640857704669
Loss at iteration [512]: 0.0024145493136553987
Loss at iteration [513]: 0.002414515310834691
Loss at iteration [514]: 0.0024144704619042364
Loss at iteration [515]: 0.0024144041865473333
Loss at iteration [516]: 0.0024143733201318104
Loss at iteration [517]: 0.0024143051802162654
Loss at iteration [518]: 0.00241424211604923
Loss at iteration [519]: 0.00241424211604923
Loss at iteration [520]: 0.0024142142464900393
Loss at iteration [521]: 0.002414168660301579
Loss at iteration [522]: 0.002414133344558064
Loss at iteration [523]: 0.002414095582253493
Loss at iteration [524]: 0.0024140399003010813
Loss at iteration [525]: 0.002413993307592487
Loss at iteration [526]: 0.002413907120994834
Loss at iteration [527]: 0.002413735390432705
Loss at iteration [528]: 0.002413735390432705
Loss at iteration [529]: 0.0024136606770702364
Loss at iteration [530]: 0.002413609570237754
Loss at iteration [531]: 0.0024135562960300645
Loss at iteration [532]: 0.002413543582677505
Loss at iteration [533]: 0.002413503069966146
Loss at iteration [534]: 0.0024134792772029785
Loss at iteration [535]: 0.00241345947232664
Loss at iteration [536]: 0.00241343435938289
Loss at iteration [537]: 0.0024133407322269314
Loss at iteration [538]: 0.0024133407322269314
Loss at iteration [539]: 0.0024133039713351632
Loss at iteration [540]: 0.002413207900050128
Loss at iteration [541]: 0.0024131553711204817
Loss at iteration [542]: 0.002413089255147512
Loss at iteration [543]: 0.002413061069301898
Loss at iteration [544]: 0.0024129302719781164
Loss at iteration [545]: 0.002412732429676339
Loss at iteration [546]: 0.0024126153112484353
Loss at iteration [547]: 0.0024126153112484353
Loss at iteration [548]: 0.0024125625708786604
Loss at iteration [549]: 0.002412534033472632
Loss at iteration [550]: 0.0024124798712624085
Loss at iteration [551]: 0.002412366911871017
Loss at iteration [552]: 0.0024123095618018516
Loss at iteration [553]: 0.00241224357659238
Loss at iteration [554]: 0.0024122064958220692
Loss at iteration [555]: 0.00241213112422176
Loss at iteration [556]: 0.0024121117687394577
Loss at iteration [557]: 0.0024121117687394577
Loss at iteration [558]: 0.002412092877393048
Loss at iteration [559]: 0.002412069924711922
Loss at iteration [560]: 0.002412016059389205
Loss at iteration [561]: 0.002411988744535596
Loss at iteration [562]: 0.002411948078473155
Loss at iteration [563]: 0.00241190330318919
Loss at iteration [564]: 0.002411874808052645
Loss at iteration [565]: 0.0024117412570605692
Loss at iteration [566]: 0.0024117412570605692
Loss at iteration [567]: 0.002411699191938856
Loss at iteration [568]: 0.0024116715281173075
Loss at iteration [569]: 0.0024116526628089933
Loss at iteration [570]: 0.0024116205513745994
Loss at iteration [571]: 0.002411525769429909
Loss at iteration [572]: 0.0024114542045319534
Loss at iteration [573]: 0.0024113998878766297
Loss at iteration [574]: 0.00241128009712414
Loss at iteration [575]: 0.00241128009712414
Loss at iteration [576]: 0.0024112607256117297
Loss at iteration [577]: 0.0024112162100403123
Loss at iteration [578]: 0.0024111117982180946
Loss at iteration [579]: 0.0024110829074048255
Loss at iteration [580]: 0.0024110387126205484
Loss at iteration [581]: 0.002411004740358606
Loss at iteration [582]: 0.0024107700272794864
Loss at iteration [583]: 0.002410521108182103
Loss at iteration [584]: 0.002410521108182103
Loss at iteration [585]: 0.0024104171087662323
Loss at iteration [586]: 0.002410378027050685
Loss at iteration [587]: 0.002410092295624735
Loss at iteration [588]: 0.0024099796533520665
Loss at iteration [589]: 0.002409813806083983
Loss at iteration [590]: 0.0024097688352903808
Loss at iteration [591]: 0.0024096325497914585
Loss at iteration [592]: 0.002409591769835096
Loss at iteration [593]: 0.002409591769835096
Loss at iteration [594]: 0.002409554458091013
Loss at iteration [595]: 0.0024095381200497814
Loss at iteration [596]: 0.0024095045130297977
Loss at iteration [597]: 0.002409479955297225
Loss at iteration [598]: 0.002409409104637472
Loss at iteration [599]: 0.0024093477221880906
Loss at iteration [600]: 0.002409288334979197
Loss at iteration [601]: 0.0024091298617916986
Loss at iteration [602]: 0.002409065410751765
Loss at iteration [603]: 0.002409065410751765
Loss at iteration [604]: 0.0024089709559635677
Loss at iteration [605]: 0.002408926301465566
Loss at iteration [606]: 0.0024089020218973356
Loss at iteration [607]: 0.0024088841190032435
Loss at iteration [608]: 0.002408832485044806
Loss at iteration [609]: 0.0024087555127402177
Loss at iteration [610]: 0.002408707207690781
Loss at iteration [611]: 0.002408486222674095
Loss at iteration [612]: 0.002408486222674095
Loss at iteration [613]: 0.0024084662816727925
Loss at iteration [614]: 0.0024084354166159067
Loss at iteration [615]: 0.002408377561156874
Loss at iteration [616]: 0.0024083454416902843
Loss at iteration [617]: 0.0024083061551406913
Loss at iteration [618]: 0.002408282153021545
Loss at iteration [619]: 0.002408205946328602
Loss at iteration [620]: 0.002408205946328602
Loss at iteration [621]: 0.0024081475111717232
Loss at iteration [622]: 0.002408131512030654
Loss at iteration [623]: 0.0024081022181788344
Loss at iteration [624]: 0.0024080807650497284
Loss at iteration [625]: 0.002408034373842708
Loss at iteration [626]: 0.002408003289344441
Loss at iteration [627]: 0.0024078370405409915
Loss at iteration [628]: 0.0024076534848926083
Loss at iteration [629]: 0.0024075176250933196
Loss at iteration [630]: 0.0024075176250933196
Loss at iteration [631]: 0.0024074395984453744
Loss at iteration [632]: 0.002407341640299373
Loss at iteration [633]: 0.0024070770936803097
Loss at iteration [634]: 0.002407036402241262
Loss at iteration [635]: 0.0024069963974869928
Loss at iteration [636]: 0.00240689182688226
Loss at iteration [637]: 0.002406854431693503
Loss at iteration [638]: 0.0024067405169060406
Loss at iteration [639]: 0.0024067405169060406
Loss at iteration [640]: 0.0024066936778618627
Loss at iteration [641]: 0.0024066012420223665
Loss at iteration [642]: 0.0024065878686716554
Loss at iteration [643]: 0.002406568164762017
Loss at iteration [644]: 0.0024065137508999307
Loss at iteration [645]: 0.002406489994504427
Loss at iteration [646]: 0.0024064166881363246
Loss at iteration [647]: 0.002406262460909324
Loss at iteration [648]: 0.002406208397451514
Loss at iteration [649]: 0.002406208397451514
Loss at iteration [650]: 0.0024061846983629382
Loss at iteration [651]: 0.002406110012348438
Loss at iteration [652]: 0.002406097209175945
Loss at iteration [653]: 0.0024060317703058958
Loss at iteration [654]: 0.002405950747145185
Loss at iteration [655]: 0.0024058760603408425
Loss at iteration [656]: 0.0024057332345840633
Loss at iteration [657]: 0.0024056223707686524
Loss at iteration [658]: 0.0024056223707686524
Loss at iteration [659]: 0.0024055140319751422
Loss at iteration [660]: 0.002405438082047122
Loss at iteration [661]: 0.002405420079011635
Loss at iteration [662]: 0.0024053525593484256
Loss at iteration [663]: 0.0024052374851712887
Loss at iteration [664]: 0.002405173540712289
Loss at iteration [665]: 0.002405127001947049
Loss at iteration [666]: 0.0024050685843650258
Loss at iteration [667]: 0.0024050469591263878
Loss at iteration [668]: 0.0024050469591263878
Loss at iteration [669]: 0.0024050370492259996
Loss at iteration [670]: 0.0024049646821601486
Loss at iteration [671]: 0.002404900857474093
Loss at iteration [672]: 0.002404842268229866
Loss at iteration [673]: 0.0024047979326683763
Loss at iteration [674]: 0.0024047621241812507
Loss at iteration [675]: 0.0024045885776303217
Loss at iteration [676]: 0.0024045561389532425
Loss at iteration [677]: 0.0024044866599791477
Loss at iteration [678]: 0.0024044866599791477
Loss at iteration [679]: 0.0024044589187841186
Loss at iteration [680]: 0.0024044364644172827
Loss at iteration [681]: 0.0024043632634478816
Loss at iteration [682]: 0.002404317448017218
Loss at iteration [683]: 0.002404274916599044
Loss at iteration [684]: 0.002404002468723664
Loss at iteration [685]: 0.0024037450810662385
Loss at iteration [686]: 0.0024035939780728877
Loss at iteration [687]: 0.0024035939780728877
Loss at iteration [688]: 0.002403527418783667
Loss at iteration [689]: 0.002403481922452099
Loss at iteration [690]: 0.002403416551960506
Loss at iteration [691]: 0.0024032920581892087
Loss at iteration [692]: 0.0024031499781217765
Loss at iteration [693]: 0.0024030128550488056
Loss at iteration [694]: 0.002402936848910886
Loss at iteration [695]: 0.002402883468629877
Loss at iteration [696]: 0.002402883468629877
Loss at iteration [697]: 0.0024028441304185843
Loss at iteration [698]: 0.0024028109218200196
Loss at iteration [699]: 0.0024025737732715946
Loss at iteration [700]: 0.002402526319422213
Loss at iteration [701]: 0.0024024693963339825
Loss at iteration [702]: 0.0024023228795773443
Loss at iteration [703]: 0.002402258887335305
Loss at iteration [704]: 0.002402199337905956
Loss at iteration [705]: 0.002402199337905956
Loss at iteration [706]: 0.002402156081556929
Loss at iteration [707]: 0.002402115622186118
Loss at iteration [708]: 0.0024020038946235634
Loss at iteration [709]: 0.0024019704155486816
Loss at iteration [710]: 0.0024018844452109336
Loss at iteration [711]: 0.0024017259675550833
Loss at iteration [712]: 0.002401686246579632
Loss at iteration [713]: 0.0024015854967240832
Loss at iteration [714]: 0.0024015854967240832
Loss at iteration [715]: 0.0024015064714565885
Loss at iteration [716]: 0.0024014635337831452
Loss at iteration [717]: 0.002401412308792151
Loss at iteration [718]: 0.0024013131485012347
Loss at iteration [719]: 0.0024012544482059635
Loss at iteration [720]: 0.0024012228454524474
Loss at iteration [721]: 0.0024011418777774873
Loss at iteration [722]: 0.002401032372264122
Loss at iteration [723]: 0.0024009763937442056
Loss at iteration [724]: 0.0024009763937442056
Loss at iteration [725]: 0.0024009485993105183
Loss at iteration [726]: 0.002400836877105409
Loss at iteration [727]: 0.002400795577104309
Loss at iteration [728]: 0.0024007586666073273
Loss at iteration [729]: 0.0024007144260642177
Loss at iteration [730]: 0.0024006925428836892
Loss at iteration [731]: 0.002400601196975107
Loss at iteration [732]: 0.0024005826320062126
Loss at iteration [733]: 0.002400526102873298
Loss at iteration [734]: 0.002400526102873298
Loss at iteration [735]: 0.0024005000734410916
Loss at iteration [736]: 0.0024004813786668994
Loss at iteration [737]: 0.002400447042688138
Loss at iteration [738]: 0.0024004215536958073
Loss at iteration [739]: 0.002400348471080277
Loss at iteration [740]: 0.0024002864836268033
Loss at iteration [741]: 0.0023998155722606516
Loss at iteration [742]: 0.002398643797092893
Loss at iteration [743]: 0.002398643797092893
Loss at iteration [744]: 0.0023983624522460954
Loss at iteration [745]: 0.002398256466353521
Loss at iteration [746]: 0.0023976185213973513
Loss at iteration [747]: 0.0023975625975908637
Loss at iteration [748]: 0.002397436311341237
Loss at iteration [749]: 0.0023972798017417992
Loss at iteration [750]: 0.0023970442074637845
Loss at iteration [751]: 0.002396817505207606
Loss at iteration [752]: 0.002396766097599421
Loss at iteration [753]: 0.0023967025046065164
Loss at iteration [754]: 0.0023967025046065164
Loss at iteration [755]: 0.0023966643208823213
Loss at iteration [756]: 0.0023965797187672504
Loss at iteration [757]: 0.002396300695509815
Loss at iteration [758]: 0.002396188669525572
Loss at iteration [759]: 0.00239601266501799
Loss at iteration [760]: 0.002395971692674369
Loss at iteration [761]: 0.0023958841615362614
Loss at iteration [762]: 0.0023958714654662657
Loss at iteration [763]: 0.0023957842376527132
Loss at iteration [764]: 0.0023957842376527132
Loss at iteration [765]: 0.0023957464239690563
Loss at iteration [766]: 0.002395715903835907
Loss at iteration [767]: 0.0023956229190674637
Loss at iteration [768]: 0.0023955953751453977
Loss at iteration [769]: 0.0023954358592167837
Loss at iteration [770]: 0.002395401271912138
Loss at iteration [771]: 0.0023952994404875597
Loss at iteration [772]: 0.0023951587014094974
Loss at iteration [773]: 0.0023950196621170075
Loss at iteration [774]: 0.0023950196621170075
Loss at iteration [775]: 0.002394987338075756
Loss at iteration [776]: 0.0023949090426166495
Loss at iteration [777]: 0.0023947875746857
Loss at iteration [778]: 0.0023947680592550638
Loss at iteration [779]: 0.0023947457474814125
Loss at iteration [780]: 0.0023946923016381844
Loss at iteration [781]: 0.002394671815386169
Loss at iteration [782]: 0.0023946390883421327
Loss at iteration [783]: 0.0023945912424243507
Loss at iteration [784]: 0.0023945912424243507
Loss at iteration [785]: 0.0023945802825953368
Loss at iteration [786]: 0.0023945368981055782
Loss at iteration [787]: 0.002394459744241403
Loss at iteration [788]: 0.0023943845630049368
Loss at iteration [789]: 0.002394215851381748
Loss at iteration [790]: 0.002394069114710798
Loss at iteration [791]: 0.002393786349498188
Loss at iteration [792]: 0.002393786349498188
Loss at iteration [793]: 0.0023936796027818445
Loss at iteration [794]: 0.0023935496035930167
Loss at iteration [795]: 0.002393458520504907
Loss at iteration [796]: 0.0023933907446600233
Loss at iteration [797]: 0.0023932662912994934
Loss at iteration [798]: 0.0023931943076181514
Loss at iteration [799]: 0.0023930849460485517
Loss at iteration [800]: 0.0023929470447584082
Loss at iteration [801]: 0.0023929470447584082
Loss at iteration [802]: 0.0023928380251825943
Loss at iteration [803]: 0.002392814443231956
Loss at iteration [804]: 0.0023926987976872153
Loss at iteration [805]: 0.0023926168696492057
Loss at iteration [806]: 0.002392581976782753
Loss at iteration [807]: 0.002392471472177196
Loss at iteration [808]: 0.0023923442430956246
Loss at iteration [809]: 0.002392249881004055
Loss at iteration [810]: 0.002392249881004055
Loss at iteration [811]: 0.002392202300546367
Loss at iteration [812]: 0.0023919806079272642
Loss at iteration [813]: 0.0023919413557163074
Loss at iteration [814]: 0.0023919130061928555
Loss at iteration [815]: 0.002391692179135827
Loss at iteration [816]: 0.0023916515040790015
Loss at iteration [817]: 0.0023915855122043066
Loss at iteration [818]: 0.0023913622396526746
Loss at iteration [819]: 0.0023913622396526746
Loss at iteration [820]: 0.0023913065662168483
Loss at iteration [821]: 0.002391198976175041
Loss at iteration [822]: 0.0023911614933503572
Loss at iteration [823]: 0.002391130506214695
Loss at iteration [824]: 0.002391092355028716
Loss at iteration [825]: 0.002391077591224468
Loss at iteration [826]: 0.002391033922791187
Loss at iteration [827]: 0.0023909562942184726
Loss at iteration [828]: 0.002390909057498885
Loss at iteration [829]: 0.0023907596223382704
Loss at iteration [830]: 0.0023907596223382704
Loss at iteration [831]: 0.0023907335479642126
Loss at iteration [832]: 0.0023907125125487666
Loss at iteration [833]: 0.0023906485050923763
Loss at iteration [834]: 0.0023906029989969263
Loss at iteration [835]: 0.0023905266320877203
Loss at iteration [836]: 0.002390461733985246
Loss at iteration [837]: 0.0023903861513845954
Loss at iteration [838]: 0.002390367963882991
Loss at iteration [839]: 0.002390337205872774
Loss at iteration [840]: 0.002390337205872774
Loss at iteration [841]: 0.002390320705494115
Loss at iteration [842]: 0.00239028764173589
Loss at iteration [843]: 0.0023901375729506623
Loss at iteration [844]: 0.002390048326231881
Loss at iteration [845]: 0.0023900074470147477
Loss at iteration [846]: 0.002389871809661466
Loss at iteration [847]: 0.0023897696379489002
Loss at iteration [848]: 0.002389695692658736
Loss at iteration [849]: 0.002389695692658736
Loss at iteration [850]: 0.0023896741943747867
Loss at iteration [851]: 0.00238960471800187
Loss at iteration [852]: 0.002389535968181214
Loss at iteration [853]: 0.002389492311646677
Loss at iteration [854]: 0.0023894114288123412
Loss at iteration [855]: 0.00238939069583627
Loss at iteration [856]: 0.002389370569998547
Loss at iteration [857]: 0.002389329274386404
Loss at iteration [858]: 0.002389329274386404
Loss at iteration [859]: 0.0023893124229497505
Loss at iteration [860]: 0.00238928423733722
Loss at iteration [861]: 0.0023892464313390533
Loss at iteration [862]: 0.0023892191463238065
Loss at iteration [863]: 0.0023889936713547685
Loss at iteration [864]: 0.0023888018423010066
Loss at iteration [865]: 0.002388386375035743
Loss at iteration [866]: 0.0023872911128427956
Loss at iteration [867]: 0.0023832310463153326
Loss at iteration [868]: 0.0023832310463153326
Loss at iteration [869]: 0.00238021066900853
Loss at iteration [870]: 0.002379881052048961
Loss at iteration [871]: 0.0023769841687170724
Loss at iteration [872]: 0.002376558431958074
Loss at iteration [873]: 0.0023760581147280148
Loss at iteration [874]: 0.0023753773919451644
Loss at iteration [875]: 0.0023751575476149886
Loss at iteration [876]: 0.0023749721474712712
Loss at iteration [877]: 0.0023748526955526604
Loss at iteration [878]: 0.0023748526955526604
Loss at iteration [879]: 0.0023748056346452394
Loss at iteration [880]: 0.0023746113326550285
Loss at iteration [881]: 0.0023745313976023296
Loss at iteration [882]: 0.0023744061630389746
Loss at iteration [883]: 0.002374189250560763
Loss at iteration [884]: 0.0023741351300887955
Loss at iteration [885]: 0.0023740786168363957
Loss at iteration [886]: 0.002373984248610505
Loss at iteration [887]: 0.002373984248610505
Loss at iteration [888]: 0.002373956878699223
Loss at iteration [889]: 0.0023739064675717234
Loss at iteration [890]: 0.0023738080167394877
Loss at iteration [891]: 0.0023737507274674
Loss at iteration [892]: 0.002373609497555564
Loss at iteration [893]: 0.002373555175097917
Loss at iteration [894]: 0.0023734128702760475
Loss at iteration [895]: 0.0023732988261886195
Loss at iteration [896]: 0.00237324020917033
Loss at iteration [897]: 0.002373075430980203
Loss at iteration [898]: 0.002373075430980203
Loss at iteration [899]: 0.0023730178032314187
Loss at iteration [900]: 0.00237289155349736
Loss at iteration [901]: 0.002372775685646958
Loss at iteration [902]: 0.002372764489301957
Loss at iteration [903]: 0.0023727385225676364
Loss at iteration [904]: 0.002372642585994654
Loss at iteration [905]: 0.002372581285168148
Loss at iteration [906]: 0.0023725513395116463
Loss at iteration [907]: 0.0023725219439946254
Loss at iteration [908]: 0.00237244995921771
Loss at iteration [909]: 0.00237244995921771
Loss at iteration [910]: 0.0023724336360602736
Loss at iteration [911]: 0.0023723828574742614
Loss at iteration [912]: 0.0023722853304463387
Loss at iteration [913]: 0.00237224945811988
Loss at iteration [914]: 0.0023720713306496846
Loss at iteration [915]: 0.0023719735130511665
Loss at iteration [916]: 0.0023718214828442235
Loss at iteration [917]: 0.002371497753822387
Loss at iteration [918]: 0.002371497753822387
Loss at iteration [919]: 0.002371418092346981
Loss at iteration [920]: 0.00237127383083876
Loss at iteration [921]: 0.002371187196734078
Loss at iteration [922]: 0.0023711399440503405
Loss at iteration [923]: 0.0023710606333825148
Loss at iteration [924]: 0.0023709383390108016
Loss at iteration [925]: 0.0023708682050955702
Loss at iteration [926]: 0.0023706733670099795
Loss at iteration [927]: 0.0023706733670099795
Loss at iteration [928]: 0.0023706079882324154
Loss at iteration [929]: 0.0023705496078782816
Loss at iteration [930]: 0.002370433300569539
Loss at iteration [931]: 0.0023703796734611204
Loss at iteration [932]: 0.0023702287823420157
Loss at iteration [933]: 0.0023701597030407265
Loss at iteration [934]: 0.0023701245793130506
Loss at iteration [935]: 0.0023699618836439604
Loss at iteration [936]: 0.0023699618836439604
Loss at iteration [937]: 0.0023699195596600556
Loss at iteration [938]: 0.0023698938299538826
Loss at iteration [939]: 0.0023697783381446563
Loss at iteration [940]: 0.002369690403688449
Loss at iteration [941]: 0.002369634948351237
Loss at iteration [942]: 0.002369582016376359
Loss at iteration [943]: 0.0023695519784691123
Loss at iteration [944]: 0.0023694948521677003
Loss at iteration [945]: 0.0023694765879674266
Loss at iteration [946]: 0.0023694765879674266
Loss at iteration [947]: 0.002369465783561117
Loss at iteration [948]: 0.002369446829422714
Loss at iteration [949]: 0.002369399055562456
Loss at iteration [950]: 0.002369327524357728
Loss at iteration [951]: 0.002369252236079186
Loss at iteration [952]: 0.0023692002429590558
Loss at iteration [953]: 0.0023691301207712776
Loss at iteration [954]: 0.0023689911822631746
Loss at iteration [955]: 0.0023689447925618837
Loss at iteration [956]: 0.0023689447925618837
Loss at iteration [957]: 0.002368903466153042
Loss at iteration [958]: 0.0023688631585398515
Loss at iteration [959]: 0.0023688307461192035
Loss at iteration [960]: 0.0023688065264147414
Loss at iteration [961]: 0.0023687657328991074
Loss at iteration [962]: 0.002368704576447884
Loss at iteration [963]: 0.0023686779323069886
Loss at iteration [964]: 0.002368617640860388
Loss at iteration [965]: 0.00236847117096793
Loss at iteration [966]: 0.00236847117096793
Loss at iteration [967]: 0.0023684053277790397
Loss at iteration [968]: 0.0023683330711917554
Loss at iteration [969]: 0.0023682645532335407
Loss at iteration [970]: 0.002368232357515306
Loss at iteration [971]: 0.002368187650191851
Loss at iteration [972]: 0.0023681381679510147
Loss at iteration [973]: 0.002368046100144493
Loss at iteration [974]: 0.002368009621065531
Loss at iteration [975]: 0.0023679819945696574
Loss at iteration [976]: 0.0023679819945696574
Loss at iteration [977]: 0.002367964569472913
Loss at iteration [978]: 0.0023679427754339072
Loss at iteration [979]: 0.002367906941976789
Loss at iteration [980]: 0.0023678733324511256
Loss at iteration [981]: 0.0023678204756322407
Loss at iteration [982]: 0.0023677919570268386
Loss at iteration [983]: 0.0023677386481932654
Loss at iteration [984]: 0.002367680507681376
Loss at iteration [985]: 0.002367680507681376
Loss at iteration [986]: 0.0023676627589768605
Loss at iteration [987]: 0.00236762918529961
Loss at iteration [988]: 0.0023675945020161836
Loss at iteration [989]: 0.0023675845588306227
Loss at iteration [990]: 0.002367558687740754
Loss at iteration [991]: 0.002367534586887904
Loss at iteration [992]: 0.0023675036585738348
Loss at iteration [993]: 0.002367472348441363
Loss at iteration [994]: 0.002367442655007514
Loss at iteration [995]: 0.002367442655007514
Loss at iteration [996]: 0.002367414487698731
Loss at iteration [997]: 0.002367379899309629
Loss at iteration [998]: 0.0023673362436994402
Loss at iteration [999]: 0.002367317809888602
Loss at iteration [1000]: 0.00236729868317106
Loss at iteration [1001]: 0.0023672560798950744
Loss at iteration [1002]: 0.0023672331646817524
Loss at iteration [1003]: 0.002367195571651987
Loss at iteration [1004]: 0.002367195571651987
Loss at iteration [1005]: 0.0023671768404986766
Loss at iteration [1006]: 0.0023671472801041578
Loss at iteration [1007]: 0.002367112781581275
Loss at iteration [1008]: 0.002367073339248709
Loss at iteration [1009]: 0.002367049551744681
Loss at iteration [1010]: 0.002366969557966004
Loss at iteration [1011]: 0.0023669400842307713
Loss at iteration [1012]: 0.0023667942874960046
Loss at iteration [1013]: 0.0023666223461561563
Loss at iteration [1014]: 0.0023666223461561563
Loss at iteration [1015]: 0.0023665565788318446
Loss at iteration [1016]: 0.0023664732890678534
Loss at iteration [1017]: 0.002366252669137809
Loss at iteration [1018]: 0.0023662186194618666
Loss at iteration [1019]: 0.0023662022050362705
Loss at iteration [1020]: 0.002366169048595913
Loss at iteration [1021]: 0.0023661179805529926
Loss at iteration [1022]: 0.0023660872732090272
Loss at iteration [1023]: 0.0023660327554620845
Loss at iteration [1024]: 0.0023660327554620845
Loss at iteration [1025]: 0.0023659985614036282
Loss at iteration [1026]: 0.002365951102377477
Loss at iteration [1027]: 0.002365915084783581
Loss at iteration [1028]: 0.002365888909738321
Loss at iteration [1029]: 0.0023658324489960734
Loss at iteration [1030]: 0.0023658097978804253
Loss at iteration [1031]: 0.0023657130816874777
Loss at iteration [1032]: 0.002365637452174611
Loss at iteration [1033]: 0.0023653800257876847
Loss at iteration [1034]: 0.0023653800257876847
Loss at iteration [1035]: 0.0023652772966120932
Loss at iteration [1036]: 0.0023651337488424687
Loss at iteration [1037]: 0.0023650007607445003
Loss at iteration [1038]: 0.002364962577757456
Loss at iteration [1039]: 0.0023648818897094416
Loss at iteration [1040]: 0.0023648235367511386
Loss at iteration [1041]: 0.002364788141252857
Loss at iteration [1042]: 0.0023647162319700554
Loss at iteration [1043]: 0.0023647162319700554
Loss at iteration [1044]: 0.0023646924744838774
Loss at iteration [1045]: 0.0023646582109975653
Loss at iteration [1046]: 0.002364618006472097
Loss at iteration [1047]: 0.0023645721636420826
Loss at iteration [1048]: 0.002364480963255551
Loss at iteration [1049]: 0.002364372845947889
Loss at iteration [1050]: 0.0023641787979889455
Loss at iteration [1051]: 0.0023641787979889455
Loss at iteration [1052]: 0.002364113809255134
Loss at iteration [1053]: 0.0023640225819214376
Loss at iteration [1054]: 0.002363935865235061
Loss at iteration [1055]: 0.0023638956995840617
Loss at iteration [1056]: 0.0023638714606316192
Loss at iteration [1057]: 0.0023638350153215113
Loss at iteration [1058]: 0.002363776534585993
Loss at iteration [1059]: 0.0023637403427317996
Loss at iteration [1060]: 0.0023637074256157345
Loss at iteration [1061]: 0.0023637074256157345
Loss at iteration [1062]: 0.002363693279919368
Loss at iteration [1063]: 0.0023636685458876894
Loss at iteration [1064]: 0.002363651970836708
Loss at iteration [1065]: 0.002363549293205993
Loss at iteration [1066]: 0.002363525705381692
Loss at iteration [1067]: 0.0023634328265511864
Loss at iteration [1068]: 0.002363378375545975
Loss at iteration [1069]: 0.0023633295676421637
Loss at iteration [1070]: 0.0023633295676421637
Loss at iteration [1071]: 0.0023632916010001585
Loss at iteration [1072]: 0.0023632808874084225
Loss at iteration [1073]: 0.0023632325758895082
Loss at iteration [1074]: 0.0023631991967806207
Loss at iteration [1075]: 0.0023631744404120163
Loss at iteration [1076]: 0.0023631538220050133
Loss at iteration [1077]: 0.0023631351840607655
Loss at iteration [1078]: 0.0023631194975501854
Loss at iteration [1079]: 0.002363104241942855
Loss at iteration [1080]: 0.002363081064992488
Loss at iteration [1081]: 0.002363081064992488
Loss at iteration [1082]: 0.002363055160163507
Loss at iteration [1083]: 0.0023630287577758578
Loss at iteration [1084]: 0.0023630085224477684
Loss at iteration [1085]: 0.0023629821931996408
Loss at iteration [1086]: 0.0023629295784211024
Loss at iteration [1087]: 0.0023628963851012397
Loss at iteration [1088]: 0.0023628529474566647
Loss at iteration [1089]: 0.0023628133227453726
Loss at iteration [1090]: 0.0023627471092605924
Loss at iteration [1091]: 0.0023627471092605924
Loss at iteration [1092]: 0.0023627174395787123
Loss at iteration [1093]: 0.0023627055859919838
Loss at iteration [1094]: 0.0023626817379556668
Loss at iteration [1095]: 0.002362649869497672
Loss at iteration [1096]: 0.002362432562920741
Loss at iteration [1097]: 0.0023620796026028327
Loss at iteration [1098]: 0.002362032406264384
Loss at iteration [1099]: 0.002361672553445494
Loss at iteration [1100]: 0.002361672553445494
Loss at iteration [1101]: 0.0023614108044578254
Loss at iteration [1102]: 0.002361368506925048
Loss at iteration [1103]: 0.0023611846129714073
Loss at iteration [1104]: 0.002361155140770331
Loss at iteration [1105]: 0.0023610314136136503
Loss at iteration [1106]: 0.002360970322378829
Loss at iteration [1107]: 0.0023609277369271682
Loss at iteration [1108]: 0.002360821891607079
Loss at iteration [1109]: 0.00236075825833561
Loss at iteration [1110]: 0.002360647294430022
Loss at iteration [1111]: 0.002360647294430022
Loss at iteration [1112]: 0.0023605998853212275
Loss at iteration [1113]: 0.002360554891836181
Loss at iteration [1114]: 0.002360468459837854
Loss at iteration [1115]: 0.0023604517212344506
Loss at iteration [1116]: 0.002360427453689856
Loss at iteration [1117]: 0.002360390928097199
Loss at iteration [1118]: 0.002360343402305289
Loss at iteration [1119]: 0.00236029051384763
