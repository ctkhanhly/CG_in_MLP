Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.001
Beta type                             :HS
Total number of function evaluations  : 3041
Total number of iterations            : 757
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 41.768595695495605
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.93022405416902%
Percentage of parameters < 1e-7       : 49.93022405416902%
Percentage of parameters < 1e-6       : 49.93171658776968%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.003981548753607205
Loss at iteration [2]: 0.0038911687536178183
Loss at iteration [3]: 0.003497605210546144
Loss at iteration [4]: 0.002920635078223435
Loss at iteration [5]: 0.0028506528718937048
Loss at iteration [6]: 0.00279172307453168
Loss at iteration [7]: 0.002755715030376374
Loss at iteration [8]: 0.002732225730573472
Loss at iteration [9]: 0.002732225730573472
Loss at iteration [10]: 0.0027183455651730243
Loss at iteration [11]: 0.002709216753193287
Loss at iteration [12]: 0.0026559156281249284
Loss at iteration [13]: 0.00263106093935745
Loss at iteration [14]: 0.0026232804411827435
Loss at iteration [15]: 0.0026185893261282973
Loss at iteration [16]: 0.002614072728646878
Loss at iteration [17]: 0.002614072728646878
Loss at iteration [18]: 0.002610725829217892
Loss at iteration [19]: 0.002593909755079666
Loss at iteration [20]: 0.0025395714056404336
Loss at iteration [21]: 0.002537479836919918
Loss at iteration [22]: 0.0025370006239583023
Loss at iteration [23]: 0.002533663310823732
Loss at iteration [24]: 0.002533663310823732
Loss at iteration [25]: 0.0025315116170657936
Loss at iteration [26]: 0.0025311213877953704
Loss at iteration [27]: 0.0025270038447735474
Loss at iteration [28]: 0.0025254156905409326
Loss at iteration [29]: 0.002524331209208855
Loss at iteration [30]: 0.0025165119826394853
Loss at iteration [31]: 0.0025146255509109526
Loss at iteration [32]: 0.0025146255509109526
Loss at iteration [33]: 0.0025142664318113265
Loss at iteration [34]: 0.0025139464300894697
Loss at iteration [35]: 0.0025119003484124726
Loss at iteration [36]: 0.002511204610017306
Loss at iteration [37]: 0.0025107631743821984
Loss at iteration [38]: 0.002507690287430339
Loss at iteration [39]: 0.002507470349474975
Loss at iteration [40]: 0.002507470349474975
Loss at iteration [41]: 0.0025073315504387833
Loss at iteration [42]: 0.0025062743601260646
Loss at iteration [43]: 0.002506109937952087
Loss at iteration [44]: 0.002505766879656602
Loss at iteration [45]: 0.0025027134214438774
Loss at iteration [46]: 0.002502181414594832
Loss at iteration [47]: 0.002501056363511855
Loss at iteration [48]: 0.002501056363511855
Loss at iteration [49]: 0.0025005124426457116
Loss at iteration [50]: 0.002499175105134113
Loss at iteration [51]: 0.002497157632665952
Loss at iteration [52]: 0.0024971345636439378
Loss at iteration [53]: 0.002496866718711143
Loss at iteration [54]: 0.0024965040859602195
Loss at iteration [55]: 0.002496283972886704
Loss at iteration [56]: 0.0024956664545584453
Loss at iteration [57]: 0.0024956664545584453
Loss at iteration [58]: 0.002495409659167745
Loss at iteration [59]: 0.0024948276750428175
Loss at iteration [60]: 0.0024945690659071763
Loss at iteration [61]: 0.002492541108283245
Loss at iteration [62]: 0.002489837124596835
Loss at iteration [63]: 0.002489409592583857
Loss at iteration [64]: 0.002489409592583857
Loss at iteration [65]: 0.002489101277655229
Loss at iteration [66]: 0.002488143165193684
Loss at iteration [67]: 0.002486337986749535
Loss at iteration [68]: 0.002486203376985678
Loss at iteration [69]: 0.0024853986628304695
Loss at iteration [70]: 0.0024850342597714328
Loss at iteration [71]: 0.002484959531615813
Loss at iteration [72]: 0.002484959531615813
Loss at iteration [73]: 0.0024848009046904472
Loss at iteration [74]: 0.002484049124995382
Loss at iteration [75]: 0.002483968350539838
Loss at iteration [76]: 0.002483636082594829
Loss at iteration [77]: 0.0024826444778866434
Loss at iteration [78]: 0.0024825807591536617
Loss at iteration [79]: 0.0024823717484200214
Loss at iteration [80]: 0.0024823717484200214
Loss at iteration [81]: 0.0024821945750147954
Loss at iteration [82]: 0.0024819805866204588
Loss at iteration [83]: 0.0024811271498092185
Loss at iteration [84]: 0.002481087407063004
Loss at iteration [85]: 0.0024808523624540143
Loss at iteration [86]: 0.002480756287119841
Loss at iteration [87]: 0.0024806029948325425
Loss at iteration [88]: 0.0024806029948325425
Loss at iteration [89]: 0.0024804516699147715
Loss at iteration [90]: 0.00248033960371413
Loss at iteration [91]: 0.002479978261061426
Loss at iteration [92]: 0.0024797801695652056
Loss at iteration [93]: 0.002479232237693458
Loss at iteration [94]: 0.0024789216705854156
Loss at iteration [95]: 0.00247858382176128
Loss at iteration [96]: 0.00247858382176128
Loss at iteration [97]: 0.002478392365682172
Loss at iteration [98]: 0.0024779069097562315
Loss at iteration [99]: 0.002477615091987015
Loss at iteration [100]: 0.002477518350123371
Loss at iteration [101]: 0.0024766458516958263
Loss at iteration [102]: 0.002475621554901188
Loss at iteration [103]: 0.002475437490315945
Loss at iteration [104]: 0.002475437490315945
Loss at iteration [105]: 0.002475322299957298
Loss at iteration [106]: 0.0024747802416001516
Loss at iteration [107]: 0.0024742201665500924
Loss at iteration [108]: 0.0024741112279756438
Loss at iteration [109]: 0.0024740106158977694
Loss at iteration [110]: 0.0024734481657888237
Loss at iteration [111]: 0.0024733781875665656
Loss at iteration [112]: 0.0024733781875665656
Loss at iteration [113]: 0.002473338883331576
Loss at iteration [114]: 0.0024728995211141997
Loss at iteration [115]: 0.0024724253044960884
Loss at iteration [116]: 0.0024723638531524315
Loss at iteration [117]: 0.002472309366196588
Loss at iteration [118]: 0.0024722697387017036
Loss at iteration [119]: 0.0024722697387017036
Loss at iteration [120]: 0.002472236561806808
Loss at iteration [121]: 0.002470524383946926
Loss at iteration [122]: 0.002469894950849226
Loss at iteration [123]: 0.0024635492001672554
Loss at iteration [124]: 0.002462852849583471
Loss at iteration [125]: 0.002462631878676781
Loss at iteration [126]: 0.002462631878676781
Loss at iteration [127]: 0.0024621850292991577
Loss at iteration [128]: 0.0024614121727861977
Loss at iteration [129]: 0.0024612041220865863
Loss at iteration [130]: 0.002459631572315271
Loss at iteration [131]: 0.0024594736354440687
Loss at iteration [132]: 0.0024590095375711454
Loss at iteration [133]: 0.0024590095375711454
Loss at iteration [134]: 0.002458807626132028
Loss at iteration [135]: 0.0024587450201838923
Loss at iteration [136]: 0.0024582711876266405
Loss at iteration [137]: 0.002458113647450409
Loss at iteration [138]: 0.0024567643050734367
Loss at iteration [139]: 0.0024561352185091976
Loss at iteration [140]: 0.002455680768227721
Loss at iteration [141]: 0.002455680768227721
Loss at iteration [142]: 0.002455610317363825
Loss at iteration [143]: 0.0024552180269526934
Loss at iteration [144]: 0.002455061976487562
Loss at iteration [145]: 0.002454879024221093
Loss at iteration [146]: 0.002454776623124306
Loss at iteration [147]: 0.0024547199790836845
Loss at iteration [148]: 0.0024547199790836845
Loss at iteration [149]: 0.0024546712519991814
Loss at iteration [150]: 0.0024545792869638456
Loss at iteration [151]: 0.0024542837440274553
Loss at iteration [152]: 0.0024541865840137117
Loss at iteration [153]: 0.0024531023723966083
Loss at iteration [154]: 0.0024527499524117907
Loss at iteration [155]: 0.0024527499524117907
Loss at iteration [156]: 0.002452339278344708
Loss at iteration [157]: 0.0024516245058868176
Loss at iteration [158]: 0.002451517563561051
Loss at iteration [159]: 0.0024512405759707203
Loss at iteration [160]: 0.002450401054479333
Loss at iteration [161]: 0.0024498595459720974
Loss at iteration [162]: 0.0024498595459720974
Loss at iteration [163]: 0.002449722338941422
Loss at iteration [164]: 0.002449583774059475
Loss at iteration [165]: 0.002449215158694694
Loss at iteration [166]: 0.0024491390827601567
Loss at iteration [167]: 0.0024487550677017612
Loss at iteration [168]: 0.0024484530496756414
Loss at iteration [169]: 0.002448321495116909
Loss at iteration [170]: 0.002448321495116909
Loss at iteration [171]: 0.002448261623630117
Loss at iteration [172]: 0.0024482336347166197
Loss at iteration [173]: 0.002447604341131206
Loss at iteration [174]: 0.0024475592777853192
Loss at iteration [175]: 0.0024471013341901384
Loss at iteration [176]: 0.002445951381997627
Loss at iteration [177]: 0.002445951381997627
Loss at iteration [178]: 0.002445885120298502
Loss at iteration [179]: 0.0024458238498149395
Loss at iteration [180]: 0.0024454873549857414
Loss at iteration [181]: 0.002445446507498393
Loss at iteration [182]: 0.0024449285545458245
Loss at iteration [183]: 0.002444874962526233
Loss at iteration [184]: 0.0024447529000199563
Loss at iteration [185]: 0.0024447529000199563
Loss at iteration [186]: 0.0024446096530463765
Loss at iteration [187]: 0.00244457528367027
Loss at iteration [188]: 0.0024444421142144138
Loss at iteration [189]: 0.00244429478689512
Loss at iteration [190]: 0.00244423501776004
Loss at iteration [191]: 0.0024439707263047487
Loss at iteration [192]: 0.0024439707263047487
Loss at iteration [193]: 0.0024438241282091445
Loss at iteration [194]: 0.0024437937426374863
Loss at iteration [195]: 0.0024435831753049243
Loss at iteration [196]: 0.002443547100241573
Loss at iteration [197]: 0.0024433886965614547
Loss at iteration [198]: 0.0024427313357547574
Loss at iteration [199]: 0.0024425769002823236
Loss at iteration [200]: 0.0024425769002823236
Loss at iteration [201]: 0.002442490491221048
Loss at iteration [202]: 0.002442144839259958
Loss at iteration [203]: 0.0024414617956529934
Loss at iteration [204]: 0.002441451287764021
Loss at iteration [205]: 0.00244128626923502
Loss at iteration [206]: 0.0024412243489609045
Loss at iteration [207]: 0.0024411009993334626
Loss at iteration [208]: 0.0024411009993334626
Loss at iteration [209]: 0.0024410661661572746
Loss at iteration [210]: 0.002440872035142072
Loss at iteration [211]: 0.0024403205713774442
Loss at iteration [212]: 0.002440146225679364
Loss at iteration [213]: 0.002440129815831365
Loss at iteration [214]: 0.0024396981144121656
Loss at iteration [215]: 0.0024396981144121656
Loss at iteration [216]: 0.0024396143009309157
Loss at iteration [217]: 0.0024395622971108435
Loss at iteration [218]: 0.002439352182305278
Loss at iteration [219]: 0.002439212726483175
Loss at iteration [220]: 0.00243887118801655
Loss at iteration [221]: 0.0024384411095473924
Loss at iteration [222]: 0.002438154919720682
Loss at iteration [223]: 0.002438154919720682
Loss at iteration [224]: 0.002438066065849147
Loss at iteration [225]: 0.0024377810154832783
Loss at iteration [226]: 0.002437683141872738
Loss at iteration [227]: 0.0024376307045798225
Loss at iteration [228]: 0.0024371927595306844
Loss at iteration [229]: 0.002437008068270495
Loss at iteration [230]: 0.002436733727302832
Loss at iteration [231]: 0.002436733727302832
Loss at iteration [232]: 0.002436676725578117
Loss at iteration [233]: 0.0024366599629189385
Loss at iteration [234]: 0.002436512034977957
Loss at iteration [235]: 0.0024360300295278264
Loss at iteration [236]: 0.0024359486258426636
Loss at iteration [237]: 0.0024359231202147054
Loss at iteration [238]: 0.002435852684890916
Loss at iteration [239]: 0.002435852684890916
Loss at iteration [240]: 0.0024357923258184664
Loss at iteration [241]: 0.002435750019497774
Loss at iteration [242]: 0.0024354867124140875
Loss at iteration [243]: 0.0024354363811675966
Loss at iteration [244]: 0.0024353229355835327
Loss at iteration [245]: 0.002434957864058542
Loss at iteration [246]: 0.0024348984103235187
Loss at iteration [247]: 0.0024348984103235187
Loss at iteration [248]: 0.0024348790715962923
Loss at iteration [249]: 0.0024347756258572646
Loss at iteration [250]: 0.0024345695703978393
Loss at iteration [251]: 0.0024345112103459684
Loss at iteration [252]: 0.0024344100212745777
Loss at iteration [253]: 0.00243430447260063
Loss at iteration [254]: 0.002434264949575285
Loss at iteration [255]: 0.002434264949575285
Loss at iteration [256]: 0.0024342459337949726
Loss at iteration [257]: 0.002434193695005954
Loss at iteration [258]: 0.002434099736762146
Loss at iteration [259]: 0.002434070399215977
Loss at iteration [260]: 0.0024339784343990053
Loss at iteration [261]: 0.002433924982798255
Loss at iteration [262]: 0.002433894826010276
Loss at iteration [263]: 0.002433894826010276
Loss at iteration [264]: 0.002433874617928704
Loss at iteration [265]: 0.0024338510754216194
Loss at iteration [266]: 0.0024337522762234603
Loss at iteration [267]: 0.0024336854413811496
Loss at iteration [268]: 0.0024334634492199823
Loss at iteration [269]: 0.0024329346173963652
Loss at iteration [270]: 0.0024329346173963652
Loss at iteration [271]: 0.002432683681050457
Loss at iteration [272]: 0.002432376722521335
Loss at iteration [273]: 0.00243220506318874
Loss at iteration [274]: 0.00243212377130096
Loss at iteration [275]: 0.0024320086332479704
Loss at iteration [276]: 0.0024319033476591996
Loss at iteration [277]: 0.0024314578667726395
Loss at iteration [278]: 0.0024314578667726395
Loss at iteration [279]: 0.0024313198294095427
Loss at iteration [280]: 0.0024312737424464917
Loss at iteration [281]: 0.0024310871417814227
Loss at iteration [282]: 0.0024310446849264435
Loss at iteration [283]: 0.0024309016876022113
Loss at iteration [284]: 0.0024308789598877075
Loss at iteration [285]: 0.00243030594414447
Loss at iteration [286]: 0.00243030594414447
Loss at iteration [287]: 0.002430002745621819
Loss at iteration [288]: 0.0024299831820460923
Loss at iteration [289]: 0.002429702166132639
Loss at iteration [290]: 0.002429479481165367
Loss at iteration [291]: 0.0024293009892511434
Loss at iteration [292]: 0.002429198349876187
Loss at iteration [293]: 0.002429198349876187
Loss at iteration [294]: 0.0024291594847404637
Loss at iteration [295]: 0.002429034913436437
Loss at iteration [296]: 0.002428917936419605
Loss at iteration [297]: 0.002428876594093144
Loss at iteration [298]: 0.002428826818258928
Loss at iteration [299]: 0.0024287034231594952
Loss at iteration [300]: 0.002428593101701466
Loss at iteration [301]: 0.002428593101701466
Loss at iteration [302]: 0.002428574736105262
Loss at iteration [303]: 0.002428554780795107
Loss at iteration [304]: 0.0024284156689018542
Loss at iteration [305]: 0.0024283318584695204
Loss at iteration [306]: 0.0024281940209735513
Loss at iteration [307]: 0.002428127707664706
Loss at iteration [308]: 0.0024280287244305403
Loss at iteration [309]: 0.0024280287244305403
Loss at iteration [310]: 0.002427972437866211
Loss at iteration [311]: 0.0024278081270798883
Loss at iteration [312]: 0.002427750115253679
Loss at iteration [313]: 0.002427694342045368
Loss at iteration [314]: 0.0024275718486500755
Loss at iteration [315]: 0.002427425917722326
Loss at iteration [316]: 0.0024273749608684746
Loss at iteration [317]: 0.0024273749608684746
Loss at iteration [318]: 0.00242729680068097
Loss at iteration [319]: 0.0024271819971062444
Loss at iteration [320]: 0.002427170262180224
Loss at iteration [321]: 0.002427012913461622
Loss at iteration [322]: 0.002426831317885191
Loss at iteration [323]: 0.002426767551178717
Loss at iteration [324]: 0.002426767551178717
Loss at iteration [325]: 0.0024267240614012156
Loss at iteration [326]: 0.0024266536864979344
Loss at iteration [327]: 0.002426419555206577
Loss at iteration [328]: 0.002426324904519627
Loss at iteration [329]: 0.002426221268282244
Loss at iteration [330]: 0.0024258968998202768
Loss at iteration [331]: 0.0024258330125023204
Loss at iteration [332]: 0.0024258330125023204
Loss at iteration [333]: 0.002425811927969388
Loss at iteration [334]: 0.0024256624654639216
Loss at iteration [335]: 0.0024255566601520816
Loss at iteration [336]: 0.0024255070413213304
Loss at iteration [337]: 0.002425081324881663
Loss at iteration [338]: 0.0024249086327487734
Loss at iteration [339]: 0.0024249086327487734
Loss at iteration [340]: 0.0024248405067471357
Loss at iteration [341]: 0.0024248181630826615
Loss at iteration [342]: 0.002424479567648528
Loss at iteration [343]: 0.002424455577295953
Loss at iteration [344]: 0.0024243503836245566
Loss at iteration [345]: 0.0024236817391540457
Loss at iteration [346]: 0.0024236817391540457
Loss at iteration [347]: 0.0024232408245497264
Loss at iteration [348]: 0.0024229301483236886
Loss at iteration [349]: 0.002422276216551924
Loss at iteration [350]: 0.0024221372608027973
Loss at iteration [351]: 0.0024220742450431227
Loss at iteration [352]: 0.002421848283200147
Loss at iteration [353]: 0.002421529770658593
Loss at iteration [354]: 0.002421422708023326
Loss at iteration [355]: 0.002421422708023326
Loss at iteration [356]: 0.002421344189202217
Loss at iteration [357]: 0.0024212119615174233
Loss at iteration [358]: 0.002421045946940567
Loss at iteration [359]: 0.0024208110322113774
Loss at iteration [360]: 0.0024205991010385016
Loss at iteration [361]: 0.002420498753577125
Loss at iteration [362]: 0.002420454170485175
Loss at iteration [363]: 0.002420454170485175
Loss at iteration [364]: 0.0024204221456219795
Loss at iteration [365]: 0.0024203456084673637
Loss at iteration [366]: 0.0024202196621770024
Loss at iteration [367]: 0.002420160300506495
Loss at iteration [368]: 0.0024200945387842228
Loss at iteration [369]: 0.0024199952016392134
Loss at iteration [370]: 0.0024197315007625916
Loss at iteration [371]: 0.0024197315007625916
Loss at iteration [372]: 0.0024196932357619235
Loss at iteration [373]: 0.002419580687652294
Loss at iteration [374]: 0.0024194750758855764
Loss at iteration [375]: 0.0024192712691935413
Loss at iteration [376]: 0.002418824947344712
Loss at iteration [377]: 0.002418743903090342
Loss at iteration [378]: 0.002418743903090342
Loss at iteration [379]: 0.0024186978109657594
Loss at iteration [380]: 0.0024185402642282632
Loss at iteration [381]: 0.0024181881576565392
Loss at iteration [382]: 0.0024180327183441104
Loss at iteration [383]: 0.0024178550506321716
Loss at iteration [384]: 0.0024174241850504602
Loss at iteration [385]: 0.0024174241850504602
Loss at iteration [386]: 0.002417377124230528
Loss at iteration [387]: 0.0024173459921410603
Loss at iteration [388]: 0.0024171376364219743
Loss at iteration [389]: 0.002417104292141606
Loss at iteration [390]: 0.0024170077896191336
Loss at iteration [391]: 0.0024168607495366383
Loss at iteration [392]: 0.0024168607495366383
Loss at iteration [393]: 0.0024168267925339364
Loss at iteration [394]: 0.002416768995683528
Loss at iteration [395]: 0.0024165290985849286
Loss at iteration [396]: 0.0024163788676288952
Loss at iteration [397]: 0.0024162914735378683
Loss at iteration [398]: 0.002416261774092502
Loss at iteration [399]: 0.002416158033537199
Loss at iteration [400]: 0.002416158033537199
Loss at iteration [401]: 0.002416095218692726
Loss at iteration [402]: 0.0024160682017729837
Loss at iteration [403]: 0.0024159491355622833
Loss at iteration [404]: 0.002415896641570394
Loss at iteration [405]: 0.002415863698116157
Loss at iteration [406]: 0.0024158349134820924
Loss at iteration [407]: 0.002415667868931064
Loss at iteration [408]: 0.002415667868931064
Loss at iteration [409]: 0.002415608477369805
Loss at iteration [410]: 0.002415518231815899
Loss at iteration [411]: 0.0024154784435862033
Loss at iteration [412]: 0.002415395083616793
Loss at iteration [413]: 0.002415251384721576
Loss at iteration [414]: 0.002414899366995849
Loss at iteration [415]: 0.002413829855670728
Loss at iteration [416]: 0.002413829855670728
Loss at iteration [417]: 0.0024135452184314847
Loss at iteration [418]: 0.0024132660242674677
Loss at iteration [419]: 0.0024126565963828494
Loss at iteration [420]: 0.002412541081591891
Loss at iteration [421]: 0.0024124549991949157
Loss at iteration [422]: 0.002412035446279016
Loss at iteration [423]: 0.002411849385455112
Loss at iteration [424]: 0.002411849385455112
Loss at iteration [425]: 0.00241181579756419
Loss at iteration [426]: 0.002411716153638402
Loss at iteration [427]: 0.002411560983245377
Loss at iteration [428]: 0.00241152811132667
Loss at iteration [429]: 0.002411477304210867
Loss at iteration [430]: 0.00241144296395042
Loss at iteration [431]: 0.0024112906802156234
Loss at iteration [432]: 0.0024112906802156234
Loss at iteration [433]: 0.0024112493442841605
Loss at iteration [434]: 0.0024110524639099845
Loss at iteration [435]: 0.002411009951444154
Loss at iteration [436]: 0.002410974681159751
Loss at iteration [437]: 0.0024106282619657583
Loss at iteration [438]: 0.0024103017540866925
Loss at iteration [439]: 0.0024103017540866925
Loss at iteration [440]: 0.0024101864627881515
Loss at iteration [441]: 0.0024100741339135607
Loss at iteration [442]: 0.0024098394047194606
Loss at iteration [443]: 0.002409755877743774
Loss at iteration [444]: 0.002409579818193433
Loss at iteration [445]: 0.0024094913651849553
Loss at iteration [446]: 0.0024094913651849553
Loss at iteration [447]: 0.002409396304589628
Loss at iteration [448]: 0.0024093068579505495
Loss at iteration [449]: 0.0024091448165223994
Loss at iteration [450]: 0.0024090827581755942
Loss at iteration [451]: 0.002409047956742089
Loss at iteration [452]: 0.002408673464535605
Loss at iteration [453]: 0.0024085649659229018
Loss at iteration [454]: 0.0024085649659229018
Loss at iteration [455]: 0.002408514975123872
Loss at iteration [456]: 0.002408146349107786
Loss at iteration [457]: 0.0024081308731146635
Loss at iteration [458]: 0.002407934266593246
Loss at iteration [459]: 0.0024078394739457634
Loss at iteration [460]: 0.0024077930611819706
Loss at iteration [461]: 0.0024077930611819706
Loss at iteration [462]: 0.0024077598902459917
Loss at iteration [463]: 0.002407664680997895
Loss at iteration [464]: 0.0024073842104666496
Loss at iteration [465]: 0.0024072475339097094
Loss at iteration [466]: 0.0024070312109377564
Loss at iteration [467]: 0.0024069341709456555
Loss at iteration [468]: 0.0024067343690784134
Loss at iteration [469]: 0.0024067343690784134
Loss at iteration [470]: 0.00240668375560737
Loss at iteration [471]: 0.0024065365540075346
Loss at iteration [472]: 0.002406378441151296
Loss at iteration [473]: 0.002406225815396105
Loss at iteration [474]: 0.002406075191011008
Loss at iteration [475]: 0.002406022525840295
Loss at iteration [476]: 0.002405930019973541
Loss at iteration [477]: 0.002405930019973541
Loss at iteration [478]: 0.002405902480330361
Loss at iteration [479]: 0.0024057142148018094
Loss at iteration [480]: 0.0024057017499188016
Loss at iteration [481]: 0.0024055977403341327
Loss at iteration [482]: 0.002405554749305612
Loss at iteration [483]: 0.0024052927053142773
Loss at iteration [484]: 0.0024052927053142773
Loss at iteration [485]: 0.0024051770728572316
Loss at iteration [486]: 0.0024049723415733347
Loss at iteration [487]: 0.002404836811777694
Loss at iteration [488]: 0.002404827756166134
Loss at iteration [489]: 0.0024047561592638335
Loss at iteration [490]: 0.002404643589158358
Loss at iteration [491]: 0.0024045523174843477
Loss at iteration [492]: 0.0024045523174843477
Loss at iteration [493]: 0.0024045037703121497
Loss at iteration [494]: 0.0024043926023241233
Loss at iteration [495]: 0.002404312934073036
Loss at iteration [496]: 0.002404289045509615
Loss at iteration [497]: 0.002404188137848695
Loss at iteration [498]: 0.0024040255017749065
Loss at iteration [499]: 0.0024039788156462343
Loss at iteration [500]: 0.0024039788156462343
Loss at iteration [501]: 0.0024039416866720583
Loss at iteration [502]: 0.0024038675668016894
Loss at iteration [503]: 0.0024037188512564213
Loss at iteration [504]: 0.0024036960294917566
Loss at iteration [505]: 0.0024036478205704628
Loss at iteration [506]: 0.0024033889289429688
Loss at iteration [507]: 0.0024033889289429688
Loss at iteration [508]: 0.0024033509940111224
Loss at iteration [509]: 0.0024032844034422083
Loss at iteration [510]: 0.0024031041441866323
Loss at iteration [511]: 0.0024030697459740706
Loss at iteration [512]: 0.0024029535844351203
Loss at iteration [513]: 0.0024028905784052156
Loss at iteration [514]: 0.0024028218549798437
Loss at iteration [515]: 0.0024028218549798437
Loss at iteration [516]: 0.0024027757752959882
Loss at iteration [517]: 0.0024027488937652325
Loss at iteration [518]: 0.002402640377599067
Loss at iteration [519]: 0.002402591541215441
Loss at iteration [520]: 0.0024025536155844298
Loss at iteration [521]: 0.0024025109220545543
Loss at iteration [522]: 0.0024023950154535086
Loss at iteration [523]: 0.0024023950154535086
Loss at iteration [524]: 0.00240234244766535
Loss at iteration [525]: 0.0024023245941388115
Loss at iteration [526]: 0.002402228193475178
Loss at iteration [527]: 0.002402167148360148
Loss at iteration [528]: 0.0024021367403650774
Loss at iteration [529]: 0.0024020156139653724
Loss at iteration [530]: 0.0024019385813529026
Loss at iteration [531]: 0.0024019385813529026
Loss at iteration [532]: 0.0024019071319195144
Loss at iteration [533]: 0.0024018596474961345
Loss at iteration [534]: 0.0024017983576931396
Loss at iteration [535]: 0.002401769384800163
Loss at iteration [536]: 0.002401725773509167
Loss at iteration [537]: 0.0024017022971467602
Loss at iteration [538]: 0.002401664761893457
Loss at iteration [539]: 0.002401664761893457
Loss at iteration [540]: 0.0024016445570283107
Loss at iteration [541]: 0.002401628588590784
Loss at iteration [542]: 0.0024015915065727983
Loss at iteration [543]: 0.002401441590056781
Loss at iteration [544]: 0.002401362584687775
Loss at iteration [545]: 0.0024010807465035693
Loss at iteration [546]: 0.0024010807465035693
Loss at iteration [547]: 0.002400868988628754
Loss at iteration [548]: 0.0024005727749100768
Loss at iteration [549]: 0.0024003853087694134
Loss at iteration [550]: 0.0024003498052781384
Loss at iteration [551]: 0.0024002818298847427
Loss at iteration [552]: 0.0024002083975227227
Loss at iteration [553]: 0.0024000299430410195
Loss at iteration [554]: 0.0024000299430410195
Loss at iteration [555]: 0.0023999914606536396
Loss at iteration [556]: 0.0023998327113520313
Loss at iteration [557]: 0.0023997871999992372
Loss at iteration [558]: 0.0023997746211911536
Loss at iteration [559]: 0.0023996664670355165
Loss at iteration [560]: 0.0023995250391246267
Loss at iteration [561]: 0.0023994287116467948
Loss at iteration [562]: 0.0023994287116467948
Loss at iteration [563]: 0.002399384852661958
Loss at iteration [564]: 0.002399244981110203
Loss at iteration [565]: 0.0023992018645132276
Loss at iteration [566]: 0.0023991593150283104
Loss at iteration [567]: 0.002399085544206634
Loss at iteration [568]: 0.0023990526979804487
Loss at iteration [569]: 0.0023990228048427647
Loss at iteration [570]: 0.0023990228048427647
Loss at iteration [571]: 0.0023990056336472684
Loss at iteration [572]: 0.0023989722682211914
Loss at iteration [573]: 0.0023988989820695875
Loss at iteration [574]: 0.0023988710609157146
Loss at iteration [575]: 0.002398840358239288
Loss at iteration [576]: 0.002398785314883651
Loss at iteration [577]: 0.0023986275808945286
Loss at iteration [578]: 0.0023986275808945286
Loss at iteration [579]: 0.002398579477001058
Loss at iteration [580]: 0.0023984836951590933
Loss at iteration [581]: 0.0023984643194717497
Loss at iteration [582]: 0.0023983948360847373
Loss at iteration [583]: 0.002398331945831518
Loss at iteration [584]: 0.00239825929608491
Loss at iteration [585]: 0.00239825929608491
Loss at iteration [586]: 0.002398076773526644
Loss at iteration [587]: 0.0023980425359006493
Loss at iteration [588]: 0.00239791771960104
Loss at iteration [589]: 0.002397831937285948
Loss at iteration [590]: 0.0023978181238180855
Loss at iteration [591]: 0.0023978011198070373
Loss at iteration [592]: 0.0023978011198070373
Loss at iteration [593]: 0.0023977880728026607
Loss at iteration [594]: 0.00239771207591288
Loss at iteration [595]: 0.002397583645442943
Loss at iteration [596]: 0.002397552644563312
Loss at iteration [597]: 0.002397268910136262
Loss at iteration [598]: 0.0023956158470942344
Loss at iteration [599]: 0.0023956158470942344
Loss at iteration [600]: 0.00239494312621967
Loss at iteration [601]: 0.00239483046863996
Loss at iteration [602]: 0.0023942723580741413
Loss at iteration [603]: 0.0023941867716480492
Loss at iteration [604]: 0.0023940238004628535
Loss at iteration [605]: 0.00239298814622125
Loss at iteration [606]: 0.002392884744169331
Loss at iteration [607]: 0.002392884744169331
Loss at iteration [608]: 0.002392801222013511
Loss at iteration [609]: 0.002392128725211877
Loss at iteration [610]: 0.002391824158040624
Loss at iteration [611]: 0.0023917750739801172
Loss at iteration [612]: 0.0023914959729174072
Loss at iteration [613]: 0.0023914336338771497
Loss at iteration [614]: 0.00239091115930171
Loss at iteration [615]: 0.00239091115930171
Loss at iteration [616]: 0.002390792605300998
Loss at iteration [617]: 0.002390722814687051
Loss at iteration [618]: 0.0023905159249086104
Loss at iteration [619]: 0.0023904956222822586
Loss at iteration [620]: 0.002390415940675142
Loss at iteration [621]: 0.0023901238027807968
Loss at iteration [622]: 0.002390064112336452
Loss at iteration [623]: 0.002390064112336452
Loss at iteration [624]: 0.0023900293902184605
Loss at iteration [625]: 0.002389974549622375
Loss at iteration [626]: 0.0023898777511952517
Loss at iteration [627]: 0.0023898728671832574
Loss at iteration [628]: 0.002389834868125893
Loss at iteration [629]: 0.002389692447367707
Loss at iteration [630]: 0.002389534027150999
Loss at iteration [631]: 0.002389534027150999
Loss at iteration [632]: 0.0023894952213549237
Loss at iteration [633]: 0.0023894068092362554
Loss at iteration [634]: 0.0023892972399898677
Loss at iteration [635]: 0.002389253023346737
Loss at iteration [636]: 0.002389209001248484
Loss at iteration [637]: 0.0023891407524110524
Loss at iteration [638]: 0.0023891407524110524
Loss at iteration [639]: 0.0023891177970230375
Loss at iteration [640]: 0.0023890717072745505
Loss at iteration [641]: 0.002388784044805965
Loss at iteration [642]: 0.002388747784606798
Loss at iteration [643]: 0.0023885439883288596
Loss at iteration [644]: 0.0023885009633872693
Loss at iteration [645]: 0.0023883937078483224
Loss at iteration [646]: 0.0023883937078483224
Loss at iteration [647]: 0.0023883537350493952
Loss at iteration [648]: 0.0023883126317375404
Loss at iteration [649]: 0.00238820819423047
Loss at iteration [650]: 0.002388173991585478
Loss at iteration [651]: 0.0023881426118715038
Loss at iteration [652]: 0.002388110095408558
Loss at iteration [653]: 0.002387974573108813
Loss at iteration [654]: 0.002387974573108813
Loss at iteration [655]: 0.002387932369368537
Loss at iteration [656]: 0.0023879143362877464
Loss at iteration [657]: 0.002387828919155271
Loss at iteration [658]: 0.002387816958712988
Loss at iteration [659]: 0.0023877959854824026
Loss at iteration [660]: 0.0023877195521683496
Loss at iteration [661]: 0.0023876469842546008
Loss at iteration [662]: 0.0023876469842546008
Loss at iteration [663]: 0.002387592094835281
Loss at iteration [664]: 0.002387560353278921
Loss at iteration [665]: 0.0023874114521547024
Loss at iteration [666]: 0.002387378386428172
Loss at iteration [667]: 0.0023872141910355237
Loss at iteration [668]: 0.002387056129925436
Loss at iteration [669]: 0.002387056129925436
Loss at iteration [670]: 0.0023869969730386416
Loss at iteration [671]: 0.0023868912839956993
Loss at iteration [672]: 0.0023868113431351253
Loss at iteration [673]: 0.0023867544810753194
Loss at iteration [674]: 0.002386714887360211
Loss at iteration [675]: 0.0023864939494864894
Loss at iteration [676]: 0.0023864939494864894
Loss at iteration [677]: 0.0023864707239278486
Loss at iteration [678]: 0.0023864199339950973
Loss at iteration [679]: 0.002386301721238085
Loss at iteration [680]: 0.0023862420621518316
Loss at iteration [681]: 0.0023859297181283083
Loss at iteration [682]: 0.0023859009572915826
Loss at iteration [683]: 0.002385840007582168
Loss at iteration [684]: 0.002385840007582168
Loss at iteration [685]: 0.002385801073964875
Loss at iteration [686]: 0.0023857864718007772
Loss at iteration [687]: 0.002385748422494081
Loss at iteration [688]: 0.0023857093025143115
Loss at iteration [689]: 0.0023856135457052956
Loss at iteration [690]: 0.0023855723248021993
Loss at iteration [691]: 0.0023854310366392857
Loss at iteration [692]: 0.0023854310366392857
Loss at iteration [693]: 0.0023853701728286767
Loss at iteration [694]: 0.002385352546751528
Loss at iteration [695]: 0.0023853055382259828
Loss at iteration [696]: 0.002385202974081121
Loss at iteration [697]: 0.002385142677362949
Loss at iteration [698]: 0.002384836212871466
Loss at iteration [699]: 0.002384836212871466
Loss at iteration [700]: 0.002384737397734238
Loss at iteration [701]: 0.002384677671121026
Loss at iteration [702]: 0.0023840949505146273
Loss at iteration [703]: 0.0023839836109448534
Loss at iteration [704]: 0.0023837523020507853
Loss at iteration [705]: 0.0023836765732685333
Loss at iteration [706]: 0.002383478490798947
Loss at iteration [707]: 0.002383478490798947
Loss at iteration [708]: 0.002383366353251419
Loss at iteration [709]: 0.0023833244452054255
Loss at iteration [710]: 0.0023827943957446473
Loss at iteration [711]: 0.0023827363909374234
Loss at iteration [712]: 0.0023826030344690657
Loss at iteration [713]: 0.0023824411276502115
Loss at iteration [714]: 0.002382401868012028
Loss at iteration [715]: 0.002382401868012028
Loss at iteration [716]: 0.002382381533405626
Loss at iteration [717]: 0.0023821323184264193
Loss at iteration [718]: 0.0023821186211055636
Loss at iteration [719]: 0.0023818874417925007
Loss at iteration [720]: 0.0023818433501850184
Loss at iteration [721]: 0.002381637782041802
Loss at iteration [722]: 0.002381637782041802
Loss at iteration [723]: 0.0023815770811658273
Loss at iteration [724]: 0.0023814093053346826
Loss at iteration [725]: 0.002381355005540719
Loss at iteration [726]: 0.0023813492165079327
Loss at iteration [727]: 0.002381289183495928
Loss at iteration [728]: 0.002381231284225336
Loss at iteration [729]: 0.0023812010910977217
Loss at iteration [730]: 0.0023812010910977217
Loss at iteration [731]: 0.0023811816222159593
Loss at iteration [732]: 0.0023811345406945564
Loss at iteration [733]: 0.002381085482339451
Loss at iteration [734]: 0.002381061805686179
Loss at iteration [735]: 0.0023808833843810256
Loss at iteration [736]: 0.0023808165655420654
Loss at iteration [737]: 0.0023808165655420654
Loss at iteration [738]: 0.0023807916780220813
Loss at iteration [739]: 0.0023807201926191327
Loss at iteration [740]: 0.00238061635083011
Loss at iteration [741]: 0.0023805818732800734
Loss at iteration [742]: 0.0023805466397414324
Loss at iteration [743]: 0.0023805466397414324
Loss at iteration [744]: 0.002380525031668523
Loss at iteration [745]: 0.002380491935202854
Loss at iteration [746]: 0.002380420873721485
Loss at iteration [747]: 0.0023803917874331447
Loss at iteration [748]: 0.0023803378439737604
Loss at iteration [749]: 0.0023802240385264063
Loss at iteration [750]: 0.002380197890733498
Loss at iteration [751]: 0.002380197890733498
Loss at iteration [752]: 0.0023801741351141647
Loss at iteration [753]: 0.002380076915000576
Loss at iteration [754]: 0.0023800542650509546
Loss at iteration [755]: 0.002380040730449277
Loss at iteration [756]: 0.0023799992556938513
Loss at iteration [757]: 0.0023799163101649455
Loss at iteration [758]: 0.0023799163101649455
Loss at iteration [759]: 0.0023798900521364605
Loss at iteration [760]: 0.0023798223180640514
Loss at iteration [761]: 0.0023797858129028515
Loss at iteration [762]: 0.0023797544685062733
Loss at iteration [763]: 0.0023796818022370435
Loss at iteration [764]: 0.0023793384046627004
Loss at iteration [765]: 0.0023793384046627004
Loss at iteration [766]: 0.0023792491790040693
Loss at iteration [767]: 0.0023791373421537276
Loss at iteration [768]: 0.0023788990582398776
Loss at iteration [769]: 0.0023788645610733215
Loss at iteration [770]: 0.0023787677432611436
Loss at iteration [771]: 0.0023784258600728567
Loss at iteration [772]: 0.0023783685856248503
Loss at iteration [773]: 0.0023783685856248503
Loss at iteration [774]: 0.0023783489141483093
Loss at iteration [775]: 0.002378310439478043
Loss at iteration [776]: 0.002378202816718172
Loss at iteration [777]: 0.0023781335766385778
Loss at iteration [778]: 0.002378001753774779
Loss at iteration [779]: 0.002377949229676862
Loss at iteration [780]: 0.0023779081614010906
Loss at iteration [781]: 0.0023779081614010906
Loss at iteration [782]: 0.002377871162262484
Loss at iteration [783]: 0.0023777394452974576
Loss at iteration [784]: 0.0023776554035878435
Loss at iteration [785]: 0.0023776455800207075
Loss at iteration [786]: 0.0023776162842629746
Loss at iteration [787]: 0.0023774640738790593
Loss at iteration [788]: 0.0023774640738790593
Loss at iteration [789]: 0.002377435169902955
Loss at iteration [790]: 0.002377392037194724
Loss at iteration [791]: 0.0023772621876873967
Loss at iteration [792]: 0.002377234424623444
Loss at iteration [793]: 0.0023771678399902765
Loss at iteration [794]: 0.002377014607813779
Loss at iteration [795]: 0.002376904824179472
Loss at iteration [796]: 0.002376904824179472
Loss at iteration [797]: 0.0023768459178306013
Loss at iteration [798]: 0.002376787696908736
Loss at iteration [799]: 0.0023766487722516913
Loss at iteration [800]: 0.002376626130160245
Loss at iteration [801]: 0.002376596404366941
Loss at iteration [802]: 0.002376510458395998
Loss at iteration [803]: 0.0023764920983029877
Loss at iteration [804]: 0.0023764920983029877
Loss at iteration [805]: 0.0023764777547330396
Loss at iteration [806]: 0.002376413193577506
Loss at iteration [807]: 0.0023763795127976837
Loss at iteration [808]: 0.002376336696541817
Loss at iteration [809]: 0.0023761170411672333
Loss at iteration [810]: 0.002376025646762518
Loss at iteration [811]: 0.002376025646762518
Loss at iteration [812]: 0.002375996210900485
Loss at iteration [813]: 0.0023759633701904855
Loss at iteration [814]: 0.0023759048180668333
Loss at iteration [815]: 0.0023758087470349
Loss at iteration [816]: 0.002375749761723385
Loss at iteration [817]: 0.0023757168359243006
Loss at iteration [818]: 0.002375614617304336
Loss at iteration [819]: 0.002375614617304336
Loss at iteration [820]: 0.0023755610149073813
Loss at iteration [821]: 0.0023755063108374668
Loss at iteration [822]: 0.0023753211816011977
Loss at iteration [823]: 0.00237531190374764
Loss at iteration [824]: 0.0023751750667519215
Loss at iteration [825]: 0.002375028152546185
Loss at iteration [826]: 0.0023750081140073904
Loss at iteration [827]: 0.0023750081140073904
Loss at iteration [828]: 0.0023749907022023588
Loss at iteration [829]: 0.002374802687164534
Loss at iteration [830]: 0.002374774487486157
Loss at iteration [831]: 0.0023747424606203335
Loss at iteration [832]: 0.0023745898243900704
Loss at iteration [833]: 0.002374526252590674
Loss at iteration [834]: 0.002374526252590674
Loss at iteration [835]: 0.002374500183282668
Loss at iteration [836]: 0.0023744550954280835
Loss at iteration [837]: 0.0023742920003367767
Loss at iteration [838]: 0.0023742592733411467
Loss at iteration [839]: 0.0023742060113892973
Loss at iteration [840]: 0.0023741808382564266
Loss at iteration [841]: 0.0023741209522549897
Loss at iteration [842]: 0.0023741209522549897
Loss at iteration [843]: 0.002374076018490666
Loss at iteration [844]: 0.0023740466594383844
Loss at iteration [845]: 0.0023739892520940453
Loss at iteration [846]: 0.0023739471667822924
Loss at iteration [847]: 0.002373904145173252
Loss at iteration [848]: 0.0023736562135077983
Loss at iteration [849]: 0.0023735185145832265
Loss at iteration [850]: 0.0023735185145832265
Loss at iteration [851]: 0.002373445298670804
Loss at iteration [852]: 0.002373309767461694
Loss at iteration [853]: 0.002372720623012257
Loss at iteration [854]: 0.0023726885109142696
Loss at iteration [855]: 0.002372565681802117
Loss at iteration [856]: 0.002372412725950341
Loss at iteration [857]: 0.002372412725950341
Loss at iteration [858]: 0.002372392955662502
Loss at iteration [859]: 0.002372347287542239
Loss at iteration [860]: 0.0023722833133841487
Loss at iteration [861]: 0.002372259105655314
Loss at iteration [862]: 0.0023722238900197087
Loss at iteration [863]: 0.0023721723597951248
Loss at iteration [864]: 0.0023721280950587216
