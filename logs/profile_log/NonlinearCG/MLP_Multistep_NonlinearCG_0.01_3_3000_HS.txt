Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :HS
Total number of function evaluations  : 3031
Total number of iterations            : 694
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 8.48536205291748
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 50.28677279075987%
Percentage of parameters < 1e-7       : 50.28677279075987%
Percentage of parameters < 1e-6       : 50.28825866014205%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.257244829896874
Loss at iteration [2]: 1.2515215472804604
Loss at iteration [3]: 1.248578002425746
Loss at iteration [4]: 1.2446822955866705
Loss at iteration [5]: 1.2287269475605287
Loss at iteration [6]: 1.2082375081820733
Loss at iteration [7]: 1.1700657449434424
Loss at iteration [8]: 1.1584223234023592
Loss at iteration [9]: 1.150455727628999
Loss at iteration [10]: 1.150455727628999
Loss at iteration [11]: 1.1469294526297102
Loss at iteration [12]: 1.1261888770002608
Loss at iteration [13]: 1.1092518226842256
Loss at iteration [14]: 1.1030500385025281
Loss at iteration [15]: 1.095558522353806
Loss at iteration [16]: 1.0896151430332262
Loss at iteration [17]: 1.0798260064150984
Loss at iteration [18]: 1.0752804189741902
Loss at iteration [19]: 1.0631814198584746
Loss at iteration [20]: 1.0605227611058279
Loss at iteration [21]: 1.0378774717279913
Loss at iteration [22]: 1.0378774717279913
Loss at iteration [23]: 1.031990477600037
Loss at iteration [24]: 1.024206503774317
Loss at iteration [25]: 1.0159122930494262
Loss at iteration [26]: 1.013167762148178
Loss at iteration [27]: 1.0080994234207876
Loss at iteration [28]: 1.0052442010295082
Loss at iteration [29]: 1.0016095957436022
Loss at iteration [30]: 0.998531410136957
Loss at iteration [31]: 0.9964362092011373
Loss at iteration [32]: 0.9947491507693836
Loss at iteration [33]: 0.9933367324048985
Loss at iteration [34]: 0.9911142734302028
Loss at iteration [35]: 0.9882226291460591
Loss at iteration [36]: 0.9882226291460591
Loss at iteration [37]: 0.9852217561342407
Loss at iteration [38]: 0.9835758607290365
Loss at iteration [39]: 0.9816869671641676
Loss at iteration [40]: 0.9807431182475277
Loss at iteration [41]: 0.9785486027366815
Loss at iteration [42]: 0.9762245735484352
Loss at iteration [43]: 0.9621509860695311
Loss at iteration [44]: 0.9583804142391817
Loss at iteration [45]: 0.9375113073617519
Loss at iteration [46]: 0.932357692391689
Loss at iteration [47]: 0.9172598533025658
Loss at iteration [48]: 0.9118673106015667
Loss at iteration [49]: 0.9118673106015667
Loss at iteration [50]: 0.908746387011844
Loss at iteration [51]: 0.9044309025339602
Loss at iteration [52]: 0.900203544005607
Loss at iteration [53]: 0.8948683558301611
Loss at iteration [54]: 0.8939480317707491
Loss at iteration [55]: 0.8922669540504461
Loss at iteration [56]: 0.8910990481100648
Loss at iteration [57]: 0.8885745546277807
Loss at iteration [58]: 0.8874262804864252
Loss at iteration [59]: 0.8846381014204125
Loss at iteration [60]: 0.8830883333989564
Loss at iteration [61]: 0.8811634808892529
Loss at iteration [62]: 0.8801361233798395
Loss at iteration [63]: 0.8781298684089163
Loss at iteration [64]: 0.8781298684089163
Loss at iteration [65]: 0.8773251581288692
Loss at iteration [66]: 0.8767523999972887
Loss at iteration [67]: 0.8756688781105518
Loss at iteration [68]: 0.8733706207496974
Loss at iteration [69]: 0.8711369317524618
Loss at iteration [70]: 0.8621333372404124
Loss at iteration [71]: 0.857427978539978
Loss at iteration [72]: 0.8500938051637925
Loss at iteration [73]: 0.8431971322835687
Loss at iteration [74]: 0.840034207627946
Loss at iteration [75]: 0.8376642949311668
Loss at iteration [76]: 0.8337021157661098
Loss at iteration [77]: 0.8296439783862698
Loss at iteration [78]: 0.8287291793158554
Loss at iteration [79]: 0.8263728163208867
Loss at iteration [80]: 0.8263728163208867
Loss at iteration [81]: 0.8240800007984175
Loss at iteration [82]: 0.8226404365503092
Loss at iteration [83]: 0.822147666514421
Loss at iteration [84]: 0.8201974553611556
Loss at iteration [85]: 0.8193614795036567
Loss at iteration [86]: 0.8182520674476497
Loss at iteration [87]: 0.817430095691898
Loss at iteration [88]: 0.8164390885066897
Loss at iteration [89]: 0.8157590536876662
Loss at iteration [90]: 0.8148342580434812
Loss at iteration [91]: 0.8133818598556961
Loss at iteration [92]: 0.8121667786827566
Loss at iteration [93]: 0.810417397432846
Loss at iteration [94]: 0.8085022257822971
Loss at iteration [95]: 0.807708604166903
Loss at iteration [96]: 0.8061569559236735
Loss at iteration [97]: 0.8061569559236735
Loss at iteration [98]: 0.805612461473347
Loss at iteration [99]: 0.8050013700922284
Loss at iteration [100]: 0.8045582989793615
Loss at iteration [101]: 0.8040191675629159
Loss at iteration [102]: 0.8035655560195751
Loss at iteration [103]: 0.8028018849217858
Loss at iteration [104]: 0.8018468757894491
Loss at iteration [105]: 0.8013742117582551
Loss at iteration [106]: 0.8008842947558361
Loss at iteration [107]: 0.8002632594911931
Loss at iteration [108]: 0.7993953136803046
Loss at iteration [109]: 0.7978393940797761
Loss at iteration [110]: 0.7967854470676929
Loss at iteration [111]: 0.7957442493753042
Loss at iteration [112]: 0.7944200153982459
Loss at iteration [113]: 0.792165267123537
Loss at iteration [114]: 0.7914547811259343
Loss at iteration [115]: 0.7883755626005429
Loss at iteration [116]: 0.7883755626005429
Loss at iteration [117]: 0.7872368361412314
Loss at iteration [118]: 0.7850106695161077
Loss at iteration [119]: 0.7833666374087366
Loss at iteration [120]: 0.7822700278477704
Loss at iteration [121]: 0.7819608534393735
Loss at iteration [122]: 0.7815411725223261
Loss at iteration [123]: 0.78096434834538
Loss at iteration [124]: 0.780313229994234
Loss at iteration [125]: 0.7798984491274384
Loss at iteration [126]: 0.7794940421846269
Loss at iteration [127]: 0.7790638065707978
Loss at iteration [128]: 0.7785046800165818
Loss at iteration [129]: 0.77784212160419
Loss at iteration [130]: 0.7774311025593684
Loss at iteration [131]: 0.7769219775665914
Loss at iteration [132]: 0.7761651890437733
Loss at iteration [133]: 0.7751230209723491
Loss at iteration [134]: 0.7751230209723491
Loss at iteration [135]: 0.7746136320426998
Loss at iteration [136]: 0.7741147305500049
Loss at iteration [137]: 0.7738541565755528
Loss at iteration [138]: 0.7735643857515996
Loss at iteration [139]: 0.7733537974046328
Loss at iteration [140]: 0.7730187046359818
Loss at iteration [141]: 0.7727058126353514
Loss at iteration [142]: 0.772397162194711
Loss at iteration [143]: 0.7720097075984775
Loss at iteration [144]: 0.7714730470363919
Loss at iteration [145]: 0.7709908118084305
Loss at iteration [146]: 0.7707156575982428
Loss at iteration [147]: 0.7703195783293176
Loss at iteration [148]: 0.7697792375983665
Loss at iteration [149]: 0.7689031060275122
Loss at iteration [150]: 0.7678476011600759
Loss at iteration [151]: 0.7656073575001606
Loss at iteration [152]: 0.7629074572748212
Loss at iteration [153]: 0.7629074572748212
Loss at iteration [154]: 0.7619521813803494
Loss at iteration [155]: 0.760314606911466
Loss at iteration [156]: 0.7594765802032347
Loss at iteration [157]: 0.7585135773716515
Loss at iteration [158]: 0.7571951857184291
Loss at iteration [159]: 0.7568043830018637
Loss at iteration [160]: 0.7561290506566983
Loss at iteration [161]: 0.7558435204823216
Loss at iteration [162]: 0.7552815826613455
Loss at iteration [163]: 0.7551250332299108
Loss at iteration [164]: 0.7547053441080761
Loss at iteration [165]: 0.7543951223661168
Loss at iteration [166]: 0.7542500900281892
Loss at iteration [167]: 0.7540052489237701
Loss at iteration [168]: 0.7540052489237701
Loss at iteration [169]: 0.753833690714254
Loss at iteration [170]: 0.7536807520957528
Loss at iteration [171]: 0.7535866768920387
Loss at iteration [172]: 0.7534868749276433
Loss at iteration [173]: 0.7533253645617981
Loss at iteration [174]: 0.7531893402395534
Loss at iteration [175]: 0.7530990930770597
Loss at iteration [176]: 0.7528646636411598
Loss at iteration [177]: 0.7526933043880062
Loss at iteration [178]: 0.7523178222444825
Loss at iteration [179]: 0.7515883736103566
Loss at iteration [180]: 0.7513524811874448
Loss at iteration [181]: 0.7509413723661884
Loss at iteration [182]: 0.7502757531286491
Loss at iteration [183]: 0.7496851498541341
Loss at iteration [184]: 0.7496851498541341
Loss at iteration [185]: 0.749458454473657
Loss at iteration [186]: 0.7492717106662858
Loss at iteration [187]: 0.7491115561600017
Loss at iteration [188]: 0.7489301985407676
Loss at iteration [189]: 0.7488674839992043
Loss at iteration [190]: 0.748669978938724
Loss at iteration [191]: 0.7485657686667772
Loss at iteration [192]: 0.7484287736413803
Loss at iteration [193]: 0.7482848044608971
Loss at iteration [194]: 0.7481773850917182
Loss at iteration [195]: 0.748048479595863
Loss at iteration [196]: 0.7478914664848187
Loss at iteration [197]: 0.7478914664848187
Loss at iteration [198]: 0.7477456635490307
Loss at iteration [199]: 0.7476974513073671
Loss at iteration [200]: 0.7476698761830384
Loss at iteration [201]: 0.7476099282124067
Loss at iteration [202]: 0.7475766421485891
Loss at iteration [203]: 0.7475223414917453
Loss at iteration [204]: 0.7474244157434878
Loss at iteration [205]: 0.7473459439714576
Loss at iteration [206]: 0.7473298709549604
Loss at iteration [207]: 0.7471777692659017
Loss at iteration [208]: 0.7470141680627925
Loss at iteration [209]: 0.7464946455349062
Loss at iteration [210]: 0.7459886737263807
Loss at iteration [211]: 0.7449440701441312
Loss at iteration [212]: 0.7449440701441312
Loss at iteration [213]: 0.7444988442776045
Loss at iteration [214]: 0.7442640579960821
Loss at iteration [215]: 0.7439866262535038
Loss at iteration [216]: 0.7437466122455634
Loss at iteration [217]: 0.743521125681481
Loss at iteration [218]: 0.7432989627750359
Loss at iteration [219]: 0.7431045113317019
Loss at iteration [220]: 0.7430672652216067
Loss at iteration [221]: 0.7429456239758266
Loss at iteration [222]: 0.7426347959040568
Loss at iteration [223]: 0.7423465415336388
Loss at iteration [224]: 0.7419590553474255
Loss at iteration [225]: 0.7417367833882957
Loss at iteration [226]: 0.7413434278476213
Loss at iteration [227]: 0.7411608385273951
Loss at iteration [228]: 0.7411608385273951
Loss at iteration [229]: 0.7409161744046789
Loss at iteration [230]: 0.7407592957578599
Loss at iteration [231]: 0.7406263246873486
Loss at iteration [232]: 0.7405128641698092
Loss at iteration [233]: 0.7404146917326286
Loss at iteration [234]: 0.7403118209732047
Loss at iteration [235]: 0.7401954105520385
Loss at iteration [236]: 0.7400939469991906
Loss at iteration [237]: 0.7400395917554462
Loss at iteration [238]: 0.7399733545052114
Loss at iteration [239]: 0.7399066914866503
Loss at iteration [240]: 0.7396737146902389
Loss at iteration [241]: 0.7386219306398178
Loss at iteration [242]: 0.7380348628485353
Loss at iteration [243]: 0.7380348628485353
Loss at iteration [244]: 0.7376799695525477
Loss at iteration [245]: 0.7375341746878319
Loss at iteration [246]: 0.7374152663536199
Loss at iteration [247]: 0.7373302639526612
Loss at iteration [248]: 0.7372108025873298
Loss at iteration [249]: 0.737105665554316
Loss at iteration [250]: 0.7369440939427175
Loss at iteration [251]: 0.736824206916157
Loss at iteration [252]: 0.73675144832898
Loss at iteration [253]: 0.7366795524343647
Loss at iteration [254]: 0.7365415724686659
Loss at iteration [255]: 0.7364492636619463
Loss at iteration [256]: 0.7364492636619463
Loss at iteration [257]: 0.7363341081351531
Loss at iteration [258]: 0.7363163679761563
Loss at iteration [259]: 0.736280137030346
Loss at iteration [260]: 0.7362486808592608
Loss at iteration [261]: 0.7362148198573268
Loss at iteration [262]: 0.7361989628325774
Loss at iteration [263]: 0.7361849428793
Loss at iteration [264]: 0.7361498341557098
Loss at iteration [265]: 0.7360877590665931
Loss at iteration [266]: 0.7359316188686992
Loss at iteration [267]: 0.735748458057433
Loss at iteration [268]: 0.735748458057433
Loss at iteration [269]: 0.7354924680378886
Loss at iteration [270]: 0.7353719600407739
Loss at iteration [271]: 0.7352786959326031
Loss at iteration [272]: 0.7352492099968893
Loss at iteration [273]: 0.7352107765310096
Loss at iteration [274]: 0.735120637437597
Loss at iteration [275]: 0.7350247392821047
Loss at iteration [276]: 0.7349321438513526
Loss at iteration [277]: 0.7348464906717791
Loss at iteration [278]: 0.7347804349493934
Loss at iteration [279]: 0.7346990697710583
Loss at iteration [280]: 0.7346190214909903
Loss at iteration [281]: 0.7346190214909903
Loss at iteration [282]: 0.7344986893741235
Loss at iteration [283]: 0.7344441538021966
Loss at iteration [284]: 0.7343995915449582
Loss at iteration [285]: 0.734369603319825
Loss at iteration [286]: 0.7343233642803076
Loss at iteration [287]: 0.7343060248363328
Loss at iteration [288]: 0.7342688446719449
Loss at iteration [289]: 0.7342329169837872
Loss at iteration [290]: 0.7341856064027766
Loss at iteration [291]: 0.7341651348835103
Loss at iteration [292]: 0.7341651348835103
Loss at iteration [293]: 0.734114455992133
Loss at iteration [294]: 0.7340971967378258
Loss at iteration [295]: 0.7340828231060821
Loss at iteration [296]: 0.7340701712995046
Loss at iteration [297]: 0.7340599251468989
Loss at iteration [298]: 0.7340451280159288
Loss at iteration [299]: 0.734029645099543
Loss at iteration [300]: 0.7340125268139294
Loss at iteration [301]: 0.7340125268139294
Loss at iteration [302]: 0.7339934274239296
Loss at iteration [303]: 0.7339866352857599
Loss at iteration [304]: 0.733978105695362
Loss at iteration [305]: 0.7339707725262583
Loss at iteration [306]: 0.7339692222107312
Loss at iteration [307]: 0.7339584584153553
Loss at iteration [308]: 0.7339511584715698
Loss at iteration [309]: 0.7339414654898915
Loss at iteration [310]: 0.7339414654898915
Loss at iteration [311]: 0.7339217384751549
Loss at iteration [312]: 0.7339124561404494
Loss at iteration [313]: 0.7339109566574095
Loss at iteration [314]: 0.7339009933789045
Loss at iteration [315]: 0.7338945636943524
Loss at iteration [316]: 0.7338848438361176
Loss at iteration [317]: 0.7338770613375747
Loss at iteration [318]: 0.7338593430751086
Loss at iteration [319]: 0.7338593430751086
Loss at iteration [320]: 0.7338388307728918
Loss at iteration [321]: 0.7338254346817306
Loss at iteration [322]: 0.7338164170891952
Loss at iteration [323]: 0.733814210790616
Loss at iteration [324]: 0.7338096848997957
Loss at iteration [325]: 0.7337987179572656
Loss at iteration [326]: 0.733787261020381
Loss at iteration [327]: 0.7337695584348483
Loss at iteration [328]: 0.7337695584348483
Loss at iteration [329]: 0.733748084521256
Loss at iteration [330]: 0.7337419182580831
Loss at iteration [331]: 0.7337371043034411
Loss at iteration [332]: 0.7337354348253413
Loss at iteration [333]: 0.7337285900769945
Loss at iteration [334]: 0.7337158189956368
Loss at iteration [335]: 0.7337141953873055
Loss at iteration [336]: 0.7336979276827076
Loss at iteration [337]: 0.7336979276827076
Loss at iteration [338]: 0.733675935878948
Loss at iteration [339]: 0.7336709313078721
Loss at iteration [340]: 0.7336638639694022
Loss at iteration [341]: 0.7336545388635989
Loss at iteration [342]: 0.7336485350192546
Loss at iteration [343]: 0.7336369462254663
Loss at iteration [344]: 0.7336313164418802
Loss at iteration [345]: 0.7336207001880396
Loss at iteration [346]: 0.7336207001880396
Loss at iteration [347]: 0.7336025933483872
Loss at iteration [348]: 0.733596787912282
Loss at iteration [349]: 0.7335937262789504
Loss at iteration [350]: 0.7335904072604845
Loss at iteration [351]: 0.7335845867649886
Loss at iteration [352]: 0.7335778130557208
Loss at iteration [353]: 0.7335658785827418
Loss at iteration [354]: 0.7335481907680652
Loss at iteration [355]: 0.7335481907680652
Loss at iteration [356]: 0.7335214582816221
Loss at iteration [357]: 0.7335093938565391
Loss at iteration [358]: 0.7334991898331517
Loss at iteration [359]: 0.7334904418503064
Loss at iteration [360]: 0.7334861936917665
Loss at iteration [361]: 0.7334827014745001
Loss at iteration [362]: 0.733476326505945
Loss at iteration [363]: 0.7334617492982446
Loss at iteration [364]: 0.7334617492982446
Loss at iteration [365]: 0.7334455456884755
Loss at iteration [366]: 0.7334373016919876
Loss at iteration [367]: 0.733433579698718
Loss at iteration [368]: 0.7334292848029564
Loss at iteration [369]: 0.7334275419714759
Loss at iteration [370]: 0.7334102153497282
Loss at iteration [371]: 0.7334013162202583
Loss at iteration [372]: 0.733381265985126
Loss at iteration [373]: 0.733381265985126
Loss at iteration [374]: 0.7333604936852066
Loss at iteration [375]: 0.7333462735852382
Loss at iteration [376]: 0.7333433925277801
Loss at iteration [377]: 0.7333342837057698
Loss at iteration [378]: 0.7333292862963747
Loss at iteration [379]: 0.7333196958848144
Loss at iteration [380]: 0.7333105441554467
Loss at iteration [381]: 0.7333035712822191
Loss at iteration [382]: 0.7333035712822191
Loss at iteration [383]: 0.7332839505864028
Loss at iteration [384]: 0.7332742834076409
Loss at iteration [385]: 0.7332684058226364
Loss at iteration [386]: 0.7332650363039762
Loss at iteration [387]: 0.733259012333632
Loss at iteration [388]: 0.733245892297233
Loss at iteration [389]: 0.7332437392136456
Loss at iteration [390]: 0.7332345082403798
Loss at iteration [391]: 0.7332345082403798
Loss at iteration [392]: 0.733210615062415
Loss at iteration [393]: 0.7332056164308444
Loss at iteration [394]: 0.7331998818850477
Loss at iteration [395]: 0.7331960069819474
Loss at iteration [396]: 0.7331902761855017
Loss at iteration [397]: 0.7331779353924649
Loss at iteration [398]: 0.7331646458393783
Loss at iteration [399]: 0.7331514952193551
Loss at iteration [400]: 0.7331514952193551
Loss at iteration [401]: 0.7331360423956935
Loss at iteration [402]: 0.7331292393412752
Loss at iteration [403]: 0.7331215318022527
Loss at iteration [404]: 0.733112640065186
Loss at iteration [405]: 0.7331034520825925
Loss at iteration [406]: 0.7330967091773357
Loss at iteration [407]: 0.7330892415000064
Loss at iteration [408]: 0.7330795876195917
Loss at iteration [409]: 0.7330795876195917
Loss at iteration [410]: 0.7330663264276329
Loss at iteration [411]: 0.7330610241403933
Loss at iteration [412]: 0.7330553610187346
Loss at iteration [413]: 0.7330474033455994
Loss at iteration [414]: 0.7330457339938239
Loss at iteration [415]: 0.7330392585019122
Loss at iteration [416]: 0.733027865932729
Loss at iteration [417]: 0.7330124343611335
Loss at iteration [418]: 0.7330124343611335
Loss at iteration [419]: 0.7329978501472689
Loss at iteration [420]: 0.7329890960754427
Loss at iteration [421]: 0.7329880636527673
Loss at iteration [422]: 0.7329803928006247
Loss at iteration [423]: 0.7329745479609574
Loss at iteration [424]: 0.7329660159729222
Loss at iteration [425]: 0.7329462740200637
Loss at iteration [426]: 0.7329342568859427
Loss at iteration [427]: 0.7329342568859427
Loss at iteration [428]: 0.7329096042866832
Loss at iteration [429]: 0.7328994215189605
Loss at iteration [430]: 0.7328926297739847
Loss at iteration [431]: 0.7328821426350999
Loss at iteration [432]: 0.7328796362764132
Loss at iteration [433]: 0.7328722397961936
Loss at iteration [434]: 0.7328686381526069
Loss at iteration [435]: 0.7328575430114433
Loss at iteration [436]: 0.7328575430114433
Loss at iteration [437]: 0.7328420833192555
Loss at iteration [438]: 0.7328332949754036
Loss at iteration [439]: 0.7328297766528182
Loss at iteration [440]: 0.7328272869109477
Loss at iteration [441]: 0.7328215978332474
Loss at iteration [442]: 0.7328109422933945
Loss at iteration [443]: 0.7328045977623735
Loss at iteration [444]: 0.732796304445298
Loss at iteration [445]: 0.732796304445298
Loss at iteration [446]: 0.7327835218332646
Loss at iteration [447]: 0.7327765343728809
Loss at iteration [448]: 0.7327738322692502
Loss at iteration [449]: 0.7327701301311579
Loss at iteration [450]: 0.7327664972407509
Loss at iteration [451]: 0.7327625586704573
Loss at iteration [452]: 0.7327597363698231
Loss at iteration [453]: 0.7327597363698231
Loss at iteration [454]: 0.7327560717590563
Loss at iteration [455]: 0.7327546186916469
Loss at iteration [456]: 0.7327531898487413
Loss at iteration [457]: 0.732751813741396
Loss at iteration [458]: 0.7327500415445817
Loss at iteration [459]: 0.732749767374835
Loss at iteration [460]: 0.732749767374835
Loss at iteration [461]: 0.7327484808710488
Loss at iteration [462]: 0.7327477433658565
Loss at iteration [463]: 0.7327472278084722
Loss at iteration [464]: 0.7327463899599121
Loss at iteration [465]: 0.7327459753154641
Loss at iteration [466]: 0.7327453368558914
Loss at iteration [467]: 0.7327453368558914
Loss at iteration [468]: 0.7327440874242463
Loss at iteration [469]: 0.7327434178572483
Loss at iteration [470]: 0.7327431001887592
Loss at iteration [471]: 0.7327421751919005
Loss at iteration [472]: 0.7327407336683347
Loss at iteration [473]: 0.7327395570660887
Loss at iteration [474]: 0.7327395570660887
Loss at iteration [475]: 0.7327356944412347
Loss at iteration [476]: 0.7327333642579784
Loss at iteration [477]: 0.7327328431724823
Loss at iteration [478]: 0.732732447034167
Loss at iteration [479]: 0.7327312796227669
Loss at iteration [480]: 0.7327283180719402
Loss at iteration [481]: 0.7327283180719402
Loss at iteration [482]: 0.7327252937862188
Loss at iteration [483]: 0.7327247584128447
Loss at iteration [484]: 0.7327236424210706
Loss at iteration [485]: 0.7327234440040287
Loss at iteration [486]: 0.7327227534302201
Loss at iteration [487]: 0.7327216103853382
Loss at iteration [488]: 0.7327216103853382
Loss at iteration [489]: 0.732717220237778
Loss at iteration [490]: 0.7327163921388016
Loss at iteration [491]: 0.7327152246739178
Loss at iteration [492]: 0.7327141494726033
Loss at iteration [493]: 0.7327133733097998
Loss at iteration [494]: 0.7327116535779743
Loss at iteration [495]: 0.7327116535779743
Loss at iteration [496]: 0.7327093520998355
Loss at iteration [497]: 0.732708534077948
Loss at iteration [498]: 0.7327081538336647
Loss at iteration [499]: 0.7327075919903356
Loss at iteration [500]: 0.7327069962777988
Loss at iteration [501]: 0.7327064673993333
Loss at iteration [502]: 0.7327064673993333
Loss at iteration [503]: 0.7327045462880423
Loss at iteration [504]: 0.7327040044800995
Loss at iteration [505]: 0.7327036006285818
Loss at iteration [506]: 0.7327033276206099
Loss at iteration [507]: 0.7327029501569605
Loss at iteration [508]: 0.7327029501569605
Loss at iteration [509]: 0.7327023763389457
Loss at iteration [510]: 0.7327021450995879
Loss at iteration [511]: 0.732702050110333
Loss at iteration [512]: 0.7327018584876103
Loss at iteration [513]: 0.7327016585767391
Loss at iteration [514]: 0.7327016585767391
Loss at iteration [515]: 0.7327014474505846
Loss at iteration [516]: 0.7327013674479375
Loss at iteration [517]: 0.7327013377180083
Loss at iteration [518]: 0.7327012712196
Loss at iteration [519]: 0.7327010144776755
Loss at iteration [520]: 0.7327010144776755
Loss at iteration [521]: 0.7327007858084947
Loss at iteration [522]: 0.7327006541246974
Loss at iteration [523]: 0.7327005985403647
Loss at iteration [524]: 0.7327005583527068
Loss at iteration [525]: 0.7327005193619366
Loss at iteration [526]: 0.7327005193619366
Loss at iteration [527]: 0.7327001989546676
Loss at iteration [528]: 0.7327001902766758
Loss at iteration [529]: 0.7327000923506067
Loss at iteration [530]: 0.7327000421658567
Loss at iteration [531]: 0.7326998881147344
Loss at iteration [532]: 0.7326998881147344
Loss at iteration [533]: 0.7326996094786935
Loss at iteration [534]: 0.7326994989349671
Loss at iteration [535]: 0.732699370853983
Loss at iteration [536]: 0.732699329189725
Loss at iteration [537]: 0.7326991945303103
Loss at iteration [538]: 0.7326991945303103
Loss at iteration [539]: 0.7326991181403149
Loss at iteration [540]: 0.7326990178372416
Loss at iteration [541]: 0.7326989668283541
Loss at iteration [542]: 0.732698822632952
Loss at iteration [543]: 0.7326987136795565
Loss at iteration [544]: 0.7326987136795565
Loss at iteration [545]: 0.7326986696870832
Loss at iteration [546]: 0.7326986087532439
Loss at iteration [547]: 0.732698533617499
Loss at iteration [548]: 0.7326984468303839
Loss at iteration [549]: 0.7326983657390168
Loss at iteration [550]: 0.7326983657390168
Loss at iteration [551]: 0.7326982841548776
Loss at iteration [552]: 0.732698253421838
Loss at iteration [553]: 0.732698225133548
Loss at iteration [554]: 0.7326981890396235
Loss at iteration [555]: 0.7326981890396235
Loss at iteration [556]: 0.7326981587564587
Loss at iteration [557]: 0.7326981473320073
Loss at iteration [558]: 0.7326981353189482
Loss at iteration [559]: 0.732698119985427
Loss at iteration [560]: 0.732698119985427
Loss at iteration [561]: 0.7326980928767551
Loss at iteration [562]: 0.7326980784268091
Loss at iteration [563]: 0.7326980723166441
Loss at iteration [564]: 0.7326980690368542
Loss at iteration [565]: 0.7326980690368542
Loss at iteration [566]: 0.7326980653348322
Loss at iteration [567]: 0.7326980532193625
Loss at iteration [568]: 0.732698037783207
Loss at iteration [569]: 0.7326980267174364
Loss at iteration [570]: 0.7326980267174364
Loss at iteration [571]: 0.7326979874994461
Loss at iteration [572]: 0.7326979768394418
Loss at iteration [573]: 0.7326979631279873
Loss at iteration [574]: 0.7326979443120484
Loss at iteration [575]: 0.7326979443120484
Loss at iteration [576]: 0.7326979352275376
Loss at iteration [577]: 0.7326979323259589
Loss at iteration [578]: 0.7326979217332302
Loss at iteration [579]: 0.7326979196536224
Loss at iteration [580]: 0.7326979196536224
Loss at iteration [581]: 0.7326979049144625
Loss at iteration [582]: 0.7326978996056766
Loss at iteration [583]: 0.7326978981979463
Loss at iteration [584]: 0.7326978943808391
Loss at iteration [585]: 0.7326978943808391
Loss at iteration [586]: 0.7326978883992801
Loss at iteration [587]: 0.732697887785807
Loss at iteration [588]: 0.732697887785807
Loss at iteration [589]: 0.7326978852765552
Loss at iteration [590]: 0.7326978841929745
Loss at iteration [591]: 0.7326978826524808
Loss at iteration [592]: 0.7326978817198384
Loss at iteration [593]: 0.7326978817198384
Loss at iteration [594]: 0.7326978758280982
Loss at iteration [595]: 0.7326978748025914
Loss at iteration [596]: 0.7326978734817425
Loss at iteration [597]: 0.7326978723343612
Loss at iteration [598]: 0.7326978723343612
Loss at iteration [599]: 0.7326978666037482
Loss at iteration [600]: 0.7326978660074458
Loss at iteration [601]: 0.7326978660074458
Loss at iteration [602]: 0.7326978636372808
Loss at iteration [603]: 0.7326978624905769
Loss at iteration [604]: 0.7326978607000593
Loss at iteration [605]: 0.7326978594820329
Loss at iteration [606]: 0.7326978594820329
Loss at iteration [607]: 0.7326978572358245
Loss at iteration [608]: 0.7326978562420382
Loss at iteration [609]: 0.7326978562420382
Loss at iteration [610]: 0.7326978555639565
Loss at iteration [611]: 0.7326978555639565
Loss at iteration [612]: 0.7326978550947445
Loss at iteration [613]: 0.7326978550947445
Loss at iteration [614]: 0.7326978542599346
Loss at iteration [615]: 0.7326978542599346
Loss at iteration [616]: 0.7326978538894935
Loss at iteration [617]: 0.7326978538894935
Loss at iteration [618]: 0.7326978538177228
Loss at iteration [619]: 0.7326978538177228
Loss at iteration [620]: 0.7326978538177228
Loss at iteration [621]: 0.7326978538177228
Loss at iteration [622]: 0.7326978538177228
Loss at iteration [623]: 0.7326978538177228
Loss at iteration [624]: 0.7326978538177228
Loss at iteration [625]: 0.7326978538177228
Loss at iteration [626]: 0.7326978538177228
Loss at iteration [627]: 0.7326978538177228
Loss at iteration [628]: 0.7326978538177228
Loss at iteration [629]: 0.7326978538177228
Loss at iteration [630]: 0.7326978538177228
Loss at iteration [631]: 0.7326978538177228
Loss at iteration [632]: 0.7326978538177228
Loss at iteration [633]: 0.7326978538177228
Loss at iteration [634]: 0.7326978538177228
Loss at iteration [635]: 0.7326978538177228
Loss at iteration [636]: 0.7326978538177228
Loss at iteration [637]: 0.7326978538177228
Loss at iteration [638]: 0.7326978538177228
Loss at iteration [639]: 0.7326978538177228
Loss at iteration [640]: 0.7326978538177228
Loss at iteration [641]: 0.7326978538177228
Loss at iteration [642]: 0.7326978538177228
Loss at iteration [643]: 0.7326978538177228
Loss at iteration [644]: 0.7326978538177228
Loss at iteration [645]: 0.7326978538177228
Loss at iteration [646]: 0.7326978538177228
Loss at iteration [647]: 0.7326978538177228
Loss at iteration [648]: 0.7326978538177228
Loss at iteration [649]: 0.7326978538177228
Loss at iteration [650]: 0.7326978538177228
Loss at iteration [651]: 0.7326978538177228
Loss at iteration [652]: 0.7326978538177228
Loss at iteration [653]: 0.7326978538177228
Loss at iteration [654]: 0.7326978538177228
Loss at iteration [655]: 0.7326978538177228
Loss at iteration [656]: 0.7326978538177228
Loss at iteration [657]: 0.7326978538177228
Loss at iteration [658]: 0.7326978538177228
Loss at iteration [659]: 0.7326978538177228
Loss at iteration [660]: 0.7326978538177228
Loss at iteration [661]: 0.7326978538177228
Loss at iteration [662]: 0.7326978538177228
Loss at iteration [663]: 0.7326978538177228
Loss at iteration [664]: 0.7326978538177228
Loss at iteration [665]: 0.7326978538177228
Loss at iteration [666]: 0.7326978538177228
Loss at iteration [667]: 0.7326978538177228
Loss at iteration [668]: 0.7326978538177228
Loss at iteration [669]: 0.7326978538177228
Loss at iteration [670]: 0.7326978538177228
Loss at iteration [671]: 0.7326978538177228
Loss at iteration [672]: 0.7326978538177228
Loss at iteration [673]: 0.7326978538177228
Loss at iteration [674]: 0.7326978538177228
Loss at iteration [675]: 0.7326978538177228
Loss at iteration [676]: 0.7326978538177228
Loss at iteration [677]: 0.7326978538177228
Loss at iteration [678]: 0.7326978538177228
Loss at iteration [679]: 0.7326978538177228
Loss at iteration [680]: 0.7326978538177228
Loss at iteration [681]: 0.7326978538177228
Loss at iteration [682]: 0.7326978538177228
Loss at iteration [683]: 0.7326978538177228
Loss at iteration [684]: 0.7326978538177228
Loss at iteration [685]: 0.7326978538177228
Loss at iteration [686]: 0.7326978538177228
Loss at iteration [687]: 0.7326978538177228
Loss at iteration [688]: 0.7326978538177228
Loss at iteration [689]: 0.7326978538177228
Loss at iteration [690]: 0.7326978538177228
Loss at iteration [691]: 0.7326978538177228
Loss at iteration [692]: 0.7326978538177228
Loss at iteration [693]: 0.7326978538177228
Loss at iteration [694]: 0.7326978538177228
Loss at iteration [695]: 0.7326978538177228
Loss at iteration [696]: 0.7326978538177228
Loss at iteration [697]: 0.7326978538177228
Loss at iteration [698]: 0.7326978538177228
Loss at iteration [699]: 0.7326978538177228
Loss at iteration [700]: 0.7326978538177228
Loss at iteration [701]: 0.7326978538177228
Loss at iteration [702]: 0.7326978538177228
Loss at iteration [703]: 0.7326978538177228
Loss at iteration [704]: 0.7326978538177228
Loss at iteration [705]: 0.7326978538177228
Loss at iteration [706]: 0.7326978538177228
Loss at iteration [707]: 0.7326978538177228
Loss at iteration [708]: 0.7326978538177228
Loss at iteration [709]: 0.7326978538177228
Loss at iteration [710]: 0.7326978538177228
Loss at iteration [711]: 0.7326978538177228
Loss at iteration [712]: 0.7326978538177228
Loss at iteration [713]: 0.7326978538177228
Loss at iteration [714]: 0.7326978538177228
Loss at iteration [715]: 0.7326978538177228
Loss at iteration [716]: 0.7326978538177228
Loss at iteration [717]: 0.7326978538177228
Loss at iteration [718]: 0.7326978538177228
Loss at iteration [719]: 0.7326978538177228
Loss at iteration [720]: 0.7326978538177228
Loss at iteration [721]: 0.7326978538177228
Loss at iteration [722]: 0.7326978538177228
Loss at iteration [723]: 0.7326978538177228
Loss at iteration [724]: 0.7326978538177228
Loss at iteration [725]: 0.7326978538177228
Loss at iteration [726]: 0.7326978538177228
Loss at iteration [727]: 0.7326978538177228
Loss at iteration [728]: 0.7326978538177228
Loss at iteration [729]: 0.7326978538177228
Loss at iteration [730]: 0.7326978538177228
Loss at iteration [731]: 0.7326978538177228
Loss at iteration [732]: 0.7326978538177228
Loss at iteration [733]: 0.7326978538177228
Loss at iteration [734]: 0.7326978538177228
Loss at iteration [735]: 0.7326978538177228
Loss at iteration [736]: 0.7326978538177228
Loss at iteration [737]: 0.7326978538177228
Loss at iteration [738]: 0.7326978538177228
Loss at iteration [739]: 0.7326978538177228
Loss at iteration [740]: 0.7326978538177228
Loss at iteration [741]: 0.7326978538177228
Loss at iteration [742]: 0.7326978538177228
Loss at iteration [743]: 0.7326978538177228
Loss at iteration [744]: 0.7326978538177228
Loss at iteration [745]: 0.7326978538177228
Loss at iteration [746]: 0.7326978538177228
Loss at iteration [747]: 0.7326978538177228
Loss at iteration [748]: 0.7326978538177228
Loss at iteration [749]: 0.7326978538177228
Loss at iteration [750]: 0.7326978538177228
Loss at iteration [751]: 0.7326978538177228
Loss at iteration [752]: 0.7326978538177228
Loss at iteration [753]: 0.7326978538177228
Loss at iteration [754]: 0.7326978538177228
Loss at iteration [755]: 0.7326978538177228
Loss at iteration [756]: 0.7326978538177228
Loss at iteration [757]: 0.7326978538177228
Loss at iteration [758]: 0.7326978538177228
Loss at iteration [759]: 0.7326978538177228
Loss at iteration [760]: 0.7326978538177228
Loss at iteration [761]: 0.7326978538177228
Loss at iteration [762]: 0.7326978538177228
Loss at iteration [763]: 0.7326978538177228
Loss at iteration [764]: 0.7326978538177228
Loss at iteration [765]: 0.7326978538177228
Loss at iteration [766]: 0.7326978538177228
Loss at iteration [767]: 0.7326978538177228
Loss at iteration [768]: 0.7326978538177228
Loss at iteration [769]: 0.7326978538177228
Loss at iteration [770]: 0.7326978538177228
Loss at iteration [771]: 0.7326978538177228
Loss at iteration [772]: 0.7326978538177228
Loss at iteration [773]: 0.7326978538177228
Loss at iteration [774]: 0.7326978538177228
Loss at iteration [775]: 0.7326978538177228
Loss at iteration [776]: 0.7326978538177228
Loss at iteration [777]: 0.7326978538177228
Loss at iteration [778]: 0.7326978538177228
Loss at iteration [779]: 0.7326978538177228
Loss at iteration [780]: 0.7326978538177228
Loss at iteration [781]: 0.7326978538177228
Loss at iteration [782]: 0.7326978538177228
Loss at iteration [783]: 0.7326978538177228
Loss at iteration [784]: 0.7326978538177228
Loss at iteration [785]: 0.7326978538177228
Loss at iteration [786]: 0.7326978538177228
Loss at iteration [787]: 0.7326978538177228
Loss at iteration [788]: 0.7326978538177228
Loss at iteration [789]: 0.7326978538177228
Loss at iteration [790]: 0.7326978538177228
Loss at iteration [791]: 0.7326978538177228
Loss at iteration [792]: 0.7326978538177228
Loss at iteration [793]: 0.7326978538177228
Loss at iteration [794]: 0.7326978538177228
Loss at iteration [795]: 0.7326978538177228
Loss at iteration [796]: 0.7326978538177228
Loss at iteration [797]: 0.7326978538177228
Loss at iteration [798]: 0.7326978538177228
Loss at iteration [799]: 0.7326978538177228
Loss at iteration [800]: 0.7326978538177228
Loss at iteration [801]: 0.7326978538177228
Loss at iteration [802]: 0.7326978538177228
Loss at iteration [803]: 0.7326978538177228
Loss at iteration [804]: 0.7326978538177228
Loss at iteration [805]: 0.7326978538177228
Loss at iteration [806]: 0.7326978538177228
Loss at iteration [807]: 0.7326978538177228
Loss at iteration [808]: 0.7326978538177228
Loss at iteration [809]: 0.7326978538177228
Loss at iteration [810]: 0.7326978538177228
Loss at iteration [811]: 0.7326978538177228
Loss at iteration [812]: 0.7326978538177228
Loss at iteration [813]: 0.7326978538177228
Loss at iteration [814]: 0.7326978538177228
Loss at iteration [815]: 0.7326978538177228
Loss at iteration [816]: 0.7326978538177228
Loss at iteration [817]: 0.7326978538177228
Loss at iteration [818]: 0.7326978538177228
Loss at iteration [819]: 0.7326978538177228
Loss at iteration [820]: 0.7326978538177228
Loss at iteration [821]: 0.7326978538177228
Loss at iteration [822]: 0.7326978538177228
Loss at iteration [823]: 0.7326978538177228
Loss at iteration [824]: 0.7326978538177228
Loss at iteration [825]: 0.7326978538177228
Loss at iteration [826]: 0.7326978538177228
Loss at iteration [827]: 0.7326978538177228
Loss at iteration [828]: 0.7326978538177228
Loss at iteration [829]: 0.7326978538177228
Loss at iteration [830]: 0.7326978538177228
Loss at iteration [831]: 0.7326978538177228
Loss at iteration [832]: 0.7326978538177228
Loss at iteration [833]: 0.7326978538177228
Loss at iteration [834]: 0.7326978538177228
Loss at iteration [835]: 0.7326978538177228
Loss at iteration [836]: 0.7326978538177228
Loss at iteration [837]: 0.7326978538177228
Loss at iteration [838]: 0.7326978538177228
Loss at iteration [839]: 0.7326978538177228
Loss at iteration [840]: 0.7326978538177228
Loss at iteration [841]: 0.7326978538177228
Loss at iteration [842]: 0.7326978538177228
Loss at iteration [843]: 0.7326978538177228
Loss at iteration [844]: 0.7326978538177228
Loss at iteration [845]: 0.7326978538177228
Loss at iteration [846]: 0.7326978538177228
Loss at iteration [847]: 0.7326978538177228
Loss at iteration [848]: 0.7326978538177228
Loss at iteration [849]: 0.7326978538177228
Loss at iteration [850]: 0.7326978538177228
Loss at iteration [851]: 0.7326978538177228
Loss at iteration [852]: 0.7326978538177228
Loss at iteration [853]: 0.7326978538177228
Loss at iteration [854]: 0.7326978538177228
Loss at iteration [855]: 0.7326978538177228
Loss at iteration [856]: 0.7326978538177228
Loss at iteration [857]: 0.7326978538177228
Loss at iteration [858]: 0.7326978538177228
Loss at iteration [859]: 0.7326978538177228
Loss at iteration [860]: 0.7326978538177228
Loss at iteration [861]: 0.7326978538177228
Loss at iteration [862]: 0.7326978538177228
Loss at iteration [863]: 0.7326978538177228
Loss at iteration [864]: 0.7326978538177228
Loss at iteration [865]: 0.7326978538177228
Loss at iteration [866]: 0.7326978538177228
Loss at iteration [867]: 0.7326978538177228
Loss at iteration [868]: 0.7326978538177228
Loss at iteration [869]: 0.7326978538177228
Loss at iteration [870]: 0.7326978538177228
Loss at iteration [871]: 0.7326978538177228
Loss at iteration [872]: 0.7326978538177228
Loss at iteration [873]: 0.7326978538177228
Loss at iteration [874]: 0.7326978538177228
Loss at iteration [875]: 0.7326978538177228
Loss at iteration [876]: 0.7326978538177228
Loss at iteration [877]: 0.7326978538177228
Loss at iteration [878]: 0.7326978538177228
Loss at iteration [879]: 0.7326978538177228
Loss at iteration [880]: 0.7326978538177228
Loss at iteration [881]: 0.7326978538177228
Loss at iteration [882]: 0.7326978538177228
Loss at iteration [883]: 0.7326978538177228
Loss at iteration [884]: 0.7326978538177228
Loss at iteration [885]: 0.7326978538177228
Loss at iteration [886]: 0.7326978538177228
Loss at iteration [887]: 0.7326978538177228
Loss at iteration [888]: 0.7326978538177228
Loss at iteration [889]: 0.7326978538177228
Loss at iteration [890]: 0.7326978538177228
Loss at iteration [891]: 0.7326978538177228
Loss at iteration [892]: 0.7326978538177228
Loss at iteration [893]: 0.7326978538177228
Loss at iteration [894]: 0.7326978538177228
Loss at iteration [895]: 0.7326978538177228
Loss at iteration [896]: 0.7326978538177228
Loss at iteration [897]: 0.7326978538177228
Loss at iteration [898]: 0.7326978538177228
Loss at iteration [899]: 0.7326978538177228
Loss at iteration [900]: 0.7326978538177228
