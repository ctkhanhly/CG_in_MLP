Model name                            : MLP_Multistep
The number of input features          : 5
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.1
Beta type                             :HS
Total number of function evaluations  : 563
Total number of iterations            : 255
Max number of iterations              : 3000
Number of samples in training data    : 122
Number of samples in tests data       : 52
Total training time                   : 1.2461650371551514
Total number of parameters            : 202302
Percentage of parameters < 1e-9       : 50.10429951260986%
Percentage of parameters < 1e-7       : 50.10429951260986%
Percentage of parameters < 1e-6       : 50.10479382309616%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.5183075564285926
Loss at iteration [2]: 0.5144431781861153
Loss at iteration [3]: 0.5088758057359136
Loss at iteration [4]: 0.5007585338123663
Loss at iteration [5]: 0.47108166105747207
Loss at iteration [6]: 0.4484812105929192
Loss at iteration [7]: 0.4338185429693033
Loss at iteration [8]: 0.4275482680942555
Loss at iteration [9]: 0.42181937439153777
Loss at iteration [10]: 0.41676224040318965
Loss at iteration [11]: 0.4048473363470231
Loss at iteration [12]: 0.39299786749666
Loss at iteration [13]: 0.38267582210843665
Loss at iteration [14]: 0.38267582210843665
Loss at iteration [15]: 0.37383905683434054
Loss at iteration [16]: 0.36084483273060775
Loss at iteration [17]: 0.3333878438791175
Loss at iteration [18]: 0.32984550228842113
Loss at iteration [19]: 0.32199293723876765
Loss at iteration [20]: 0.3057708030714426
Loss at iteration [21]: 0.30465775230949016
Loss at iteration [22]: 0.29662228334043067
Loss at iteration [23]: 0.29314011751756425
Loss at iteration [24]: 0.2862839620775518
Loss at iteration [25]: 0.28562368913010727
Loss at iteration [26]: 0.28395570515764457
Loss at iteration [27]: 0.281065663294199
Loss at iteration [28]: 0.281065663294199
Loss at iteration [29]: 0.28036872859056694
Loss at iteration [30]: 0.27513468210573827
Loss at iteration [31]: 0.2717551601337951
Loss at iteration [32]: 0.2698967786243946
Loss at iteration [33]: 0.26879548590038227
Loss at iteration [34]: 0.2650902836788994
Loss at iteration [35]: 0.26478884674406533
Loss at iteration [36]: 0.262192651631348
Loss at iteration [37]: 0.2612149627496895
Loss at iteration [38]: 0.25970139220887
Loss at iteration [39]: 0.25867439617047
Loss at iteration [40]: 0.25839668998015025
Loss at iteration [41]: 0.2579671582699649
Loss at iteration [42]: 0.25664643863255976
Loss at iteration [43]: 0.25664643863255976
Loss at iteration [44]: 0.2560739811762201
Loss at iteration [45]: 0.25339633376183784
Loss at iteration [46]: 0.25294597988355505
Loss at iteration [47]: 0.249402930647628
Loss at iteration [48]: 0.24890714349820892
Loss at iteration [49]: 0.2477234568663416
Loss at iteration [50]: 0.24741212259166218
Loss at iteration [51]: 0.24515109697640275
Loss at iteration [52]: 0.24496667391058116
Loss at iteration [53]: 0.24313996719835052
Loss at iteration [54]: 0.24294569300938146
Loss at iteration [55]: 0.24103079707933134
Loss at iteration [56]: 0.2407044318598507
Loss at iteration [57]: 0.23979207014584364
Loss at iteration [58]: 0.23979207014584364
Loss at iteration [59]: 0.23961616518850984
Loss at iteration [60]: 0.23809101724394136
Loss at iteration [61]: 0.23802195071202692
Loss at iteration [62]: 0.23722831631656152
Loss at iteration [63]: 0.2371182749086646
Loss at iteration [64]: 0.23586326289520457
Loss at iteration [65]: 0.2356875368965239
Loss at iteration [66]: 0.23525601515904268
Loss at iteration [67]: 0.2350480655492559
Loss at iteration [68]: 0.23447050709721695
Loss at iteration [69]: 0.23424773675956134
Loss at iteration [70]: 0.23189595400751195
Loss at iteration [71]: 0.23189595400751195
Loss at iteration [72]: 0.23160745460390134
Loss at iteration [73]: 0.22597762098193735
Loss at iteration [74]: 0.22571974589498775
Loss at iteration [75]: 0.22397892993306154
Loss at iteration [76]: 0.22382591979297173
Loss at iteration [77]: 0.2230762272444432
Loss at iteration [78]: 0.222809129778193
Loss at iteration [79]: 0.2222625525315064
Loss at iteration [80]: 0.22214355256987336
Loss at iteration [81]: 0.22146533496983922
Loss at iteration [82]: 0.22102998183942665
Loss at iteration [83]: 0.2200011262518335
Loss at iteration [84]: 0.2197837503090625
Loss at iteration [85]: 0.2197837503090625
Loss at iteration [86]: 0.21948848069903915
Loss at iteration [87]: 0.21831822677087773
Loss at iteration [88]: 0.21792748101342813
Loss at iteration [89]: 0.2175743637319267
Loss at iteration [90]: 0.21746035121140594
Loss at iteration [91]: 0.21709213156888452
Loss at iteration [92]: 0.2168092474322149
Loss at iteration [93]: 0.21579127724021446
Loss at iteration [94]: 0.21544219580353205
Loss at iteration [95]: 0.21488179925969084
Loss at iteration [96]: 0.21404331367140214
Loss at iteration [97]: 0.21404331367140214
Loss at iteration [98]: 0.21383247040078554
Loss at iteration [99]: 0.2125759959016301
Loss at iteration [100]: 0.2124054051716692
Loss at iteration [101]: 0.2122942206856906
Loss at iteration [102]: 0.21187019336116208
Loss at iteration [103]: 0.21163891820720265
Loss at iteration [104]: 0.2113047812394432
Loss at iteration [105]: 0.21120884226279754
Loss at iteration [106]: 0.21059478976274604
Loss at iteration [107]: 0.2104516452427845
Loss at iteration [108]: 0.20944552754206555
Loss at iteration [109]: 0.20910002370693448
Loss at iteration [110]: 0.20910002370693448
Loss at iteration [111]: 0.20894933459182424
Loss at iteration [112]: 0.20769603084853552
Loss at iteration [113]: 0.20761543974932667
Loss at iteration [114]: 0.20706598350315666
Loss at iteration [115]: 0.20697812476163627
Loss at iteration [116]: 0.20667033799732162
Loss at iteration [117]: 0.20651109103891427
Loss at iteration [118]: 0.20616202491872995
Loss at iteration [119]: 0.20580507339010687
Loss at iteration [120]: 0.20496428759110213
Loss at iteration [121]: 0.20470941529357337
Loss at iteration [122]: 0.20470941529357337
Loss at iteration [123]: 0.20447854161539822
Loss at iteration [124]: 0.2029160136956446
Loss at iteration [125]: 0.2028047479190266
Loss at iteration [126]: 0.20213481821186957
Loss at iteration [127]: 0.20202059135524214
Loss at iteration [128]: 0.2013075714947858
Loss at iteration [129]: 0.20113857020096731
Loss at iteration [130]: 0.20102057072701646
Loss at iteration [131]: 0.20097014439754438
Loss at iteration [132]: 0.1994000074271887
Loss at iteration [133]: 0.1992690747663523
Loss at iteration [134]: 0.19776430532160877
Loss at iteration [135]: 0.19750569691488118
Loss at iteration [136]: 0.19750569691488118
Loss at iteration [137]: 0.19729922320890772
Loss at iteration [138]: 0.19672568834668896
Loss at iteration [139]: 0.19648852920419277
Loss at iteration [140]: 0.19642155436268538
Loss at iteration [141]: 0.19627013320264886
Loss at iteration [142]: 0.19623688278365534
Loss at iteration [143]: 0.19605610755247932
Loss at iteration [144]: 0.19553078743975802
Loss at iteration [145]: 0.1954198265008293
Loss at iteration [146]: 0.1953156514082338
Loss at iteration [147]: 0.19522107088683557
Loss at iteration [148]: 0.19514205432977483
Loss at iteration [149]: 0.19514205432977483
Loss at iteration [150]: 0.19504692234644888
Loss at iteration [151]: 0.19449487958893055
Loss at iteration [152]: 0.19445024384956391
Loss at iteration [153]: 0.19407350324057981
Loss at iteration [154]: 0.19375979401551716
Loss at iteration [155]: 0.19335812701739646
Loss at iteration [156]: 0.192917575918217
Loss at iteration [157]: 0.19270322234028553
Loss at iteration [158]: 0.19203492507024514
Loss at iteration [159]: 0.19190954852062658
Loss at iteration [160]: 0.1910945387329843
Loss at iteration [161]: 0.18937704415247908
Loss at iteration [162]: 0.18937704415247908
Loss at iteration [163]: 0.1885727877537676
Loss at iteration [164]: 0.18389103196538023
Loss at iteration [165]: 0.18316235129577546
Loss at iteration [166]: 0.18250174479225845
Loss at iteration [167]: 0.18212056192998585
Loss at iteration [168]: 0.18189433305669148
Loss at iteration [169]: 0.1816857932054411
Loss at iteration [170]: 0.1814060342141797
Loss at iteration [171]: 0.181252472052909
Loss at iteration [172]: 0.1807422970435165
Loss at iteration [173]: 0.18026577149382522
Loss at iteration [174]: 0.1796461616859709
Loss at iteration [175]: 0.1796461616859709
Loss at iteration [176]: 0.17943205138733376
Loss at iteration [177]: 0.17755520772455213
Loss at iteration [178]: 0.1773212760952984
Loss at iteration [179]: 0.17678700534743366
Loss at iteration [180]: 0.17671821535800783
Loss at iteration [181]: 0.17652461614983408
Loss at iteration [182]: 0.17611970707935193
Loss at iteration [183]: 0.1755423341391881
Loss at iteration [184]: 0.1753604800548408
Loss at iteration [185]: 0.1747086735460493
Loss at iteration [186]: 0.17436851645848914
Loss at iteration [187]: 0.17201510281386384
Loss at iteration [188]: 0.17201510281386384
Loss at iteration [189]: 0.1717354229489343
Loss at iteration [190]: 0.17020572812351556
Loss at iteration [191]: 0.17005333254271884
Loss at iteration [192]: 0.16969223099133315
Loss at iteration [193]: 0.16910840293545493
Loss at iteration [194]: 0.16877701997500047
Loss at iteration [195]: 0.16764919485165478
Loss at iteration [196]: 0.16742706480742459
Loss at iteration [197]: 0.16701309727416022
Loss at iteration [198]: 0.16694947341188884
Loss at iteration [199]: 0.1667237926968221
Loss at iteration [200]: 0.166126197676094
Loss at iteration [201]: 0.166126197676094
Loss at iteration [202]: 0.1660392698968398
Loss at iteration [203]: 0.16595737631094873
Loss at iteration [204]: 0.16590923699567345
Loss at iteration [205]: 0.1657200371150715
Loss at iteration [206]: 0.165608718638952
Loss at iteration [207]: 0.1650379008564085
Loss at iteration [208]: 0.16491089855442864
Loss at iteration [209]: 0.1642476844254006
Loss at iteration [210]: 0.16346888175402133
Loss at iteration [211]: 0.1621705712488745
Loss at iteration [212]: 0.16167312110108584
Loss at iteration [213]: 0.16167312110108584
Loss at iteration [214]: 0.16143186764425888
Loss at iteration [215]: 0.16121918087923373
Loss at iteration [216]: 0.16080230767646916
Loss at iteration [217]: 0.1607182697932621
Loss at iteration [218]: 0.15959935660853783
Loss at iteration [219]: 0.15927322346095116
Loss at iteration [220]: 0.15889333688580923
Loss at iteration [221]: 0.15855197831744286
Loss at iteration [222]: 0.15823146432895532
Loss at iteration [223]: 0.15809918864310896
Loss at iteration [224]: 0.15792629801930716
Loss at iteration [225]: 0.15792629801930716
Loss at iteration [226]: 0.15778146963436776
Loss at iteration [227]: 0.1573348667290559
Loss at iteration [228]: 0.15726916985893855
Loss at iteration [229]: 0.15690547548003403
Loss at iteration [230]: 0.1568547185110317
Loss at iteration [231]: 0.1564047627992298
Loss at iteration [232]: 0.1562048707599776
Loss at iteration [233]: 0.15584462847911393
Loss at iteration [234]: 0.15561452262463657
Loss at iteration [235]: 0.15529140766119628
Loss at iteration [236]: 0.1547037124769528
Loss at iteration [237]: 0.1547037124769528
Loss at iteration [238]: 0.15405790386390278
Loss at iteration [239]: 0.15325010716001522
Loss at iteration [240]: 0.1531915995603053
Loss at iteration [241]: 0.15293127499942233
Loss at iteration [242]: 0.15289241400722123
Loss at iteration [243]: 0.15276252592192727
Loss at iteration [244]: 0.15260902374176338
Loss at iteration [245]: 0.15246424542894685
Loss at iteration [246]: 0.152373914555003
Loss at iteration [247]: 0.15207099930124596
Loss at iteration [248]: 0.15207099930124596
Loss at iteration [249]: 0.15193905232207006
Loss at iteration [250]: 0.15189868275479412
Loss at iteration [251]: 0.15179614191911167
Loss at iteration [252]: 0.1517223941795437
Loss at iteration [253]: 0.1516799397084115
Loss at iteration [254]: 0.1516284212359805
Loss at iteration [255]: 0.15157058941498275
Loss at iteration [256]: 0.1514897155974388
Loss at iteration [257]: 0.15122182470643608
Loss at iteration [258]: 0.15122182470643608
Loss at iteration [259]: 0.15077938056952245
Loss at iteration [260]: 0.15070189204116732
Loss at iteration [261]: 0.15044369645910083
Loss at iteration [262]: 0.15034072325076334
Loss at iteration [263]: 0.15012179037207687
Loss at iteration [264]: 0.150064718086482
