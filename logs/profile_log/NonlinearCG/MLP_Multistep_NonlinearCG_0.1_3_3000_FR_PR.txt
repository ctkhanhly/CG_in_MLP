Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.1
Beta type                             :FR_PR
Total number of function evaluations  : 3038
Total number of iterations            : 802
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 8.648952722549438
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 49.89598914324772%
Percentage of parameters < 1e-7       : 49.89598914324772%
Percentage of parameters < 1e-6       : 49.89747501262989%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.1819352789065338
Loss at iteration [2]: 1.173212607545277
Loss at iteration [3]: 1.1544885833947165
Loss at iteration [4]: 1.1450804139961959
Loss at iteration [5]: 1.1396138541519307
Loss at iteration [6]: 1.1277875636611203
Loss at iteration [7]: 1.1204364835584086
Loss at iteration [8]: 1.1062630777264146
Loss at iteration [9]: 1.0909308206656836
Loss at iteration [10]: 1.07980977879732
Loss at iteration [11]: 1.0531990883933076
Loss at iteration [12]: 1.0453930886265892
Loss at iteration [13]: 1.0248790750294656
Loss at iteration [14]: 1.0190047156699198
Loss at iteration [15]: 0.992142045490313
Loss at iteration [16]: 0.992142045490313
Loss at iteration [17]: 0.9864507460286241
Loss at iteration [18]: 0.9713571354756809
Loss at iteration [19]: 0.9635143602397958
Loss at iteration [20]: 0.9583910377801675
Loss at iteration [21]: 0.9426050934620825
Loss at iteration [22]: 0.9398384073373143
Loss at iteration [23]: 0.9358830104542666
Loss at iteration [24]: 0.9326660960484693
Loss at iteration [25]: 0.9180676020530075
Loss at iteration [26]: 0.9141246828424364
Loss at iteration [27]: 0.9067548536872196
Loss at iteration [28]: 0.8967951548936779
Loss at iteration [29]: 0.8967951548936779
Loss at iteration [30]: 0.8904243239551647
Loss at iteration [31]: 0.8872011766572939
Loss at iteration [32]: 0.8801157111250144
Loss at iteration [33]: 0.8773273767383414
Loss at iteration [34]: 0.8691682870363847
Loss at iteration [35]: 0.8669551839400125
Loss at iteration [36]: 0.8633445183059102
Loss at iteration [37]: 0.8599620009991672
Loss at iteration [38]: 0.8568733526728385
Loss at iteration [39]: 0.8529048767317597
Loss at iteration [40]: 0.8509650459481427
Loss at iteration [41]: 0.8476650282810333
Loss at iteration [42]: 0.8476650282810333
Loss at iteration [43]: 0.8462929610583791
Loss at iteration [44]: 0.8395371619611735
Loss at iteration [45]: 0.8381932271556052
Loss at iteration [46]: 0.8365145811050866
Loss at iteration [47]: 0.833302772750218
Loss at iteration [48]: 0.8309020020175466
Loss at iteration [49]: 0.8297180896144462
Loss at iteration [50]: 0.8277114386691607
Loss at iteration [51]: 0.826504620129651
Loss at iteration [52]: 0.8228459253182511
Loss at iteration [53]: 0.8207097004001422
Loss at iteration [54]: 0.8169079365461624
Loss at iteration [55]: 0.8169079365461624
Loss at iteration [56]: 0.8154357354899088
Loss at iteration [57]: 0.810244104627063
Loss at iteration [58]: 0.8090045608869229
Loss at iteration [59]: 0.8073500568505103
Loss at iteration [60]: 0.80682725699454
Loss at iteration [61]: 0.8051960391254511
Loss at iteration [62]: 0.8042722387729799
Loss at iteration [63]: 0.8020663998486137
Loss at iteration [64]: 0.801157840838136
Loss at iteration [65]: 0.7989601176066505
Loss at iteration [66]: 0.7982966366882167
Loss at iteration [67]: 0.7964721451966943
Loss at iteration [68]: 0.7964721451966943
Loss at iteration [69]: 0.7957065485081884
Loss at iteration [70]: 0.793770713784977
Loss at iteration [71]: 0.792602176474404
Loss at iteration [72]: 0.7919004961146694
Loss at iteration [73]: 0.7907165545060744
Loss at iteration [74]: 0.7901989594613861
Loss at iteration [75]: 0.7890372182844783
Loss at iteration [76]: 0.7879511792398137
Loss at iteration [77]: 0.7871395403256355
Loss at iteration [78]: 0.7859169425352196
Loss at iteration [79]: 0.7852456601506818
Loss at iteration [80]: 0.784146198524033
Loss at iteration [81]: 0.784146198524033
Loss at iteration [82]: 0.7834893246578081
Loss at iteration [83]: 0.7817134432768735
Loss at iteration [84]: 0.7813315398982649
Loss at iteration [85]: 0.7797742470471328
Loss at iteration [86]: 0.7793154959104339
Loss at iteration [87]: 0.7787458572333464
Loss at iteration [88]: 0.7774969119489648
Loss at iteration [89]: 0.7769505410113394
Loss at iteration [90]: 0.7760531636736349
Loss at iteration [91]: 0.775671294334291
Loss at iteration [92]: 0.7751168998283103
Loss at iteration [93]: 0.7745933539410177
Loss at iteration [94]: 0.7745933539410177
Loss at iteration [95]: 0.7741694088357164
Loss at iteration [96]: 0.772703296029162
Loss at iteration [97]: 0.7722238383466634
Loss at iteration [98]: 0.7718725686840682
Loss at iteration [99]: 0.7714795052060611
Loss at iteration [100]: 0.7709528278433937
Loss at iteration [101]: 0.7707340712900976
Loss at iteration [102]: 0.7702646377750357
Loss at iteration [103]: 0.7698923474612668
Loss at iteration [104]: 0.7694830112422668
Loss at iteration [105]: 0.7685856284479575
Loss at iteration [106]: 0.7678538236651935
Loss at iteration [107]: 0.7678538236651935
Loss at iteration [108]: 0.7673861792235035
Loss at iteration [109]: 0.766618708421361
Loss at iteration [110]: 0.7663438689055955
Loss at iteration [111]: 0.7660441930380747
Loss at iteration [112]: 0.7657827788034022
Loss at iteration [113]: 0.7654293792170953
Loss at iteration [114]: 0.7653031145475238
Loss at iteration [115]: 0.7650231618267025
Loss at iteration [116]: 0.7648334683659448
Loss at iteration [117]: 0.7646681482490141
Loss at iteration [118]: 0.7646681482490141
Loss at iteration [119]: 0.7645568829627878
Loss at iteration [120]: 0.7644616696882096
Loss at iteration [121]: 0.7643541311495743
Loss at iteration [122]: 0.764291681298626
Loss at iteration [123]: 0.7642478521799299
Loss at iteration [124]: 0.764176888477748
Loss at iteration [125]: 0.764050013463744
Loss at iteration [126]: 0.7637727913852876
Loss at iteration [127]: 0.7635027541488175
Loss at iteration [128]: 0.7635027541488175
Loss at iteration [129]: 0.7632869867686553
Loss at iteration [130]: 0.7631361410136389
Loss at iteration [131]: 0.7630412335209378
Loss at iteration [132]: 0.7630019721278468
Loss at iteration [133]: 0.7629143672842581
Loss at iteration [134]: 0.7627819529190367
Loss at iteration [135]: 0.7626753296361464
Loss at iteration [136]: 0.7625699261607455
Loss at iteration [137]: 0.7625699261607455
Loss at iteration [138]: 0.7624071220605415
Loss at iteration [139]: 0.7623367275133666
Loss at iteration [140]: 0.7623066799097629
Loss at iteration [141]: 0.7622598518648033
Loss at iteration [142]: 0.762219787149294
Loss at iteration [143]: 0.7620818564531813
Loss at iteration [144]: 0.7619169881861205
Loss at iteration [145]: 0.7617015176598031
Loss at iteration [146]: 0.7614950398948604
Loss at iteration [147]: 0.7614950398948604
Loss at iteration [148]: 0.761204179263444
Loss at iteration [149]: 0.7607605334367249
Loss at iteration [150]: 0.7606002732807507
Loss at iteration [151]: 0.7604858183028689
Loss at iteration [152]: 0.7603227595027752
Loss at iteration [153]: 0.7602080731371893
Loss at iteration [154]: 0.760084277053499
Loss at iteration [155]: 0.7600025737808583
Loss at iteration [156]: 0.7599089901406445
Loss at iteration [157]: 0.7599089901406445
Loss at iteration [158]: 0.7598215576269284
Loss at iteration [159]: 0.7597748064134532
Loss at iteration [160]: 0.759744141749078
Loss at iteration [161]: 0.7596500378814629
Loss at iteration [162]: 0.7594974023047216
Loss at iteration [163]: 0.7593999468345703
Loss at iteration [164]: 0.7593398276903498
Loss at iteration [165]: 0.7590491551688541
Loss at iteration [166]: 0.7581776930941216
Loss at iteration [167]: 0.7581776930941216
Loss at iteration [168]: 0.757340087971193
Loss at iteration [169]: 0.7562633433161252
Loss at iteration [170]: 0.7558474683819809
Loss at iteration [171]: 0.7552568970876805
Loss at iteration [172]: 0.754778115903572
Loss at iteration [173]: 0.7546252034549482
Loss at iteration [174]: 0.7543095653687174
Loss at iteration [175]: 0.7539809018573478
Loss at iteration [176]: 0.7535615564955451
Loss at iteration [177]: 0.7533323298644853
Loss at iteration [178]: 0.7529476414677311
Loss at iteration [179]: 0.7529476414677311
Loss at iteration [180]: 0.7523038608629977
Loss at iteration [181]: 0.7521983179136197
Loss at iteration [182]: 0.751884416850617
Loss at iteration [183]: 0.7516175952402072
Loss at iteration [184]: 0.7513490681711561
Loss at iteration [185]: 0.7510359691053499
Loss at iteration [186]: 0.7508220282937239
Loss at iteration [187]: 0.7506374685749644
Loss at iteration [188]: 0.7504286467265695
Loss at iteration [189]: 0.7504286467265695
Loss at iteration [190]: 0.7501646768685739
Loss at iteration [191]: 0.7500681117497616
Loss at iteration [192]: 0.7500047663162839
Loss at iteration [193]: 0.7499336802728672
Loss at iteration [194]: 0.7498506380433569
Loss at iteration [195]: 0.7497361913328429
Loss at iteration [196]: 0.7495566613834412
Loss at iteration [197]: 0.7493318057900326
Loss at iteration [198]: 0.7493318057900326
Loss at iteration [199]: 0.749102811659826
Loss at iteration [200]: 0.7490258294582214
Loss at iteration [201]: 0.7489858030558781
Loss at iteration [202]: 0.7489373548123418
Loss at iteration [203]: 0.7488545589041988
Loss at iteration [204]: 0.7487809938159641
Loss at iteration [205]: 0.7486680061248375
Loss at iteration [206]: 0.7484724057961265
Loss at iteration [207]: 0.7484724057961265
Loss at iteration [208]: 0.7482518083035314
Loss at iteration [209]: 0.7481660603570452
Loss at iteration [210]: 0.748091662515031
Loss at iteration [211]: 0.7480694567015558
Loss at iteration [212]: 0.7480168141953119
Loss at iteration [213]: 0.7479907157356769
Loss at iteration [214]: 0.7479504272014087
Loss at iteration [215]: 0.7478595322260695
Loss at iteration [216]: 0.7478595322260695
Loss at iteration [217]: 0.7476811880428466
Loss at iteration [218]: 0.7476167614014132
Loss at iteration [219]: 0.747576193147198
Loss at iteration [220]: 0.7475191695506062
Loss at iteration [221]: 0.7474996033535757
Loss at iteration [222]: 0.7474341237378166
Loss at iteration [223]: 0.7473179795582816
Loss at iteration [224]: 0.7470767492485314
Loss at iteration [225]: 0.7470767492485314
Loss at iteration [226]: 0.7468593569954196
Loss at iteration [227]: 0.7467861777696702
Loss at iteration [228]: 0.7466708259826963
Loss at iteration [229]: 0.7465846077908429
Loss at iteration [230]: 0.7465288076707053
Loss at iteration [231]: 0.7464478320658118
Loss at iteration [232]: 0.7464060935903353
Loss at iteration [233]: 0.7463264771663236
Loss at iteration [234]: 0.7463264771663236
Loss at iteration [235]: 0.7461468602810571
Loss at iteration [236]: 0.7460671411525484
Loss at iteration [237]: 0.7459995612570517
Loss at iteration [238]: 0.7459664993171142
Loss at iteration [239]: 0.745955512675439
Loss at iteration [240]: 0.7459023157144248
Loss at iteration [241]: 0.7457956877479645
Loss at iteration [242]: 0.7456632131515464
Loss at iteration [243]: 0.7456632131515464
Loss at iteration [244]: 0.745444472523053
Loss at iteration [245]: 0.7453215962183993
Loss at iteration [246]: 0.7452727389891973
Loss at iteration [247]: 0.7452341332241701
Loss at iteration [248]: 0.7451861703441361
Loss at iteration [249]: 0.7451354631137549
Loss at iteration [250]: 0.7449856681504821
Loss at iteration [251]: 0.744796027146639
Loss at iteration [252]: 0.744796027146639
Loss at iteration [253]: 0.7446484101298433
Loss at iteration [254]: 0.7445606907447938
Loss at iteration [255]: 0.7444873138124942
Loss at iteration [256]: 0.7444242456948005
Loss at iteration [257]: 0.744414274710027
Loss at iteration [258]: 0.7443563055023528
Loss at iteration [259]: 0.7441392835090331
Loss at iteration [260]: 0.7439296054658828
Loss at iteration [261]: 0.7439296054658828
Loss at iteration [262]: 0.743805488627108
Loss at iteration [263]: 0.7437131216971334
Loss at iteration [264]: 0.7436615336044635
Loss at iteration [265]: 0.7436004827982587
Loss at iteration [266]: 0.7435562885208926
Loss at iteration [267]: 0.7434870558582566
Loss at iteration [268]: 0.7434163592397501
Loss at iteration [269]: 0.743342069477053
Loss at iteration [270]: 0.743342069477053
Loss at iteration [271]: 0.7431904712655595
Loss at iteration [272]: 0.7431267500152687
Loss at iteration [273]: 0.7430561601380841
Loss at iteration [274]: 0.7430024919513267
Loss at iteration [275]: 0.7429988113189777
Loss at iteration [276]: 0.74290984896905
Loss at iteration [277]: 0.7427965980029462
Loss at iteration [278]: 0.7427281723083856
Loss at iteration [279]: 0.7427281723083856
Loss at iteration [280]: 0.7425734440265818
Loss at iteration [281]: 0.7424974408603936
Loss at iteration [282]: 0.7424616215695038
Loss at iteration [283]: 0.7423919803615512
Loss at iteration [284]: 0.7423222772175784
Loss at iteration [285]: 0.7422668290665301
Loss at iteration [286]: 0.742180223114897
Loss at iteration [287]: 0.7420481789050052
Loss at iteration [288]: 0.7420481789050052
Loss at iteration [289]: 0.7419152265080962
Loss at iteration [290]: 0.7418217017633313
Loss at iteration [291]: 0.7417761459251249
Loss at iteration [292]: 0.7417463877339648
Loss at iteration [293]: 0.7417003777174486
Loss at iteration [294]: 0.7416071823626381
Loss at iteration [295]: 0.7415442528881185
Loss at iteration [296]: 0.741539630949801
Loss at iteration [297]: 0.7412897941066735
Loss at iteration [298]: 0.7412897941066735
Loss at iteration [299]: 0.7408721927505699
Loss at iteration [300]: 0.7403614885435752
Loss at iteration [301]: 0.7401478950346161
Loss at iteration [302]: 0.7397290406053465
Loss at iteration [303]: 0.7394770139621385
Loss at iteration [304]: 0.7392952846248726
Loss at iteration [305]: 0.7389720015875827
Loss at iteration [306]: 0.7387202430767628
Loss at iteration [307]: 0.7383997624484907
Loss at iteration [308]: 0.7380583391573493
Loss at iteration [309]: 0.7380583391573493
Loss at iteration [310]: 0.7378203081278373
Loss at iteration [311]: 0.7377176350156057
Loss at iteration [312]: 0.7376106643062023
Loss at iteration [313]: 0.7374900183484074
Loss at iteration [314]: 0.7373824368293309
Loss at iteration [315]: 0.7372128144343628
Loss at iteration [316]: 0.7370500727256135
Loss at iteration [317]: 0.7368270761320145
Loss at iteration [318]: 0.7368270761320145
Loss at iteration [319]: 0.7366535729387034
Loss at iteration [320]: 0.7365866403327476
Loss at iteration [321]: 0.7365350336096439
Loss at iteration [322]: 0.7364689917506264
Loss at iteration [323]: 0.7363549019189383
Loss at iteration [324]: 0.7362744548819598
Loss at iteration [325]: 0.7361087292359294
Loss at iteration [326]: 0.7358798368480364
Loss at iteration [327]: 0.7358798368480364
Loss at iteration [328]: 0.735658392988121
Loss at iteration [329]: 0.7355587580751011
Loss at iteration [330]: 0.735460491652877
Loss at iteration [331]: 0.7353475196786672
Loss at iteration [332]: 0.7352732874730087
Loss at iteration [333]: 0.7351490733624384
Loss at iteration [334]: 0.7350677507925312
Loss at iteration [335]: 0.7349466645255137
Loss at iteration [336]: 0.7349466645255137
Loss at iteration [337]: 0.7348326983217159
Loss at iteration [338]: 0.7347866980225912
Loss at iteration [339]: 0.7347266269331636
Loss at iteration [340]: 0.7346924280600381
Loss at iteration [341]: 0.7346403731817949
Loss at iteration [342]: 0.7345644718462104
Loss at iteration [343]: 0.7345265077262458
Loss at iteration [344]: 0.7344929681537974
Loss at iteration [345]: 0.7344929681537974
Loss at iteration [346]: 0.7343922691047106
Loss at iteration [347]: 0.7343647987786354
Loss at iteration [348]: 0.7343041673093313
Loss at iteration [349]: 0.7342682539362687
Loss at iteration [350]: 0.7342269622569243
Loss at iteration [351]: 0.7341988219726834
Loss at iteration [352]: 0.7341400084972032
Loss at iteration [353]: 0.7340514742513523
Loss at iteration [354]: 0.7340514742513523
Loss at iteration [355]: 0.7339221998685453
Loss at iteration [356]: 0.7338397840414783
Loss at iteration [357]: 0.7337887357683619
Loss at iteration [358]: 0.7337601707126611
Loss at iteration [359]: 0.7337114693015537
Loss at iteration [360]: 0.7336802200981697
Loss at iteration [361]: 0.7336432293478485
Loss at iteration [362]: 0.7335878509013033
Loss at iteration [363]: 0.7335878509013033
Loss at iteration [364]: 0.7334036396022805
Loss at iteration [365]: 0.7333020428787774
Loss at iteration [366]: 0.7332573751895631
Loss at iteration [367]: 0.7332060226138531
Loss at iteration [368]: 0.7331850230223709
Loss at iteration [369]: 0.733155786228839
Loss at iteration [370]: 0.7330802354717649
Loss at iteration [371]: 0.7330207596461835
Loss at iteration [372]: 0.7330207596461835
Loss at iteration [373]: 0.7328530315213232
Loss at iteration [374]: 0.7327694102237121
Loss at iteration [375]: 0.7327128432829225
Loss at iteration [376]: 0.732688388339398
Loss at iteration [377]: 0.7326353419524172
Loss at iteration [378]: 0.7325992734960112
Loss at iteration [379]: 0.732539223177571
Loss at iteration [380]: 0.7324593656697289
Loss at iteration [381]: 0.7324593656697289
Loss at iteration [382]: 0.7323479607937864
Loss at iteration [383]: 0.7323051316045266
Loss at iteration [384]: 0.7322733207099087
Loss at iteration [385]: 0.7322385009656419
Loss at iteration [386]: 0.7321931286808225
Loss at iteration [387]: 0.732138100037751
Loss at iteration [388]: 0.7321258524006022
Loss at iteration [389]: 0.7321237357764921
Loss at iteration [390]: 0.7321237357764921
Loss at iteration [391]: 0.732048313819388
Loss at iteration [392]: 0.732021140203937
Loss at iteration [393]: 0.7319855207834359
Loss at iteration [394]: 0.7319603325876267
Loss at iteration [395]: 0.7319486520206933
Loss at iteration [396]: 0.731911391435897
Loss at iteration [397]: 0.7318752516056598
Loss at iteration [398]: 0.7318752516056598
Loss at iteration [399]: 0.7318517702551914
Loss at iteration [400]: 0.7318410933500572
Loss at iteration [401]: 0.7318308012385402
Loss at iteration [402]: 0.7318188808907747
Loss at iteration [403]: 0.7318087095946856
Loss at iteration [404]: 0.7317990251199405
Loss at iteration [405]: 0.7317990251199405
Loss at iteration [406]: 0.7317721250170652
Loss at iteration [407]: 0.731762311412227
Loss at iteration [408]: 0.7317534970386831
Loss at iteration [409]: 0.7317434128759298
Loss at iteration [410]: 0.7317430892448356
Loss at iteration [411]: 0.7317227491153241
Loss at iteration [412]: 0.7317227491153241
Loss at iteration [413]: 0.7316987293609676
Loss at iteration [414]: 0.7316899479702953
Loss at iteration [415]: 0.7316883397146715
Loss at iteration [416]: 0.7316845070078501
Loss at iteration [417]: 0.7316695137803988
Loss at iteration [418]: 0.731661722591603
Loss at iteration [419]: 0.731661722591603
Loss at iteration [420]: 0.7316320928019807
Loss at iteration [421]: 0.7316210781582108
Loss at iteration [422]: 0.7316125891689845
Loss at iteration [423]: 0.731606151410807
Loss at iteration [424]: 0.7315945290671533
Loss at iteration [425]: 0.7315754777759181
Loss at iteration [426]: 0.7315754777759181
Loss at iteration [427]: 0.7315534409046878
Loss at iteration [428]: 0.7315454520327648
Loss at iteration [429]: 0.7315399676268989
Loss at iteration [430]: 0.7315357303388536
Loss at iteration [431]: 0.7315247438944983
Loss at iteration [432]: 0.7315077397986957
Loss at iteration [433]: 0.7315077397986957
Loss at iteration [434]: 0.7314849383057739
Loss at iteration [435]: 0.7314807122741178
Loss at iteration [436]: 0.7314734354881817
Loss at iteration [437]: 0.7314662697623507
Loss at iteration [438]: 0.731451364356326
Loss at iteration [439]: 0.731435382229266
Loss at iteration [440]: 0.731435382229266
Loss at iteration [441]: 0.7314025814035285
Loss at iteration [442]: 0.7313950427488602
Loss at iteration [443]: 0.73138521791397
Loss at iteration [444]: 0.7313804197700577
Loss at iteration [445]: 0.7313749539934892
Loss at iteration [446]: 0.7313575700727569
Loss at iteration [447]: 0.7313575700727569
Loss at iteration [448]: 0.7313455517120341
Loss at iteration [449]: 0.7313407902721192
Loss at iteration [450]: 0.7313294510109666
Loss at iteration [451]: 0.73132308655341
Loss at iteration [452]: 0.7313141851111445
Loss at iteration [453]: 0.7312976532496537
Loss at iteration [454]: 0.7312976532496537
Loss at iteration [455]: 0.7312432035307217
Loss at iteration [456]: 0.7312292746204581
Loss at iteration [457]: 0.7312229648983478
Loss at iteration [458]: 0.7312151840593011
Loss at iteration [459]: 0.7312051416976925
Loss at iteration [460]: 0.7311834373978353
Loss at iteration [461]: 0.7311834373978353
Loss at iteration [462]: 0.7311609195492257
Loss at iteration [463]: 0.7311512954428993
Loss at iteration [464]: 0.731146344221441
Loss at iteration [465]: 0.7311401963726081
Loss at iteration [466]: 0.7311303656803387
Loss at iteration [467]: 0.7310995534991259
Loss at iteration [468]: 0.7310995534991259
Loss at iteration [469]: 0.731067210755531
Loss at iteration [470]: 0.7310582603900522
Loss at iteration [471]: 0.731053665680826
Loss at iteration [472]: 0.7310420225280421
Loss at iteration [473]: 0.7310322753323409
Loss at iteration [474]: 0.7310253814375699
Loss at iteration [475]: 0.7310253814375699
Loss at iteration [476]: 0.731008742492078
Loss at iteration [477]: 0.7310010874677105
Loss at iteration [478]: 0.7309979236656375
Loss at iteration [479]: 0.730989029419953
Loss at iteration [480]: 0.7309827828176517
Loss at iteration [481]: 0.730967833667393
Loss at iteration [482]: 0.730967833667393
Loss at iteration [483]: 0.7309543739302028
Loss at iteration [484]: 0.7309474042064946
Loss at iteration [485]: 0.7309439441183573
Loss at iteration [486]: 0.7309370186123136
Loss at iteration [487]: 0.7309291765426551
Loss at iteration [488]: 0.7309254577572082
Loss at iteration [489]: 0.7309254577572082
Loss at iteration [490]: 0.7309091955464709
Loss at iteration [491]: 0.7309038349064202
Loss at iteration [492]: 0.730898155522093
Loss at iteration [493]: 0.730892894098187
Loss at iteration [494]: 0.7308915987592851
Loss at iteration [495]: 0.7308866466678887
Loss at iteration [496]: 0.7308866466678887
Loss at iteration [497]: 0.7308516192055033
Loss at iteration [498]: 0.7308432364272506
Loss at iteration [499]: 0.7308393504578914
Loss at iteration [500]: 0.7308300292538195
Loss at iteration [501]: 0.7308161162394641
Loss at iteration [502]: 0.7308020525815634
Loss at iteration [503]: 0.7308020525815634
Loss at iteration [504]: 0.730787467951371
Loss at iteration [505]: 0.7307834252807978
Loss at iteration [506]: 0.7307775870654853
Loss at iteration [507]: 0.7307612669211823
Loss at iteration [508]: 0.7307496176816597
Loss at iteration [509]: 0.7307421993623611
Loss at iteration [510]: 0.7307421993623611
Loss at iteration [511]: 0.7307141882763966
Loss at iteration [512]: 0.7307094481130164
Loss at iteration [513]: 0.7307062086035263
Loss at iteration [514]: 0.7306977510146532
Loss at iteration [515]: 0.7306917091779308
Loss at iteration [516]: 0.730686900908016
Loss at iteration [517]: 0.730686900908016
Loss at iteration [518]: 0.7306731067143266
Loss at iteration [519]: 0.7306657785644134
Loss at iteration [520]: 0.7306567222496156
Loss at iteration [521]: 0.7306559849974453
Loss at iteration [522]: 0.7306501699107083
Loss at iteration [523]: 0.7306448930042789
Loss at iteration [524]: 0.7306448930042789
Loss at iteration [525]: 0.7306167570363353
Loss at iteration [526]: 0.7306128331397879
Loss at iteration [527]: 0.7306039303075453
Loss at iteration [528]: 0.7305944076290745
Loss at iteration [529]: 0.7305814686821387
Loss at iteration [530]: 0.7305707328027101
Loss at iteration [531]: 0.7305707328027101
Loss at iteration [532]: 0.7305470818372857
Loss at iteration [533]: 0.730543093917106
Loss at iteration [534]: 0.7305344172308825
Loss at iteration [535]: 0.7305304594128709
Loss at iteration [536]: 0.7305198460175027
Loss at iteration [537]: 0.7305016754791938
Loss at iteration [538]: 0.7305016754791938
Loss at iteration [539]: 0.7304694768453566
Loss at iteration [540]: 0.7304604870027671
Loss at iteration [541]: 0.7304531319118543
Loss at iteration [542]: 0.7304478128154464
Loss at iteration [543]: 0.7304398505009183
Loss at iteration [544]: 0.7304276400782319
Loss at iteration [545]: 0.7304276400782319
Loss at iteration [546]: 0.7304134080076722
Loss at iteration [547]: 0.7304073933482378
Loss at iteration [548]: 0.7304047149897439
Loss at iteration [549]: 0.7304015795313014
Loss at iteration [550]: 0.7303889097699134
Loss at iteration [551]: 0.7303838491543491
Loss at iteration [552]: 0.7303838491543491
Loss at iteration [553]: 0.7303609131605258
Loss at iteration [554]: 0.7303530620658633
Loss at iteration [555]: 0.7303472348244848
Loss at iteration [556]: 0.7303424024655869
Loss at iteration [557]: 0.7303363847702088
Loss at iteration [558]: 0.7303290128839485
Loss at iteration [559]: 0.7303290128839485
Loss at iteration [560]: 0.7303165803972724
Loss at iteration [561]: 0.730310993471819
Loss at iteration [562]: 0.7303033830315431
Loss at iteration [563]: 0.7302999863438432
Loss at iteration [564]: 0.7302959888667587
Loss at iteration [565]: 0.7302755517594075
Loss at iteration [566]: 0.7302755517594075
Loss at iteration [567]: 0.7302229452074094
Loss at iteration [568]: 0.7302022317350518
Loss at iteration [569]: 0.7301914719660932
Loss at iteration [570]: 0.730176038930225
Loss at iteration [571]: 0.7301528540508296
Loss at iteration [572]: 0.7301488379126045
Loss at iteration [573]: 0.7301488379126045
Loss at iteration [574]: 0.7301255979548207
Loss at iteration [575]: 0.7301199364756755
Loss at iteration [576]: 0.7301156119659664
Loss at iteration [577]: 0.7301110835344164
Loss at iteration [578]: 0.7301041816718731
Loss at iteration [579]: 0.730101071127568
Loss at iteration [580]: 0.730101071127568
Loss at iteration [581]: 0.7300707375531725
Loss at iteration [582]: 0.7300653540260529
Loss at iteration [583]: 0.7300609835546211
Loss at iteration [584]: 0.7300531517349297
Loss at iteration [585]: 0.7300400203549504
Loss at iteration [586]: 0.7300299640182492
Loss at iteration [587]: 0.7300299640182492
Loss at iteration [588]: 0.7299987011936107
Loss at iteration [589]: 0.7299879603767349
Loss at iteration [590]: 0.7299831645651931
Loss at iteration [591]: 0.7299795879200571
Loss at iteration [592]: 0.7299795570496962
Loss at iteration [593]: 0.7299699598223622
Loss at iteration [594]: 0.7299699598223622
Loss at iteration [595]: 0.7299380981279028
Loss at iteration [596]: 0.7299323927809857
Loss at iteration [597]: 0.72992827930358
Loss at iteration [598]: 0.7299216140773078
Loss at iteration [599]: 0.7299105260907532
Loss at iteration [600]: 0.7298968038824887
Loss at iteration [601]: 0.7298968038824887
Loss at iteration [602]: 0.7298752825656836
Loss at iteration [603]: 0.7298663386243447
Loss at iteration [604]: 0.7298592584017703
Loss at iteration [605]: 0.7298567500011081
Loss at iteration [606]: 0.7298499908372008
Loss at iteration [607]: 0.7298420098654932
Loss at iteration [608]: 0.7298420098654932
Loss at iteration [609]: 0.7298211121329029
Loss at iteration [610]: 0.7298135095300682
Loss at iteration [611]: 0.7298115026347194
Loss at iteration [612]: 0.7298056553285183
Loss at iteration [613]: 0.7297946246652215
Loss at iteration [614]: 0.7297796636703638
Loss at iteration [615]: 0.7297796636703638
Loss at iteration [616]: 0.7297590665979135
Loss at iteration [617]: 0.7297496898328182
Loss at iteration [618]: 0.7297429577091981
Loss at iteration [619]: 0.7297391121958352
Loss at iteration [620]: 0.7297353169546157
Loss at iteration [621]: 0.72972096310806
Loss at iteration [622]: 0.72972096310806
Loss at iteration [623]: 0.7297068244709932
Loss at iteration [624]: 0.7297019001636269
Loss at iteration [625]: 0.7296978034400914
Loss at iteration [626]: 0.7296925691369507
Loss at iteration [627]: 0.7296923061220824
Loss at iteration [628]: 0.7296878367443331
Loss at iteration [629]: 0.7296878367443331
Loss at iteration [630]: 0.7296630449627879
Loss at iteration [631]: 0.729655397387267
Loss at iteration [632]: 0.7296489417460186
Loss at iteration [633]: 0.7296447516141733
Loss at iteration [634]: 0.7296393894113431
Loss at iteration [635]: 0.7296308121431232
Loss at iteration [636]: 0.7296308121431232
Loss at iteration [637]: 0.7296050449554696
Loss at iteration [638]: 0.7295999085698754
Loss at iteration [639]: 0.7295951248335638
Loss at iteration [640]: 0.7295892231308774
Loss at iteration [641]: 0.7295809091793066
Loss at iteration [642]: 0.7295756452849578
Loss at iteration [643]: 0.7295756452849578
Loss at iteration [644]: 0.7295445633055357
Loss at iteration [645]: 0.7295349423227839
Loss at iteration [646]: 0.7295280078147997
Loss at iteration [647]: 0.7295201314818809
Loss at iteration [648]: 0.7295135236381269
Loss at iteration [649]: 0.7295050318304291
Loss at iteration [650]: 0.7295050318304291
Loss at iteration [651]: 0.7294870267418452
Loss at iteration [652]: 0.7294810869407887
Loss at iteration [653]: 0.7294807715692434
Loss at iteration [654]: 0.7294745869266686
Loss at iteration [655]: 0.7294631875272105
Loss at iteration [656]: 0.7294489866159017
Loss at iteration [657]: 0.7294489866159017
Loss at iteration [658]: 0.7294093540382208
Loss at iteration [659]: 0.7293964806385133
Loss at iteration [660]: 0.7293866152342017
Loss at iteration [661]: 0.7293766022137005
Loss at iteration [662]: 0.7293681377622938
Loss at iteration [663]: 0.7293583959306366
Loss at iteration [664]: 0.7293583959306366
Loss at iteration [665]: 0.7293345773596196
Loss at iteration [666]: 0.72932978584535
Loss at iteration [667]: 0.7293284243252763
Loss at iteration [668]: 0.7293251728187647
Loss at iteration [669]: 0.7293160763459143
Loss at iteration [670]: 0.7293025633070315
Loss at iteration [671]: 0.7293025633070315
Loss at iteration [672]: 0.7292839532413702
Loss at iteration [673]: 0.7292772273554703
Loss at iteration [674]: 0.7292676787515333
Loss at iteration [675]: 0.7292574727001863
Loss at iteration [676]: 0.7292510421682263
Loss at iteration [677]: 0.7292453911969824
Loss at iteration [678]: 0.7292453911969824
Loss at iteration [679]: 0.7292239610407792
Loss at iteration [680]: 0.729216911389934
Loss at iteration [681]: 0.7292151974982499
Loss at iteration [682]: 0.7292123871935546
Loss at iteration [683]: 0.7292025347344939
Loss at iteration [684]: 0.7291946090784043
Loss at iteration [685]: 0.7291946090784043
Loss at iteration [686]: 0.7291834756104204
Loss at iteration [687]: 0.7291776059917048
Loss at iteration [688]: 0.7291707225473849
Loss at iteration [689]: 0.7291637258958761
Loss at iteration [690]: 0.7291562992666812
Loss at iteration [691]: 0.7291431657032696
Loss at iteration [692]: 0.7291431657032696
Loss at iteration [693]: 0.7291164293693222
Loss at iteration [694]: 0.7291066145938997
Loss at iteration [695]: 0.7291040786204133
Loss at iteration [696]: 0.7291023849113647
Loss at iteration [697]: 0.7290942617152806
Loss at iteration [698]: 0.7290817770735756
Loss at iteration [699]: 0.7290817770735756
Loss at iteration [700]: 0.7290599788949821
Loss at iteration [701]: 0.7290565364246953
Loss at iteration [702]: 0.7290545045640903
Loss at iteration [703]: 0.7290456384821504
Loss at iteration [704]: 0.7290418706121214
Loss at iteration [705]: 0.729040740003769
Loss at iteration [706]: 0.729040740003769
Loss at iteration [707]: 0.729032078735283
Loss at iteration [708]: 0.7290275739995076
Loss at iteration [709]: 0.7290266101591837
Loss at iteration [710]: 0.729023599558707
Loss at iteration [711]: 0.7290193169560711
Loss at iteration [712]: 0.7290074364073539
Loss at iteration [713]: 0.7290074364073539
Loss at iteration [714]: 0.7289962405686815
Loss at iteration [715]: 0.728987069205539
Loss at iteration [716]: 0.7289812980767659
Loss at iteration [717]: 0.7289744165803653
Loss at iteration [718]: 0.7289714766042656
Loss at iteration [719]: 0.7289647785204734
Loss at iteration [720]: 0.7289647785204734
Loss at iteration [721]: 0.7289461282484964
Loss at iteration [722]: 0.7289377335729789
Loss at iteration [723]: 0.7289324217030425
Loss at iteration [724]: 0.7289279885173718
Loss at iteration [725]: 0.7289162551827704
Loss at iteration [726]: 0.7289100892875087
Loss at iteration [727]: 0.7289100892875087
Loss at iteration [728]: 0.7289029714329034
Loss at iteration [729]: 0.7289001774923793
Loss at iteration [730]: 0.7288935460148717
Loss at iteration [731]: 0.728889529348326
Loss at iteration [732]: 0.7288847227739121
Loss at iteration [733]: 0.7288828766864585
Loss at iteration [734]: 0.7288828766864585
Loss at iteration [735]: 0.728872900053461
Loss at iteration [736]: 0.7288696506221143
Loss at iteration [737]: 0.7288663376481105
Loss at iteration [738]: 0.7288625915496433
Loss at iteration [739]: 0.7288601664029118
Loss at iteration [740]: 0.7288601664029118
Loss at iteration [741]: 0.7288570262481263
Loss at iteration [742]: 0.7288560485569028
Loss at iteration [743]: 0.7288547730278135
Loss at iteration [744]: 0.7288525826044555
Loss at iteration [745]: 0.7288509196351369
Loss at iteration [746]: 0.7288509196351369
Loss at iteration [747]: 0.7288475235683443
Loss at iteration [748]: 0.7288457053525285
Loss at iteration [749]: 0.7288439448674255
Loss at iteration [750]: 0.7288424152104231
Loss at iteration [751]: 0.7288416348807293
Loss at iteration [752]: 0.7288416348807293
Loss at iteration [753]: 0.7288364155165173
Loss at iteration [754]: 0.7288354838783058
Loss at iteration [755]: 0.7288342383392994
Loss at iteration [756]: 0.7288331506255998
Loss at iteration [757]: 0.7288319964671062
Loss at iteration [758]: 0.7288319964671062
Loss at iteration [759]: 0.7288284590271021
Loss at iteration [760]: 0.7288277236116479
Loss at iteration [761]: 0.728826986880627
Loss at iteration [762]: 0.7288269824361616
Loss at iteration [763]: 0.7288267086985879
Loss at iteration [764]: 0.7288267086985879
Loss at iteration [765]: 0.7288251318640587
Loss at iteration [766]: 0.7288238014845846
Loss at iteration [767]: 0.7288234176361184
Loss at iteration [768]: 0.7288228894570303
Loss at iteration [769]: 0.728822497228649
Loss at iteration [770]: 0.728822497228649
Loss at iteration [771]: 0.7288189757294704
Loss at iteration [772]: 0.7288171643801415
Loss at iteration [773]: 0.7288168373675783
Loss at iteration [774]: 0.7288160531786864
Loss at iteration [775]: 0.7288148111463028
Loss at iteration [776]: 0.7288148111463028
Loss at iteration [777]: 0.7288137049437223
Loss at iteration [778]: 0.7288128059653616
Loss at iteration [779]: 0.7288125174977043
Loss at iteration [780]: 0.7288123706290452
Loss at iteration [781]: 0.7288105287019223
Loss at iteration [782]: 0.7288105287019223
Loss at iteration [783]: 0.7288033783005091
Loss at iteration [784]: 0.7288012362902127
Loss at iteration [785]: 0.7288003872837664
Loss at iteration [786]: 0.7287993710468824
Loss at iteration [787]: 0.7287966199843984
Loss at iteration [788]: 0.7287966199843984
Loss at iteration [789]: 0.7287941389025677
Loss at iteration [790]: 0.7287934050842947
Loss at iteration [791]: 0.728792773020464
Loss at iteration [792]: 0.7287920875437625
Loss at iteration [793]: 0.7287912104971519
Loss at iteration [794]: 0.7287912104971519
Loss at iteration [795]: 0.7287874182448012
Loss at iteration [796]: 0.7287864775629954
Loss at iteration [797]: 0.7287856063389933
Loss at iteration [798]: 0.7287854282722787
Loss at iteration [799]: 0.7287842712146474
Loss at iteration [800]: 0.7287842712146474
Loss at iteration [801]: 0.7287807577601085
Loss at iteration [802]: 0.728778898973508
Loss at iteration [803]: 0.7287781384503096
Loss at iteration [804]: 0.7287769264724512
Loss at iteration [805]: 0.7287755691463089
Loss at iteration [806]: 0.7287755691463089
Loss at iteration [807]: 0.7287728584244719
Loss at iteration [808]: 0.7287714899337612
Loss at iteration [809]: 0.728770685344644
Loss at iteration [810]: 0.7287695229905958
Loss at iteration [811]: 0.7287684819947915
Loss at iteration [812]: 0.7287684819947915
Loss at iteration [813]: 0.7287655508175487
Loss at iteration [814]: 0.7287644472041738
Loss at iteration [815]: 0.7287640842454852
Loss at iteration [816]: 0.7287635532704259
Loss at iteration [817]: 0.728762806323436
Loss at iteration [818]: 0.728762806323436
Loss at iteration [819]: 0.7287602342090449
Loss at iteration [820]: 0.7287592436568248
Loss at iteration [821]: 0.7287584580390118
Loss at iteration [822]: 0.7287579738721555
Loss at iteration [823]: 0.7287575764861282
Loss at iteration [824]: 0.7287575764861282
Loss at iteration [825]: 0.7287536909255351
Loss at iteration [826]: 0.7287532419350875
Loss at iteration [827]: 0.7287520083903442
Loss at iteration [828]: 0.7287507046149382
Loss at iteration [829]: 0.7287499337410386
Loss at iteration [830]: 0.7287499337410386
Loss at iteration [831]: 0.728749285506021
Loss at iteration [832]: 0.7287483189068609
Loss at iteration [833]: 0.7287479407590485
Loss at iteration [834]: 0.728747713455492
Loss at iteration [835]: 0.7287463585876802
Loss at iteration [836]: 0.7287463585876802
Loss at iteration [837]: 0.7287449765358722
Loss at iteration [838]: 0.7287438711795217
Loss at iteration [839]: 0.7287428406260166
Loss at iteration [840]: 0.7287415585243995
Loss at iteration [841]: 0.7287390757423381
Loss at iteration [842]: 0.7287390757423381
Loss at iteration [843]: 0.7287354697024494
Loss at iteration [844]: 0.7287339945032639
Loss at iteration [845]: 0.7287331764931272
Loss at iteration [846]: 0.7287320576242691
Loss at iteration [847]: 0.7287307366039028
Loss at iteration [848]: 0.7287307366039028
Loss at iteration [849]: 0.7287271645916397
Loss at iteration [850]: 0.7287263226656912
Loss at iteration [851]: 0.7287253659365558
Loss at iteration [852]: 0.7287250339081854
Loss at iteration [853]: 0.7287242308871257
Loss at iteration [854]: 0.7287242308871257
Loss at iteration [855]: 0.7287223553484158
Loss at iteration [856]: 0.728721765687274
Loss at iteration [857]: 0.7287208559698197
Loss at iteration [858]: 0.7287198934117641
Loss at iteration [859]: 0.7287194712942797
Loss at iteration [860]: 0.7287194712942797
Loss at iteration [861]: 0.728716344559946
Loss at iteration [862]: 0.7287154885984176
Loss at iteration [863]: 0.72871484384744
Loss at iteration [864]: 0.7287148218584671
Loss at iteration [865]: 0.7287136579790746
Loss at iteration [866]: 0.7287136579790746
Loss at iteration [867]: 0.7287135600153729
Loss at iteration [868]: 0.7287119427168975
Loss at iteration [869]: 0.7287113180079905
Loss at iteration [870]: 0.7287101304908385
Loss at iteration [871]: 0.7287097398668055
Loss at iteration [872]: 0.7287097398668055
Loss at iteration [873]: 0.7287060963405267
Loss at iteration [874]: 0.7287048359816698
Loss at iteration [875]: 0.7287037735271874
Loss at iteration [876]: 0.7287033980944095
Loss at iteration [877]: 0.7287020584009753
Loss at iteration [878]: 0.7287020584009753
Loss at iteration [879]: 0.7287011281578082
Loss at iteration [880]: 0.728700671608951
Loss at iteration [881]: 0.7286997502059552
Loss at iteration [882]: 0.7286992795186396
Loss at iteration [883]: 0.7286987079986518
Loss at iteration [884]: 0.7286987079986518
Loss at iteration [885]: 0.7286977024723404
Loss at iteration [886]: 0.7286970581466229
Loss at iteration [887]: 0.7286966919110498
Loss at iteration [888]: 0.7286963366078993
Loss at iteration [889]: 0.7286954003046008
Loss at iteration [890]: 0.7286954003046008
Loss at iteration [891]: 0.7286946563154013
Loss at iteration [892]: 0.7286943521785568
Loss at iteration [893]: 0.7286935885052572
Loss at iteration [894]: 0.7286932476413532
Loss at iteration [895]: 0.7286926030036515
Loss at iteration [896]: 0.7286926030036515
Loss at iteration [897]: 0.7286919173100093
Loss at iteration [898]: 0.7286915694902077
Loss at iteration [899]: 0.7286912921713088
Loss at iteration [900]: 0.7286910601328853
Loss at iteration [901]: 0.7286910601328853
Loss at iteration [902]: 0.7286907793739664
Loss at iteration [903]: 0.7286907402217099
Loss at iteration [904]: 0.7286905885655485
Loss at iteration [905]: 0.7286904587696713
Loss at iteration [906]: 0.7286904587696713
Loss at iteration [907]: 0.7286898821140965
Loss at iteration [908]: 0.7286896901731137
Loss at iteration [909]: 0.7286894000423246
Loss at iteration [910]: 0.7286892144945246
