Model name                            : MLP_Multistep
The number of input features          : 5
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :FR_PR
Total number of function evaluations  : 485
Total number of iterations            : 237
Max number of iterations              : 3000
Number of samples in training data    : 122
Number of samples in tests data       : 52
Total training time                   : 1.0958828926086426
Total number of parameters            : 202302
Percentage of parameters < 1e-9       : 49.91794445927376%
Percentage of parameters < 1e-7       : 49.91794445927376%
Percentage of parameters < 1e-6       : 49.91843876976006%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.5882378173815089
Loss at iteration [2]: 0.5842982002519567
Loss at iteration [3]: 0.5753527831732391
Loss at iteration [4]: 0.5696710605197712
Loss at iteration [5]: 0.5655258803135828
Loss at iteration [6]: 0.5458141466478944
Loss at iteration [7]: 0.5424489904818612
Loss at iteration [8]: 0.5360764196114384
Loss at iteration [9]: 0.5346210222985336
Loss at iteration [10]: 0.52175415392158
Loss at iteration [11]: 0.5196329647563749
Loss at iteration [12]: 0.5122784323871522
Loss at iteration [13]: 0.5122784323871522
Loss at iteration [14]: 0.5072924822545498
Loss at iteration [15]: 0.49166446104458356
Loss at iteration [16]: 0.4868640726364224
Loss at iteration [17]: 0.4779354867374027
Loss at iteration [18]: 0.46666292573394846
Loss at iteration [19]: 0.4559804043049271
Loss at iteration [20]: 0.4397918771183657
Loss at iteration [21]: 0.3903064187021221
Loss at iteration [22]: 0.38165496839323604
Loss at iteration [23]: 0.3533399078122406
Loss at iteration [24]: 0.349056113322724
Loss at iteration [25]: 0.349056113322724
Loss at iteration [26]: 0.34620556991736806
Loss at iteration [27]: 0.33895116138850573
Loss at iteration [28]: 0.33759106533914235
Loss at iteration [29]: 0.33476567696748544
Loss at iteration [30]: 0.3297023815143753
Loss at iteration [31]: 0.32718577334874
Loss at iteration [32]: 0.3234168956039647
Loss at iteration [33]: 0.31852483279020083
Loss at iteration [34]: 0.3154099176539996
Loss at iteration [35]: 0.30671612701433876
Loss at iteration [36]: 0.3048912260676715
Loss at iteration [37]: 0.29923999908437154
Loss at iteration [38]: 0.2982999171300525
Loss at iteration [39]: 0.2982999171300525
Loss at iteration [40]: 0.29780904097589483
Loss at iteration [41]: 0.292899866227142
Loss at iteration [42]: 0.292251104866965
Loss at iteration [43]: 0.28937706416137265
Loss at iteration [44]: 0.28190492817105917
Loss at iteration [45]: 0.2812579284686888
Loss at iteration [46]: 0.2760062625183442
Loss at iteration [47]: 0.27554376935334496
Loss at iteration [48]: 0.2694405250835546
Loss at iteration [49]: 0.2694405250835546
Loss at iteration [50]: 0.2688472634215684
Loss at iteration [51]: 0.2669473447062608
Loss at iteration [52]: 0.2659356047834234
Loss at iteration [53]: 0.2646621107702441
Loss at iteration [54]: 0.26372304144135944
Loss at iteration [55]: 0.26288076178924213
Loss at iteration [56]: 0.2624015330583831
Loss at iteration [57]: 0.26129192697301556
Loss at iteration [58]: 0.25994972065001326
Loss at iteration [59]: 0.25904806668267194
Loss at iteration [60]: 0.2582601536345501
Loss at iteration [61]: 0.2566285357073653
Loss at iteration [62]: 0.2561707793620313
Loss at iteration [63]: 0.2561707793620313
Loss at iteration [64]: 0.2560493332093168
Loss at iteration [65]: 0.2548329813131241
Loss at iteration [66]: 0.2543922287939439
Loss at iteration [67]: 0.25378960036525966
Loss at iteration [68]: 0.2534456666524841
Loss at iteration [69]: 0.2526995789552402
Loss at iteration [70]: 0.2518170812759047
Loss at iteration [71]: 0.2507518862006033
Loss at iteration [72]: 0.2500147112868161
Loss at iteration [73]: 0.2494855688898761
Loss at iteration [74]: 0.24765569125066134
Loss at iteration [75]: 0.246773368891395
Loss at iteration [76]: 0.24582668382272505
Loss at iteration [77]: 0.24582668382272505
Loss at iteration [78]: 0.24548972547634362
Loss at iteration [79]: 0.2442448964760481
Loss at iteration [80]: 0.24336876223820117
Loss at iteration [81]: 0.24262878529832832
Loss at iteration [82]: 0.24210158393532283
Loss at iteration [83]: 0.2414612046086616
Loss at iteration [84]: 0.24123367791787015
Loss at iteration [85]: 0.24074481141047285
Loss at iteration [86]: 0.24049185025626946
Loss at iteration [87]: 0.23978488041932267
Loss at iteration [88]: 0.23971363469370796
Loss at iteration [89]: 0.23937271482333317
Loss at iteration [90]: 0.2390693557786866
Loss at iteration [91]: 0.238798208746069
Loss at iteration [92]: 0.2384734735713146
Loss at iteration [93]: 0.2384734735713146
Loss at iteration [94]: 0.23823959229407363
Loss at iteration [95]: 0.23807836015933395
Loss at iteration [96]: 0.23777180574586324
Loss at iteration [97]: 0.23766995491072504
Loss at iteration [98]: 0.23712069645888348
Loss at iteration [99]: 0.23689599074757403
Loss at iteration [100]: 0.2368929651255579
Loss at iteration [101]: 0.2357891976996112
Loss at iteration [102]: 0.2355815946815754
Loss at iteration [103]: 0.23545748737853783
Loss at iteration [104]: 0.235244770928306
Loss at iteration [105]: 0.2350627886181096
Loss at iteration [106]: 0.23476296866270024
Loss at iteration [107]: 0.23227938498721337
Loss at iteration [108]: 0.23227938498721337
Loss at iteration [109]: 0.23073055570971934
Loss at iteration [110]: 0.23065994329840206
Loss at iteration [111]: 0.2303353730570133
Loss at iteration [112]: 0.2302183832227404
Loss at iteration [113]: 0.22988102547264072
Loss at iteration [114]: 0.22971840614439762
Loss at iteration [115]: 0.22944318570381694
Loss at iteration [116]: 0.22926899541872853
Loss at iteration [117]: 0.22915949058849316
Loss at iteration [118]: 0.2289287065132177
Loss at iteration [119]: 0.22832267422345257
Loss at iteration [120]: 0.22771771780848474
Loss at iteration [121]: 0.22717355580830847
Loss at iteration [122]: 0.22663965545682047
Loss at iteration [123]: 0.2262086639427115
Loss at iteration [124]: 0.22589881251211807
Loss at iteration [125]: 0.22589881251211807
Loss at iteration [126]: 0.22539338396574216
Loss at iteration [127]: 0.22454056922385793
Loss at iteration [128]: 0.22443754515985978
Loss at iteration [129]: 0.2242278452660353
Loss at iteration [130]: 0.2239245256886117
Loss at iteration [131]: 0.22382252824349594
Loss at iteration [132]: 0.22370788099168742
Loss at iteration [133]: 0.2233326998721152
Loss at iteration [134]: 0.2227285024950381
Loss at iteration [135]: 0.22253438504391315
Loss at iteration [136]: 0.22179367645525455
Loss at iteration [137]: 0.2217013781033531
Loss at iteration [138]: 0.22131682512496686
Loss at iteration [139]: 0.22066433335450136
Loss at iteration [140]: 0.21907900453130702
Loss at iteration [141]: 0.21818274340885324
Loss at iteration [142]: 0.21818274340885324
Loss at iteration [143]: 0.21786147649890203
Loss at iteration [144]: 0.21774670294481402
Loss at iteration [145]: 0.21659999107094355
Loss at iteration [146]: 0.21612814174387238
Loss at iteration [147]: 0.2154031539390636
Loss at iteration [148]: 0.21508417154213857
Loss at iteration [149]: 0.21499974487388612
Loss at iteration [150]: 0.21457379391908166
Loss at iteration [151]: 0.21436251961440944
Loss at iteration [152]: 0.2140494708864733
Loss at iteration [153]: 0.21379518387228302
Loss at iteration [154]: 0.2136737771152453
Loss at iteration [155]: 0.2135053786163179
Loss at iteration [156]: 0.21337275886266227
Loss at iteration [157]: 0.2131536456291527
Loss at iteration [158]: 0.2131536456291527
Loss at iteration [159]: 0.21283421490650686
Loss at iteration [160]: 0.21247403825398703
Loss at iteration [161]: 0.21240206729358185
Loss at iteration [162]: 0.21230042991097955
Loss at iteration [163]: 0.21218604574903555
Loss at iteration [164]: 0.21214237349704831
Loss at iteration [165]: 0.21202348914342306
Loss at iteration [166]: 0.211881813404709
Loss at iteration [167]: 0.2117585212234111
Loss at iteration [168]: 0.21154550758796448
Loss at iteration [169]: 0.21133094967713992
Loss at iteration [170]: 0.2110637750903097
Loss at iteration [171]: 0.21086792149144612
Loss at iteration [172]: 0.21086792149144612
Loss at iteration [173]: 0.21074943544191374
Loss at iteration [174]: 0.21074624807001185
Loss at iteration [175]: 0.21066750807888243
Loss at iteration [176]: 0.21059747823473682
Loss at iteration [177]: 0.21049807138357074
Loss at iteration [178]: 0.21037419521540332
Loss at iteration [179]: 0.21027334994656985
Loss at iteration [180]: 0.21011685475551203
Loss at iteration [181]: 0.21000120773220265
Loss at iteration [182]: 0.20983678096726158
Loss at iteration [183]: 0.20964159088023648
Loss at iteration [184]: 0.20942379368915656
Loss at iteration [185]: 0.20942379368915656
Loss at iteration [186]: 0.20917543770081654
Loss at iteration [187]: 0.20862993434829422
Loss at iteration [188]: 0.2085679109885395
Loss at iteration [189]: 0.20850762849960017
Loss at iteration [190]: 0.2083607384335125
Loss at iteration [191]: 0.20809606698075264
Loss at iteration [192]: 0.20797524056907754
Loss at iteration [193]: 0.207830197700991
Loss at iteration [194]: 0.20747761566208872
Loss at iteration [195]: 0.2072507780279123
Loss at iteration [196]: 0.2067919497034452
Loss at iteration [197]: 0.2065752750974346
Loss at iteration [198]: 0.2061820336850908
Loss at iteration [199]: 0.20583869811933733
Loss at iteration [200]: 0.20547356133536984
Loss at iteration [201]: 0.20547356133536984
Loss at iteration [202]: 0.20540081602941448
Loss at iteration [203]: 0.20523545863062967
Loss at iteration [204]: 0.2049122234306236
Loss at iteration [205]: 0.20455881132279052
Loss at iteration [206]: 0.20442717720081935
Loss at iteration [207]: 0.2044227937242094
Loss at iteration [208]: 0.20429785840204656
Loss at iteration [209]: 0.20407542504414702
Loss at iteration [210]: 0.20384813792685422
Loss at iteration [211]: 0.20377532034253157
Loss at iteration [212]: 0.20363015284645763
Loss at iteration [213]: 0.20355276961463367
Loss at iteration [214]: 0.20341533346163196
Loss at iteration [215]: 0.20341533346163196
Loss at iteration [216]: 0.20335958229414328
Loss at iteration [217]: 0.20324826040539892
Loss at iteration [218]: 0.20321553594007782
Loss at iteration [219]: 0.20314572470805237
Loss at iteration [220]: 0.20307497177704156
Loss at iteration [221]: 0.2030250338724057
Loss at iteration [222]: 0.2029873321003913
Loss at iteration [223]: 0.20293231434727688
Loss at iteration [224]: 0.20285682175156589
Loss at iteration [225]: 0.20277187453615106
Loss at iteration [226]: 0.2026389733923272
Loss at iteration [227]: 0.20247257220273232
Loss at iteration [228]: 0.20247257220273232
Loss at iteration [229]: 0.2024624572412274
Loss at iteration [230]: 0.2024018293410483
Loss at iteration [231]: 0.20236825388522092
Loss at iteration [232]: 0.20227815459444728
Loss at iteration [233]: 0.20220973284666882
Loss at iteration [234]: 0.20217465917746894
Loss at iteration [235]: 0.20216440316694834
Loss at iteration [236]: 0.20210466973105562
Loss at iteration [237]: 0.20208738633080603
Loss at iteration [238]: 0.20206369016975265
Loss at iteration [239]: 0.20195291517709066
Loss at iteration [240]: 0.20195291517709066
Loss at iteration [241]: 0.20184229038009738
Loss at iteration [242]: 0.20169055693927113
Loss at iteration [243]: 0.20164756847024792
Loss at iteration [244]: 0.2015860494091853
Loss at iteration [245]: 0.20155257796429588
Loss at iteration [246]: 0.20152059935734148
Loss at iteration [247]: 0.20149707574406645
