Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :HS
Total number of function evaluations  : 3028
Total number of iterations            : 935
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 8.658651351928711
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 49.89797030242395%
Percentage of parameters < 1e-7       : 49.89797030242395%
Percentage of parameters < 1e-6       : 49.898465592218%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.2574618503599728
Loss at iteration [2]: 1.2531326488054813
Loss at iteration [3]: 1.2477673583976028
Loss at iteration [4]: 1.2395389599379232
Loss at iteration [5]: 1.1987397799994517
Loss at iteration [6]: 1.1987397799994517
Loss at iteration [7]: 1.1859876564835794
Loss at iteration [8]: 1.1742845573905683
Loss at iteration [9]: 1.166512281612887
Loss at iteration [10]: 1.152978223055301
Loss at iteration [11]: 1.140727507951293
Loss at iteration [12]: 1.140727507951293
Loss at iteration [13]: 1.13732444285844
Loss at iteration [14]: 1.1304064288757094
Loss at iteration [15]: 1.1161260967461366
Loss at iteration [16]: 1.1115277654012765
Loss at iteration [17]: 1.1063470037731409
Loss at iteration [18]: 1.1063470037731409
Loss at iteration [19]: 1.1015694952003192
Loss at iteration [20]: 1.098179231776322
Loss at iteration [21]: 1.097073449347586
Loss at iteration [22]: 1.0932258117871227
Loss at iteration [23]: 1.0871896324140753
Loss at iteration [24]: 1.0871896324140753
Loss at iteration [25]: 1.0838281838734671
Loss at iteration [26]: 1.0793668460443535
Loss at iteration [27]: 1.0743961225186134
Loss at iteration [28]: 1.0685685651256567
Loss at iteration [29]: 1.0549930518962043
Loss at iteration [30]: 1.0549930518962043
Loss at iteration [31]: 1.0522175739140234
Loss at iteration [32]: 1.048362511713497
Loss at iteration [33]: 1.042510080187869
Loss at iteration [34]: 1.039179592153292
Loss at iteration [35]: 1.029288858140646
Loss at iteration [36]: 1.029288858140646
Loss at iteration [37]: 1.0268028050826292
Loss at iteration [38]: 1.0205732438543342
Loss at iteration [39]: 1.015915819451375
Loss at iteration [40]: 1.011920097968692
Loss at iteration [41]: 0.9994060199982373
Loss at iteration [42]: 0.9994060199982373
Loss at iteration [43]: 0.9952515745848127
Loss at iteration [44]: 0.9845321847265208
Loss at iteration [45]: 0.9827564813005792
Loss at iteration [46]: 0.9785917626298329
Loss at iteration [47]: 0.9739712745294851
Loss at iteration [48]: 0.9739712745294851
Loss at iteration [49]: 0.9721148640564082
Loss at iteration [50]: 0.9678892315702566
Loss at iteration [51]: 0.9665063732137787
Loss at iteration [52]: 0.963560149551122
Loss at iteration [53]: 0.9592156076941003
Loss at iteration [54]: 0.9592156076941003
Loss at iteration [55]: 0.9577931653604412
Loss at iteration [56]: 0.955054444945961
Loss at iteration [57]: 0.9515812243856953
Loss at iteration [58]: 0.9500751613903847
Loss at iteration [59]: 0.9372749463395992
Loss at iteration [60]: 0.9372749463395992
Loss at iteration [61]: 0.9337191872981219
Loss at iteration [62]: 0.9319269840325225
Loss at iteration [63]: 0.9273148931984897
Loss at iteration [64]: 0.9256657281319951
Loss at iteration [65]: 0.9225715493057254
Loss at iteration [66]: 0.9203700601276135
Loss at iteration [67]: 0.9203700601276135
Loss at iteration [68]: 0.9193950007567059
Loss at iteration [69]: 0.9175385010200383
Loss at iteration [70]: 0.9168486243649266
Loss at iteration [71]: 0.915020535615519
Loss at iteration [72]: 0.9134829478765923
Loss at iteration [73]: 0.9115842207002011
Loss at iteration [74]: 0.9115842207002011
Loss at iteration [75]: 0.9100802307189519
Loss at iteration [76]: 0.9082720458740081
Loss at iteration [77]: 0.9072603672600434
Loss at iteration [78]: 0.9052444718825006
Loss at iteration [79]: 0.9043376751990292
Loss at iteration [80]: 0.9032094242048723
Loss at iteration [81]: 0.9032094242048723
Loss at iteration [82]: 0.9027132246368033
Loss at iteration [83]: 0.9018485884761941
Loss at iteration [84]: 0.901393822235951
Loss at iteration [85]: 0.9000412193842868
Loss at iteration [86]: 0.8981784876999835
Loss at iteration [87]: 0.8807512324292073
Loss at iteration [88]: 0.8807512324292073
Loss at iteration [89]: 0.8708736741597249
Loss at iteration [90]: 0.8667922343691795
Loss at iteration [91]: 0.8634847012460598
Loss at iteration [92]: 0.8589292385302834
Loss at iteration [93]: 0.8554854701875937
Loss at iteration [94]: 0.8528453462294668
Loss at iteration [95]: 0.8528453462294668
Loss at iteration [96]: 0.8516995445972749
Loss at iteration [97]: 0.8500334077450921
Loss at iteration [98]: 0.8485096620088896
Loss at iteration [99]: 0.8469272142730958
Loss at iteration [100]: 0.8445047919072595
Loss at iteration [101]: 0.8434678015830488
Loss at iteration [102]: 0.8434678015830488
Loss at iteration [103]: 0.8426214970046481
Loss at iteration [104]: 0.8417696776182484
Loss at iteration [105]: 0.8410216709851324
Loss at iteration [106]: 0.8403123005532577
Loss at iteration [107]: 0.8396745522692246
Loss at iteration [108]: 0.8386681546961239
Loss at iteration [109]: 0.8386681546961239
Loss at iteration [110]: 0.8376894142648206
Loss at iteration [111]: 0.8366341762958491
Loss at iteration [112]: 0.8361411377519603
Loss at iteration [113]: 0.8352817701386881
Loss at iteration [114]: 0.8348688076890763
Loss at iteration [115]: 0.8342005727003428
Loss at iteration [116]: 0.8342005727003428
Loss at iteration [117]: 0.833760408377387
Loss at iteration [118]: 0.8334323901127444
Loss at iteration [119]: 0.832807549876353
Loss at iteration [120]: 0.8321991847050934
Loss at iteration [121]: 0.8305968267965886
Loss at iteration [122]: 0.8305968267965886
Loss at iteration [123]: 0.8295572374507484
Loss at iteration [124]: 0.8292208717324707
Loss at iteration [125]: 0.8285290191765101
Loss at iteration [126]: 0.828102263863764
Loss at iteration [127]: 0.8278295945758235
Loss at iteration [128]: 0.8274776086863883
Loss at iteration [129]: 0.8268022524306704
Loss at iteration [130]: 0.8268022524306704
Loss at iteration [131]: 0.8263897851518004
Loss at iteration [132]: 0.8261055549534119
Loss at iteration [133]: 0.8259075857660675
Loss at iteration [134]: 0.8256113779524874
Loss at iteration [135]: 0.8254162221779164
Loss at iteration [136]: 0.8251606820283488
Loss at iteration [137]: 0.8248328827442375
Loss at iteration [138]: 0.8248328827442375
Loss at iteration [139]: 0.8246310434086711
Loss at iteration [140]: 0.8244439156512913
Loss at iteration [141]: 0.8241834923472713
Loss at iteration [142]: 0.823382634147562
Loss at iteration [143]: 0.8210303626761545
Loss at iteration [144]: 0.8202328006185842
Loss at iteration [145]: 0.8113149918291397
Loss at iteration [146]: 0.8113149918291397
Loss at iteration [147]: 0.8096336624026446
Loss at iteration [148]: 0.8071129065241782
Loss at iteration [149]: 0.8048014782258619
Loss at iteration [150]: 0.8036776052822323
Loss at iteration [151]: 0.8020815960810634
Loss at iteration [152]: 0.8012277075317006
Loss at iteration [153]: 0.8012277075317006
Loss at iteration [154]: 0.8009280927670388
Loss at iteration [155]: 0.8002523666059357
Loss at iteration [156]: 0.7996241275148155
Loss at iteration [157]: 0.7992179586544877
Loss at iteration [158]: 0.7986923174278093
Loss at iteration [159]: 0.7982826022759404
Loss at iteration [160]: 0.7982826022759404
Loss at iteration [161]: 0.7979673449294369
Loss at iteration [162]: 0.7977780238715293
Loss at iteration [163]: 0.7974879684387427
Loss at iteration [164]: 0.7971998088328984
Loss at iteration [165]: 0.7965534909273074
Loss at iteration [166]: 0.7959948137654347
Loss at iteration [167]: 0.7947318753935918
Loss at iteration [168]: 0.7947318753935918
Loss at iteration [169]: 0.7941910890732204
Loss at iteration [170]: 0.7938222585975191
Loss at iteration [171]: 0.7933144382559657
Loss at iteration [172]: 0.7930120068006445
Loss at iteration [173]: 0.7926687590014935
Loss at iteration [174]: 0.7924337984090638
Loss at iteration [175]: 0.791806576616571
Loss at iteration [176]: 0.791806576616571
Loss at iteration [177]: 0.7914153216577722
Loss at iteration [178]: 0.7910602113621384
Loss at iteration [179]: 0.7908543092940179
Loss at iteration [180]: 0.7905820597183338
Loss at iteration [181]: 0.7904238229072676
Loss at iteration [182]: 0.790320140450108
Loss at iteration [183]: 0.790320140450108
Loss at iteration [184]: 0.7900521997120574
Loss at iteration [185]: 0.7898145411526731
Loss at iteration [186]: 0.7897168275006751
Loss at iteration [187]: 0.7896328169416106
Loss at iteration [188]: 0.7894297565292145
Loss at iteration [189]: 0.7889221243708552
Loss at iteration [190]: 0.7884443703346977
Loss at iteration [191]: 0.7884443703346977
Loss at iteration [192]: 0.7881572615052759
Loss at iteration [193]: 0.787795488698048
Loss at iteration [194]: 0.7875490548464824
Loss at iteration [195]: 0.787324466683046
Loss at iteration [196]: 0.7872031686401043
Loss at iteration [197]: 0.7870605925507816
Loss at iteration [198]: 0.7869205715976677
Loss at iteration [199]: 0.7869205715976677
Loss at iteration [200]: 0.786801027766686
Loss at iteration [201]: 0.7866905118569214
Loss at iteration [202]: 0.7866178201216245
Loss at iteration [203]: 0.7865246576689163
Loss at iteration [204]: 0.7864507443579395
Loss at iteration [205]: 0.7863952602018841
Loss at iteration [206]: 0.7862806191043219
Loss at iteration [207]: 0.7861385587419538
Loss at iteration [208]: 0.7861385587419538
Loss at iteration [209]: 0.7860886275683473
Loss at iteration [210]: 0.7860535737606618
Loss at iteration [211]: 0.7860193520766492
Loss at iteration [212]: 0.7859463365852024
Loss at iteration [213]: 0.7858244595297431
Loss at iteration [214]: 0.7855983861497526
Loss at iteration [215]: 0.7848477967336615
Loss at iteration [216]: 0.7848477967336615
Loss at iteration [217]: 0.7844469364812713
Loss at iteration [218]: 0.7841339059610384
Loss at iteration [219]: 0.7840171397227028
Loss at iteration [220]: 0.7839460789924053
Loss at iteration [221]: 0.7838808216655139
Loss at iteration [222]: 0.7837266678818049
Loss at iteration [223]: 0.7834260975584627
Loss at iteration [224]: 0.7834260975584627
Loss at iteration [225]: 0.7831123390349155
Loss at iteration [226]: 0.7827836769295545
Loss at iteration [227]: 0.7826713867641348
Loss at iteration [228]: 0.7824897767566654
Loss at iteration [229]: 0.782359172126663
Loss at iteration [230]: 0.7822426693982903
Loss at iteration [231]: 0.7821408203453837
Loss at iteration [232]: 0.7821408203453837
Loss at iteration [233]: 0.7820854350974955
Loss at iteration [234]: 0.7820595693074736
Loss at iteration [235]: 0.7819889376071733
Loss at iteration [236]: 0.7818955041462126
Loss at iteration [237]: 0.7817913951677132
Loss at iteration [238]: 0.7816357822112104
Loss at iteration [239]: 0.7814018562167668
Loss at iteration [240]: 0.7814018562167668
Loss at iteration [241]: 0.7812298579912436
Loss at iteration [242]: 0.7811977222371076
Loss at iteration [243]: 0.781105177485828
Loss at iteration [244]: 0.7810373788381736
Loss at iteration [245]: 0.7809291847968212
Loss at iteration [246]: 0.7807788045923367
Loss at iteration [247]: 0.7805858871891356
Loss at iteration [248]: 0.7805858871891356
Loss at iteration [249]: 0.7805411030364277
Loss at iteration [250]: 0.7804798008881525
Loss at iteration [251]: 0.7804323400690892
Loss at iteration [252]: 0.7803506219411374
Loss at iteration [253]: 0.7802529484431777
Loss at iteration [254]: 0.7801180219471094
Loss at iteration [255]: 0.7796665685497804
Loss at iteration [256]: 0.7789767866126581
Loss at iteration [257]: 0.7789767866126581
Loss at iteration [258]: 0.7785829292909795
Loss at iteration [259]: 0.7783092833862055
Loss at iteration [260]: 0.7780961959860468
Loss at iteration [261]: 0.7779637377912955
Loss at iteration [262]: 0.7777502423877499
Loss at iteration [263]: 0.777615815711541
Loss at iteration [264]: 0.7774344003209215
Loss at iteration [265]: 0.7774344003209215
Loss at iteration [266]: 0.7772521294435485
Loss at iteration [267]: 0.7771637258894944
Loss at iteration [268]: 0.777070883945625
Loss at iteration [269]: 0.7770258013785869
Loss at iteration [270]: 0.7769708391836837
Loss at iteration [271]: 0.7769347441644715
Loss at iteration [272]: 0.7768897872825392
Loss at iteration [273]: 0.7768897872825392
Loss at iteration [274]: 0.7768609942941416
Loss at iteration [275]: 0.7768358507265816
Loss at iteration [276]: 0.7768120345206161
Loss at iteration [277]: 0.7767715866512781
Loss at iteration [278]: 0.7767358765943063
Loss at iteration [279]: 0.7766436852298854
Loss at iteration [280]: 0.7765765830350363
Loss at iteration [281]: 0.7764635457350356
Loss at iteration [282]: 0.7764635457350356
Loss at iteration [283]: 0.7763779404755068
Loss at iteration [284]: 0.7763379912495857
Loss at iteration [285]: 0.7763202328540575
Loss at iteration [286]: 0.7762760752475282
Loss at iteration [287]: 0.7762300000853891
Loss at iteration [288]: 0.7761554058091285
Loss at iteration [289]: 0.7761275598843121
Loss at iteration [290]: 0.7760021546278375
Loss at iteration [291]: 0.7760021546278375
Loss at iteration [292]: 0.7759202717716802
Loss at iteration [293]: 0.7759030726718098
Loss at iteration [294]: 0.7758780532552522
Loss at iteration [295]: 0.7758582534913137
Loss at iteration [296]: 0.7758370985219355
Loss at iteration [297]: 0.7758246640260454
Loss at iteration [298]: 0.7758033688517654
Loss at iteration [299]: 0.7757841323217405
Loss at iteration [300]: 0.7757456413020895
Loss at iteration [301]: 0.7757096858589461
Loss at iteration [302]: 0.7757096858589461
Loss at iteration [303]: 0.7756801066769647
Loss at iteration [304]: 0.7756687931817181
Loss at iteration [305]: 0.7756621795451746
Loss at iteration [306]: 0.7756526173973932
Loss at iteration [307]: 0.7756429456866885
Loss at iteration [308]: 0.7756378216565688
Loss at iteration [309]: 0.7756267103702428
Loss at iteration [310]: 0.7756119329917697
Loss at iteration [311]: 0.7755855964016796
Loss at iteration [312]: 0.7754258846142471
Loss at iteration [313]: 0.7754258846142471
Loss at iteration [314]: 0.7752903017579796
Loss at iteration [315]: 0.7752367372112692
Loss at iteration [316]: 0.7752150354532543
Loss at iteration [317]: 0.7751851320905728
Loss at iteration [318]: 0.7751450265916768
Loss at iteration [319]: 0.7751327912792835
Loss at iteration [320]: 0.7751037381829988
Loss at iteration [321]: 0.7750614788614751
Loss at iteration [322]: 0.774970606614855
Loss at iteration [323]: 0.774970606614855
Loss at iteration [324]: 0.7748547574420562
Loss at iteration [325]: 0.7748105380686682
Loss at iteration [326]: 0.7747800827711627
Loss at iteration [327]: 0.7747211136940129
Loss at iteration [328]: 0.7746619608156501
Loss at iteration [329]: 0.7745370217629092
Loss at iteration [330]: 0.7745370217629092
Loss at iteration [331]: 0.7743810472285335
Loss at iteration [332]: 0.7743389749752028
Loss at iteration [333]: 0.7743104880690208
Loss at iteration [334]: 0.7742576704343201
Loss at iteration [335]: 0.7742017607150868
Loss at iteration [336]: 0.7741626145656535
Loss at iteration [337]: 0.7740962173818523
Loss at iteration [338]: 0.774016803238379
Loss at iteration [339]: 0.7739331039467907
Loss at iteration [340]: 0.7739331039467907
Loss at iteration [341]: 0.7738930659284912
Loss at iteration [342]: 0.7738638075464016
Loss at iteration [343]: 0.7738281376239214
Loss at iteration [344]: 0.7738010626850318
Loss at iteration [345]: 0.7737778805695791
Loss at iteration [346]: 0.7737291620955599
Loss at iteration [347]: 0.7736822360055214
Loss at iteration [348]: 0.7736122912193095
Loss at iteration [349]: 0.7733227026474314
Loss at iteration [350]: 0.7733227026474314
Loss at iteration [351]: 0.7731387232466783
Loss at iteration [352]: 0.77304502393745
Loss at iteration [353]: 0.7729641368352645
Loss at iteration [354]: 0.7729021191257587
Loss at iteration [355]: 0.7727946163622699
Loss at iteration [356]: 0.7726881095077414
Loss at iteration [357]: 0.7726291921434458
Loss at iteration [358]: 0.7725586376537055
Loss at iteration [359]: 0.7725586376537055
Loss at iteration [360]: 0.7724735556960437
Loss at iteration [361]: 0.7724392822299749
Loss at iteration [362]: 0.7723997298244947
Loss at iteration [363]: 0.7723768150522643
Loss at iteration [364]: 0.7723432369650799
Loss at iteration [365]: 0.7723197701228859
Loss at iteration [366]: 0.772292644280969
Loss at iteration [367]: 0.7722537783871865
Loss at iteration [368]: 0.7722537783871865
Loss at iteration [369]: 0.7722429453398262
Loss at iteration [370]: 0.772235754734603
Loss at iteration [371]: 0.7722227961187327
Loss at iteration [372]: 0.7721934237349591
Loss at iteration [373]: 0.7721799594903579
Loss at iteration [374]: 0.772142626556068
Loss at iteration [375]: 0.7720908639356612
Loss at iteration [376]: 0.7720225674425972
Loss at iteration [377]: 0.7719333584979294
Loss at iteration [378]: 0.7717867199052881
Loss at iteration [379]: 0.7717867199052881
Loss at iteration [380]: 0.7717345018209402
Loss at iteration [381]: 0.7717106230314902
Loss at iteration [382]: 0.7716922592285277
Loss at iteration [383]: 0.7716630437711658
Loss at iteration [384]: 0.7716393588158768
Loss at iteration [385]: 0.7716247442669036
Loss at iteration [386]: 0.7716039647577482
Loss at iteration [387]: 0.7715721732921924
Loss at iteration [388]: 0.7715322155848067
Loss at iteration [389]: 0.7715322155848067
Loss at iteration [390]: 0.7715030848423235
Loss at iteration [391]: 0.7714892428209406
Loss at iteration [392]: 0.7714771383260918
Loss at iteration [393]: 0.7714579004886208
Loss at iteration [394]: 0.7714263551211817
Loss at iteration [395]: 0.771391229192777
Loss at iteration [396]: 0.7713521513142044
Loss at iteration [397]: 0.7712988306860863
Loss at iteration [398]: 0.7712353411531652
Loss at iteration [399]: 0.7712353411531652
Loss at iteration [400]: 0.7711923649417833
Loss at iteration [401]: 0.7711627893258992
Loss at iteration [402]: 0.7711548140238715
Loss at iteration [403]: 0.7711218228184809
Loss at iteration [404]: 0.7710875027768656
Loss at iteration [405]: 0.7710781075518075
Loss at iteration [406]: 0.771042832411486
Loss at iteration [407]: 0.77102202740122
Loss at iteration [408]: 0.7710028151983225
Loss at iteration [409]: 0.7709899597364842
Loss at iteration [410]: 0.7709540795058846
Loss at iteration [411]: 0.7709540795058846
Loss at iteration [412]: 0.7709389970641114
Loss at iteration [413]: 0.770928314264582
Loss at iteration [414]: 0.7709175682207547
Loss at iteration [415]: 0.7709043240429216
Loss at iteration [416]: 0.7709007901039789
Loss at iteration [417]: 0.7708885026451281
Loss at iteration [418]: 0.77087537672371
Loss at iteration [419]: 0.7708699178581516
Loss at iteration [420]: 0.7708524479490372
Loss at iteration [421]: 0.7708345550856477
Loss at iteration [422]: 0.7708194595869898
Loss at iteration [423]: 0.7707664052040982
Loss at iteration [424]: 0.7707664052040982
Loss at iteration [425]: 0.7707030860173036
Loss at iteration [426]: 0.7706733649635317
Loss at iteration [427]: 0.7706525296070179
Loss at iteration [428]: 0.7706327997327171
Loss at iteration [429]: 0.7706269597692079
Loss at iteration [430]: 0.770608675556658
Loss at iteration [431]: 0.7705895148035068
Loss at iteration [432]: 0.7705756832449577
Loss at iteration [433]: 0.7705611905814579
Loss at iteration [434]: 0.7705470558453195
Loss at iteration [435]: 0.770504297855359
Loss at iteration [436]: 0.770504297855359
Loss at iteration [437]: 0.7704729298541867
Loss at iteration [438]: 0.7704575705249019
Loss at iteration [439]: 0.7704359836158992
Loss at iteration [440]: 0.7704176486530361
Loss at iteration [441]: 0.7703944719168281
Loss at iteration [442]: 0.7703799346349602
Loss at iteration [443]: 0.770323673112004
Loss at iteration [444]: 0.7698779977079308
Loss at iteration [445]: 0.7698779977079308
Loss at iteration [446]: 0.7697728337196884
Loss at iteration [447]: 0.7695673358894195
Loss at iteration [448]: 0.7694729344431955
Loss at iteration [449]: 0.7693829450084496
Loss at iteration [450]: 0.7692859708591279
Loss at iteration [451]: 0.7691615785054684
Loss at iteration [452]: 0.7690691762967801
Loss at iteration [453]: 0.7688935836355946
Loss at iteration [454]: 0.7688935836355946
Loss at iteration [455]: 0.7687253783213894
Loss at iteration [456]: 0.7686729779981125
Loss at iteration [457]: 0.768605250842309
Loss at iteration [458]: 0.7685303688257771
Loss at iteration [459]: 0.7684677191115169
Loss at iteration [460]: 0.7683833165296745
Loss at iteration [461]: 0.7681543794115947
Loss at iteration [462]: 0.7681543794115947
Loss at iteration [463]: 0.7679899595063244
Loss at iteration [464]: 0.7679272341096395
Loss at iteration [465]: 0.7678520921267797
Loss at iteration [466]: 0.7678262452451944
Loss at iteration [467]: 0.7677466899830175
Loss at iteration [468]: 0.7677043894389968
Loss at iteration [469]: 0.7676570251850388
Loss at iteration [470]: 0.7676147800230672
Loss at iteration [471]: 0.7676147800230672
Loss at iteration [472]: 0.7675988739499854
Loss at iteration [473]: 0.7675639105731814
Loss at iteration [474]: 0.7675231944931099
Loss at iteration [475]: 0.767508641772174
Loss at iteration [476]: 0.7674882061807663
Loss at iteration [477]: 0.7674505053693799
Loss at iteration [478]: 0.767416444125106
Loss at iteration [479]: 0.7673781833583873
Loss at iteration [480]: 0.7673063387272822
Loss at iteration [481]: 0.7673063387272822
Loss at iteration [482]: 0.7672574394037578
Loss at iteration [483]: 0.7672307756005892
Loss at iteration [484]: 0.7672078085400154
Loss at iteration [485]: 0.7671714633057437
Loss at iteration [486]: 0.7671519255898681
Loss at iteration [487]: 0.7671257221311819
Loss at iteration [488]: 0.7671055604260981
Loss at iteration [489]: 0.7670435557984983
Loss at iteration [490]: 0.7670435557984983
Loss at iteration [491]: 0.7669411462019061
Loss at iteration [492]: 0.7669235127293679
Loss at iteration [493]: 0.7668949410767237
Loss at iteration [494]: 0.7668590642157189
Loss at iteration [495]: 0.7668460505420871
Loss at iteration [496]: 0.7668225856414876
Loss at iteration [497]: 0.7667725928133684
Loss at iteration [498]: 0.7667164191251058
Loss at iteration [499]: 0.7667164191251058
Loss at iteration [500]: 0.7666931135436255
Loss at iteration [501]: 0.7666768258696705
Loss at iteration [502]: 0.7666683389870734
Loss at iteration [503]: 0.7666514057446133
Loss at iteration [504]: 0.7666248595395452
Loss at iteration [505]: 0.7666036580459483
Loss at iteration [506]: 0.7665805763422828
Loss at iteration [507]: 0.7665585037319504
Loss at iteration [508]: 0.7665223540102681
Loss at iteration [509]: 0.7665223540102681
Loss at iteration [510]: 0.7664766382982192
Loss at iteration [511]: 0.7664611738331218
Loss at iteration [512]: 0.7664418872600781
Loss at iteration [513]: 0.7664268198613114
Loss at iteration [514]: 0.7664174780885803
Loss at iteration [515]: 0.7664106743488542
Loss at iteration [516]: 0.7663958648530554
Loss at iteration [517]: 0.7663820995798317
Loss at iteration [518]: 0.7663611987398322
Loss at iteration [519]: 0.7663261475932811
Loss at iteration [520]: 0.7662753146233
Loss at iteration [521]: 0.7662753146233
Loss at iteration [522]: 0.7662506940113851
Loss at iteration [523]: 0.7662311824403142
Loss at iteration [524]: 0.7662192035078754
Loss at iteration [525]: 0.7662029311688431
Loss at iteration [526]: 0.7661859371183629
Loss at iteration [527]: 0.7660743975110065
Loss at iteration [528]: 0.7658445042694081
Loss at iteration [529]: 0.7645401107387544
Loss at iteration [530]: 0.7595933849533896
Loss at iteration [531]: 0.7595933849533896
Loss at iteration [532]: 0.7590553173653094
Loss at iteration [533]: 0.7576897774801247
Loss at iteration [534]: 0.7572461881543826
Loss at iteration [535]: 0.7565291980387869
Loss at iteration [536]: 0.7562986752583152
Loss at iteration [537]: 0.7559357157372598
Loss at iteration [538]: 0.7555561271574799
Loss at iteration [539]: 0.7552147676509153
Loss at iteration [540]: 0.7552147676509153
Loss at iteration [541]: 0.754970453049388
Loss at iteration [542]: 0.7546953016093466
Loss at iteration [543]: 0.7545362845990161
Loss at iteration [544]: 0.7542721224525841
Loss at iteration [545]: 0.7540233753250845
Loss at iteration [546]: 0.7537309952379533
Loss at iteration [547]: 0.7537309952379533
Loss at iteration [548]: 0.7535756970626919
Loss at iteration [549]: 0.7534479940809142
Loss at iteration [550]: 0.7533519532381867
Loss at iteration [551]: 0.7532747672491189
Loss at iteration [552]: 0.7531631844399351
Loss at iteration [553]: 0.752883595994909
Loss at iteration [554]: 0.7525625240935352
Loss at iteration [555]: 0.7525625240935352
Loss at iteration [556]: 0.7524123947852672
Loss at iteration [557]: 0.7522582223641981
Loss at iteration [558]: 0.7520664879513934
Loss at iteration [559]: 0.7519869870734225
Loss at iteration [560]: 0.7518226657121866
Loss at iteration [561]: 0.75160288641661
Loss at iteration [562]: 0.75160288641661
Loss at iteration [563]: 0.751402206215291
Loss at iteration [564]: 0.7511981794829967
Loss at iteration [565]: 0.7510115181912908
Loss at iteration [566]: 0.7508699763377339
Loss at iteration [567]: 0.7507346732058608
Loss at iteration [568]: 0.7504926184034078
Loss at iteration [569]: 0.7501769579145955
Loss at iteration [570]: 0.7500289761699508
Loss at iteration [571]: 0.7500289761699508
Loss at iteration [572]: 0.7499718879960827
Loss at iteration [573]: 0.7498437736324567
Loss at iteration [574]: 0.7497888778363866
Loss at iteration [575]: 0.7497201842425012
Loss at iteration [576]: 0.7496505682734884
Loss at iteration [577]: 0.7495350630916254
Loss at iteration [578]: 0.7492742628282011
Loss at iteration [579]: 0.7481005955064269
Loss at iteration [580]: 0.7481005955064269
Loss at iteration [581]: 0.7477287710338781
Loss at iteration [582]: 0.7472712484900491
Loss at iteration [583]: 0.7471307364243075
Loss at iteration [584]: 0.7469457693635227
Loss at iteration [585]: 0.7467664181121304
Loss at iteration [586]: 0.7466532542035044
Loss at iteration [587]: 0.7465398734624193
Loss at iteration [588]: 0.7463259795724284
Loss at iteration [589]: 0.7463259795724284
Loss at iteration [590]: 0.7461772859566563
Loss at iteration [591]: 0.7461019223199032
Loss at iteration [592]: 0.7460306228611546
Loss at iteration [593]: 0.7459463584665821
Loss at iteration [594]: 0.7458466401157449
Loss at iteration [595]: 0.7456937822887381
Loss at iteration [596]: 0.7455553616764681
Loss at iteration [597]: 0.7454052465196765
Loss at iteration [598]: 0.7454052465196765
Loss at iteration [599]: 0.7452975435290785
Loss at iteration [600]: 0.7452750254187194
Loss at iteration [601]: 0.7452224157684733
Loss at iteration [602]: 0.7451443363797399
Loss at iteration [603]: 0.7450257027044385
Loss at iteration [604]: 0.7447683535587554
Loss at iteration [605]: 0.7444382969981173
Loss at iteration [606]: 0.7444382969981173
Loss at iteration [607]: 0.7442174244601055
Loss at iteration [608]: 0.7441095553205953
Loss at iteration [609]: 0.7439940015193456
Loss at iteration [610]: 0.7439531197527115
Loss at iteration [611]: 0.7438601590401145
Loss at iteration [612]: 0.7436338501110952
Loss at iteration [613]: 0.74309780996236
Loss at iteration [614]: 0.74309780996236
Loss at iteration [615]: 0.7428131304959523
Loss at iteration [616]: 0.7426361534227524
Loss at iteration [617]: 0.7424313068277493
Loss at iteration [618]: 0.7423398906050099
Loss at iteration [619]: 0.7422259537347142
Loss at iteration [620]: 0.7421047870410683
Loss at iteration [621]: 0.7420278977289775
Loss at iteration [622]: 0.7420278977289775
Loss at iteration [623]: 0.7419222444034272
Loss at iteration [624]: 0.7418944924868113
Loss at iteration [625]: 0.7418350174182422
Loss at iteration [626]: 0.741799489853356
Loss at iteration [627]: 0.7417644610238002
Loss at iteration [628]: 0.7416600390884475
Loss at iteration [629]: 0.7414896573363658
Loss at iteration [630]: 0.7412628233222791
Loss at iteration [631]: 0.7412628233222791
Loss at iteration [632]: 0.7411616361951721
Loss at iteration [633]: 0.7410961793639024
Loss at iteration [634]: 0.7410618009068204
Loss at iteration [635]: 0.7410125679064314
Loss at iteration [636]: 0.7409509786209765
Loss at iteration [637]: 0.7408790963699776
Loss at iteration [638]: 0.7407320792379842
Loss at iteration [639]: 0.7407320792379842
Loss at iteration [640]: 0.7406711574039164
Loss at iteration [641]: 0.7406477867390476
Loss at iteration [642]: 0.7406214991560074
Loss at iteration [643]: 0.7405859024078919
Loss at iteration [644]: 0.740568646776936
Loss at iteration [645]: 0.7405438120484464
Loss at iteration [646]: 0.7405073091424588
Loss at iteration [647]: 0.7404710341485512
Loss at iteration [648]: 0.7404466145394187
Loss at iteration [649]: 0.7404178674106675
Loss at iteration [650]: 0.7403899521530808
Loss at iteration [651]: 0.7403899521530808
Loss at iteration [652]: 0.7403786430806195
Loss at iteration [653]: 0.7403700159048676
Loss at iteration [654]: 0.7403552837325215
Loss at iteration [655]: 0.7403094787075979
Loss at iteration [656]: 0.7401779554211656
Loss at iteration [657]: 0.7400283090551424
Loss at iteration [658]: 0.739544521379735
Loss at iteration [659]: 0.7375876319428997
Loss at iteration [660]: 0.7375876319428997
Loss at iteration [661]: 0.7372213512467003
Loss at iteration [662]: 0.7364761357829217
Loss at iteration [663]: 0.7361986427875027
Loss at iteration [664]: 0.7359713741012754
Loss at iteration [665]: 0.7357832381372571
Loss at iteration [666]: 0.7355985746168728
Loss at iteration [667]: 0.7354912885747692
Loss at iteration [668]: 0.7354912885747692
Loss at iteration [669]: 0.7354190489635164
Loss at iteration [670]: 0.7353594017073649
Loss at iteration [671]: 0.7352752638750396
Loss at iteration [672]: 0.7351766411564811
Loss at iteration [673]: 0.7350627708478386
Loss at iteration [674]: 0.7349083583804896
Loss at iteration [675]: 0.7347466266026681
Loss at iteration [676]: 0.7345863802286053
Loss at iteration [677]: 0.7345863802286053
Loss at iteration [678]: 0.7344613278630806
Loss at iteration [679]: 0.7344280897594051
Loss at iteration [680]: 0.7343538135231393
Loss at iteration [681]: 0.7343273687326002
Loss at iteration [682]: 0.7342823799278485
Loss at iteration [683]: 0.7342333658617697
Loss at iteration [684]: 0.7341871559256123
Loss at iteration [685]: 0.7341871559256123
Loss at iteration [686]: 0.7340832823174032
Loss at iteration [687]: 0.734043520183983
Loss at iteration [688]: 0.7340216327084411
Loss at iteration [689]: 0.7340015261941761
Loss at iteration [690]: 0.7339792422396063
Loss at iteration [691]: 0.7339624448815116
Loss at iteration [692]: 0.7339386271459031
Loss at iteration [693]: 0.7339104569956707
Loss at iteration [694]: 0.7338658369074119
Loss at iteration [695]: 0.7338658369074119
Loss at iteration [696]: 0.7338429818232929
Loss at iteration [697]: 0.7338235914932855
Loss at iteration [698]: 0.7338121032481072
Loss at iteration [699]: 0.7338017765656545
Loss at iteration [700]: 0.7337838610197331
Loss at iteration [701]: 0.7337625542900191
Loss at iteration [702]: 0.7337406542778724
Loss at iteration [703]: 0.7337225169468821
Loss at iteration [704]: 0.7336944200745932
Loss at iteration [705]: 0.7335938232667212
Loss at iteration [706]: 0.7330139045565265
Loss at iteration [707]: 0.7330139045565265
Loss at iteration [708]: 0.7326440325358405
Loss at iteration [709]: 0.7324756529126873
Loss at iteration [710]: 0.7323750930225325
Loss at iteration [711]: 0.7322251145250265
Loss at iteration [712]: 0.7321736883161067
Loss at iteration [713]: 0.7320958725923172
Loss at iteration [714]: 0.7320513729256408
Loss at iteration [715]: 0.7320513729256408
Loss at iteration [716]: 0.7319546366063545
Loss at iteration [717]: 0.731917931663194
Loss at iteration [718]: 0.731883665126125
Loss at iteration [719]: 0.7318613275739517
Loss at iteration [720]: 0.7318423448319902
Loss at iteration [721]: 0.7318269856445387
Loss at iteration [722]: 0.7318060716622717
Loss at iteration [723]: 0.7317865365689598
Loss at iteration [724]: 0.7317652422936991
Loss at iteration [725]: 0.7317463934715429
Loss at iteration [726]: 0.7317463934715429
Loss at iteration [727]: 0.7317272587785048
Loss at iteration [728]: 0.7317197977720438
Loss at iteration [729]: 0.7317181330178281
Loss at iteration [730]: 0.7317069640621101
Loss at iteration [731]: 0.7316868395425388
Loss at iteration [732]: 0.7316620338708554
Loss at iteration [733]: 0.7316419187568646
Loss at iteration [734]: 0.7316195845952436
Loss at iteration [735]: 0.7315967984804846
Loss at iteration [736]: 0.7315776504158166
Loss at iteration [737]: 0.7315547646346939
Loss at iteration [738]: 0.7315547646346939
Loss at iteration [739]: 0.7315272458838024
Loss at iteration [740]: 0.7315107387409018
Loss at iteration [741]: 0.7314990555784464
Loss at iteration [742]: 0.7314922243723626
Loss at iteration [743]: 0.7314878147606703
Loss at iteration [744]: 0.7314865428034729
Loss at iteration [745]: 0.7314738898359644
Loss at iteration [746]: 0.7314420759977324
Loss at iteration [747]: 0.7314202702907397
Loss at iteration [748]: 0.7313934430365456
Loss at iteration [749]: 0.7313677319195657
Loss at iteration [750]: 0.7313677319195657
Loss at iteration [751]: 0.7313225526293411
Loss at iteration [752]: 0.7313021908058381
Loss at iteration [753]: 0.7312936550402538
Loss at iteration [754]: 0.7312799589161989
Loss at iteration [755]: 0.7312625814242921
Loss at iteration [756]: 0.7312474287521404
Loss at iteration [757]: 0.7312281885798443
Loss at iteration [758]: 0.7312069992758976
Loss at iteration [759]: 0.7311810981922464
Loss at iteration [760]: 0.7311535668117378
Loss at iteration [761]: 0.7311535668117378
Loss at iteration [762]: 0.731136096563898
Loss at iteration [763]: 0.7311282658705334
Loss at iteration [764]: 0.7311205132579603
Loss at iteration [765]: 0.7311140391876557
Loss at iteration [766]: 0.7311004741524492
Loss at iteration [767]: 0.7310789883196709
Loss at iteration [768]: 0.7310351578359994
Loss at iteration [769]: 0.7309028432522711
Loss at iteration [770]: 0.730063029098485
Loss at iteration [771]: 0.730063029098485
Loss at iteration [772]: 0.729672294719814
Loss at iteration [773]: 0.7293376994953054
Loss at iteration [774]: 0.7291773602059805
Loss at iteration [775]: 0.7290447314833745
Loss at iteration [776]: 0.728945481183501
Loss at iteration [777]: 0.7288825857985758
Loss at iteration [778]: 0.7288171516209087
Loss at iteration [779]: 0.7287840359381051
Loss at iteration [780]: 0.7287840359381051
Loss at iteration [781]: 0.7287528726939798
Loss at iteration [782]: 0.7287313530482407
Loss at iteration [783]: 0.7286594964717048
Loss at iteration [784]: 0.7286232380961073
Loss at iteration [785]: 0.7285852555452251
Loss at iteration [786]: 0.7285170322424437
Loss at iteration [787]: 0.7284469649861932
Loss at iteration [788]: 0.7284469649861932
Loss at iteration [789]: 0.7284098349979883
Loss at iteration [790]: 0.7283970226790881
Loss at iteration [791]: 0.7283824553197623
Loss at iteration [792]: 0.7283653904360832
Loss at iteration [793]: 0.7283552033134513
Loss at iteration [794]: 0.7283364609881586
Loss at iteration [795]: 0.7283030336519504
Loss at iteration [796]: 0.7282129777860918
Loss at iteration [797]: 0.7280608000310856
Loss at iteration [798]: 0.7280608000310856
Loss at iteration [799]: 0.7279728508387735
Loss at iteration [800]: 0.7279454957705218
Loss at iteration [801]: 0.7279131803330827
Loss at iteration [802]: 0.7278889905159028
Loss at iteration [803]: 0.7278461189097598
Loss at iteration [804]: 0.7278109442369897
Loss at iteration [805]: 0.727780002415975
Loss at iteration [806]: 0.7277574267080111
Loss at iteration [807]: 0.7277574267080111
Loss at iteration [808]: 0.7277441778766518
Loss at iteration [809]: 0.7277353363321488
Loss at iteration [810]: 0.7277290492314323
Loss at iteration [811]: 0.7277170059125075
Loss at iteration [812]: 0.7276983323929256
Loss at iteration [813]: 0.7276795893680053
Loss at iteration [814]: 0.7276675683619366
Loss at iteration [815]: 0.727656875923292
Loss at iteration [816]: 0.7276489218825078
Loss at iteration [817]: 0.7276303355193249
Loss at iteration [818]: 0.7276057155539999
Loss at iteration [819]: 0.7276057155539999
Loss at iteration [820]: 0.7275924337084096
Loss at iteration [821]: 0.7275873981298062
Loss at iteration [822]: 0.727583004999199
Loss at iteration [823]: 0.7275754527895566
Loss at iteration [824]: 0.7275711410783328
Loss at iteration [825]: 0.7275657780439564
Loss at iteration [826]: 0.7275599760158531
Loss at iteration [827]: 0.7275507530271103
Loss at iteration [828]: 0.727546972943779
Loss at iteration [829]: 0.72754150876716
Loss at iteration [830]: 0.7275360943136373
Loss at iteration [831]: 0.7275127628841725
Loss at iteration [832]: 0.7274794434573391
Loss at iteration [833]: 0.7274472321715655
Loss at iteration [834]: 0.7274072211272749
Loss at iteration [835]: 0.7274072211272749
Loss at iteration [836]: 0.7273866451892941
Loss at iteration [837]: 0.7273812065462409
Loss at iteration [838]: 0.7273721031092873
Loss at iteration [839]: 0.7273679694977774
Loss at iteration [840]: 0.7273639386216959
Loss at iteration [841]: 0.727360193367075
Loss at iteration [842]: 0.7273554665299352
Loss at iteration [843]: 0.7273522250428219
Loss at iteration [844]: 0.7273483668931323
Loss at iteration [845]: 0.7273445059455572
Loss at iteration [846]: 0.7273397019202604
Loss at iteration [847]: 0.7273354684758384
Loss at iteration [848]: 0.7273297868715914
Loss at iteration [849]: 0.7273244357375027
Loss at iteration [850]: 0.7273204427786232
Loss at iteration [851]: 0.7273163227913367
Loss at iteration [852]: 0.7273112645255544
Loss at iteration [853]: 0.7273057939726224
Loss at iteration [854]: 0.7273047917965461
Loss at iteration [855]: 0.7272699807336492
Loss at iteration [856]: 0.7272699807336492
Loss at iteration [857]: 0.7272492971716288
Loss at iteration [858]: 0.7272374820977674
Loss at iteration [859]: 0.7272328118104132
Loss at iteration [860]: 0.7272262107247366
Loss at iteration [861]: 0.7272212255803181
Loss at iteration [862]: 0.7272165140208814
Loss at iteration [863]: 0.7272129802920082
Loss at iteration [864]: 0.7272096859151664
Loss at iteration [865]: 0.727205774727318
Loss at iteration [866]: 0.7272011557690717
Loss at iteration [867]: 0.7271965481567298
Loss at iteration [868]: 0.7271936944215465
Loss at iteration [869]: 0.7271911069504428
Loss at iteration [870]: 0.7271864729572896
Loss at iteration [871]: 0.7271819876230893
Loss at iteration [872]: 0.7271777734957758
Loss at iteration [873]: 0.7271722149634764
Loss at iteration [874]: 0.7271680191766495
Loss at iteration [875]: 0.7271635112216461
Loss at iteration [876]: 0.7271525504405666
Loss at iteration [877]: 0.7271525504405666
Loss at iteration [878]: 0.727140505805351
Loss at iteration [879]: 0.7271365181633654
Loss at iteration [880]: 0.7271303408589249
Loss at iteration [881]: 0.7271250392864915
Loss at iteration [882]: 0.7271214147880062
Loss at iteration [883]: 0.7271175689076399
Loss at iteration [884]: 0.7271138134318736
Loss at iteration [885]: 0.7271099659797773
Loss at iteration [886]: 0.7271074248709607
Loss at iteration [887]: 0.72710355873178
Loss at iteration [888]: 0.7270977355123267
Loss at iteration [889]: 0.7270941823029404
Loss at iteration [890]: 0.727090134278395
Loss at iteration [891]: 0.7270854959954735
Loss at iteration [892]: 0.7270796557930587
Loss at iteration [893]: 0.7270750975681365
Loss at iteration [894]: 0.7270698661281896
Loss at iteration [895]: 0.7270640317805402
Loss at iteration [896]: 0.7270589007267159
Loss at iteration [897]: 0.7270536239439935
Loss at iteration [898]: 0.7270536239439935
Loss at iteration [899]: 0.7270513712914213
Loss at iteration [900]: 0.7270511890870678
Loss at iteration [901]: 0.7270491372285188
Loss at iteration [902]: 0.7270469725277142
Loss at iteration [903]: 0.7270424767233551
Loss at iteration [904]: 0.7270386388519032
Loss at iteration [905]: 0.7270354537972011
Loss at iteration [906]: 0.727031487594253
Loss at iteration [907]: 0.7270211317015277
Loss at iteration [908]: 0.7270041136642215
Loss at iteration [909]: 0.7269057557728255
Loss at iteration [910]: 0.7224318609629634
Loss at iteration [911]: 0.7083517525844887
Loss at iteration [912]: 0.7005332573360474
Loss at iteration [913]: 0.7005332573360474
Loss at iteration [914]: 0.694411598900203
Loss at iteration [915]: 0.689138842886101
Loss at iteration [916]: 0.682772887360074
Loss at iteration [917]: 0.678530149480832
Loss at iteration [918]: 0.6774774962609752
Loss at iteration [919]: 0.6753711589326226
Loss at iteration [920]: 0.6744530741052712
Loss at iteration [921]: 0.6744530741052712
Loss at iteration [922]: 0.6739914193574612
Loss at iteration [923]: 0.6717448867110518
Loss at iteration [924]: 0.6707259500631123
Loss at iteration [925]: 0.6689459040560182
Loss at iteration [926]: 0.6678393140560106
Loss at iteration [927]: 0.6664068324358337
Loss at iteration [928]: 0.6658868364039577
Loss at iteration [929]: 0.6658868364039577
Loss at iteration [930]: 0.6653782342108046
Loss at iteration [931]: 0.6647231721576895
Loss at iteration [932]: 0.6642160257191333
Loss at iteration [933]: 0.6635807224559694
Loss at iteration [934]: 0.6628638224058276
Loss at iteration [935]: 0.6624737583631716
Loss at iteration [936]: 0.6619627754158827
Loss at iteration [937]: 0.6619627754158827
Loss at iteration [938]: 0.661698054127488
Loss at iteration [939]: 0.6615071671528161
Loss at iteration [940]: 0.6613736611340882
Loss at iteration [941]: 0.6612066066387823
Loss at iteration [942]: 0.6610887585512171
Loss at iteration [943]: 0.6606234372218605
Loss at iteration [944]: 0.660182683342642
Loss at iteration [945]: 0.660182683342642
Loss at iteration [946]: 0.6595886697704814
Loss at iteration [947]: 0.6591657474644631
Loss at iteration [948]: 0.6585275955012903
Loss at iteration [949]: 0.6583499297277312
Loss at iteration [950]: 0.658017130650249
Loss at iteration [951]: 0.6577739913199451
Loss at iteration [952]: 0.6576247529671019
Loss at iteration [953]: 0.6576247529671019
Loss at iteration [954]: 0.6572933174257675
Loss at iteration [955]: 0.6571340251567702
Loss at iteration [956]: 0.6570996260008581
Loss at iteration [957]: 0.6570140928852068
Loss at iteration [958]: 0.6568625630941847
Loss at iteration [959]: 0.6564477035755137
Loss at iteration [960]: 0.6561390339909358
Loss at iteration [961]: 0.655704978657417
Loss at iteration [962]: 0.655704978657417
Loss at iteration [963]: 0.6554047863280532
Loss at iteration [964]: 0.6550461357471629
Loss at iteration [965]: 0.6548618432606483
Loss at iteration [966]: 0.6547041310073533
Loss at iteration [967]: 0.6545692870691123
Loss at iteration [968]: 0.6543269317428081
Loss at iteration [969]: 0.654139523786367
Loss at iteration [970]: 0.6539622469878205
Loss at iteration [971]: 0.6539622469878205
Loss at iteration [972]: 0.6537729910622272
Loss at iteration [973]: 0.6536598682466376
Loss at iteration [974]: 0.6536076177363698
Loss at iteration [975]: 0.6535672164659714
Loss at iteration [976]: 0.6535037013347358
Loss at iteration [977]: 0.6533769044055603
Loss at iteration [978]: 0.6531680757732297
Loss at iteration [979]: 0.6528519170865119
Loss at iteration [980]: 0.6524636977557279
Loss at iteration [981]: 0.6524636977557279
Loss at iteration [982]: 0.6522676103635604
Loss at iteration [983]: 0.6521517812653504
Loss at iteration [984]: 0.6520864510422835
Loss at iteration [985]: 0.6520261902390533
Loss at iteration [986]: 0.6519633264119801
Loss at iteration [987]: 0.6519149198163252
Loss at iteration [988]: 0.6518666549561947
Loss at iteration [989]: 0.6517813699796686
Loss at iteration [990]: 0.6517813699796686
Loss at iteration [991]: 0.651710966658181
Loss at iteration [992]: 0.6516904932553962
Loss at iteration [993]: 0.6516633373945496
Loss at iteration [994]: 0.651622619772327
Loss at iteration [995]: 0.6515761272065685
Loss at iteration [996]: 0.6515154314759577
Loss at iteration [997]: 0.6514669763007149
Loss at iteration [998]: 0.6514669763007149
Loss at iteration [999]: 0.6514237145939165
Loss at iteration [1000]: 0.6513843945059199
Loss at iteration [1001]: 0.6513738237147307
Loss at iteration [1002]: 0.6513575214185895
Loss at iteration [1003]: 0.6513314547038311
Loss at iteration [1004]: 0.6513258014956741
Loss at iteration [1005]: 0.6513093463710777
Loss at iteration [1006]: 0.651288530445268
Loss at iteration [1007]: 0.651275846476413
Loss at iteration [1008]: 0.6512675393969727
Loss at iteration [1009]: 0.6512648886696056
Loss at iteration [1010]: 0.6512220972029857
Loss at iteration [1011]: 0.6511858916561686
Loss at iteration [1012]: 0.6511858916561686
Loss at iteration [1013]: 0.6511594014687146
Loss at iteration [1014]: 0.6511544085300153
Loss at iteration [1015]: 0.6511437170640361
Loss at iteration [1016]: 0.6511315395663922
Loss at iteration [1017]: 0.6511110490941041
Loss at iteration [1018]: 0.6510794788626513
Loss at iteration [1019]: 0.6510277566017978
Loss at iteration [1020]: 0.6509953439583795
Loss at iteration [1021]: 0.6509558583114817
Loss at iteration [1022]: 0.6509098825998226
Loss at iteration [1023]: 0.6508633991974627
Loss at iteration [1024]: 0.65080895653139
Loss at iteration [1025]: 0.6507395852257221
Loss at iteration [1026]: 0.6507395852257221
Loss at iteration [1027]: 0.6507111939469308
Loss at iteration [1028]: 0.6506901075258136
Loss at iteration [1029]: 0.6506509929213004
Loss at iteration [1030]: 0.6506394112974424
Loss at iteration [1031]: 0.6505959261398423
Loss at iteration [1032]: 0.6505073382900107
Loss at iteration [1033]: 0.6500975205621607
Loss at iteration [1034]: 0.6499252595197533
Loss at iteration [1035]: 0.6497171048855848
Loss at iteration [1036]: 0.6497171048855848
Loss at iteration [1037]: 0.6496523109240339
Loss at iteration [1038]: 0.6496093621327033
Loss at iteration [1039]: 0.6495609761301828
Loss at iteration [1040]: 0.6495307707390999
Loss at iteration [1041]: 0.649505186091811
Loss at iteration [1042]: 0.6494861951977056
Loss at iteration [1043]: 0.6494519841938861
Loss at iteration [1044]: 0.6494111619220214
Loss at iteration [1045]: 0.6493780518292894
