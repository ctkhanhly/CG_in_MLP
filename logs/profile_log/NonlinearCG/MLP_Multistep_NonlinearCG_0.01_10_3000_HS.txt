Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :HS
Total number of function evaluations  : 425
Total number of iterations            : 189
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 0.7904424667358398
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 50.232658803159836%
Percentage of parameters < 1e-7       : 50.233150682236285%
Percentage of parameters < 1e-6       : 50.23413444038918%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.6617857758396698
Loss at iteration [2]: 0.6437255691159732
Loss at iteration [3]: 0.5912433353192559
Loss at iteration [4]: 0.5743407409636886
Loss at iteration [5]: 0.5093815644923962
Loss at iteration [6]: 0.40403818247528356
Loss at iteration [7]: 0.3359159949638323
Loss at iteration [8]: 0.30820693848720004
Loss at iteration [9]: 0.28974381250265174
Loss at iteration [10]: 0.28465964470950544
Loss at iteration [11]: 0.28465964470950544
Loss at iteration [12]: 0.2807484484702426
Loss at iteration [13]: 0.26657503230774665
Loss at iteration [14]: 0.26558865665824555
Loss at iteration [15]: 0.2576918546614726
Loss at iteration [16]: 0.2540169089372323
Loss at iteration [17]: 0.23660927384287403
Loss at iteration [18]: 0.23293914604240804
Loss at iteration [19]: 0.22735696338649602
Loss at iteration [20]: 0.2262901521825663
Loss at iteration [21]: 0.22450347377028024
Loss at iteration [22]: 0.22450347377028024
Loss at iteration [23]: 0.22406702564069667
Loss at iteration [24]: 0.22005225498976194
Loss at iteration [25]: 0.2196628369487619
Loss at iteration [26]: 0.21638603604774564
Loss at iteration [27]: 0.21618920075486786
Loss at iteration [28]: 0.21585227119801179
Loss at iteration [29]: 0.2158142105143402
Loss at iteration [30]: 0.21574369331308144
Loss at iteration [31]: 0.21567611115477972
Loss at iteration [32]: 0.21554973975430705
Loss at iteration [33]: 0.2153831771097826
Loss at iteration [34]: 0.21525852625103814
Loss at iteration [35]: 0.21524399717310388
Loss at iteration [36]: 0.21524399717310388
Loss at iteration [37]: 0.21507266649172832
Loss at iteration [38]: 0.21504554481386867
Loss at iteration [39]: 0.21503561017108774
Loss at iteration [40]: 0.21501498782206216
Loss at iteration [41]: 0.2150011678591461
Loss at iteration [42]: 0.21499278547621653
Loss at iteration [43]: 0.21498109658897566
Loss at iteration [44]: 0.21496164978255958
Loss at iteration [45]: 0.21494873483898266
Loss at iteration [46]: 0.21493563425317813
Loss at iteration [47]: 0.2149212354335329
Loss at iteration [48]: 0.21490847081074377
Loss at iteration [49]: 0.21490847081074377
Loss at iteration [50]: 0.21490233877638173
Loss at iteration [51]: 0.2148720850946357
Loss at iteration [52]: 0.21485424437703188
Loss at iteration [53]: 0.21484924359307409
Loss at iteration [54]: 0.21484575576738715
Loss at iteration [55]: 0.2148449874219555
Loss at iteration [56]: 0.2148439569986823
Loss at iteration [57]: 0.21484249715290143
Loss at iteration [58]: 0.2148419329817057
Loss at iteration [59]: 0.21484113828230836
Loss at iteration [60]: 0.2148407197664867
Loss at iteration [61]: 0.21483991740217545
Loss at iteration [62]: 0.21483991740217545
Loss at iteration [63]: 0.21483954972580005
Loss at iteration [64]: 0.2148342700913421
Loss at iteration [65]: 0.21483324348092067
Loss at iteration [66]: 0.21482194783839698
Loss at iteration [67]: 0.2148082613760571
Loss at iteration [68]: 0.21480552112425613
Loss at iteration [69]: 0.21476096858763447
Loss at iteration [70]: 0.2147193804173353
Loss at iteration [71]: 0.21471841588606466
Loss at iteration [72]: 0.21469740599258905
Loss at iteration [73]: 0.21469580603814306
Loss at iteration [74]: 0.21469580603814306
Loss at iteration [75]: 0.21469416224628132
Loss at iteration [76]: 0.21468683263628371
Loss at iteration [77]: 0.21468677002655062
Loss at iteration [78]: 0.21468394842169408
Loss at iteration [79]: 0.21468271379910275
Loss at iteration [80]: 0.2146800308044433
Loss at iteration [81]: 0.21467629441019348
Loss at iteration [82]: 0.21467605951822297
Loss at iteration [83]: 0.21467200538094153
Loss at iteration [84]: 0.21466946617625704
Loss at iteration [85]: 0.21466901175828074
Loss at iteration [86]: 0.2146658983733868
Loss at iteration [87]: 0.2146658983733868
Loss at iteration [88]: 0.21466537028289215
Loss at iteration [89]: 0.21466330686175825
Loss at iteration [90]: 0.21466086680134336
Loss at iteration [91]: 0.21465915594820617
Loss at iteration [92]: 0.2146590089405678
Loss at iteration [93]: 0.2146534514275483
Loss at iteration [94]: 0.21465154186190583
Loss at iteration [95]: 0.21465089345806626
Loss at iteration [96]: 0.21464599726177774
Loss at iteration [97]: 0.21464538903138058
Loss at iteration [98]: 0.2146398035825209
Loss at iteration [99]: 0.2146398035825209
Loss at iteration [100]: 0.21463936714801204
Loss at iteration [101]: 0.21463729165350515
Loss at iteration [102]: 0.21463575292811082
Loss at iteration [103]: 0.21463561915688445
Loss at iteration [104]: 0.21463422334411075
Loss at iteration [105]: 0.21463387687409102
Loss at iteration [106]: 0.21463348717747205
Loss at iteration [107]: 0.21463347981574493
Loss at iteration [108]: 0.21463343662183587
Loss at iteration [109]: 0.21463311549886138
Loss at iteration [110]: 0.21463285302709267
Loss at iteration [111]: 0.21463279301975147
Loss at iteration [112]: 0.21463275680403468
Loss at iteration [113]: 0.21463275680403468
Loss at iteration [114]: 0.21463273589392415
Loss at iteration [115]: 0.21463186486300476
Loss at iteration [116]: 0.21463142001404514
Loss at iteration [117]: 0.21463014276360995
Loss at iteration [118]: 0.21462905819818626
Loss at iteration [119]: 0.21462830913935135
Loss at iteration [120]: 0.2146282073200804
Loss at iteration [121]: 0.21462811780872026
Loss at iteration [122]: 0.21462795160906498
Loss at iteration [123]: 0.2146277844733521
Loss at iteration [124]: 0.21462757727233053
Loss at iteration [125]: 0.2146272927454273
Loss at iteration [126]: 0.2146272927454273
Loss at iteration [127]: 0.21462717755524777
Loss at iteration [128]: 0.21462669592105563
Loss at iteration [129]: 0.21462660072778622
Loss at iteration [130]: 0.21462614694170862
Loss at iteration [131]: 0.21462445775615796
Loss at iteration [132]: 0.21462336073717422
Loss at iteration [133]: 0.21461790160588443
Loss at iteration [134]: 0.21461639466495763
Loss at iteration [135]: 0.21461147891454235
Loss at iteration [136]: 0.21460986733818926
Loss at iteration [137]: 0.2145955095198962
Loss at iteration [138]: 0.21459507220917512
Loss at iteration [139]: 0.21458538922467185
Loss at iteration [140]: 0.21458538922467185
Loss at iteration [141]: 0.21458393733382716
Loss at iteration [142]: 0.21458256530638245
Loss at iteration [143]: 0.21458256379352747
Loss at iteration [144]: 0.21458229762541056
Loss at iteration [145]: 0.21458201390084883
Loss at iteration [146]: 0.2145816048966987
Loss at iteration [147]: 0.21458132834658217
Loss at iteration [148]: 0.21458078023561
Loss at iteration [149]: 0.2145805750930067
Loss at iteration [150]: 0.21458023741450188
Loss at iteration [151]: 0.2145802096979593
Loss at iteration [152]: 0.21458010982682008
Loss at iteration [153]: 0.21458008953207677
Loss at iteration [154]: 0.21458008953207677
Loss at iteration [155]: 0.21458007127816817
Loss at iteration [156]: 0.21457979821386167
Loss at iteration [157]: 0.21457949108895444
Loss at iteration [158]: 0.2145794196926236
Loss at iteration [159]: 0.21457907532785678
Loss at iteration [160]: 0.21457904322591156
Loss at iteration [161]: 0.21457866912714516
Loss at iteration [162]: 0.2145785980052947
Loss at iteration [163]: 0.21457845591982758
Loss at iteration [164]: 0.21457817393929465
Loss at iteration [165]: 0.2145781526973231
Loss at iteration [166]: 0.21457784554812195
Loss at iteration [167]: 0.21457784554812195
Loss at iteration [168]: 0.2145776747917309
Loss at iteration [169]: 0.2145776656956639
Loss at iteration [170]: 0.21457746616932932
Loss at iteration [171]: 0.21457739656924713
Loss at iteration [172]: 0.2145770578931608
Loss at iteration [173]: 0.2145759265163011
Loss at iteration [174]: 0.21457575064660062
Loss at iteration [175]: 0.21457427391236786
Loss at iteration [176]: 0.21457412283855054
Loss at iteration [177]: 0.2145734241506065
Loss at iteration [178]: 0.21457240006382486
Loss at iteration [179]: 0.21457240006382486
Loss at iteration [180]: 0.21457230851581918
Loss at iteration [181]: 0.21457194894571585
Loss at iteration [182]: 0.21457192543069561
Loss at iteration [183]: 0.21457165656613428
Loss at iteration [184]: 0.21457158327374437
Loss at iteration [185]: 0.21457127759373115
Loss at iteration [186]: 0.21457097563758135
Loss at iteration [187]: 0.21457082484329623
Loss at iteration [188]: 0.2145707959433273
Loss at iteration [189]: 0.21457078557201892
Loss at iteration [190]: 0.214570651981405
Loss at iteration [191]: 0.21457064492249936
Loss at iteration [192]: 0.21457052615521788
Loss at iteration [193]: 0.21457052615521788
Loss at iteration [194]: 0.21457051262593443
Loss at iteration [195]: 0.21457037807114207
Loss at iteration [196]: 0.21457033977890927
Loss at iteration [197]: 0.2145703103138056
Loss at iteration [198]: 0.21457030988026796
