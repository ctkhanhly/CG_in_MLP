Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :HS
Total number of function evaluations  : 883
Total number of iterations            : 201
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 1.5146946907043457
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 50.14264493216988%
Percentage of parameters < 1e-7       : 50.14264493216988%
Percentage of parameters < 1e-6       : 50.14362869032277%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.6581193418703558
Loss at iteration [2]: 0.646679299992081
Loss at iteration [3]: 0.46075390337295397
Loss at iteration [4]: 0.40392550357127577
Loss at iteration [5]: 0.37131990125948183
Loss at iteration [6]: 0.36121667156603926
Loss at iteration [7]: 0.36121667156603926
Loss at iteration [8]: 0.3554990219074364
Loss at iteration [9]: 0.3152414878582623
Loss at iteration [10]: 0.30803352154220864
Loss at iteration [11]: 0.3009915501855688
Loss at iteration [12]: 0.26674407204583045
Loss at iteration [13]: 0.26107682423184747
Loss at iteration [14]: 0.26107682423184747
Loss at iteration [15]: 0.243306034297154
Loss at iteration [16]: 0.24199354055225497
Loss at iteration [17]: 0.23830809958305682
Loss at iteration [18]: 0.22699393735701803
Loss at iteration [19]: 0.2262788124382126
Loss at iteration [20]: 0.22348438860825368
Loss at iteration [21]: 0.22348438860825368
Loss at iteration [22]: 0.22307122726725534
Loss at iteration [23]: 0.2221167958375829
Loss at iteration [24]: 0.21920634061690045
Loss at iteration [25]: 0.21913123203697416
Loss at iteration [26]: 0.21869258729517654
Loss at iteration [27]: 0.21842479325196856
Loss at iteration [28]: 0.21842479325196856
Loss at iteration [29]: 0.2183851418833944
Loss at iteration [30]: 0.2181766845354819
Loss at iteration [31]: 0.2181185945644379
Loss at iteration [32]: 0.21700759059572577
Loss at iteration [33]: 0.21672430851866265
Loss at iteration [34]: 0.2162739963988339
Loss at iteration [35]: 0.2162739963988339
Loss at iteration [36]: 0.21613771439570698
Loss at iteration [37]: 0.21577847326706284
Loss at iteration [38]: 0.21568709773570674
Loss at iteration [39]: 0.21564208964053103
Loss at iteration [40]: 0.2150319444213735
Loss at iteration [41]: 0.21500997416990544
Loss at iteration [42]: 0.21500997416990544
Loss at iteration [43]: 0.2150036468515633
Loss at iteration [44]: 0.21493899641360892
Loss at iteration [45]: 0.214930496041944
Loss at iteration [46]: 0.2149047138089937
Loss at iteration [47]: 0.21483463926546353
Loss at iteration [48]: 0.21482301125409706
Loss at iteration [49]: 0.21482301125409706
Loss at iteration [50]: 0.21481764171904688
Loss at iteration [51]: 0.2148101253447365
Loss at iteration [52]: 0.21478143825362017
Loss at iteration [53]: 0.21476789136297178
Loss at iteration [54]: 0.21476356510209427
Loss at iteration [55]: 0.21474763183238543
Loss at iteration [56]: 0.21474763183238543
Loss at iteration [57]: 0.21474415237243893
Loss at iteration [58]: 0.21472751669967516
Loss at iteration [59]: 0.21472395810488806
Loss at iteration [60]: 0.21472188538218498
Loss at iteration [61]: 0.2147196729709597
Loss at iteration [62]: 0.21471946711116727
Loss at iteration [63]: 0.21471570235995785
Loss at iteration [64]: 0.21471570235995785
Loss at iteration [65]: 0.2147131365285018
Loss at iteration [66]: 0.21471284061816845
Loss at iteration [67]: 0.21470997076661585
Loss at iteration [68]: 0.21470634537931987
Loss at iteration [69]: 0.2146786709873174
Loss at iteration [70]: 0.21466667482810906
Loss at iteration [71]: 0.21466667482810906
Loss at iteration [72]: 0.2146653482092386
Loss at iteration [73]: 0.2146618504595499
Loss at iteration [74]: 0.21466031129227378
Loss at iteration [75]: 0.21465995765723492
Loss at iteration [76]: 0.21465867889611975
Loss at iteration [77]: 0.2146583987393239
Loss at iteration [78]: 0.214656080329711
Loss at iteration [79]: 0.214656080329711
Loss at iteration [80]: 0.21465542116039316
Loss at iteration [81]: 0.21465455906148428
Loss at iteration [82]: 0.21465286775572218
Loss at iteration [83]: 0.21464762358486178
Loss at iteration [84]: 0.21464745588064518
Loss at iteration [85]: 0.21463532582297642
Loss at iteration [86]: 0.21463532582297642
Loss at iteration [87]: 0.21463480347700856
Loss at iteration [88]: 0.21463404164696956
Loss at iteration [89]: 0.2146315622983654
Loss at iteration [90]: 0.21463026372959013
Loss at iteration [91]: 0.21463014545260725
Loss at iteration [92]: 0.2146150534137367
Loss at iteration [93]: 0.2146150534137367
Loss at iteration [94]: 0.21461425905129278
Loss at iteration [95]: 0.21461318042714445
Loss at iteration [96]: 0.21461048676898484
Loss at iteration [97]: 0.21461005169842343
Loss at iteration [98]: 0.21460788251157262
Loss at iteration [99]: 0.21460727117452655
Loss at iteration [100]: 0.21460727117452655
Loss at iteration [101]: 0.21460693551205018
Loss at iteration [102]: 0.21460639353578323
Loss at iteration [103]: 0.21460524975291953
Loss at iteration [104]: 0.2146042389593267
Loss at iteration [105]: 0.21460421106538038
Loss at iteration [106]: 0.21460362084849438
Loss at iteration [107]: 0.21460353056891596
Loss at iteration [108]: 0.21460353056891596
Loss at iteration [109]: 0.2146034459536695
Loss at iteration [110]: 0.21460273348783165
Loss at iteration [111]: 0.2146014909107587
Loss at iteration [112]: 0.2146008643982504
Loss at iteration [113]: 0.21460067667632826
Loss at iteration [114]: 0.214600134841271
Loss at iteration [115]: 0.214600134841271
Loss at iteration [116]: 0.21460006512798918
Loss at iteration [117]: 0.21459966957999463
Loss at iteration [118]: 0.2145988109026901
Loss at iteration [119]: 0.21459737706141116
Loss at iteration [120]: 0.21459721101534898
Loss at iteration [121]: 0.21458770388018938
Loss at iteration [122]: 0.21458770388018938
Loss at iteration [123]: 0.2145872929798962
Loss at iteration [124]: 0.21458657610456686
Loss at iteration [125]: 0.2145847368543915
Loss at iteration [126]: 0.21458406552446516
Loss at iteration [127]: 0.2145839385023626
Loss at iteration [128]: 0.21458246668330602
Loss at iteration [129]: 0.21458246668330602
Loss at iteration [130]: 0.21458207704284765
Loss at iteration [131]: 0.21458205588487972
Loss at iteration [132]: 0.21458139848223468
Loss at iteration [133]: 0.21458137469311162
Loss at iteration [134]: 0.21458107414599115
Loss at iteration [135]: 0.21458067519148627
Loss at iteration [136]: 0.21458067519148627
Loss at iteration [137]: 0.21458063536502486
Loss at iteration [138]: 0.21458034066322362
Loss at iteration [139]: 0.21458027094421336
Loss at iteration [140]: 0.21457930818375898
Loss at iteration [141]: 0.21457927329315957
Loss at iteration [142]: 0.21457869135421603
Loss at iteration [143]: 0.21457869135421603
Loss at iteration [144]: 0.21457834016158628
Loss at iteration [145]: 0.2145782439250677
Loss at iteration [146]: 0.2145780000922798
Loss at iteration [147]: 0.21457783763801566
Loss at iteration [148]: 0.2145777753698535
Loss at iteration [149]: 0.21457745988002752
Loss at iteration [150]: 0.21457745988002752
Loss at iteration [151]: 0.2145773083161591
Loss at iteration [152]: 0.2145770202757734
Loss at iteration [153]: 0.21457679998713308
Loss at iteration [154]: 0.21457653159005727
Loss at iteration [155]: 0.21457651588861604
Loss at iteration [156]: 0.21457341737301155
Loss at iteration [157]: 0.21457341737301155
Loss at iteration [158]: 0.21457311155417724
Loss at iteration [159]: 0.2145728381427635
Loss at iteration [160]: 0.2145727750379503
Loss at iteration [161]: 0.2145726985358809
Loss at iteration [162]: 0.2145725780446064
Loss at iteration [163]: 0.2145723501772196
Loss at iteration [164]: 0.21457201125307035
Loss at iteration [165]: 0.21457201125307035
Loss at iteration [166]: 0.21457187685957466
Loss at iteration [167]: 0.2145716177126262
Loss at iteration [168]: 0.21457161047315326
Loss at iteration [169]: 0.2145715297748426
Loss at iteration [170]: 0.21457151309157826
Loss at iteration [171]: 0.21457145054333057
Loss at iteration [172]: 0.21457139280433157
Loss at iteration [173]: 0.21457139280433157
Loss at iteration [174]: 0.21457135982309555
Loss at iteration [175]: 0.214571325508307
Loss at iteration [176]: 0.214571227733951
Loss at iteration [177]: 0.21457121053238232
Loss at iteration [178]: 0.21457111808302876
Loss at iteration [179]: 0.21457108669044075
Loss at iteration [180]: 0.21457108669044075
Loss at iteration [181]: 0.21457106892845543
Loss at iteration [182]: 0.2145710260841212
Loss at iteration [183]: 0.2145708771659607
Loss at iteration [184]: 0.21457078616841166
Loss at iteration [185]: 0.21457077529707347
Loss at iteration [186]: 0.21456922221181302
Loss at iteration [187]: 0.21456922221181302
Loss at iteration [188]: 0.21456877297992763
Loss at iteration [189]: 0.21456841698983337
Loss at iteration [190]: 0.21456836942381421
Loss at iteration [191]: 0.21456832973025872
Loss at iteration [192]: 0.2145682776362066
Loss at iteration [193]: 0.21456821363641454
Loss at iteration [194]: 0.21456814766338642
Loss at iteration [195]: 0.21456814766338642
Loss at iteration [196]: 0.21456812185580743
Loss at iteration [197]: 0.21456807589662855
Loss at iteration [198]: 0.21456799163596585
Loss at iteration [199]: 0.21456790654970456
Loss at iteration [200]: 0.21456789781331914
Loss at iteration [201]: 0.2145678369515453
Loss at iteration [202]: 0.2145678369515453
Loss at iteration [203]: 0.21456779347372262
Loss at iteration [204]: 0.21456778438318275
Loss at iteration [205]: 0.21456775615564946
Loss at iteration [206]: 0.21456773965163875
Loss at iteration [207]: 0.2145677069820458
Loss at iteration [208]: 0.2145676698376934
Loss at iteration [209]: 0.2145676698376934
Loss at iteration [210]: 0.21456765261545083
Loss at iteration [211]: 0.2145676279916806
Loss at iteration [212]: 0.21456759875559003
Loss at iteration [213]: 0.21456759671305262
Loss at iteration [214]: 0.21456750217493462
Loss at iteration [215]: 0.21456749315880502
Loss at iteration [216]: 0.21456749315880502
Loss at iteration [217]: 0.21456748917108717
Loss at iteration [218]: 0.21456747326425946
Loss at iteration [219]: 0.21456744632461353
Loss at iteration [220]: 0.21456743684880047
Loss at iteration [221]: 0.21456736556494224
Loss at iteration [222]: 0.2145673454250315
Loss at iteration [223]: 0.2145673454250315
Loss at iteration [224]: 0.21456732847435142
Loss at iteration [225]: 0.21456729817669182
Loss at iteration [226]: 0.2145672827521799
Loss at iteration [227]: 0.21456724146822043
Loss at iteration [228]: 0.21456722830089928
Loss at iteration [229]: 0.21456722793862817
