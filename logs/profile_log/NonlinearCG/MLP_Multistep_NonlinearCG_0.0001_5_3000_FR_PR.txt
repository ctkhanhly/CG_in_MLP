Model name                            : MLP_Multistep
The number of input features          : 5
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :FR_PR
Total number of function evaluations  : 2148
Total number of iterations            : 607
Max number of iterations              : 3000
Number of samples in training data    : 122
Number of samples in tests data       : 52
Total training time                   : 4.481926441192627
Total number of parameters            : 202302
Percentage of parameters < 1e-9       : 49.84231495486945%
Percentage of parameters < 1e-7       : 49.84231495486945%
Percentage of parameters < 1e-6       : 49.84330357584206%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.6756029529364459
Loss at iteration [2]: 0.6710870032051472
Loss at iteration [3]: 0.6551557789495268
Loss at iteration [4]: 0.6333519786678196
Loss at iteration [5]: 0.6186498345909336
Loss at iteration [6]: 0.6119824043279443
Loss at iteration [7]: 0.6119824043279443
Loss at iteration [8]: 0.6078246352648818
Loss at iteration [9]: 0.6045231937649406
Loss at iteration [10]: 0.587768424035733
Loss at iteration [11]: 0.570346725695819
Loss at iteration [12]: 0.5653496287726109
Loss at iteration [13]: 0.5653496287726109
Loss at iteration [14]: 0.5640092860832909
Loss at iteration [15]: 0.5457932025854664
Loss at iteration [16]: 0.5436313559889978
Loss at iteration [17]: 0.5276217830123212
Loss at iteration [18]: 0.518042405735549
Loss at iteration [19]: 0.4978782823757044
Loss at iteration [20]: 0.4978782823757044
Loss at iteration [21]: 0.49517192417091566
Loss at iteration [22]: 0.46529791442452306
Loss at iteration [23]: 0.460239463593192
Loss at iteration [24]: 0.45345216098794
Loss at iteration [25]: 0.4486085257113822
Loss at iteration [26]: 0.4375403653417791
Loss at iteration [27]: 0.4375403653417791
Loss at iteration [28]: 0.43587734756832663
Loss at iteration [29]: 0.42981607761968305
Loss at iteration [30]: 0.41698578538515707
Loss at iteration [31]: 0.4033038110318718
Loss at iteration [32]: 0.3880850824537398
Loss at iteration [33]: 0.3679779049677418
Loss at iteration [34]: 0.3679779049677418
Loss at iteration [35]: 0.36509071229978485
Loss at iteration [36]: 0.3502288541273081
Loss at iteration [37]: 0.3477301537117297
Loss at iteration [38]: 0.3428872965399503
Loss at iteration [39]: 0.3375804950652926
Loss at iteration [40]: 0.33319653650752173
Loss at iteration [41]: 0.33079629204646116
Loss at iteration [42]: 0.33079629204646116
Loss at iteration [43]: 0.3288728294207824
Loss at iteration [44]: 0.32676003820387095
Loss at iteration [45]: 0.3248435737864252
Loss at iteration [46]: 0.32339722789600317
Loss at iteration [47]: 0.3201896523151102
Loss at iteration [48]: 0.31232996469238994
Loss at iteration [49]: 0.2951476646466038
Loss at iteration [50]: 0.2951476646466038
Loss at iteration [51]: 0.2932750321307941
Loss at iteration [52]: 0.2849769233081944
Loss at iteration [53]: 0.28464617399491976
Loss at iteration [54]: 0.28201444773236606
Loss at iteration [55]: 0.28118566724951877
Loss at iteration [56]: 0.2774449984716989
Loss at iteration [57]: 0.2774449984716989
Loss at iteration [58]: 0.27625032857341447
Loss at iteration [59]: 0.27501967558869655
Loss at iteration [60]: 0.27472352551020923
Loss at iteration [61]: 0.2726810243447674
Loss at iteration [62]: 0.27024212693484606
Loss at iteration [63]: 0.26943079744184406
Loss at iteration [64]: 0.26943079744184406
Loss at iteration [65]: 0.2692014033247084
Loss at iteration [66]: 0.2680003825047939
Loss at iteration [67]: 0.2678896048754048
Loss at iteration [68]: 0.2675792813458776
Loss at iteration [69]: 0.26739630421027544
Loss at iteration [70]: 0.26613529459137325
Loss at iteration [71]: 0.26613529459137325
Loss at iteration [72]: 0.2659350570425697
Loss at iteration [73]: 0.26535644887623344
Loss at iteration [74]: 0.26524041003224824
Loss at iteration [75]: 0.2642348709483486
Loss at iteration [76]: 0.2640927480084593
Loss at iteration [77]: 0.26370702342281743
Loss at iteration [78]: 0.263572551914941
Loss at iteration [79]: 0.263572551914941
Loss at iteration [80]: 0.26349820842234817
Loss at iteration [81]: 0.26314539316618335
Loss at iteration [82]: 0.2630580130237052
Loss at iteration [83]: 0.2616842799805646
Loss at iteration [84]: 0.26139007020879457
Loss at iteration [85]: 0.26066023416098616
Loss at iteration [86]: 0.2603910797531355
Loss at iteration [87]: 0.2603910797531355
Loss at iteration [88]: 0.2602277954948034
Loss at iteration [89]: 0.2601148426167002
Loss at iteration [90]: 0.26001249242311714
Loss at iteration [91]: 0.2595951633097163
Loss at iteration [92]: 0.25954580670312144
Loss at iteration [93]: 0.25937526682879253
Loss at iteration [94]: 0.259062419320266
Loss at iteration [95]: 0.25890820341168863
Loss at iteration [96]: 0.25890820341168863
Loss at iteration [97]: 0.25882865822753387
Loss at iteration [98]: 0.258786143033518
Loss at iteration [99]: 0.25864703632634317
Loss at iteration [100]: 0.2585928117978934
Loss at iteration [101]: 0.2585210759115351
Loss at iteration [102]: 0.25841281077695893
Loss at iteration [103]: 0.2581436185707531
Loss at iteration [104]: 0.25803188134027477
Loss at iteration [105]: 0.25803188134027477
Loss at iteration [106]: 0.25795840608841014
Loss at iteration [107]: 0.25781291001249906
Loss at iteration [108]: 0.2577676717366248
Loss at iteration [109]: 0.2575901532982301
Loss at iteration [110]: 0.25740306924975037
Loss at iteration [111]: 0.2570768401357875
Loss at iteration [112]: 0.2570768401357875
Loss at iteration [113]: 0.25695702528814357
Loss at iteration [114]: 0.2566394303037276
Loss at iteration [115]: 0.2565135733519045
Loss at iteration [116]: 0.25636222900469724
Loss at iteration [117]: 0.25629283280939147
Loss at iteration [118]: 0.2561342517570735
Loss at iteration [119]: 0.25595389603548707
Loss at iteration [120]: 0.25595389603548707
Loss at iteration [121]: 0.25578070842049755
Loss at iteration [122]: 0.2555291189841767
Loss at iteration [123]: 0.25542018618583856
Loss at iteration [124]: 0.2553520692064447
Loss at iteration [125]: 0.25525080795476074
Loss at iteration [126]: 0.25496654419775966
Loss at iteration [127]: 0.2549166427069812
Loss at iteration [128]: 0.25479244863719064
Loss at iteration [129]: 0.25479244863719064
Loss at iteration [130]: 0.25476219096850444
Loss at iteration [131]: 0.2547153677138091
Loss at iteration [132]: 0.2544953648281346
Loss at iteration [133]: 0.2544019217673824
Loss at iteration [134]: 0.2540850386136508
Loss at iteration [135]: 0.25382349545019445
Loss at iteration [136]: 0.25382349545019445
Loss at iteration [137]: 0.2535660809182067
Loss at iteration [138]: 0.25350466630924584
Loss at iteration [139]: 0.25336197503948527
Loss at iteration [140]: 0.25329280510214247
Loss at iteration [141]: 0.2532112487196958
Loss at iteration [142]: 0.25315880651203554
Loss at iteration [143]: 0.25315880651203554
Loss at iteration [144]: 0.2531009366739028
Loss at iteration [145]: 0.2530997661253054
Loss at iteration [146]: 0.25307563766624264
Loss at iteration [147]: 0.25300926996103484
Loss at iteration [148]: 0.2528895470832
Loss at iteration [149]: 0.25270258289069236
Loss at iteration [150]: 0.25250556455431894
Loss at iteration [151]: 0.2522442337371323
Loss at iteration [152]: 0.2522442337371323
Loss at iteration [153]: 0.2521631379448039
Loss at iteration [154]: 0.2520969387486085
Loss at iteration [155]: 0.25204702971077425
Loss at iteration [156]: 0.2518641472777919
Loss at iteration [157]: 0.25175645509891875
Loss at iteration [158]: 0.2517261163158141
Loss at iteration [159]: 0.25167418912437384
Loss at iteration [160]: 0.25167418912437384
Loss at iteration [161]: 0.2516735872196156
Loss at iteration [162]: 0.25163905814857673
Loss at iteration [163]: 0.2515764882543538
Loss at iteration [164]: 0.2515289330590623
Loss at iteration [165]: 0.2514360574766593
Loss at iteration [166]: 0.25140386710046825
Loss at iteration [167]: 0.2513758497549705
Loss at iteration [168]: 0.2513630460660253
Loss at iteration [169]: 0.2513630460660253
Loss at iteration [170]: 0.2513509881643775
Loss at iteration [171]: 0.25131530797481255
Loss at iteration [172]: 0.25130071047359914
Loss at iteration [173]: 0.25124038409153004
Loss at iteration [174]: 0.2512180321368478
Loss at iteration [175]: 0.25110777308637156
Loss at iteration [176]: 0.2507742525284492
Loss at iteration [177]: 0.2507742525284492
Loss at iteration [178]: 0.2506232603845575
Loss at iteration [179]: 0.2504143668886016
Loss at iteration [180]: 0.25031408851824377
Loss at iteration [181]: 0.25011304356536085
Loss at iteration [182]: 0.2500728753653165
Loss at iteration [183]: 0.2500230574969835
Loss at iteration [184]: 0.24973351931980386
Loss at iteration [185]: 0.24973351931980386
Loss at iteration [186]: 0.2496714491581045
Loss at iteration [187]: 0.24925950948900955
Loss at iteration [188]: 0.24918782938467918
Loss at iteration [189]: 0.24915414179898035
Loss at iteration [190]: 0.2490392694518099
Loss at iteration [191]: 0.24880555760659864
Loss at iteration [192]: 0.24873324343055533
Loss at iteration [193]: 0.24873324343055533
Loss at iteration [194]: 0.24866926237714293
Loss at iteration [195]: 0.24857282975224196
Loss at iteration [196]: 0.24848793311494366
Loss at iteration [197]: 0.24841532983821707
Loss at iteration [198]: 0.24837254415334717
Loss at iteration [199]: 0.24827461180897195
Loss at iteration [200]: 0.24814899514675193
Loss at iteration [201]: 0.24798580507077086
Loss at iteration [202]: 0.24798580507077086
Loss at iteration [203]: 0.24780336693797664
Loss at iteration [204]: 0.24756945095241942
Loss at iteration [205]: 0.24747445612743574
Loss at iteration [206]: 0.2474268921443763
Loss at iteration [207]: 0.24728743223970703
Loss at iteration [208]: 0.24724058766371762
Loss at iteration [209]: 0.24724058766371762
Loss at iteration [210]: 0.24717955463680907
Loss at iteration [211]: 0.24713535991622124
Loss at iteration [212]: 0.24706415561698933
Loss at iteration [213]: 0.24692602334040853
Loss at iteration [214]: 0.2467203766456643
Loss at iteration [215]: 0.24665653334731932
Loss at iteration [216]: 0.2464661618606674
Loss at iteration [217]: 0.2464661618606674
Loss at iteration [218]: 0.24640229971897945
Loss at iteration [219]: 0.24637973631410667
Loss at iteration [220]: 0.24627568164053423
Loss at iteration [221]: 0.2462050372191435
Loss at iteration [222]: 0.24605263114021933
Loss at iteration [223]: 0.2459577835441518
Loss at iteration [224]: 0.2457705608154796
Loss at iteration [225]: 0.2457705608154796
Loss at iteration [226]: 0.24571917074333882
Loss at iteration [227]: 0.2456758204779776
Loss at iteration [228]: 0.2456167782161497
Loss at iteration [229]: 0.24558993968492504
Loss at iteration [230]: 0.24553890913474938
Loss at iteration [231]: 0.24552491645032623
Loss at iteration [232]: 0.2454546477503686
Loss at iteration [233]: 0.24541826596547575
Loss at iteration [234]: 0.24541826596547575
Loss at iteration [235]: 0.24535342721583248
Loss at iteration [236]: 0.2452826858832333
Loss at iteration [237]: 0.24518574472115995
Loss at iteration [238]: 0.24513959934362134
Loss at iteration [239]: 0.2449802118757994
Loss at iteration [240]: 0.24450984072861512
Loss at iteration [241]: 0.24443390104828738
Loss at iteration [242]: 0.24443390104828738
Loss at iteration [243]: 0.24438228268792486
Loss at iteration [244]: 0.24399883064768538
Loss at iteration [245]: 0.2439787505709505
Loss at iteration [246]: 0.24383034357416075
Loss at iteration [247]: 0.2436940571100908
Loss at iteration [248]: 0.24352382175433165
Loss at iteration [249]: 0.24332336491718254
Loss at iteration [250]: 0.24332336491718254
Loss at iteration [251]: 0.2432260394733457
Loss at iteration [252]: 0.24313142106867666
Loss at iteration [253]: 0.24286869930930854
Loss at iteration [254]: 0.242810424658915
Loss at iteration [255]: 0.2425868113981079
Loss at iteration [256]: 0.24246113634744457
Loss at iteration [257]: 0.24246113634744457
Loss at iteration [258]: 0.24239014330085598
Loss at iteration [259]: 0.2421963766726135
Loss at iteration [260]: 0.2421223030824415
Loss at iteration [261]: 0.24187991886862964
Loss at iteration [262]: 0.24185342520233943
Loss at iteration [263]: 0.24182531213309186
Loss at iteration [264]: 0.24167626316100632
Loss at iteration [265]: 0.24167626316100632
Loss at iteration [266]: 0.24163286260346728
Loss at iteration [267]: 0.24154601803635345
Loss at iteration [268]: 0.24149549914956916
Loss at iteration [269]: 0.24139609042408164
Loss at iteration [270]: 0.2412914883787543
Loss at iteration [271]: 0.24111787006997837
Loss at iteration [272]: 0.24101948358829814
Loss at iteration [273]: 0.24101948358829814
Loss at iteration [274]: 0.24096332942984297
Loss at iteration [275]: 0.24081381942390326
Loss at iteration [276]: 0.24070612364018634
Loss at iteration [277]: 0.24064811012167103
Loss at iteration [278]: 0.24052639397926745
Loss at iteration [279]: 0.24042400504476535
Loss at iteration [280]: 0.24032015872216508
Loss at iteration [281]: 0.24032015872216508
Loss at iteration [282]: 0.24025317483751327
Loss at iteration [283]: 0.2402438865580111
Loss at iteration [284]: 0.24021313186473103
Loss at iteration [285]: 0.24019235662996008
Loss at iteration [286]: 0.24016396827516204
Loss at iteration [287]: 0.23995870974508418
Loss at iteration [288]: 0.2399193841235955
Loss at iteration [289]: 0.23990300045286977
Loss at iteration [290]: 0.23990300045286977
Loss at iteration [291]: 0.23986857311148527
Loss at iteration [292]: 0.23986731659243904
Loss at iteration [293]: 0.23981911294384534
Loss at iteration [294]: 0.23971930483161016
Loss at iteration [295]: 0.23966799950518652
Loss at iteration [296]: 0.23962935779967784
Loss at iteration [297]: 0.23958567913065568
Loss at iteration [298]: 0.2395112112418794
Loss at iteration [299]: 0.2395112112418794
Loss at iteration [300]: 0.2394970167109429
Loss at iteration [301]: 0.23948362246331134
Loss at iteration [302]: 0.2394716221568798
Loss at iteration [303]: 0.23946662815176495
Loss at iteration [304]: 0.2394474976288096
Loss at iteration [305]: 0.2394383635025889
Loss at iteration [306]: 0.23941766670601722
Loss at iteration [307]: 0.23940609065854074
Loss at iteration [308]: 0.23939932846591172
Loss at iteration [309]: 0.23936115446206824
Loss at iteration [310]: 0.23936115446206824
Loss at iteration [311]: 0.23935540244500933
Loss at iteration [312]: 0.2393474112269071
Loss at iteration [313]: 0.23933607111289684
Loss at iteration [314]: 0.23931208787140387
Loss at iteration [315]: 0.23922195517540754
Loss at iteration [316]: 0.23908552626232477
Loss at iteration [317]: 0.2388193977445172
Loss at iteration [318]: 0.23868536931133133
Loss at iteration [319]: 0.23868536931133133
Loss at iteration [320]: 0.23849731011918573
Loss at iteration [321]: 0.2384863285492027
Loss at iteration [322]: 0.23844888430475034
Loss at iteration [323]: 0.23840678465924858
Loss at iteration [324]: 0.23834694684322233
Loss at iteration [325]: 0.2382778759909445
Loss at iteration [326]: 0.2381841890080386
Loss at iteration [327]: 0.23798821305693602
Loss at iteration [328]: 0.23798821305693602
Loss at iteration [329]: 0.2379384043451414
Loss at iteration [330]: 0.23791275008642804
Loss at iteration [331]: 0.2378852097418126
Loss at iteration [332]: 0.23782573622670944
Loss at iteration [333]: 0.23780722169454338
Loss at iteration [334]: 0.23771380574094986
Loss at iteration [335]: 0.23767101989937164
Loss at iteration [336]: 0.23765166723543962
Loss at iteration [337]: 0.23765166723543962
Loss at iteration [338]: 0.23763596340418283
Loss at iteration [339]: 0.23762470762812662
Loss at iteration [340]: 0.237604763558495
Loss at iteration [341]: 0.23759486430733476
Loss at iteration [342]: 0.23755484335513433
Loss at iteration [343]: 0.2375097659012856
Loss at iteration [344]: 0.23745096383044892
Loss at iteration [345]: 0.23745096383044892
Loss at iteration [346]: 0.23740902735544675
Loss at iteration [347]: 0.2373760168703842
Loss at iteration [348]: 0.23734536734604803
Loss at iteration [349]: 0.23729467434212678
Loss at iteration [350]: 0.23728023217162647
Loss at iteration [351]: 0.23722533935828544
Loss at iteration [352]: 0.23717287207360796
Loss at iteration [353]: 0.23717287207360796
Loss at iteration [354]: 0.23715896267633213
Loss at iteration [355]: 0.23711270873321655
Loss at iteration [356]: 0.23706485107976968
Loss at iteration [357]: 0.23699834181405163
Loss at iteration [358]: 0.23688268775571864
Loss at iteration [359]: 0.2367996611778186
Loss at iteration [360]: 0.23597384141804853
Loss at iteration [361]: 0.23597384141804853
Loss at iteration [362]: 0.23561895179698433
Loss at iteration [363]: 0.23550815622916094
Loss at iteration [364]: 0.2354000172473337
Loss at iteration [365]: 0.23536383966149269
Loss at iteration [366]: 0.23526541504722942
Loss at iteration [367]: 0.23515323846234903
Loss at iteration [368]: 0.23515323846234903
Loss at iteration [369]: 0.23508937587653633
Loss at iteration [370]: 0.23503496955223355
Loss at iteration [371]: 0.2348785209321522
Loss at iteration [372]: 0.23474803711772566
Loss at iteration [373]: 0.234706352441359
Loss at iteration [374]: 0.23466067704080668
Loss at iteration [375]: 0.23459418458126996
Loss at iteration [376]: 0.23459418458126996
Loss at iteration [377]: 0.23457521650078872
Loss at iteration [378]: 0.2345034067925796
Loss at iteration [379]: 0.234426291029866
Loss at iteration [380]: 0.234418479837535
Loss at iteration [381]: 0.2342906114901047
Loss at iteration [382]: 0.234193215945706
Loss at iteration [383]: 0.23390515578440355
Loss at iteration [384]: 0.23373970616741552
Loss at iteration [385]: 0.23373970616741552
Loss at iteration [386]: 0.23367353760465664
Loss at iteration [387]: 0.2335438801689049
Loss at iteration [388]: 0.2335112015576952
Loss at iteration [389]: 0.23347813651240665
Loss at iteration [390]: 0.23337367913035292
Loss at iteration [391]: 0.2333318888283101
Loss at iteration [392]: 0.23325566347114293
Loss at iteration [393]: 0.23325566347114293
Loss at iteration [394]: 0.23321657323013698
Loss at iteration [395]: 0.2331812093948454
Loss at iteration [396]: 0.23316439687935053
Loss at iteration [397]: 0.23309218201520585
Loss at iteration [398]: 0.23298997552028775
Loss at iteration [399]: 0.23286862423668542
Loss at iteration [400]: 0.2319699625089446
Loss at iteration [401]: 0.2319699625089446
Loss at iteration [402]: 0.23175633948790048
Loss at iteration [403]: 0.23162679531804165
Loss at iteration [404]: 0.23140507195091622
Loss at iteration [405]: 0.23133307795553612
Loss at iteration [406]: 0.2311716876965456
Loss at iteration [407]: 0.2311250084960155
Loss at iteration [408]: 0.2311250084960155
Loss at iteration [409]: 0.23110041135820192
Loss at iteration [410]: 0.23106228811213728
Loss at iteration [411]: 0.23103860855215796
Loss at iteration [412]: 0.23099644947094164
Loss at iteration [413]: 0.23092979739541397
Loss at iteration [414]: 0.2308357183250684
Loss at iteration [415]: 0.23075166046055376
Loss at iteration [416]: 0.23075166046055376
Loss at iteration [417]: 0.2307044615148642
Loss at iteration [418]: 0.23065115978493636
Loss at iteration [419]: 0.23060764235847406
Loss at iteration [420]: 0.2305658309126747
Loss at iteration [421]: 0.23048214313102855
Loss at iteration [422]: 0.23041935021568413
Loss at iteration [423]: 0.23019344870990627
Loss at iteration [424]: 0.23019344870990627
Loss at iteration [425]: 0.23011580642326365
Loss at iteration [426]: 0.23001503908968288
Loss at iteration [427]: 0.22996540144439565
Loss at iteration [428]: 0.22992607313160376
Loss at iteration [429]: 0.22983966736971712
Loss at iteration [430]: 0.22978014789629278
Loss at iteration [431]: 0.22969418871777197
Loss at iteration [432]: 0.22962815760688093
Loss at iteration [433]: 0.22962815760688093
Loss at iteration [434]: 0.22959254147514063
Loss at iteration [435]: 0.22956841491433982
Loss at iteration [436]: 0.2295375234831089
Loss at iteration [437]: 0.22950991809551327
Loss at iteration [438]: 0.22949711943197762
Loss at iteration [439]: 0.2294540010650877
Loss at iteration [440]: 0.22943191849862238
Loss at iteration [441]: 0.22942318595460606
Loss at iteration [442]: 0.22937815087289978
Loss at iteration [443]: 0.22937815087289978
Loss at iteration [444]: 0.22935624241667846
Loss at iteration [445]: 0.22933387975573624
Loss at iteration [446]: 0.22929472875943935
Loss at iteration [447]: 0.22924295670011738
Loss at iteration [448]: 0.22920962861695954
Loss at iteration [449]: 0.22874423578874944
Loss at iteration [450]: 0.22862005323161544
Loss at iteration [451]: 0.22862005323161544
Loss at iteration [452]: 0.22854415663500421
Loss at iteration [453]: 0.22849579303479042
Loss at iteration [454]: 0.2283872644008444
Loss at iteration [455]: 0.2283655695992026
Loss at iteration [456]: 0.22830993991881518
Loss at iteration [457]: 0.2282967198667252
Loss at iteration [458]: 0.2282485902136121
Loss at iteration [459]: 0.2282485902136121
Loss at iteration [460]: 0.2281767703796465
Loss at iteration [461]: 0.22815701114499715
Loss at iteration [462]: 0.2280912066065905
Loss at iteration [463]: 0.22805369510125792
Loss at iteration [464]: 0.2280058799301929
Loss at iteration [465]: 0.22799301793022397
Loss at iteration [466]: 0.2279694449255062
Loss at iteration [467]: 0.22795783674460685
Loss at iteration [468]: 0.2279320766678519
Loss at iteration [469]: 0.2279320766678519
Loss at iteration [470]: 0.22791874211575106
Loss at iteration [471]: 0.2279058080478561
Loss at iteration [472]: 0.22790051579120185
Loss at iteration [473]: 0.22788958462291287
Loss at iteration [474]: 0.22785697254109716
Loss at iteration [475]: 0.2278185076529682
Loss at iteration [476]: 0.22771955579592307
Loss at iteration [477]: 0.22761715883022934
Loss at iteration [478]: 0.22761715883022934
Loss at iteration [479]: 0.22758778184175338
Loss at iteration [480]: 0.22755773887112996
Loss at iteration [481]: 0.22754856927293293
Loss at iteration [482]: 0.22748212573483007
Loss at iteration [483]: 0.22743429569561474
Loss at iteration [484]: 0.22739215146068067
Loss at iteration [485]: 0.22711359950325785
Loss at iteration [486]: 0.22700131417895214
Loss at iteration [487]: 0.22700131417895214
Loss at iteration [488]: 0.2269864874262966
Loss at iteration [489]: 0.22697077821882994
Loss at iteration [490]: 0.22689922848675623
Loss at iteration [491]: 0.22685766774824262
Loss at iteration [492]: 0.22679341257590574
Loss at iteration [493]: 0.22668070243479693
Loss at iteration [494]: 0.22652253491247587
Loss at iteration [495]: 0.2264968631560455
Loss at iteration [496]: 0.2264968631560455
Loss at iteration [497]: 0.22645664854627076
Loss at iteration [498]: 0.22638032864280577
Loss at iteration [499]: 0.22635731303210685
Loss at iteration [500]: 0.22632095212263803
Loss at iteration [501]: 0.22631386169761342
Loss at iteration [502]: 0.22630681767630947
Loss at iteration [503]: 0.22628492103877149
Loss at iteration [504]: 0.22626455983675986
Loss at iteration [505]: 0.22626100102306185
Loss at iteration [506]: 0.22626100102306185
Loss at iteration [507]: 0.2262578764576321
Loss at iteration [508]: 0.22623977744210574
Loss at iteration [509]: 0.22623330660795463
Loss at iteration [510]: 0.22621939797692775
Loss at iteration [511]: 0.2262146472146728
Loss at iteration [512]: 0.2262024518113164
Loss at iteration [513]: 0.22619482773038577
Loss at iteration [514]: 0.2261806256851429
Loss at iteration [515]: 0.2261557725300221
Loss at iteration [516]: 0.2259935790154523
Loss at iteration [517]: 0.22591637546544271
Loss at iteration [518]: 0.22591637546544271
Loss at iteration [519]: 0.22577993124195875
Loss at iteration [520]: 0.22567603579206505
Loss at iteration [521]: 0.2256213069280081
Loss at iteration [522]: 0.2255057692779344
Loss at iteration [523]: 0.22545428220275976
Loss at iteration [524]: 0.22542329578768178
Loss at iteration [525]: 0.22542329578768178
Loss at iteration [526]: 0.2254094936641772
Loss at iteration [527]: 0.2253891322878647
Loss at iteration [528]: 0.22538082698045578
Loss at iteration [529]: 0.22535116642464306
Loss at iteration [530]: 0.22532633779795247
Loss at iteration [531]: 0.22530938485411717
Loss at iteration [532]: 0.22525176316213033
Loss at iteration [533]: 0.22521308852344168
Loss at iteration [534]: 0.2251374268460215
Loss at iteration [535]: 0.2251374268460215
Loss at iteration [536]: 0.2251120611607789
Loss at iteration [537]: 0.22509830533478872
Loss at iteration [538]: 0.22506900916246048
Loss at iteration [539]: 0.22505020053516456
Loss at iteration [540]: 0.2250287934298579
Loss at iteration [541]: 0.2250123553212156
Loss at iteration [542]: 0.22499982708456556
Loss at iteration [543]: 0.22499752735320075
Loss at iteration [544]: 0.22499752735320075
Loss at iteration [545]: 0.22499624675449634
Loss at iteration [546]: 0.22499351794556122
Loss at iteration [547]: 0.224985815801716
Loss at iteration [548]: 0.22498488113025547
Loss at iteration [549]: 0.2249614985724602
Loss at iteration [550]: 0.22494736996903889
Loss at iteration [551]: 0.2249329057980197
Loss at iteration [552]: 0.2249107650763255
Loss at iteration [553]: 0.2248841326178587
Loss at iteration [554]: 0.22469579999378358
Loss at iteration [555]: 0.22436872477489384
Loss at iteration [556]: 0.22436872477489384
Loss at iteration [557]: 0.22427345162665505
Loss at iteration [558]: 0.22415555465014061
Loss at iteration [559]: 0.2240831383818707
Loss at iteration [560]: 0.22402292287126152
Loss at iteration [561]: 0.22395496485595176
Loss at iteration [562]: 0.22385375069271632
Loss at iteration [563]: 0.223728046501493
Loss at iteration [564]: 0.223728046501493
Loss at iteration [565]: 0.22365497821261932
Loss at iteration [566]: 0.2236169341434613
Loss at iteration [567]: 0.22357779246437143
Loss at iteration [568]: 0.2235128184003567
Loss at iteration [569]: 0.2234964437008239
Loss at iteration [570]: 0.22344654991256996
Loss at iteration [571]: 0.223391156895718
Loss at iteration [572]: 0.223391156895718
Loss at iteration [573]: 0.22334581188658922
Loss at iteration [574]: 0.22332306474503957
Loss at iteration [575]: 0.2232669031985154
Loss at iteration [576]: 0.22320045852590292
Loss at iteration [577]: 0.22316310292001418
Loss at iteration [578]: 0.22311823089864588
Loss at iteration [579]: 0.2230911301915809
Loss at iteration [580]: 0.2230911301915809
Loss at iteration [581]: 0.22306283162795526
Loss at iteration [582]: 0.22304120320250215
Loss at iteration [583]: 0.22300881602564226
Loss at iteration [584]: 0.22299454488717343
Loss at iteration [585]: 0.22297758415559385
Loss at iteration [586]: 0.22293240786014376
Loss at iteration [587]: 0.2229255565361016
Loss at iteration [588]: 0.22287383185201717
Loss at iteration [589]: 0.2228532306955949
Loss at iteration [590]: 0.22277552170432188
Loss at iteration [591]: 0.22277552170432188
Loss at iteration [592]: 0.22273551864011565
Loss at iteration [593]: 0.2227161907805122
Loss at iteration [594]: 0.2226989820393946
Loss at iteration [595]: 0.2226782399027112
Loss at iteration [596]: 0.22264456522948678
Loss at iteration [597]: 0.2225299712178577
Loss at iteration [598]: 0.22250584780716853
Loss at iteration [599]: 0.2224431293001935
Loss at iteration [600]: 0.2224431293001935
Loss at iteration [601]: 0.2224161811447721
Loss at iteration [602]: 0.22240278061432536
Loss at iteration [603]: 0.2223746357682982
Loss at iteration [604]: 0.22237361158505586
Loss at iteration [605]: 0.22235834806921237
Loss at iteration [606]: 0.2223511171592364
Loss at iteration [607]: 0.222339726920454
Loss at iteration [608]: 0.2222688064742093
Loss at iteration [609]: 0.22223387721293525
Loss at iteration [610]: 0.22216185219238557
Loss at iteration [611]: 0.22216185219238557
Loss at iteration [612]: 0.22212637486529413
Loss at iteration [613]: 0.222094528189483
Loss at iteration [614]: 0.2220594763532387
Loss at iteration [615]: 0.22202660735652552
Loss at iteration [616]: 0.2220049331133964
Loss at iteration [617]: 0.22197040750338856
Loss at iteration [618]: 0.2219087935156269
Loss at iteration [619]: 0.2219087935156269
Loss at iteration [620]: 0.22188252418667356
Loss at iteration [621]: 0.22187424148160143
Loss at iteration [622]: 0.2218581415616147
Loss at iteration [623]: 0.22185502194760595
Loss at iteration [624]: 0.22183747391695502
Loss at iteration [625]: 0.2218327065458714
Loss at iteration [626]: 0.2218207698254171
Loss at iteration [627]: 0.22181750793479504
Loss at iteration [628]: 0.22180122700568308
Loss at iteration [629]: 0.22179737084905696
Loss at iteration [630]: 0.22179737084905696
Loss at iteration [631]: 0.22179657741326111
Loss at iteration [632]: 0.22179017645830193
Loss at iteration [633]: 0.2217844467125281
Loss at iteration [634]: 0.22178112980122108
Loss at iteration [635]: 0.22177614799342502
Loss at iteration [636]: 0.22177370170310473
Loss at iteration [637]: 0.2217712513384292
Loss at iteration [638]: 0.22177002090295495
Loss at iteration [639]: 0.2217641992969763
Loss at iteration [640]: 0.22175788896613108
Loss at iteration [641]: 0.22175297512792166
Loss at iteration [642]: 0.22173502542400358
Loss at iteration [643]: 0.22166150024321873
Loss at iteration [644]: 0.22166150024321873
Loss at iteration [645]: 0.2216211598453841
Loss at iteration [646]: 0.22159629512699044
Loss at iteration [647]: 0.22155975573670772
Loss at iteration [648]: 0.22155664192616756
Loss at iteration [649]: 0.22154503848125057
Loss at iteration [650]: 0.22152993156684064
Loss at iteration [651]: 0.22151486648848484
Loss at iteration [652]: 0.22150574615025873
Loss at iteration [653]: 0.22150574615025873
Loss at iteration [654]: 0.22149981250037487
Loss at iteration [655]: 0.22147353251589047
Loss at iteration [656]: 0.2214663384545143
Loss at iteration [657]: 0.22146486165658186
Loss at iteration [658]: 0.22145433123760716
Loss at iteration [659]: 0.22144721156587366
Loss at iteration [660]: 0.2214427641330521
Loss at iteration [661]: 0.22143876191291467
Loss at iteration [662]: 0.22143253412342098
Loss at iteration [663]: 0.2214300905695575
Loss at iteration [664]: 0.2214300905695575
Loss at iteration [665]: 0.2214235620530463
Loss at iteration [666]: 0.22142035568526885
Loss at iteration [667]: 0.2214159866319788
Loss at iteration [668]: 0.22141384558031002
Loss at iteration [669]: 0.22140897768968854
Loss at iteration [670]: 0.22140732783416087
Loss at iteration [671]: 0.22139340334651675
Loss at iteration [672]: 0.22117152893608602
Loss at iteration [673]: 0.22072349830774068
Loss at iteration [674]: 0.22048765252267613
Loss at iteration [675]: 0.22048765252267613
Loss at iteration [676]: 0.22022652793928701
Loss at iteration [677]: 0.21985903786610222
Loss at iteration [678]: 0.21969029957472888
Loss at iteration [679]: 0.2195868725385552
Loss at iteration [680]: 0.21948498323464877
Loss at iteration [681]: 0.2193342611141184
Loss at iteration [682]: 0.21888875963000676
