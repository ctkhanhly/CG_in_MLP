Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :HS
Total number of function evaluations  : 3028
Total number of iterations            : 580
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 12.3869309425354
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 50.16881020979991%
Percentage of parameters < 1e-7       : 50.16881020979991%
Percentage of parameters < 1e-6       : 50.16881020979991%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.012744570449700501
Loss at iteration [2]: 0.010686406333597548
Loss at iteration [3]: 0.010050362932716871
Loss at iteration [4]: 0.003480888724445672
Loss at iteration [5]: 0.003278557755217679
Loss at iteration [6]: 0.0029847500620806856
Loss at iteration [7]: 0.0029847500620806856
Loss at iteration [8]: 0.00296992020819717
Loss at iteration [9]: 0.002815922588557166
Loss at iteration [10]: 0.002799045588341221
Loss at iteration [11]: 0.002709716740515664
Loss at iteration [12]: 0.0026586562977918347
Loss at iteration [13]: 0.0026586562977918347
Loss at iteration [14]: 0.0026560827216025934
Loss at iteration [15]: 0.0026118535050674648
Loss at iteration [16]: 0.002605998624956922
Loss at iteration [17]: 0.002577167570761781
Loss at iteration [18]: 0.002573840055575091
Loss at iteration [19]: 0.002573840055575091
Loss at iteration [20]: 0.0025725552783961375
Loss at iteration [21]: 0.0025584224636755566
Loss at iteration [22]: 0.0025518329231579324
Loss at iteration [23]: 0.0025515351866178784
Loss at iteration [24]: 0.002550566884189257
Loss at iteration [25]: 0.0025501404580337535
Loss at iteration [26]: 0.0025501404580337535
Loss at iteration [27]: 0.0025476684250959163
Loss at iteration [28]: 0.0025445257459386647
Loss at iteration [29]: 0.0025424103410886653
Loss at iteration [30]: 0.0025358011723510304
Loss at iteration [31]: 0.002534238261311485
Loss at iteration [32]: 0.002534238261311485
Loss at iteration [33]: 0.0025325029740582267
Loss at iteration [34]: 0.0025313942936480006
Loss at iteration [35]: 0.00253132767198992
Loss at iteration [36]: 0.0025307073875105853
Loss at iteration [37]: 0.0025290142163289244
Loss at iteration [38]: 0.0025290142163289244
Loss at iteration [39]: 0.0025281195016408154
Loss at iteration [40]: 0.002526565775814617
Loss at iteration [41]: 0.0025261819664383067
Loss at iteration [42]: 0.0025254482419199784
Loss at iteration [43]: 0.002524637567449958
Loss at iteration [44]: 0.002524637567449958
Loss at iteration [45]: 0.002524382873273408
Loss at iteration [46]: 0.002524020636694575
Loss at iteration [47]: 0.002523109427792407
Loss at iteration [48]: 0.002521995215191869
Loss at iteration [49]: 0.0025191629651082057
Loss at iteration [50]: 0.0025191629651082057
Loss at iteration [51]: 0.002518434007619591
Loss at iteration [52]: 0.002515310978169034
Loss at iteration [53]: 0.002515238087065892
Loss at iteration [54]: 0.002514831443118805
Loss at iteration [55]: 0.002512853464927261
Loss at iteration [56]: 0.002512853464927261
Loss at iteration [57]: 0.002512590615604362
Loss at iteration [58]: 0.00251168299112108
Loss at iteration [59]: 0.0025114832620544525
Loss at iteration [60]: 0.0025061217737937917
Loss at iteration [61]: 0.0025050792091223027
Loss at iteration [62]: 0.0025050792091223027
Loss at iteration [63]: 0.002504451946561042
Loss at iteration [64]: 0.002503299000013
Loss at iteration [65]: 0.002502848069136787
Loss at iteration [66]: 0.0025023094262962907
Loss at iteration [67]: 0.0025020446411160692
Loss at iteration [68]: 0.0025020446411160692
Loss at iteration [69]: 0.002501822471062529
Loss at iteration [70]: 0.0025011973705382565
Loss at iteration [71]: 0.0025011388755919453
Loss at iteration [72]: 0.002501053075876843
Loss at iteration [73]: 0.002500067192750203
Loss at iteration [74]: 0.002500067192750203
Loss at iteration [75]: 0.002499811640298667
Loss at iteration [76]: 0.002499737232966926
Loss at iteration [77]: 0.002499338566604598
Loss at iteration [78]: 0.0024992350503560733
Loss at iteration [79]: 0.002499120908659287
Loss at iteration [80]: 0.0024980283116970586
Loss at iteration [81]: 0.0024980283116970586
Loss at iteration [82]: 0.0024975614527190386
Loss at iteration [83]: 0.0024973526536347527
Loss at iteration [84]: 0.0024967786195133805
Loss at iteration [85]: 0.0024967204038527425
Loss at iteration [86]: 0.0024963207586315415
Loss at iteration [87]: 0.0024961883993837625
Loss at iteration [88]: 0.0024961883993837625
Loss at iteration [89]: 0.0024961215140639884
Loss at iteration [90]: 0.0024959761542014324
Loss at iteration [91]: 0.0024956391639895706
Loss at iteration [92]: 0.002495506789168924
Loss at iteration [93]: 0.002493591751606704
Loss at iteration [94]: 0.002493591751606704
Loss at iteration [95]: 0.002492795685400354
Loss at iteration [96]: 0.002492725789042783
Loss at iteration [97]: 0.0024923078875009725
Loss at iteration [98]: 0.0024913742302299204
Loss at iteration [99]: 0.0024906901451001873
Loss at iteration [100]: 0.0024906901451001873
Loss at iteration [101]: 0.002490532472555816
Loss at iteration [102]: 0.0024903417496749537
Loss at iteration [103]: 0.002489879588914806
Loss at iteration [104]: 0.002489835493806392
Loss at iteration [105]: 0.0024893730941018674
Loss at iteration [106]: 0.0024893730941018674
Loss at iteration [107]: 0.002489028130240932
Loss at iteration [108]: 0.0024889200920807123
Loss at iteration [109]: 0.0024884837133497574
Loss at iteration [110]: 0.00248811603803533
Loss at iteration [111]: 0.002487044892654462
Loss at iteration [112]: 0.002487044892654462
Loss at iteration [113]: 0.0024868795465739163
Loss at iteration [114]: 0.0024863180069773856
Loss at iteration [115]: 0.0024861436586121753
Loss at iteration [116]: 0.0024860110210343594
Loss at iteration [117]: 0.0024859074965634205
Loss at iteration [118]: 0.0024857609204392496
Loss at iteration [119]: 0.0024857609204392496
Loss at iteration [120]: 0.002485658735332567
Loss at iteration [121]: 0.002485612128062331
Loss at iteration [122]: 0.0024853947428231443
Loss at iteration [123]: 0.00248532897562473
Loss at iteration [124]: 0.002485232408960048
Loss at iteration [125]: 0.0024848655400587405
Loss at iteration [126]: 0.0024848655400587405
Loss at iteration [127]: 0.002484567257715778
Loss at iteration [128]: 0.00248417262842272
Loss at iteration [129]: 0.0024840958004373826
Loss at iteration [130]: 0.002483915811630536
Loss at iteration [131]: 0.0024836342540815242
Loss at iteration [132]: 0.0024836342540815242
Loss at iteration [133]: 0.002483488808928736
Loss at iteration [134]: 0.0024829993964403706
Loss at iteration [135]: 0.002482841095380716
Loss at iteration [136]: 0.0024826705170006443
Loss at iteration [137]: 0.002481364346259923
Loss at iteration [138]: 0.002481364346259923
Loss at iteration [139]: 0.002481235754760342
Loss at iteration [140]: 0.0024809545873624747
Loss at iteration [141]: 0.0024804219362028744
Loss at iteration [142]: 0.0024802838635374384
Loss at iteration [143]: 0.002479907965786777
Loss at iteration [144]: 0.002479907965786777
Loss at iteration [145]: 0.0024797338815593538
Loss at iteration [146]: 0.002479620681601389
Loss at iteration [147]: 0.0024788086534435795
Loss at iteration [148]: 0.002478581654899728
Loss at iteration [149]: 0.0024783679828582024
Loss at iteration [150]: 0.0024783679828582024
Loss at iteration [151]: 0.00247832775355585
Loss at iteration [152]: 0.0024781026613894357
Loss at iteration [153]: 0.0024779036517317116
Loss at iteration [154]: 0.0024777275030922453
Loss at iteration [155]: 0.0024775978926583025
Loss at iteration [156]: 0.0024775978926583025
Loss at iteration [157]: 0.0024771927670182224
Loss at iteration [158]: 0.002477016264277567
Loss at iteration [159]: 0.0024769271154047767
Loss at iteration [160]: 0.002476601247673114
Loss at iteration [161]: 0.0024765654115782936
Loss at iteration [162]: 0.0024765654115782936
Loss at iteration [163]: 0.002476473164249558
Loss at iteration [164]: 0.0024761775963591955
Loss at iteration [165]: 0.002475985391153028
Loss at iteration [166]: 0.002475804232162254
Loss at iteration [167]: 0.0024755978900612716
Loss at iteration [168]: 0.0024755978900612716
Loss at iteration [169]: 0.0024755679028915937
Loss at iteration [170]: 0.0024754833439474652
Loss at iteration [171]: 0.0024752931351320227
Loss at iteration [172]: 0.002475195965964572
Loss at iteration [173]: 0.002475094881563477
Loss at iteration [174]: 0.002475094881563477
Loss at iteration [175]: 0.002475045808686721
Loss at iteration [176]: 0.0024748407901324246
Loss at iteration [177]: 0.0024747235115282736
Loss at iteration [178]: 0.002474679983314907
Loss at iteration [179]: 0.0024746287923835836
Loss at iteration [180]: 0.002474594043489062
Loss at iteration [181]: 0.002474594043489062
Loss at iteration [182]: 0.002474562045588057
Loss at iteration [183]: 0.0024744283442012243
Loss at iteration [184]: 0.0024743963040656986
Loss at iteration [185]: 0.0024741279621266223
Loss at iteration [186]: 0.0024735741409953917
Loss at iteration [187]: 0.0024735741409953917
Loss at iteration [188]: 0.0024735178194560405
Loss at iteration [189]: 0.0024734695501375287
Loss at iteration [190]: 0.0024732830522740303
Loss at iteration [191]: 0.0024732613391703086
Loss at iteration [192]: 0.002473213839140567
Loss at iteration [193]: 0.002473213839140567
Loss at iteration [194]: 0.002473191528793583
Loss at iteration [195]: 0.002473167396258324
Loss at iteration [196]: 0.0024718494397610673
Loss at iteration [197]: 0.0024701960499152738
Loss at iteration [198]: 0.0024692246303057476
Loss at iteration [199]: 0.0024692246303057476
Loss at iteration [200]: 0.0024686450852132976
Loss at iteration [201]: 0.002468586113278991
Loss at iteration [202]: 0.0024679087625888783
Loss at iteration [203]: 0.0024677921614331745
Loss at iteration [204]: 0.0024675401597165423
Loss at iteration [205]: 0.0024675401597165423
Loss at iteration [206]: 0.0024672807355027282
Loss at iteration [207]: 0.002467186014244548
Loss at iteration [208]: 0.002466844412798922
Loss at iteration [209]: 0.002466751356906482
Loss at iteration [210]: 0.0024666804691749348
Loss at iteration [211]: 0.002466045222889635
Loss at iteration [212]: 0.002466045222889635
Loss at iteration [213]: 0.0024655071575000642
Loss at iteration [214]: 0.0024653761607878228
Loss at iteration [215]: 0.0024650556381807646
Loss at iteration [216]: 0.0024648175000431725
Loss at iteration [217]: 0.002464453896875523
Loss at iteration [218]: 0.002464453896875523
Loss at iteration [219]: 0.0024643941232201615
Loss at iteration [220]: 0.002464171354799652
Loss at iteration [221]: 0.0024639440064244816
Loss at iteration [222]: 0.002463786948681536
Loss at iteration [223]: 0.0024635842742524415
Loss at iteration [224]: 0.0024635842742524415
Loss at iteration [225]: 0.0024635195892336965
Loss at iteration [226]: 0.0024634202417141426
Loss at iteration [227]: 0.0024631500612639187
Loss at iteration [228]: 0.0024630463183351963
Loss at iteration [229]: 0.0024629350572531834
Loss at iteration [230]: 0.002462853828746465
Loss at iteration [231]: 0.002462853828746465
Loss at iteration [232]: 0.002462791805634392
Loss at iteration [233]: 0.002462615450482081
Loss at iteration [234]: 0.0024625422344113227
Loss at iteration [235]: 0.0024624759644012056
Loss at iteration [236]: 0.0024620934711562033
Loss at iteration [237]: 0.0024620293798039918
Loss at iteration [238]: 0.0024620293798039918
Loss at iteration [239]: 0.0024619569505796753
Loss at iteration [240]: 0.0024617992445580767
Loss at iteration [241]: 0.002461735117635039
Loss at iteration [242]: 0.0024616975607393713
Loss at iteration [243]: 0.0024616790124050553
Loss at iteration [244]: 0.0024616790124050553
Loss at iteration [245]: 0.002461665599111378
Loss at iteration [246]: 0.002461583364418594
Loss at iteration [247]: 0.0024611099242492418
Loss at iteration [248]: 0.002460970148923381
Loss at iteration [249]: 0.002460566229405691
Loss at iteration [250]: 0.002460566229405691
Loss at iteration [251]: 0.0024603084473042453
Loss at iteration [252]: 0.0024601865209880423
Loss at iteration [253]: 0.002459546886769296
Loss at iteration [254]: 0.0024595024777474286
Loss at iteration [255]: 0.002459405215849939
Loss at iteration [256]: 0.002459405215849939
Loss at iteration [257]: 0.0024593582063433483
Loss at iteration [258]: 0.00245931912257328
Loss at iteration [259]: 0.002459138434988767
Loss at iteration [260]: 0.0024586109133292074
Loss at iteration [261]: 0.0024584398660297472
Loss at iteration [262]: 0.0024584398660297472
Loss at iteration [263]: 0.0024584044869493133
Loss at iteration [264]: 0.002458315170583805
Loss at iteration [265]: 0.0024579472810419088
Loss at iteration [266]: 0.002457940505829906
Loss at iteration [267]: 0.0024578255075478864
Loss at iteration [268]: 0.0024578255075478864
Loss at iteration [269]: 0.0024578028185662424
Loss at iteration [270]: 0.002457711329661221
Loss at iteration [271]: 0.0024575946951993035
Loss at iteration [272]: 0.002457521844267081
Loss at iteration [273]: 0.0024573498381996453
Loss at iteration [274]: 0.0024573498381996453
Loss at iteration [275]: 0.0024572774497695032
Loss at iteration [276]: 0.002457175979710387
Loss at iteration [277]: 0.0024570247176476227
Loss at iteration [278]: 0.002456961906569262
Loss at iteration [279]: 0.0024568522557248105
Loss at iteration [280]: 0.0024568522557248105
Loss at iteration [281]: 0.0024568024161314007
Loss at iteration [282]: 0.0024566984797337473
Loss at iteration [283]: 0.002456596301166166
Loss at iteration [284]: 0.0024565734524733995
Loss at iteration [285]: 0.0024563324209192165
Loss at iteration [286]: 0.0024563324209192165
Loss at iteration [287]: 0.002456267835385774
Loss at iteration [288]: 0.002456250781093964
Loss at iteration [289]: 0.002456176210928744
Loss at iteration [290]: 0.0024561140511608457
Loss at iteration [291]: 0.002456014863045456
Loss at iteration [292]: 0.002456014863045456
Loss at iteration [293]: 0.0024559694501908313
Loss at iteration [294]: 0.002455949396651178
Loss at iteration [295]: 0.002455884756105631
Loss at iteration [296]: 0.002455843464515967
Loss at iteration [297]: 0.002455732687826451
Loss at iteration [298]: 0.002455732687826451
Loss at iteration [299]: 0.0024556795247811657
Loss at iteration [300]: 0.0024556613504388026
Loss at iteration [301]: 0.0024555905294758374
Loss at iteration [302]: 0.002455525704585742
Loss at iteration [303]: 0.0024554870910362915
Loss at iteration [304]: 0.0024554127866004793
Loss at iteration [305]: 0.0024554127866004793
Loss at iteration [306]: 0.0024553360775950613
Loss at iteration [307]: 0.0024553099456910235
Loss at iteration [308]: 0.0024551422374863136
Loss at iteration [309]: 0.0024549605737261993
Loss at iteration [310]: 0.002454367568262822
Loss at iteration [311]: 0.002454367568262822
Loss at iteration [312]: 0.0024542136778382736
Loss at iteration [313]: 0.002454178421704028
Loss at iteration [314]: 0.0024536767357587206
Loss at iteration [315]: 0.0024535692493908526
Loss at iteration [316]: 0.002453506040474392
Loss at iteration [317]: 0.002453506040474392
Loss at iteration [318]: 0.002453469787489573
Loss at iteration [319]: 0.0024534425022402456
Loss at iteration [320]: 0.002453161545870309
Loss at iteration [321]: 0.002453138434448285
Loss at iteration [322]: 0.00245298779438813
Loss at iteration [323]: 0.00245298779438813
Loss at iteration [324]: 0.0024528836517816797
Loss at iteration [325]: 0.00245287242707693
Loss at iteration [326]: 0.002452643122614957
Loss at iteration [327]: 0.002452539035454766
Loss at iteration [328]: 0.0024525047910521737
Loss at iteration [329]: 0.0024525047910521737
Loss at iteration [330]: 0.0024524835451731754
Loss at iteration [331]: 0.002452452063113608
Loss at iteration [332]: 0.0024522144708035774
Loss at iteration [333]: 0.002452122039604085
Loss at iteration [334]: 0.002451774402512186
Loss at iteration [335]: 0.002451774402512186
Loss at iteration [336]: 0.0024514790692238874
Loss at iteration [337]: 0.002451429627866598
Loss at iteration [338]: 0.0024511395989903575
Loss at iteration [339]: 0.0024511223648303374
Loss at iteration [340]: 0.0024509731633487433
Loss at iteration [341]: 0.0024509731633487433
Loss at iteration [342]: 0.0024509572329587124
Loss at iteration [343]: 0.0024508065346233615
Loss at iteration [344]: 0.002450693254939271
Loss at iteration [345]: 0.002450663189005407
Loss at iteration [346]: 0.002450552465918213
Loss at iteration [347]: 0.002450552465918213
Loss at iteration [348]: 0.0024505045677567752
Loss at iteration [349]: 0.002450480822126955
Loss at iteration [350]: 0.002450076848553947
Loss at iteration [351]: 0.002450056304975264
Loss at iteration [352]: 0.0024499666160597553
Loss at iteration [353]: 0.0024499666160597553
Loss at iteration [354]: 0.002449952528060974
Loss at iteration [355]: 0.002449917611898635
Loss at iteration [356]: 0.002449774533338354
Loss at iteration [357]: 0.0024497096669601126
Loss at iteration [358]: 0.0024495741934251014
Loss at iteration [359]: 0.0024495741934251014
Loss at iteration [360]: 0.0024495245777236747
Loss at iteration [361]: 0.002449453480071462
Loss at iteration [362]: 0.0024493915717309902
Loss at iteration [363]: 0.002449362693068127
Loss at iteration [364]: 0.002449312198960853
Loss at iteration [365]: 0.002449312198960853
Loss at iteration [366]: 0.002449277042704455
Loss at iteration [367]: 0.0024492387465827087
Loss at iteration [368]: 0.002449175381785689
Loss at iteration [369]: 0.002449169168313119
Loss at iteration [370]: 0.002449157759353662
Loss at iteration [371]: 0.002449157759353662
Loss at iteration [372]: 0.0024491410548083143
Loss at iteration [373]: 0.002449080710423198
Loss at iteration [374]: 0.002448866148256211
Loss at iteration [375]: 0.0024486530249090877
Loss at iteration [376]: 0.0024482486844435063
Loss at iteration [377]: 0.0024482486844435063
Loss at iteration [378]: 0.002447821156437065
Loss at iteration [379]: 0.0024475618853918254
Loss at iteration [380]: 0.0024475553171293403
Loss at iteration [381]: 0.0024474960672074154
Loss at iteration [382]: 0.002447350203251449
Loss at iteration [383]: 0.002447350203251449
Loss at iteration [384]: 0.002447135361118659
Loss at iteration [385]: 0.002447013172387145
Loss at iteration [386]: 0.0024469749400305666
Loss at iteration [387]: 0.002446895576330388
Loss at iteration [388]: 0.0024466912563164307
Loss at iteration [389]: 0.0024466912563164307
Loss at iteration [390]: 0.0024466333719478568
Loss at iteration [391]: 0.002446604306603116
Loss at iteration [392]: 0.00244649563339679
Loss at iteration [393]: 0.0024464650191647276
Loss at iteration [394]: 0.0024464454935269643
Loss at iteration [395]: 0.0024463746916077046
Loss at iteration [396]: 0.0024463746916077046
Loss at iteration [397]: 0.0024463216137659053
Loss at iteration [398]: 0.0024462984497561732
Loss at iteration [399]: 0.0024461892037614765
Loss at iteration [400]: 0.0024461163078992607
Loss at iteration [401]: 0.002445936316862577
Loss at iteration [402]: 0.002445936316862577
Loss at iteration [403]: 0.002445807782978687
Loss at iteration [404]: 0.002445653346962215
Loss at iteration [405]: 0.0024454536555429066
Loss at iteration [406]: 0.002445416320409438
Loss at iteration [407]: 0.002445287105993971
Loss at iteration [408]: 0.002445287105993971
Loss at iteration [409]: 0.0024452672389456997
Loss at iteration [410]: 0.0024451555098071406
Loss at iteration [411]: 0.002445115395557402
Loss at iteration [412]: 0.002445001948818754
Loss at iteration [413]: 0.0024448998256948362
Loss at iteration [414]: 0.0024448998256948362
Loss at iteration [415]: 0.0024448610681192693
Loss at iteration [416]: 0.0024447279743810943
Loss at iteration [417]: 0.002444692465688791
Loss at iteration [418]: 0.0024445387394590686
Loss at iteration [419]: 0.002444514704339828
Loss at iteration [420]: 0.002444514704339828
Loss at iteration [421]: 0.002444502470920082
Loss at iteration [422]: 0.002444390424956331
Loss at iteration [423]: 0.002444342845505674
Loss at iteration [424]: 0.0024442814865020916
Loss at iteration [425]: 0.0024439886806736457
Loss at iteration [426]: 0.0024439886806736457
Loss at iteration [427]: 0.0024439258031244845
Loss at iteration [428]: 0.002443761965279234
Loss at iteration [429]: 0.002443699248800854
Loss at iteration [430]: 0.0024435889149807028
Loss at iteration [431]: 0.0024435228334719235
Loss at iteration [432]: 0.0024435228334719235
Loss at iteration [433]: 0.0024435037444527677
Loss at iteration [434]: 0.0024434328077993354
Loss at iteration [435]: 0.002443381450678832
Loss at iteration [436]: 0.0024432767145590043
Loss at iteration [437]: 0.00244320890936212
Loss at iteration [438]: 0.00244320890936212
Loss at iteration [439]: 0.002443197441490836
Loss at iteration [440]: 0.002443170299365027
Loss at iteration [441]: 0.002443034281206082
Loss at iteration [442]: 0.002442875731728188
Loss at iteration [443]: 0.0024426960685774704
Loss at iteration [444]: 0.0024426960685774704
Loss at iteration [445]: 0.0024425342420560784
Loss at iteration [446]: 0.002442351612863465
Loss at iteration [447]: 0.0024422569752608745
Loss at iteration [448]: 0.0024421315373927546
Loss at iteration [449]: 0.0024420294273602137
Loss at iteration [450]: 0.0024420294273602137
Loss at iteration [451]: 0.0024420046909231392
Loss at iteration [452]: 0.0024419521681847017
Loss at iteration [453]: 0.0024418870954269704
Loss at iteration [454]: 0.0024418759994040817
Loss at iteration [455]: 0.002441868400647039
Loss at iteration [456]: 0.002441868400647039
Loss at iteration [457]: 0.00244185636090673
Loss at iteration [458]: 0.0024417847230101004
Loss at iteration [459]: 0.0024416907545464607
Loss at iteration [460]: 0.0024416574688115827
Loss at iteration [461]: 0.002441564667298498
Loss at iteration [462]: 0.002441564667298498
Loss at iteration [463]: 0.0024415485420610336
Loss at iteration [464]: 0.002441518821247119
Loss at iteration [465]: 0.002441451455898687
Loss at iteration [466]: 0.002441403094756929
Loss at iteration [467]: 0.002441318169935133
Loss at iteration [468]: 0.002441318169935133
Loss at iteration [469]: 0.0024412716745493153
Loss at iteration [470]: 0.002441118918815378
Loss at iteration [471]: 0.002441106774968929
Loss at iteration [472]: 0.0024409616339465413
Loss at iteration [473]: 0.0024408736937912854
Loss at iteration [474]: 0.0024408736937912854
Loss at iteration [475]: 0.0024408575166557367
Loss at iteration [476]: 0.0024407630226212025
Loss at iteration [477]: 0.002440741953280311
Loss at iteration [478]: 0.002440691878211023
Loss at iteration [479]: 0.002440592340594136
Loss at iteration [480]: 0.002440592340594136
Loss at iteration [481]: 0.0024405751128331286
Loss at iteration [482]: 0.002440554235214048
Loss at iteration [483]: 0.0024404682228115217
Loss at iteration [484]: 0.002440445773499319
Loss at iteration [485]: 0.0024403705918323213
Loss at iteration [486]: 0.0024403705918323213
Loss at iteration [487]: 0.0024403140667283918
Loss at iteration [488]: 0.0024402674457242616
Loss at iteration [489]: 0.002440211881049357
Loss at iteration [490]: 0.0024401947954056226
Loss at iteration [491]: 0.0024401332701979783
Loss at iteration [492]: 0.0024401332701979783
Loss at iteration [493]: 0.002440100702026891
Loss at iteration [494]: 0.002440079953284745
Loss at iteration [495]: 0.0024399940872087736
Loss at iteration [496]: 0.002439975538274897
Loss at iteration [497]: 0.0024399111190095803
Loss at iteration [498]: 0.0024399111190095803
Loss at iteration [499]: 0.0024398898826181667
Loss at iteration [500]: 0.002439780855866725
Loss at iteration [501]: 0.0024397559120467854
Loss at iteration [502]: 0.0024397178277298834
Loss at iteration [503]: 0.002439585806489642
Loss at iteration [504]: 0.002439585806489642
Loss at iteration [505]: 0.002439542232545932
Loss at iteration [506]: 0.0024394892429031646
Loss at iteration [507]: 0.002439421199559489
Loss at iteration [508]: 0.0024393855737724476
Loss at iteration [509]: 0.002439347351373961
Loss at iteration [510]: 0.002439347351373961
Loss at iteration [511]: 0.0024393310091284496
Loss at iteration [512]: 0.0024392497146476367
Loss at iteration [513]: 0.0024392254861706885
Loss at iteration [514]: 0.0024391617849203228
Loss at iteration [515]: 0.0024391500330194485
Loss at iteration [516]: 0.0024391500330194485
Loss at iteration [517]: 0.0024391314192487154
Loss at iteration [518]: 0.0024390834554840336
Loss at iteration [519]: 0.0024390448249001083
Loss at iteration [520]: 0.002438880794356441
Loss at iteration [521]: 0.002438658354307246
Loss at iteration [522]: 0.002438658354307246
Loss at iteration [523]: 0.0024385529564155918
Loss at iteration [524]: 0.0024384946769343218
Loss at iteration [525]: 0.002438379329067144
Loss at iteration [526]: 0.0024383355863891393
Loss at iteration [527]: 0.0024383016543730544
Loss at iteration [528]: 0.0024382675749476113
Loss at iteration [529]: 0.0024382675749476113
Loss at iteration [530]: 0.0024382460313979045
Loss at iteration [531]: 0.002438179517789055
Loss at iteration [532]: 0.0024381570142940124
Loss at iteration [533]: 0.0024381336159254394
Loss at iteration [534]: 0.0024380379500001832
Loss at iteration [535]: 0.0024380379500001832
Loss at iteration [536]: 0.002438022933445148
Loss at iteration [537]: 0.002437977000331826
Loss at iteration [538]: 0.002437935841344598
Loss at iteration [539]: 0.002437917555883428
Loss at iteration [540]: 0.0024375265128432507
Loss at iteration [541]: 0.0024375265128432507
Loss at iteration [542]: 0.0024366037589962117
Loss at iteration [543]: 0.0024365366506319606
Loss at iteration [544]: 0.0024362469633913736
Loss at iteration [545]: 0.002436222484763133
Loss at iteration [546]: 0.002436147655814709
Loss at iteration [547]: 0.002436147655814709
Loss at iteration [548]: 0.0024361162316726974
Loss at iteration [549]: 0.002435965739576155
Loss at iteration [550]: 0.0024358320022140317
Loss at iteration [551]: 0.0024357877903480106
Loss at iteration [552]: 0.0024353335335789133
Loss at iteration [553]: 0.0024353335335789133
Loss at iteration [554]: 0.0024352452755213505
Loss at iteration [555]: 0.0024352102207049254
Loss at iteration [556]: 0.002435047386955598
Loss at iteration [557]: 0.002435020757775896
Loss at iteration [558]: 0.002434983320323132
Loss at iteration [559]: 0.0024348813685187076
Loss at iteration [560]: 0.0024348813685187076
Loss at iteration [561]: 0.0024348406029396676
Loss at iteration [562]: 0.002434826843691219
Loss at iteration [563]: 0.0024347416154697704
Loss at iteration [564]: 0.0024346456749462903
Loss at iteration [565]: 0.0024344743394060175
Loss at iteration [566]: 0.0024344743394060175
Loss at iteration [567]: 0.002434424405553786
Loss at iteration [568]: 0.0024342629982700746
Loss at iteration [569]: 0.0024342049147190953
Loss at iteration [570]: 0.0024341324925601093
Loss at iteration [571]: 0.002434097810628455
Loss at iteration [572]: 0.002434097810628455
Loss at iteration [573]: 0.0024340856604921917
Loss at iteration [574]: 0.002433999887596609
Loss at iteration [575]: 0.002433972631849495
Loss at iteration [576]: 0.002433948017201588
Loss at iteration [577]: 0.0024337766672440646
Loss at iteration [578]: 0.0024337766672440646
Loss at iteration [579]: 0.0024335836677860556
Loss at iteration [580]: 0.0024335592109300318
Loss at iteration [581]: 0.002433481370188829
Loss at iteration [582]: 0.002433463149019018
Loss at iteration [583]: 0.002433419050371649
Loss at iteration [584]: 0.002433419050371649
Loss at iteration [585]: 0.0024333856614182268
Loss at iteration [586]: 0.0024333750412099927
Loss at iteration [587]: 0.0024333420644101064
Loss at iteration [588]: 0.002433272693774832
Loss at iteration [589]: 0.0024331744326721526
Loss at iteration [590]: 0.0024331231276724747
Loss at iteration [591]: 0.0024331231276724747
Loss at iteration [592]: 0.0024330721874328124
Loss at iteration [593]: 0.002432938919836894
Loss at iteration [594]: 0.002432926206350211
Loss at iteration [595]: 0.002432851374379031
Loss at iteration [596]: 0.002432633732490701
Loss at iteration [597]: 0.002432633732490701
Loss at iteration [598]: 0.002432610917994037
Loss at iteration [599]: 0.002432557052920106
Loss at iteration [600]: 0.0024324413247875424
Loss at iteration [601]: 0.0024323922989959927
Loss at iteration [602]: 0.002432343506087508
Loss at iteration [603]: 0.002432343506087508
Loss at iteration [604]: 0.0024323241977119076
Loss at iteration [605]: 0.002432219627558742
Loss at iteration [606]: 0.0024321757604712432
Loss at iteration [607]: 0.002432153355298028
Loss at iteration [608]: 0.0024319856858332043
Loss at iteration [609]: 0.0024319856858332043
Loss at iteration [610]: 0.002431915740312086
Loss at iteration [611]: 0.0024318976162761986
Loss at iteration [612]: 0.0024318440446659114
Loss at iteration [613]: 0.0024318044988879445
Loss at iteration [614]: 0.002431760889336461
Loss at iteration [615]: 0.002431760889336461
Loss at iteration [616]: 0.002431736517210313
Loss at iteration [617]: 0.0024316990429850403
Loss at iteration [618]: 0.0024316660819060183
Loss at iteration [619]: 0.002431659384313949
Loss at iteration [620]: 0.0024316198703931485
Loss at iteration [621]: 0.0024315105810224923
Loss at iteration [622]: 0.0024315105810224923
Loss at iteration [623]: 0.0024314937740172447
Loss at iteration [624]: 0.002431388532494774
Loss at iteration [625]: 0.002431361713052667
Loss at iteration [626]: 0.002431324841502811
Loss at iteration [627]: 0.0024312217183984045
Loss at iteration [628]: 0.0024312217183984045
Loss at iteration [629]: 0.0024311644094589936
Loss at iteration [630]: 0.0024311145638212838
Loss at iteration [631]: 0.0024310777336378096
Loss at iteration [632]: 0.002431043858918099
Loss at iteration [633]: 0.0024310155383786627
Loss at iteration [634]: 0.0024310155383786627
Loss at iteration [635]: 0.0024309995872872797
Loss at iteration [636]: 0.0024309613687712893
Loss at iteration [637]: 0.0024309184754608857
Loss at iteration [638]: 0.0024309102999777172
Loss at iteration [639]: 0.002430891604774457
Loss at iteration [640]: 0.002430891604774457
Loss at iteration [641]: 0.002430881301758066
Loss at iteration [642]: 0.002430866633992467
Loss at iteration [643]: 0.0024307877016651763
Loss at iteration [644]: 0.00243070571077688
Loss at iteration [645]: 0.002430186717524384
Loss at iteration [646]: 0.002430186717524384
Loss at iteration [647]: 0.002430148206944468
Loss at iteration [648]: 0.002430105202563166
Loss at iteration [649]: 0.002429941655305745
Loss at iteration [650]: 0.0024299162959129515
Loss at iteration [651]: 0.002429822782670642
Loss at iteration [652]: 0.002429822782670642
Loss at iteration [653]: 0.002429775743012976
Loss at iteration [654]: 0.002429645622676712
Loss at iteration [655]: 0.0024296166244655143
Loss at iteration [656]: 0.0024295780521514324
Loss at iteration [657]: 0.0024295521737356576
Loss at iteration [658]: 0.0024295521737356576
Loss at iteration [659]: 0.0024295430006478517
Loss at iteration [660]: 0.0024294977170705163
Loss at iteration [661]: 0.0024294812560939323
Loss at iteration [662]: 0.002429418891700688
Loss at iteration [663]: 0.0024292264032237205
Loss at iteration [664]: 0.0024292264032237205
Loss at iteration [665]: 0.002429185896473303
Loss at iteration [666]: 0.002429041898245697
Loss at iteration [667]: 0.002429020685119161
Loss at iteration [668]: 0.0024289889347559592
Loss at iteration [669]: 0.002428860415234802
Loss at iteration [670]: 0.002428860415234802
Loss at iteration [671]: 0.0024288476408368515
Loss at iteration [672]: 0.002428814061589823
Loss at iteration [673]: 0.0024287552623864384
Loss at iteration [674]: 0.0024287321290732094
Loss at iteration [675]: 0.0024286695385975035
Loss at iteration [676]: 0.0024286695385975035
Loss at iteration [677]: 0.002428604189711521
Loss at iteration [678]: 0.0024285914225141846
Loss at iteration [679]: 0.002428544878618546
Loss at iteration [680]: 0.0024284741979342128
Loss at iteration [681]: 0.002428434513953154
Loss at iteration [682]: 0.002428434513953154
Loss at iteration [683]: 0.002428418088576602
Loss at iteration [684]: 0.00242837252361938
Loss at iteration [685]: 0.002428314167653646
Loss at iteration [686]: 0.00242829378372458
Loss at iteration [687]: 0.002428241858485084
