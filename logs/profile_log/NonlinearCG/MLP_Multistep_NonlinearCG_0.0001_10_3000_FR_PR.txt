Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :FR_PR
Total number of function evaluations  : 1238
Total number of iterations            : 282
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 2.0709493160247803
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 50.340380320901915%
Percentage of parameters < 1e-7       : 50.34087219997836%
Percentage of parameters < 1e-6       : 50.341855958131255%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.7254984589059643
Loss at iteration [2]: 0.7068687707802307
Loss at iteration [3]: 0.6488600877289535
Loss at iteration [4]: 0.5794002558241333
Loss at iteration [5]: 0.38230805818871894
Loss at iteration [6]: 0.37477741101796647
Loss at iteration [7]: 0.37477741101796647
Loss at iteration [8]: 0.3691245368503533
Loss at iteration [9]: 0.3482746216991526
Loss at iteration [10]: 0.3442428277645404
Loss at iteration [11]: 0.31310550066381726
Loss at iteration [12]: 0.3054483742108009
Loss at iteration [13]: 0.28598770850957145
Loss at iteration [14]: 0.28598770850957145
Loss at iteration [15]: 0.28274919932086257
Loss at iteration [16]: 0.270436310486875
Loss at iteration [17]: 0.2687655194744605
Loss at iteration [18]: 0.2618702284248534
Loss at iteration [19]: 0.26059846099189754
Loss at iteration [20]: 0.2551735446109874
Loss at iteration [21]: 0.2551735446109874
Loss at iteration [22]: 0.25238145004257606
Loss at iteration [23]: 0.24798346597221926
Loss at iteration [24]: 0.24706371870726043
Loss at iteration [25]: 0.24418964346479305
Loss at iteration [26]: 0.2435500668713116
Loss at iteration [27]: 0.23875532167269486
Loss at iteration [28]: 0.23875532167269486
Loss at iteration [29]: 0.23661662913152126
Loss at iteration [30]: 0.23321526191145212
Loss at iteration [31]: 0.23269400551136313
Loss at iteration [32]: 0.22284777506855252
Loss at iteration [33]: 0.22175594411379507
Loss at iteration [34]: 0.21790759609341323
Loss at iteration [35]: 0.21790759609341323
Loss at iteration [36]: 0.21756082034494606
Loss at iteration [37]: 0.21684221160674225
Loss at iteration [38]: 0.2162958356239812
Loss at iteration [39]: 0.21623391513242946
Loss at iteration [40]: 0.21570845348549939
Loss at iteration [41]: 0.21569434399196433
Loss at iteration [42]: 0.21569434399196433
Loss at iteration [43]: 0.21568231113577988
Loss at iteration [44]: 0.21552593577664145
Loss at iteration [45]: 0.2154838917858666
Loss at iteration [46]: 0.21542151799167492
Loss at iteration [47]: 0.21541430375575196
Loss at iteration [48]: 0.21533355164975498
Loss at iteration [49]: 0.21533355164975498
Loss at iteration [50]: 0.2153214997548617
Loss at iteration [51]: 0.21529137665290093
Loss at iteration [52]: 0.21528015872869763
Loss at iteration [53]: 0.21525631746050888
Loss at iteration [54]: 0.2152474215735674
Loss at iteration [55]: 0.2152276425576369
Loss at iteration [56]: 0.2152276425576369
Loss at iteration [57]: 0.21521473975390953
Loss at iteration [58]: 0.21519665628371185
Loss at iteration [59]: 0.21518080356255279
Loss at iteration [60]: 0.21517494684628222
Loss at iteration [61]: 0.21514167174219648
Loss at iteration [62]: 0.21512573892062778
Loss at iteration [63]: 0.21512573892062778
Loss at iteration [64]: 0.2151126776367182
Loss at iteration [65]: 0.21510232000970428
Loss at iteration [66]: 0.21504327632505893
Loss at iteration [67]: 0.21499103362330077
Loss at iteration [68]: 0.2149710461020206
Loss at iteration [69]: 0.21493479665511478
Loss at iteration [70]: 0.21493479665511478
Loss at iteration [71]: 0.2149263857399206
Loss at iteration [72]: 0.21490066827929347
Loss at iteration [73]: 0.21489999864940834
Loss at iteration [74]: 0.2148958478418443
Loss at iteration [75]: 0.21489107384457734
Loss at iteration [76]: 0.21488958423071441
Loss at iteration [77]: 0.21488677298289902
Loss at iteration [78]: 0.21488677298289902
Loss at iteration [79]: 0.21488536000961528
Loss at iteration [80]: 0.21487997262203604
Loss at iteration [81]: 0.2148687697125531
Loss at iteration [82]: 0.21486459803569946
Loss at iteration [83]: 0.21484457502630142
Loss at iteration [84]: 0.21484262383528077
Loss at iteration [85]: 0.21484262383528077
Loss at iteration [86]: 0.2148412037775531
Loss at iteration [87]: 0.21483936819424485
Loss at iteration [88]: 0.21483791828408527
Loss at iteration [89]: 0.21483559769881957
Loss at iteration [90]: 0.21483240064756384
Loss at iteration [91]: 0.21482915455023782
Loss at iteration [92]: 0.21482915455023782
Loss at iteration [93]: 0.21482886060185447
Loss at iteration [94]: 0.21482730648742007
Loss at iteration [95]: 0.21482698364736239
Loss at iteration [96]: 0.21482250744058923
Loss at iteration [97]: 0.21481724691662427
Loss at iteration [98]: 0.21481084123459637
Loss at iteration [99]: 0.21480452163836786
Loss at iteration [100]: 0.21480452163836786
Loss at iteration [101]: 0.21480417802523707
Loss at iteration [102]: 0.21480236153987714
Loss at iteration [103]: 0.21478744256184942
Loss at iteration [104]: 0.21477978000041728
Loss at iteration [105]: 0.2147361721704471
Loss at iteration [106]: 0.2147361721704471
Loss at iteration [107]: 0.21473381158629193
Loss at iteration [108]: 0.21472571542249302
Loss at iteration [109]: 0.2147234882063377
Loss at iteration [110]: 0.2147217631382923
Loss at iteration [111]: 0.2147214481845873
Loss at iteration [112]: 0.21472109063950573
Loss at iteration [113]: 0.21472071037189394
Loss at iteration [114]: 0.21472071037189394
Loss at iteration [115]: 0.21472057172006734
Loss at iteration [116]: 0.21471889426229102
Loss at iteration [117]: 0.21471743811222668
Loss at iteration [118]: 0.21471719081844665
Loss at iteration [119]: 0.21471620407172715
Loss at iteration [120]: 0.21471588904047853
Loss at iteration [121]: 0.21470879589308733
Loss at iteration [122]: 0.21470879589308733
Loss at iteration [123]: 0.21470812308613138
Loss at iteration [124]: 0.2147059626799283
Loss at iteration [125]: 0.21470583297713589
Loss at iteration [126]: 0.21470238221411855
Loss at iteration [127]: 0.21470145912280708
Loss at iteration [128]: 0.21469263447720205
Loss at iteration [129]: 0.2146922070779874
Loss at iteration [130]: 0.2146922070779874
Loss at iteration [131]: 0.21469192693275377
Loss at iteration [132]: 0.21469103370828826
Loss at iteration [133]: 0.21469056001160616
Loss at iteration [134]: 0.21468894563668647
Loss at iteration [135]: 0.21468335145105666
Loss at iteration [136]: 0.214682856408086
Loss at iteration [137]: 0.214682856408086
Loss at iteration [138]: 0.2146824631551412
Loss at iteration [139]: 0.2146801893948584
Loss at iteration [140]: 0.21467994339043153
Loss at iteration [141]: 0.2146782558483554
Loss at iteration [142]: 0.2146777127960869
Loss at iteration [143]: 0.2146767014344915
Loss at iteration [144]: 0.21467640721850892
Loss at iteration [145]: 0.21467640721850892
Loss at iteration [146]: 0.21467620702603726
Loss at iteration [147]: 0.21467593474317992
Loss at iteration [148]: 0.21467567124540074
Loss at iteration [149]: 0.21467532311061058
Loss at iteration [150]: 0.2146736826006438
Loss at iteration [151]: 0.214672261059254
Loss at iteration [152]: 0.214672261059254
Loss at iteration [153]: 0.2146715070797668
Loss at iteration [154]: 0.214670401581971
Loss at iteration [155]: 0.21466823319685321
Loss at iteration [156]: 0.21465864253689615
Loss at iteration [157]: 0.21465830197564834
Loss at iteration [158]: 0.21465379620364225
Loss at iteration [159]: 0.21465379620364225
Loss at iteration [160]: 0.21465258088330236
Loss at iteration [161]: 0.21465060856083168
Loss at iteration [162]: 0.21464848232371656
Loss at iteration [163]: 0.2146466604746875
Loss at iteration [164]: 0.21464525190532543
Loss at iteration [165]: 0.21464421611265375
Loss at iteration [166]: 0.21464421611265375
Loss at iteration [167]: 0.21464388485295804
Loss at iteration [168]: 0.21464296727339932
Loss at iteration [169]: 0.21464137545484557
Loss at iteration [170]: 0.21464097566618429
Loss at iteration [171]: 0.2146396576279442
Loss at iteration [172]: 0.21463816074893574
Loss at iteration [173]: 0.21463816074893574
Loss at iteration [174]: 0.21463709774283263
Loss at iteration [175]: 0.2146354123489838
Loss at iteration [176]: 0.21463460211492474
Loss at iteration [177]: 0.21463446016007745
Loss at iteration [178]: 0.21463417076581948
Loss at iteration [179]: 0.21463398424724603
Loss at iteration [180]: 0.21463398424724603
Loss at iteration [181]: 0.21463395586707976
Loss at iteration [182]: 0.21463357246995415
Loss at iteration [183]: 0.21463304306865977
Loss at iteration [184]: 0.21463269450763395
Loss at iteration [185]: 0.21463237329060053
Loss at iteration [186]: 0.21463235220486274
Loss at iteration [187]: 0.21463189382087952
Loss at iteration [188]: 0.21463189382087952
Loss at iteration [189]: 0.2146318237793607
Loss at iteration [190]: 0.21463148909625243
Loss at iteration [191]: 0.21463102385973717
Loss at iteration [192]: 0.21462851089985008
Loss at iteration [193]: 0.21462430944900365
Loss at iteration [194]: 0.2146172395001883
Loss at iteration [195]: 0.2146172395001883
Loss at iteration [196]: 0.2146147981058868
Loss at iteration [197]: 0.21461115108745438
Loss at iteration [198]: 0.2146107906450573
Loss at iteration [199]: 0.2146107424464665
Loss at iteration [200]: 0.21460920843290382
Loss at iteration [201]: 0.21460723981534519
Loss at iteration [202]: 0.21460723981534519
Loss at iteration [203]: 0.21460711482473308
Loss at iteration [204]: 0.21460642445932812
Loss at iteration [205]: 0.21460631440854092
Loss at iteration [206]: 0.21460577475455156
Loss at iteration [207]: 0.21460569451516187
Loss at iteration [208]: 0.21460500092024015
Loss at iteration [209]: 0.21460497459420025
Loss at iteration [210]: 0.21460497459420025
Loss at iteration [211]: 0.2146048993390181
Loss at iteration [212]: 0.21460452758194512
Loss at iteration [213]: 0.21460410180696213
Loss at iteration [214]: 0.21460240644137255
Loss at iteration [215]: 0.21459180305510156
Loss at iteration [216]: 0.21459090425947455
Loss at iteration [217]: 0.21459090425947455
Loss at iteration [218]: 0.21459041091372572
Loss at iteration [219]: 0.21458824323739553
Loss at iteration [220]: 0.21458314225968375
Loss at iteration [221]: 0.21458260988769284
Loss at iteration [222]: 0.21458257431672664
Loss at iteration [223]: 0.2145822693071081
Loss at iteration [224]: 0.2145822693071081
Loss at iteration [225]: 0.21458218534346019
Loss at iteration [226]: 0.21458201580635888
Loss at iteration [227]: 0.2145815684248046
Loss at iteration [228]: 0.2145813987611758
Loss at iteration [229]: 0.2145813717860442
Loss at iteration [230]: 0.21458123297124745
Loss at iteration [231]: 0.21458123297124745
Loss at iteration [232]: 0.21458112605293658
Loss at iteration [233]: 0.2145810088656109
Loss at iteration [234]: 0.21458074652373027
Loss at iteration [235]: 0.2145803146083926
Loss at iteration [236]: 0.21458029637265139
Loss at iteration [237]: 0.21457820272409017
Loss at iteration [238]: 0.21457820272409017
Loss at iteration [239]: 0.21457713034023376
Loss at iteration [240]: 0.21457690102543803
Loss at iteration [241]: 0.21457672863501911
Loss at iteration [242]: 0.21457665931868208
Loss at iteration [243]: 0.21457660554563987
Loss at iteration [244]: 0.2145766030630566
Loss at iteration [245]: 0.2145766030630566
Loss at iteration [246]: 0.2145765427261363
Loss at iteration [247]: 0.21457649815259952
Loss at iteration [248]: 0.21457648528739842
Loss at iteration [249]: 0.21457647758237913
Loss at iteration [250]: 0.21457640984963863
Loss at iteration [251]: 0.21457638802580167
Loss at iteration [252]: 0.21457623089331465
Loss at iteration [253]: 0.21457623089331465
Loss at iteration [254]: 0.21457617065794513
Loss at iteration [255]: 0.2145761655822774
Loss at iteration [256]: 0.21457533677789567
Loss at iteration [257]: 0.21457389141281405
Loss at iteration [258]: 0.2145735736391851
Loss at iteration [259]: 0.21457182161912813
Loss at iteration [260]: 0.21457182161912813
Loss at iteration [261]: 0.21457128578691592
Loss at iteration [262]: 0.21457112566525624
Loss at iteration [263]: 0.21457098548169323
Loss at iteration [264]: 0.21457095955558514
Loss at iteration [265]: 0.2145709366848892
Loss at iteration [266]: 0.21457084246478178
Loss at iteration [267]: 0.21457084246478178
Loss at iteration [268]: 0.21457076362316815
Loss at iteration [269]: 0.2145707246474979
Loss at iteration [270]: 0.21457065522593632
Loss at iteration [271]: 0.21457050567325497
Loss at iteration [272]: 0.21457045804456634
Loss at iteration [273]: 0.21456839291375102
Loss at iteration [274]: 0.21456839291375102
Loss at iteration [275]: 0.21456834612648099
Loss at iteration [276]: 0.2145680126424326
Loss at iteration [277]: 0.21456763151110317
Loss at iteration [278]: 0.21456755296510535
Loss at iteration [279]: 0.21456754502207914
Loss at iteration [280]: 0.21456745071091646
Loss at iteration [281]: 0.21456745071091646
Loss at iteration [282]: 0.21456744851869994
Loss at iteration [283]: 0.21456738091536318
Loss at iteration [284]: 0.21456734860096013
Loss at iteration [285]: 0.21456733824022672
Loss at iteration [286]: 0.21456728077487136
Loss at iteration [287]: 0.21456718648209966
Loss at iteration [288]: 0.21456718648209966
Loss at iteration [289]: 0.2145671480815827
Loss at iteration [290]: 0.21456712299399702
Loss at iteration [291]: 0.21456709514247116
Loss at iteration [292]: 0.21456705237769422
Loss at iteration [293]: 0.21456704942939567
Loss at iteration [294]: 0.2145667336886839
Loss at iteration [295]: 0.2145667336886839
Loss at iteration [296]: 0.2145667145202796
Loss at iteration [297]: 0.21456657830508774
Loss at iteration [298]: 0.21456648030558742
Loss at iteration [299]: 0.21456644435102476
Loss at iteration [300]: 0.21456644237489297
Loss at iteration [301]: 0.21456639532770233
Loss at iteration [302]: 0.21456639532770233
Loss at iteration [303]: 0.21456638941390557
Loss at iteration [304]: 0.21456633695390104
Loss at iteration [305]: 0.21456630833405504
Loss at iteration [306]: 0.21456629659449655
Loss at iteration [307]: 0.21456624953753292
Loss at iteration [308]: 0.2145662436587836
Loss at iteration [309]: 0.2145662436587836
Loss at iteration [310]: 0.21456623519155285
Loss at iteration [311]: 0.2145662201704546
Loss at iteration [312]: 0.21456616939491438
Loss at iteration [313]: 0.21456613456990792
Loss at iteration [314]: 0.214566132235771
Loss at iteration [315]: 0.21456597373977043
Loss at iteration [316]: 0.21456597373977043
Loss at iteration [317]: 0.21456587914909755
Loss at iteration [318]: 0.2145657839768883
Loss at iteration [319]: 0.21456577968059237
Loss at iteration [320]: 0.21456577215602363
Loss at iteration [321]: 0.2145657497696055
Loss at iteration [322]: 0.2145657329560067
Loss at iteration [323]: 0.21456572945491584
