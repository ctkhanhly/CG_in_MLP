Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :HS
Total number of function evaluations  : 978
Total number of iterations            : 326
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 4.158234357833862
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.8945554994505%
Percentage of parameters < 1e-7       : 49.8945554994505%
Percentage of parameters < 1e-6       : 49.895545588657534%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.004988072340354993
Loss at iteration [2]: 0.0042940739912413605
Loss at iteration [3]: 0.004166035371573358
Loss at iteration [4]: 0.002961791346208719
Loss at iteration [5]: 0.0029277617270214284
Loss at iteration [6]: 0.0027206201285706125
Loss at iteration [7]: 0.002678401577526595
Loss at iteration [8]: 0.0026734124297503117
Loss at iteration [9]: 0.0026667743733471247
Loss at iteration [10]: 0.0026486387507851884
Loss at iteration [11]: 0.0026486387507851884
Loss at iteration [12]: 0.0026455171689077125
Loss at iteration [13]: 0.002634741377669276
Loss at iteration [14]: 0.002615964560226174
Loss at iteration [15]: 0.0026126352542074236
Loss at iteration [16]: 0.0026083289604232505
Loss at iteration [17]: 0.0026025263869178606
Loss at iteration [18]: 0.0025950262849210176
Loss at iteration [19]: 0.0025856639687633146
Loss at iteration [20]: 0.0025808389170592656
Loss at iteration [21]: 0.0025808389170592656
Loss at iteration [22]: 0.0025785822386201157
Loss at iteration [23]: 0.00257786989919408
Loss at iteration [24]: 0.0025694630439537986
Loss at iteration [25]: 0.0025629993461144134
Loss at iteration [26]: 0.0025618860096852675
Loss at iteration [27]: 0.0025577121561231567
Loss at iteration [28]: 0.0025537199172441577
Loss at iteration [29]: 0.0025528788698471186
Loss at iteration [30]: 0.0025497885013774
Loss at iteration [31]: 0.0025481559592908337
Loss at iteration [32]: 0.0025481559592908337
Loss at iteration [33]: 0.002546148641461743
Loss at iteration [34]: 0.0025446090261747696
Loss at iteration [35]: 0.0025440790524411763
Loss at iteration [36]: 0.0025367653857595526
Loss at iteration [37]: 0.0025364406710398986
Loss at iteration [38]: 0.002534104736977147
Loss at iteration [39]: 0.0025265136625018925
Loss at iteration [40]: 0.002523756754405104
Loss at iteration [41]: 0.002523756754405104
Loss at iteration [42]: 0.002522066927167434
Loss at iteration [43]: 0.0025170917681823285
Loss at iteration [44]: 0.002516561990578284
Loss at iteration [45]: 0.0025159611422855056
Loss at iteration [46]: 0.0025153204429851895
Loss at iteration [47]: 0.0025146212686067764
Loss at iteration [48]: 0.002514107769935963
Loss at iteration [49]: 0.002512073536273261
Loss at iteration [50]: 0.0025064342240728415
Loss at iteration [51]: 0.0025064342240728415
Loss at iteration [52]: 0.0025062701882409095
Loss at iteration [53]: 0.002505309398220196
Loss at iteration [54]: 0.0025027610075716565
Loss at iteration [55]: 0.00250255205491594
Loss at iteration [56]: 0.0025018052035657417
Loss at iteration [57]: 0.002501636590783848
Loss at iteration [58]: 0.0025000272767043154
Loss at iteration [59]: 0.002497782539870272
Loss at iteration [60]: 0.002497782539870272
Loss at iteration [61]: 0.0024973126213888985
Loss at iteration [62]: 0.0024957104128996866
Loss at iteration [63]: 0.0024952407177323763
Loss at iteration [64]: 0.002495075737860923
Loss at iteration [65]: 0.00249481876679268
Loss at iteration [66]: 0.002494675403577338
Loss at iteration [67]: 0.002494031113829374
Loss at iteration [68]: 0.0024933451399988768
Loss at iteration [69]: 0.0024927254291009534
Loss at iteration [70]: 0.0024927254291009534
Loss at iteration [71]: 0.002492592015816427
Loss at iteration [72]: 0.0024918670782930184
Loss at iteration [73]: 0.0024917011115784503
Loss at iteration [74]: 0.0024913026861139186
Loss at iteration [75]: 0.002491241785980343
Loss at iteration [76]: 0.002491165430220628
Loss at iteration [77]: 0.002490893751398053
Loss at iteration [78]: 0.0024904184162901502
Loss at iteration [79]: 0.0024904184162901502
Loss at iteration [80]: 0.002490244743948123
Loss at iteration [81]: 0.002489742931752742
Loss at iteration [82]: 0.0024896303762696787
Loss at iteration [83]: 0.0024895640123345658
Loss at iteration [84]: 0.0024893320985578627
Loss at iteration [85]: 0.0024892600078135
Loss at iteration [86]: 0.002489101029287255
Loss at iteration [87]: 0.0024890245109112745
Loss at iteration [88]: 0.0024889325895552507
Loss at iteration [89]: 0.0024889325895552507
Loss at iteration [90]: 0.002488856246520778
Loss at iteration [91]: 0.0024888058706474276
Loss at iteration [92]: 0.002488478633921665
Loss at iteration [93]: 0.002488412278020007
Loss at iteration [94]: 0.002488196982206183
Loss at iteration [95]: 0.0024878464537711653
Loss at iteration [96]: 0.0024875453422226732
Loss at iteration [97]: 0.0024875011204356853
Loss at iteration [98]: 0.00248697657256495
Loss at iteration [99]: 0.00248697657256495
Loss at iteration [100]: 0.002486832466696715
Loss at iteration [101]: 0.0024867908568287547
Loss at iteration [102]: 0.0024858410851898542
Loss at iteration [103]: 0.002485591485665764
Loss at iteration [104]: 0.002485396859271415
Loss at iteration [105]: 0.0024852560341419613
Loss at iteration [106]: 0.0024850289479862774
Loss at iteration [107]: 0.0024845108949794006
Loss at iteration [108]: 0.0024843936872085653
Loss at iteration [109]: 0.0024843936872085653
Loss at iteration [110]: 0.0024842792276674477
Loss at iteration [111]: 0.002484223244314714
Loss at iteration [112]: 0.002483937405879198
Loss at iteration [113]: 0.0024838283098057717
Loss at iteration [114]: 0.002483611265648509
Loss at iteration [115]: 0.002483256787777188
Loss at iteration [116]: 0.00248226838402046
Loss at iteration [117]: 0.0024797825457170282
Loss at iteration [118]: 0.0024763661068852054
Loss at iteration [119]: 0.0024763661068852054
Loss at iteration [120]: 0.0024753385264846845
Loss at iteration [121]: 0.0024743319809465854
Loss at iteration [122]: 0.002471680023102427
Loss at iteration [123]: 0.002471639192896981
Loss at iteration [124]: 0.0024713051741897907
Loss at iteration [125]: 0.0024707741398353722
Loss at iteration [126]: 0.0024705919047517916
Loss at iteration [127]: 0.0024702834538774363
Loss at iteration [128]: 0.002470149517803482
Loss at iteration [129]: 0.002470149517803482
Loss at iteration [130]: 0.002470044267521379
Loss at iteration [131]: 0.0024697392060836413
Loss at iteration [132]: 0.0024695798850611055
Loss at iteration [133]: 0.0024693902253121523
Loss at iteration [134]: 0.002469288001750349
Loss at iteration [135]: 0.002468318710633673
Loss at iteration [136]: 0.00246796776423079
Loss at iteration [137]: 0.0024666134580068034
Loss at iteration [138]: 0.0024666134580068034
Loss at iteration [139]: 0.0024659978309315633
Loss at iteration [140]: 0.002465867835842653
Loss at iteration [141]: 0.0024648616443863633
Loss at iteration [142]: 0.0024648230120892967
Loss at iteration [143]: 0.002464690124590946
Loss at iteration [144]: 0.002464302471204485
Loss at iteration [145]: 0.0024642013280128153
Loss at iteration [146]: 0.0024641171372037224
Loss at iteration [147]: 0.0024641171372037224
Loss at iteration [148]: 0.0024640165379808342
Loss at iteration [149]: 0.0024639198941377454
Loss at iteration [150]: 0.0024636149787842974
Loss at iteration [151]: 0.0024635517096223396
Loss at iteration [152]: 0.002463393932651109
Loss at iteration [153]: 0.0024629953568305143
Loss at iteration [154]: 0.002462496820723399
Loss at iteration [155]: 0.0024617310036671174
Loss at iteration [156]: 0.0024617310036671174
Loss at iteration [157]: 0.0024612482379436826
Loss at iteration [158]: 0.002459961661179916
Loss at iteration [159]: 0.002459802694770878
Loss at iteration [160]: 0.0024595971764605406
Loss at iteration [161]: 0.0024594687549334866
Loss at iteration [162]: 0.0024592935273195574
Loss at iteration [163]: 0.0024589112179306486
Loss at iteration [164]: 0.0024589112179306486
Loss at iteration [165]: 0.0024587950250334724
Loss at iteration [166]: 0.0024580846457680538
Loss at iteration [167]: 0.002457975086726364
Loss at iteration [168]: 0.0024578349609050286
Loss at iteration [169]: 0.0024578062038935833
Loss at iteration [170]: 0.0024577109959922727
Loss at iteration [171]: 0.0024576692929022636
Loss at iteration [172]: 0.0024573879141840224
Loss at iteration [173]: 0.0024572447517423007
Loss at iteration [174]: 0.0024571450868291268
Loss at iteration [175]: 0.0024571450868291268
Loss at iteration [176]: 0.0024571151021955595
Loss at iteration [177]: 0.0024570141286150075
Loss at iteration [178]: 0.002456885505055562
Loss at iteration [179]: 0.0024568152798200827
Loss at iteration [180]: 0.002455975307256578
Loss at iteration [181]: 0.0024554266175556196
Loss at iteration [182]: 0.002455083436489964
Loss at iteration [183]: 0.0024547969132454133
Loss at iteration [184]: 0.0024546874789483347
Loss at iteration [185]: 0.0024546874789483347
Loss at iteration [186]: 0.0024546474031429916
Loss at iteration [187]: 0.002454581016465772
Loss at iteration [188]: 0.0024535080276166807
Loss at iteration [189]: 0.0024532290067600542
Loss at iteration [190]: 0.0024531310587404724
Loss at iteration [191]: 0.0024530245989618556
Loss at iteration [192]: 0.002452960512056791
Loss at iteration [193]: 0.0024529309150114046
Loss at iteration [194]: 0.002452640790097821
Loss at iteration [195]: 0.002452640790097821
Loss at iteration [196]: 0.0024525484461646097
Loss at iteration [197]: 0.0024525180621270985
Loss at iteration [198]: 0.0024523889669179977
Loss at iteration [199]: 0.0024523325218858507
Loss at iteration [200]: 0.0024520294011217266
Loss at iteration [201]: 0.002451640760423965
Loss at iteration [202]: 0.002451576118387667
Loss at iteration [203]: 0.002451007685272871
Loss at iteration [204]: 0.002451007685272871
Loss at iteration [205]: 0.0024506707216754163
Loss at iteration [206]: 0.0024506243024255674
Loss at iteration [207]: 0.0024503292636374307
Loss at iteration [208]: 0.0024503158712886903
Loss at iteration [209]: 0.0024502887640088233
Loss at iteration [210]: 0.002450286234677902
Loss at iteration [211]: 0.002450269017351998
Loss at iteration [212]: 0.002450142592231178
Loss at iteration [213]: 0.002449827952453351
Loss at iteration [214]: 0.0024495304327445
Loss at iteration [215]: 0.0024495304327445
Loss at iteration [216]: 0.0024494661232001076
Loss at iteration [217]: 0.0024493641233538713
Loss at iteration [218]: 0.0024491697650693973
Loss at iteration [219]: 0.0024491054960581737
Loss at iteration [220]: 0.0024490787321596076
Loss at iteration [221]: 0.002449048467507918
Loss at iteration [222]: 0.002448955393368733
Loss at iteration [223]: 0.00244885602423326
Loss at iteration [224]: 0.002448780100224148
Loss at iteration [225]: 0.002448780100224148
Loss at iteration [226]: 0.002448738484293671
Loss at iteration [227]: 0.0024486540618593594
Loss at iteration [228]: 0.002448552181152534
Loss at iteration [229]: 0.0024485344656828795
Loss at iteration [230]: 0.0024485073731384185
Loss at iteration [231]: 0.0024484943235518314
Loss at iteration [232]: 0.002448477003678947
Loss at iteration [233]: 0.002448397536605108
Loss at iteration [234]: 0.002448163609964741
Loss at iteration [235]: 0.002448163609964741
Loss at iteration [236]: 0.002448021937234811
Loss at iteration [237]: 0.002447969881047324
Loss at iteration [238]: 0.0024477130239035404
Loss at iteration [239]: 0.0024476618505250656
Loss at iteration [240]: 0.002447554771707989
Loss at iteration [241]: 0.0024475239390302733
Loss at iteration [242]: 0.002447438620581298
Loss at iteration [243]: 0.0024474285575732833
Loss at iteration [244]: 0.002447406605444749
Loss at iteration [245]: 0.0024473188587200998
Loss at iteration [246]: 0.0024473188587200998
Loss at iteration [247]: 0.002447300304036566
Loss at iteration [248]: 0.002447285508040262
Loss at iteration [249]: 0.002447255259058254
Loss at iteration [250]: 0.0024471938806018335
Loss at iteration [251]: 0.0024470811544269055
Loss at iteration [252]: 0.0024470097482124013
Loss at iteration [253]: 0.002446881907867933
Loss at iteration [254]: 0.002446692806097625
Loss at iteration [255]: 0.002446692806097625
Loss at iteration [256]: 0.0024465570737370644
Loss at iteration [257]: 0.002446472412199018
Loss at iteration [258]: 0.0024463789460484374
Loss at iteration [259]: 0.0024463067557136935
Loss at iteration [260]: 0.002446293673649403
Loss at iteration [261]: 0.0024462118688200166
Loss at iteration [262]: 0.0024460191383985546
Loss at iteration [263]: 0.0024458611817557412
Loss at iteration [264]: 0.002445491892742728
Loss at iteration [265]: 0.002445491892742728
Loss at iteration [266]: 0.002445421074374009
Loss at iteration [267]: 0.0024453249771062436
Loss at iteration [268]: 0.0024449224407480053
Loss at iteration [269]: 0.002444824339689937
Loss at iteration [270]: 0.0024446748820514696
Loss at iteration [271]: 0.002444621009147197
Loss at iteration [272]: 0.0024445705894322388
Loss at iteration [273]: 0.0024444446145528522
Loss at iteration [274]: 0.0024444446145528522
Loss at iteration [275]: 0.002444371065296228
Loss at iteration [276]: 0.0024443496656175864
Loss at iteration [277]: 0.0024442735521127937
Loss at iteration [278]: 0.002444210758129
Loss at iteration [279]: 0.0024441776708230816
Loss at iteration [280]: 0.002444160969175604
Loss at iteration [281]: 0.0024440775204107247
Loss at iteration [282]: 0.0024439282930768086
Loss at iteration [283]: 0.0024438900409772908
Loss at iteration [284]: 0.00244370660419457
Loss at iteration [285]: 0.00244370660419457
Loss at iteration [286]: 0.002443574724609894
Loss at iteration [287]: 0.002443534727695468
Loss at iteration [288]: 0.002443256747005504
Loss at iteration [289]: 0.0024431192139996063
Loss at iteration [290]: 0.0024430744768617354
Loss at iteration [291]: 0.0024429652254989837
Loss at iteration [292]: 0.0024428295496356544
Loss at iteration [293]: 0.002442775998842606
Loss at iteration [294]: 0.002442775998842606
Loss at iteration [295]: 0.0024427628284660694
Loss at iteration [296]: 0.002442650962116457
Loss at iteration [297]: 0.002442606091703641
Loss at iteration [298]: 0.0024425373281207513
Loss at iteration [299]: 0.0024425155228079564
Loss at iteration [300]: 0.0024424825807236434
Loss at iteration [301]: 0.0024423312876434514
Loss at iteration [302]: 0.0024422747363933104
Loss at iteration [303]: 0.002442224290258622
Loss at iteration [304]: 0.002442224290258622
Loss at iteration [305]: 0.0024421847317630282
Loss at iteration [306]: 0.0024421541659044306
Loss at iteration [307]: 0.002442110811061327
Loss at iteration [308]: 0.0024420540200434035
Loss at iteration [309]: 0.0024420189523373815
Loss at iteration [310]: 0.002442001193681333
Loss at iteration [311]: 0.002441966597983323
Loss at iteration [312]: 0.0024419421681599403
Loss at iteration [313]: 0.0024417198777482753
Loss at iteration [314]: 0.002441548546621718
Loss at iteration [315]: 0.002441548546621718
Loss at iteration [316]: 0.0024414867185048664
Loss at iteration [317]: 0.002441436316837997
Loss at iteration [318]: 0.002441374586580313
Loss at iteration [319]: 0.0024412935241471634
Loss at iteration [320]: 0.0024412510796637953
Loss at iteration [321]: 0.0024411564400509986
Loss at iteration [322]: 0.0024410076190322957
Loss at iteration [323]: 0.0024403604244352473
Loss at iteration [324]: 0.0024403604244352473
Loss at iteration [325]: 0.0024402680324040332
Loss at iteration [326]: 0.00244019149204714
Loss at iteration [327]: 0.002440054153676022
Loss at iteration [328]: 0.0024399761294355174
Loss at iteration [329]: 0.0024399242502034344
Loss at iteration [330]: 0.00243987820246618
Loss at iteration [331]: 0.0024397651191877317
Loss at iteration [332]: 0.002439709541999236
Loss at iteration [333]: 0.0024395983051404635
Loss at iteration [334]: 0.0024395983051404635
Loss at iteration [335]: 0.002439546815923636
Loss at iteration [336]: 0.0024395409221408828
Loss at iteration [337]: 0.0024394941681004996
Loss at iteration [338]: 0.0024393540981219423
Loss at iteration [339]: 0.0024392901227265886
Loss at iteration [340]: 0.0024392094853438137
Loss at iteration [341]: 0.002439127932054478
Loss at iteration [342]: 0.002439030724708319
Loss at iteration [343]: 0.0024386961257947285
Loss at iteration [344]: 0.0024386961257947285
Loss at iteration [345]: 0.0024386038261232576
Loss at iteration [346]: 0.0024385709571210865
Loss at iteration [347]: 0.0024384734291324098
Loss at iteration [348]: 0.0024383329228679818
Loss at iteration [349]: 0.0024382441791554036
Loss at iteration [350]: 0.0024381010991943175
Loss at iteration [351]: 0.0024380096708335997
Loss at iteration [352]: 0.002438001076992253
Loss at iteration [353]: 0.002437992613004778
