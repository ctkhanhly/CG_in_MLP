Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :FR_PR
Total number of function evaluations  : 3040
Total number of iterations            : 1082
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 43.11123442649841
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.97151748378735%
Percentage of parameters < 1e-7       : 49.97151748378735%
Percentage of parameters < 1e-6       : 49.9725125061878%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.0032668505206767773
Loss at iteration [2]: 0.003185765934149432
Loss at iteration [3]: 0.0031382363112302924
Loss at iteration [4]: 0.002711699760982887
Loss at iteration [5]: 0.002698039905355765
Loss at iteration [6]: 0.002682458484427857
Loss at iteration [7]: 0.0026353113101999746
Loss at iteration [8]: 0.0026287144404180296
Loss at iteration [9]: 0.002610447349290974
Loss at iteration [10]: 0.002590211529832078
Loss at iteration [11]: 0.002582080196058005
Loss at iteration [12]: 0.002582080196058005
Loss at iteration [13]: 0.0025778641689790795
Loss at iteration [14]: 0.002576178192116823
Loss at iteration [15]: 0.0025649001063235313
Loss at iteration [16]: 0.0025558951217341283
Loss at iteration [17]: 0.0025483508407501618
Loss at iteration [18]: 0.0025462531078747977
Loss at iteration [19]: 0.002542851549034868
Loss at iteration [20]: 0.0025374171455837848
Loss at iteration [21]: 0.002537252558203448
Loss at iteration [22]: 0.0025366394004513377
Loss at iteration [23]: 0.0025344849260232256
Loss at iteration [24]: 0.0025344849260232256
Loss at iteration [25]: 0.002531726566481377
Loss at iteration [26]: 0.0025311321059364148
Loss at iteration [27]: 0.0025232925370314817
Loss at iteration [28]: 0.002522484914617785
Loss at iteration [29]: 0.0025083984883341784
Loss at iteration [30]: 0.0025076264922895297
Loss at iteration [31]: 0.002507270299061334
Loss at iteration [32]: 0.0025049139267408734
Loss at iteration [33]: 0.0025038430006406663
Loss at iteration [34]: 0.0025038430006406663
Loss at iteration [35]: 0.002503697085906934
Loss at iteration [36]: 0.002503488397344168
Loss at iteration [37]: 0.0025024182267678508
Loss at iteration [38]: 0.0025021192685529274
Loss at iteration [39]: 0.002501799700845527
Loss at iteration [40]: 0.002500071390896747
Loss at iteration [41]: 0.002497435522245198
Loss at iteration [42]: 0.0024961712912676794
Loss at iteration [43]: 0.002495593930927528
Loss at iteration [44]: 0.002495593930927528
Loss at iteration [45]: 0.0024952685987443784
Loss at iteration [46]: 0.0024945623810487326
Loss at iteration [47]: 0.002493636818842188
Loss at iteration [48]: 0.002493371456511466
Loss at iteration [49]: 0.0024932516382898005
Loss at iteration [50]: 0.002491185379044281
Loss at iteration [51]: 0.0024900726232277683
Loss at iteration [52]: 0.0024898284292793342
Loss at iteration [53]: 0.0024891486628243454
Loss at iteration [54]: 0.0024890343516198116
Loss at iteration [55]: 0.0024890343516198116
Loss at iteration [56]: 0.0024889815320013713
Loss at iteration [57]: 0.0024886562905387113
Loss at iteration [58]: 0.0024881663905238797
Loss at iteration [59]: 0.0024879480347533528
Loss at iteration [60]: 0.002487717650662645
Loss at iteration [61]: 0.002486771432550868
Loss at iteration [62]: 0.0024858818974152325
Loss at iteration [63]: 0.0024847498263909948
Loss at iteration [64]: 0.0024843217609740423
Loss at iteration [65]: 0.0024843217609740423
Loss at iteration [66]: 0.0024841201684828477
Loss at iteration [67]: 0.0024839635184951757
Loss at iteration [68]: 0.002483742090113524
Loss at iteration [69]: 0.0024835697300580914
Loss at iteration [70]: 0.002483450275118366
Loss at iteration [71]: 0.002483351997087522
Loss at iteration [72]: 0.00248329489371502
Loss at iteration [73]: 0.0024829270719589087
Loss at iteration [74]: 0.0024818898907946786
Loss at iteration [75]: 0.0024813607788781223
Loss at iteration [76]: 0.002480932979677094
Loss at iteration [77]: 0.002480932979677094
Loss at iteration [78]: 0.002480729837625305
Loss at iteration [79]: 0.0024805699005873852
Loss at iteration [80]: 0.002480032041209534
Loss at iteration [81]: 0.0024799789110423027
Loss at iteration [82]: 0.002479888047425363
Loss at iteration [83]: 0.0024787527982459566
Loss at iteration [84]: 0.0024774151378326734
Loss at iteration [85]: 0.0024770723364358994
Loss at iteration [86]: 0.0024767199247367573
Loss at iteration [87]: 0.0024767199247367573
Loss at iteration [88]: 0.0024764834984907277
Loss at iteration [89]: 0.0024761647611912813
Loss at iteration [90]: 0.002475462163124363
Loss at iteration [91]: 0.002475290935955059
Loss at iteration [92]: 0.002475056016837688
Loss at iteration [93]: 0.0024745937139117216
Loss at iteration [94]: 0.002474495432033604
Loss at iteration [95]: 0.002474412461725646
Loss at iteration [96]: 0.0024740348549477267
Loss at iteration [97]: 0.002471498507055461
Loss at iteration [98]: 0.002471498507055461
Loss at iteration [99]: 0.002471177280307394
Loss at iteration [100]: 0.002471076091744234
Loss at iteration [101]: 0.002470130065111527
Loss at iteration [102]: 0.0024697446006635973
Loss at iteration [103]: 0.0024697302532132263
Loss at iteration [104]: 0.002469485648569673
Loss at iteration [105]: 0.002468548120503631
Loss at iteration [106]: 0.002468334165335548
Loss at iteration [107]: 0.0024680044492844896
Loss at iteration [108]: 0.0024680044492844896
Loss at iteration [109]: 0.0024677552583370456
Loss at iteration [110]: 0.0024673872857865617
Loss at iteration [111]: 0.002467355580320267
Loss at iteration [112]: 0.0024671192544964094
Loss at iteration [113]: 0.0024668184664680456
Loss at iteration [114]: 0.0024667319252278763
Loss at iteration [115]: 0.002466501575913951
Loss at iteration [116]: 0.0024659033395632193
Loss at iteration [117]: 0.0024659033395632193
Loss at iteration [118]: 0.002465801089382461
Loss at iteration [119]: 0.0024657710271965456
Loss at iteration [120]: 0.0024650927584568983
Loss at iteration [121]: 0.0024649443094092323
Loss at iteration [122]: 0.0024646875219040115
Loss at iteration [123]: 0.00246444273646186
Loss at iteration [124]: 0.002464333623739278
Loss at iteration [125]: 0.0024642108146871963
Loss at iteration [126]: 0.002464161371814717
Loss at iteration [127]: 0.002464161371814717
Loss at iteration [128]: 0.002464090528799581
Loss at iteration [129]: 0.002464060494924066
Loss at iteration [130]: 0.0024636394380722236
Loss at iteration [131]: 0.002463583282344474
Loss at iteration [132]: 0.002463023649985306
Loss at iteration [133]: 0.0024626030642837422
Loss at iteration [134]: 0.0024623777921834043
Loss at iteration [135]: 0.002461884836797323
Loss at iteration [136]: 0.0024613593112158224
Loss at iteration [137]: 0.0024613593112158224
Loss at iteration [138]: 0.0024610911732938063
Loss at iteration [139]: 0.0024609142424881613
Loss at iteration [140]: 0.002460336114226418
Loss at iteration [141]: 0.002460273025091671
Loss at iteration [142]: 0.0024600483881015186
Loss at iteration [143]: 0.002459606654781136
Loss at iteration [144]: 0.0024594547563617617
Loss at iteration [145]: 0.0024594057370841017
Loss at iteration [146]: 0.0024589808281714892
Loss at iteration [147]: 0.0024589294315260084
Loss at iteration [148]: 0.0024589294315260084
Loss at iteration [149]: 0.00245885733336884
Loss at iteration [150]: 0.0024587322622236576
Loss at iteration [151]: 0.002458652064211274
Loss at iteration [152]: 0.0024586062744428976
Loss at iteration [153]: 0.0024585877234159503
Loss at iteration [154]: 0.002458512906683819
Loss at iteration [155]: 0.002458452631918072
Loss at iteration [156]: 0.002458393979340146
Loss at iteration [157]: 0.0024579695533556534
Loss at iteration [158]: 0.0024579695533556534
Loss at iteration [159]: 0.0024576709628430842
Loss at iteration [160]: 0.002457555335843228
Loss at iteration [161]: 0.0024572152118729528
Loss at iteration [162]: 0.002457052409735976
Loss at iteration [163]: 0.002457010157973988
Loss at iteration [164]: 0.002456539810829653
Loss at iteration [165]: 0.002456502802547756
Loss at iteration [166]: 0.002456037021809175
Loss at iteration [167]: 0.0024558570139297303
Loss at iteration [168]: 0.0024558570139297303
Loss at iteration [169]: 0.0024557452225626316
Loss at iteration [170]: 0.002455617118369383
Loss at iteration [171]: 0.002455464973970175
Loss at iteration [172]: 0.002455438291332035
Loss at iteration [173]: 0.002454875928995899
Loss at iteration [174]: 0.0024547860631502477
Loss at iteration [175]: 0.002454337785750218
Loss at iteration [176]: 0.002454288838278636
Loss at iteration [177]: 0.002454210358550963
Loss at iteration [178]: 0.002454210358550963
Loss at iteration [179]: 0.0024541428484467047
Loss at iteration [180]: 0.0024541061360931937
Loss at iteration [181]: 0.00245388709519841
Loss at iteration [182]: 0.002453662341596674
Loss at iteration [183]: 0.0024534195411432965
Loss at iteration [184]: 0.002453359540229039
Loss at iteration [185]: 0.0024533209777964013
Loss at iteration [186]: 0.002453090247173445
Loss at iteration [187]: 0.0024529985027373536
Loss at iteration [188]: 0.0024529985027373536
Loss at iteration [189]: 0.0024529530483314937
Loss at iteration [190]: 0.0024528965167714893
Loss at iteration [191]: 0.0024527953830754453
Loss at iteration [192]: 0.0024526916125521153
Loss at iteration [193]: 0.00245259115933988
Loss at iteration [194]: 0.002452313770021406
Loss at iteration [195]: 0.0024518423082713556
Loss at iteration [196]: 0.002451200979931477
Loss at iteration [197]: 0.0024508108455673643
Loss at iteration [198]: 0.0024507235693875425
Loss at iteration [199]: 0.0024507235693875425
Loss at iteration [200]: 0.002450666522741697
Loss at iteration [201]: 0.002450593362361815
Loss at iteration [202]: 0.0024501622435160724
Loss at iteration [203]: 0.002450099511718899
Loss at iteration [204]: 0.002449711875479753
Loss at iteration [205]: 0.0024495754268098325
Loss at iteration [206]: 0.0024494815981423636
Loss at iteration [207]: 0.0024490763121825
Loss at iteration [208]: 0.00244904101428179
Loss at iteration [209]: 0.0024482696252562737
Loss at iteration [210]: 0.0024482696252562737
Loss at iteration [211]: 0.0024482046720051176
Loss at iteration [212]: 0.0024481268537196797
Loss at iteration [213]: 0.0024475904218379235
Loss at iteration [214]: 0.0024473421061357928
Loss at iteration [215]: 0.0024471530371476358
Loss at iteration [216]: 0.0024471383441445056
Loss at iteration [217]: 0.002447079398333018
Loss at iteration [218]: 0.002447036259523867
Loss at iteration [219]: 0.0024468397799854696
Loss at iteration [220]: 0.0024468397799854696
Loss at iteration [221]: 0.0024468127480459572
Loss at iteration [222]: 0.002446773084817824
Loss at iteration [223]: 0.002446633060135183
Loss at iteration [224]: 0.002446602542440905
Loss at iteration [225]: 0.0024461886763597826
Loss at iteration [226]: 0.00244570532153913
Loss at iteration [227]: 0.002445285123811794
Loss at iteration [228]: 0.002445189201950311
Loss at iteration [229]: 0.002445189201950311
Loss at iteration [230]: 0.0024451413986084827
Loss at iteration [231]: 0.0024448401820799117
Loss at iteration [232]: 0.002444599915119087
Loss at iteration [233]: 0.0024444272907753794
Loss at iteration [234]: 0.002444370429713372
Loss at iteration [235]: 0.002444289208908729
Loss at iteration [236]: 0.0024441444531034173
Loss at iteration [237]: 0.0024440613903649075
Loss at iteration [238]: 0.002444010910811256
Loss at iteration [239]: 0.0024439684102758167
Loss at iteration [240]: 0.0024439684102758167
Loss at iteration [241]: 0.002443938206894489
Loss at iteration [242]: 0.0024438811860330346
Loss at iteration [243]: 0.002443693156757287
Loss at iteration [244]: 0.002443618188379032
Loss at iteration [245]: 0.0024434643668095105
Loss at iteration [246]: 0.0024432949636705795
Loss at iteration [247]: 0.0024432499637587254
Loss at iteration [248]: 0.00244319578160852
Loss at iteration [249]: 0.002443068729170344
Loss at iteration [250]: 0.0024430572439970407
Loss at iteration [251]: 0.0024430572439970407
Loss at iteration [252]: 0.002443033240892651
Loss at iteration [253]: 0.0024429490799812155
Loss at iteration [254]: 0.002442915964431357
Loss at iteration [255]: 0.0024428445286366453
Loss at iteration [256]: 0.00244265381373898
Loss at iteration [257]: 0.002442422375131555
Loss at iteration [258]: 0.0024423729014250387
Loss at iteration [259]: 0.002442336200261935
Loss at iteration [260]: 0.0024422795537135515
Loss at iteration [261]: 0.002442191858448258
Loss at iteration [262]: 0.002442191858448258
Loss at iteration [263]: 0.002442183849351908
Loss at iteration [264]: 0.002442104901850674
Loss at iteration [265]: 0.0024419932526000133
Loss at iteration [266]: 0.0024418793372554953
Loss at iteration [267]: 0.0024417815744412715
Loss at iteration [268]: 0.0024413773925045187
Loss at iteration [269]: 0.0024405545234948084
Loss at iteration [270]: 0.002439238184005601
Loss at iteration [271]: 0.002439238184005601
Loss at iteration [272]: 0.0024390155431552735
Loss at iteration [273]: 0.0024388324470466256
Loss at iteration [274]: 0.0024384763597275717
Loss at iteration [275]: 0.002438270962747827
Loss at iteration [276]: 0.0024381336995841375
Loss at iteration [277]: 0.002438080496472762
Loss at iteration [278]: 0.0024375739105969366
Loss at iteration [279]: 0.00243729833349001
Loss at iteration [280]: 0.0024370086373249793
Loss at iteration [281]: 0.0024368320869976373
Loss at iteration [282]: 0.0024368320869976373
Loss at iteration [283]: 0.0024367601310845105
Loss at iteration [284]: 0.002436706049116943
Loss at iteration [285]: 0.002436525263546067
Loss at iteration [286]: 0.0024363906874261825
Loss at iteration [287]: 0.002436268329590711
Loss at iteration [288]: 0.0024361195109688055
Loss at iteration [289]: 0.0024359416274553586
Loss at iteration [290]: 0.002435730607406814
Loss at iteration [291]: 0.00243565872595434
Loss at iteration [292]: 0.002435564203529072
Loss at iteration [293]: 0.002435564203529072
Loss at iteration [294]: 0.0024354954883936243
Loss at iteration [295]: 0.0024354724697419946
Loss at iteration [296]: 0.0024353771947688194
Loss at iteration [297]: 0.0024351421211682924
Loss at iteration [298]: 0.0024350784689459895
Loss at iteration [299]: 0.0024350134442468726
Loss at iteration [300]: 0.002434817423108907
Loss at iteration [301]: 0.0024347602344580357
Loss at iteration [302]: 0.002434668133665233
Loss at iteration [303]: 0.002434668133665233
Loss at iteration [304]: 0.0024346212200184137
Loss at iteration [305]: 0.002434602081179081
Loss at iteration [306]: 0.0024343051144144214
Loss at iteration [307]: 0.0024342817237510504
Loss at iteration [308]: 0.0024340523314407762
Loss at iteration [309]: 0.0024336362489090265
Loss at iteration [310]: 0.0024334480246162964
Loss at iteration [311]: 0.002433373166196418
Loss at iteration [312]: 0.002433373166196418
Loss at iteration [313]: 0.002433316326020133
Loss at iteration [314]: 0.0024332072095723987
Loss at iteration [315]: 0.002432996115626916
Loss at iteration [316]: 0.0024328802078053794
Loss at iteration [317]: 0.002432785757127226
Loss at iteration [318]: 0.002432737642908053
Loss at iteration [319]: 0.0024325595482271468
Loss at iteration [320]: 0.002432471970046866
Loss at iteration [321]: 0.0024324384782995765
Loss at iteration [322]: 0.002432406306012893
Loss at iteration [323]: 0.002432406306012893
Loss at iteration [324]: 0.0024323805203290624
Loss at iteration [325]: 0.0024323339771842103
Loss at iteration [326]: 0.0024322429599031594
Loss at iteration [327]: 0.0024322164621468096
Loss at iteration [328]: 0.0024321168333816985
Loss at iteration [329]: 0.0024320885175468767
Loss at iteration [330]: 0.002432069477752231
Loss at iteration [331]: 0.002432020030039501
Loss at iteration [332]: 0.0024319192471588136
Loss at iteration [333]: 0.0024319192471588136
Loss at iteration [334]: 0.0024318965086245307
Loss at iteration [335]: 0.002431806811601137
Loss at iteration [336]: 0.0024317742737761756
Loss at iteration [337]: 0.0024317627830895165
Loss at iteration [338]: 0.0024317276609122007
Loss at iteration [339]: 0.002431671114838037
Loss at iteration [340]: 0.0024316365804898693
Loss at iteration [341]: 0.002431529289111302
Loss at iteration [342]: 0.0024312349537853545
Loss at iteration [343]: 0.0024312349537853545
Loss at iteration [344]: 0.002431141092477803
Loss at iteration [345]: 0.0024310980478464443
Loss at iteration [346]: 0.002430989952697752
Loss at iteration [347]: 0.0024308983377315622
Loss at iteration [348]: 0.0024308812155867084
Loss at iteration [349]: 0.0024307930854343025
Loss at iteration [350]: 0.0024307430646287255
Loss at iteration [351]: 0.0024306594187146623
Loss at iteration [352]: 0.0024305281525054036
Loss at iteration [353]: 0.0024305281525054036
Loss at iteration [354]: 0.0024304951587206867
Loss at iteration [355]: 0.0024304597537990187
Loss at iteration [356]: 0.0024303563869591373
Loss at iteration [357]: 0.0024303247867529906
Loss at iteration [358]: 0.002430287152937178
Loss at iteration [359]: 0.002430273653916476
Loss at iteration [360]: 0.0024302400725748287
Loss at iteration [361]: 0.002430187932737401
Loss at iteration [362]: 0.002430152135420365
Loss at iteration [363]: 0.002430152135420365
Loss at iteration [364]: 0.0024300713809926877
Loss at iteration [365]: 0.002430056369922977
Loss at iteration [366]: 0.002429986499648492
Loss at iteration [367]: 0.002429927574353122
Loss at iteration [368]: 0.0024298552508256947
Loss at iteration [369]: 0.002429739221499637
Loss at iteration [370]: 0.002429637111152238
Loss at iteration [371]: 0.002429442509772295
Loss at iteration [372]: 0.002429296397261349
Loss at iteration [373]: 0.0024291987660582754
Loss at iteration [374]: 0.0024291987660582754
Loss at iteration [375]: 0.002429180185651328
Loss at iteration [376]: 0.0024291492598850313
Loss at iteration [377]: 0.0024290237152642612
Loss at iteration [378]: 0.0024289842974773755
Loss at iteration [379]: 0.002428933844028404
Loss at iteration [380]: 0.002428857054089455
Loss at iteration [381]: 0.0024288251113318443
Loss at iteration [382]: 0.002428771609204168
Loss at iteration [383]: 0.002428771609204168
Loss at iteration [384]: 0.0024287325905831656
Loss at iteration [385]: 0.002428712481028356
Loss at iteration [386]: 0.002428604005464044
Loss at iteration [387]: 0.002428568871084728
Loss at iteration [388]: 0.0024284842922417784
Loss at iteration [389]: 0.002428339194984542
Loss at iteration [390]: 0.0024281993827440294
Loss at iteration [391]: 0.00242804100979839
Loss at iteration [392]: 0.0024276976733356073
Loss at iteration [393]: 0.002427545034066725
Loss at iteration [394]: 0.002427545034066725
Loss at iteration [395]: 0.0024274869809910687
Loss at iteration [396]: 0.0024273618095059773
Loss at iteration [397]: 0.0024271507163345345
Loss at iteration [398]: 0.0024271108146436114
Loss at iteration [399]: 0.0024270287971372866
Loss at iteration [400]: 0.0024268364899162143
Loss at iteration [401]: 0.002426805495422696
Loss at iteration [402]: 0.002426665111843903
Loss at iteration [403]: 0.002426555363022436
Loss at iteration [404]: 0.002426555363022436
Loss at iteration [405]: 0.0024265185957058084
Loss at iteration [406]: 0.002426462791211673
Loss at iteration [407]: 0.0024263512472206083
Loss at iteration [408]: 0.002426309934842113
Loss at iteration [409]: 0.002426229404051166
Loss at iteration [410]: 0.002426187147141536
Loss at iteration [411]: 0.002426151629593199
Loss at iteration [412]: 0.0024260716751013313
Loss at iteration [413]: 0.0024258883587530223
Loss at iteration [414]: 0.0024258883587530223
Loss at iteration [415]: 0.002425861051272773
Loss at iteration [416]: 0.0024257655046564723
Loss at iteration [417]: 0.0024257291492721655
Loss at iteration [418]: 0.0024256706711044177
Loss at iteration [419]: 0.002425590098986414
Loss at iteration [420]: 0.002425536091440121
Loss at iteration [421]: 0.00242548695624426
Loss at iteration [422]: 0.002425464161956617
Loss at iteration [423]: 0.002425448309811869
Loss at iteration [424]: 0.002425448309811869
Loss at iteration [425]: 0.002425432625634721
Loss at iteration [426]: 0.002425384007269125
Loss at iteration [427]: 0.0024253423939908075
Loss at iteration [428]: 0.002425304128240424
Loss at iteration [429]: 0.002425208477169411
Loss at iteration [430]: 0.0024251160347332256
Loss at iteration [431]: 0.002425045589095379
Loss at iteration [432]: 0.0024246885713334354
Loss at iteration [433]: 0.0024246885713334354
Loss at iteration [434]: 0.0024244338452512806
Loss at iteration [435]: 0.002424406478312583
Loss at iteration [436]: 0.0024242294357994538
Loss at iteration [437]: 0.0024242069808206325
Loss at iteration [438]: 0.0024240131237960576
Loss at iteration [439]: 0.0024239756598011457
Loss at iteration [440]: 0.0024233875602222015
Loss at iteration [441]: 0.0024233875602222015
Loss at iteration [442]: 0.002423190229675394
Loss at iteration [443]: 0.002423156780071418
Loss at iteration [444]: 0.0024229512170565646
Loss at iteration [445]: 0.002422927170816905
Loss at iteration [446]: 0.002422862462818488
Loss at iteration [447]: 0.0024227689936384975
Loss at iteration [448]: 0.002422695247047908
Loss at iteration [449]: 0.002422643428635645
Loss at iteration [450]: 0.0024225491186400374
Loss at iteration [451]: 0.0024225491186400374
Loss at iteration [452]: 0.0024225232799424275
Loss at iteration [453]: 0.002422444250130384
Loss at iteration [454]: 0.002422387129695655
Loss at iteration [455]: 0.0024223598388450067
Loss at iteration [456]: 0.002422304944887324
Loss at iteration [457]: 0.0024222757642633807
Loss at iteration [458]: 0.0024222385260308627
Loss at iteration [459]: 0.0024220594568037388
Loss at iteration [460]: 0.0024220594568037388
Loss at iteration [461]: 0.002421990504789417
Loss at iteration [462]: 0.0024219476076249497
Loss at iteration [463]: 0.002421860772247764
Loss at iteration [464]: 0.002421796473961094
Loss at iteration [465]: 0.0024217334570454424
Loss at iteration [466]: 0.0024216152199444774
Loss at iteration [467]: 0.002421558230855054
Loss at iteration [468]: 0.0024215027510158626
Loss at iteration [469]: 0.0024214433215873333
Loss at iteration [470]: 0.0024214273596376425
Loss at iteration [471]: 0.0024214273596376425
Loss at iteration [472]: 0.0024214110535997367
Loss at iteration [473]: 0.0024213962295848734
Loss at iteration [474]: 0.0024213767835778747
Loss at iteration [475]: 0.0024213535199718228
Loss at iteration [476]: 0.002421272011062676
Loss at iteration [477]: 0.0024212327001834227
Loss at iteration [478]: 0.0024211771440936967
Loss at iteration [479]: 0.002421156754259483
Loss at iteration [480]: 0.002421135941218205
Loss at iteration [481]: 0.002421060829457496
Loss at iteration [482]: 0.002421060829457496
Loss at iteration [483]: 0.002421024132825095
Loss at iteration [484]: 0.002421011940118863
Loss at iteration [485]: 0.002420970993201384
Loss at iteration [486]: 0.002420940301780975
Loss at iteration [487]: 0.00242092640433445
Loss at iteration [488]: 0.002420903672875879
Loss at iteration [489]: 0.0024208689510308887
Loss at iteration [490]: 0.0024208442416077396
Loss at iteration [491]: 0.002420783304787607
Loss at iteration [492]: 0.002420783304787607
Loss at iteration [493]: 0.0024207485060649845
Loss at iteration [494]: 0.002420733708477811
Loss at iteration [495]: 0.002420685009986479
Loss at iteration [496]: 0.002420656992092274
Loss at iteration [497]: 0.002420616341755957
Loss at iteration [498]: 0.0024206004863586376
Loss at iteration [499]: 0.002420586405353449
Loss at iteration [500]: 0.0024205467499219375
Loss at iteration [501]: 0.0024205177313377023
Loss at iteration [502]: 0.0024205177313377023
Loss at iteration [503]: 0.0024205063062751183
Loss at iteration [504]: 0.0024204779945358884
Loss at iteration [505]: 0.002420443004165745
Loss at iteration [506]: 0.0024204216162462358
Loss at iteration [507]: 0.002420372577096276
Loss at iteration [508]: 0.002420127120449017
Loss at iteration [509]: 0.0024183482442733397
Loss at iteration [510]: 0.0024167138957425544
Loss at iteration [511]: 0.0024167138957425544
Loss at iteration [512]: 0.0024163893945602657
Loss at iteration [513]: 0.0024162717178528126
Loss at iteration [514]: 0.0024153075809525302
Loss at iteration [515]: 0.002414888785874608
Loss at iteration [516]: 0.0024143735866743576
Loss at iteration [517]: 0.0024139381416271493
Loss at iteration [518]: 0.0024139095080624165
Loss at iteration [519]: 0.002413628989225897
Loss at iteration [520]: 0.0024132948077911945
Loss at iteration [521]: 0.0024132948077911945
Loss at iteration [522]: 0.0024132143265914747
Loss at iteration [523]: 0.0024131632488656103
Loss at iteration [524]: 0.002412941756892928
Loss at iteration [525]: 0.002412880329122927
Loss at iteration [526]: 0.002412823468736301
Loss at iteration [527]: 0.002412755008697217
Loss at iteration [528]: 0.002412704688582053
Loss at iteration [529]: 0.0024123936485512796
Loss at iteration [530]: 0.002412180799451968
Loss at iteration [531]: 0.002412180799451968
Loss at iteration [532]: 0.002412034485264082
Loss at iteration [533]: 0.0024119720978212117
Loss at iteration [534]: 0.0024118682167925
Loss at iteration [535]: 0.0024118254096501847
Loss at iteration [536]: 0.0024117439760039665
Loss at iteration [537]: 0.0024117007166004
Loss at iteration [538]: 0.0024116759848525834
Loss at iteration [539]: 0.002411640470766047
Loss at iteration [540]: 0.0024116099683753316
Loss at iteration [541]: 0.00241150253080484
Loss at iteration [542]: 0.00241150253080484
Loss at iteration [543]: 0.002411485684134401
Loss at iteration [544]: 0.0024113824231899667
Loss at iteration [545]: 0.002411362499760878
Loss at iteration [546]: 0.0024113400151374916
Loss at iteration [547]: 0.0024112983807677573
Loss at iteration [548]: 0.0024112495242687134
Loss at iteration [549]: 0.0024110443643474942
Loss at iteration [550]: 0.002410968403415099
Loss at iteration [551]: 0.0024108231058845553
Loss at iteration [552]: 0.0024108231058845553
Loss at iteration [553]: 0.002410781131263604
Loss at iteration [554]: 0.0024107425829333297
Loss at iteration [555]: 0.002410671231791028
Loss at iteration [556]: 0.0024106420238131135
Loss at iteration [557]: 0.0024105154768361207
Loss at iteration [558]: 0.0024104608783943513
Loss at iteration [559]: 0.002410417404584652
Loss at iteration [560]: 0.002410234183717844
Loss at iteration [561]: 0.0024100649323935764
Loss at iteration [562]: 0.0024100649323935764
Loss at iteration [563]: 0.002410042198140378
Loss at iteration [564]: 0.00240999946132693
Loss at iteration [565]: 0.0024097617991102726
Loss at iteration [566]: 0.0024097254511441565
Loss at iteration [567]: 0.0024094355576512842
Loss at iteration [568]: 0.002409407533125611
Loss at iteration [569]: 0.00240926376651422
Loss at iteration [570]: 0.00240879274476747
Loss at iteration [571]: 0.00240879274476747
Loss at iteration [572]: 0.002408745977189132
Loss at iteration [573]: 0.002408672318435241
Loss at iteration [574]: 0.002408387282253602
Loss at iteration [575]: 0.0024083655675841437
Loss at iteration [576]: 0.0024082821567292297
Loss at iteration [577]: 0.0024081530600252627
Loss at iteration [578]: 0.00240811693779128
Loss at iteration [579]: 0.002408051426174986
Loss at iteration [580]: 0.0024080267535962017
Loss at iteration [581]: 0.002407967285863929
Loss at iteration [582]: 0.002407967285863929
Loss at iteration [583]: 0.0024079370586004042
Loss at iteration [584]: 0.0024079191982981107
Loss at iteration [585]: 0.0024078600019527695
Loss at iteration [586]: 0.0024078224839889
Loss at iteration [587]: 0.0024077367843970644
Loss at iteration [588]: 0.002407699591207701
Loss at iteration [589]: 0.0024076550412582545
Loss at iteration [590]: 0.002407614490910193
Loss at iteration [591]: 0.0024075960476527434
Loss at iteration [592]: 0.0024075395678458855
Loss at iteration [593]: 0.0024075395678458855
Loss at iteration [594]: 0.002407516531521044
Loss at iteration [595]: 0.002407503411589242
Loss at iteration [596]: 0.0024074178937699684
Loss at iteration [597]: 0.00240739648081544
Loss at iteration [598]: 0.0024072529767193425
Loss at iteration [599]: 0.0024071395516257325
Loss at iteration [600]: 0.0024070300458146074
Loss at iteration [601]: 0.002406778059754773
Loss at iteration [602]: 0.002405482690845203
Loss at iteration [603]: 0.002405482690845203
Loss at iteration [604]: 0.0024049611379083685
Loss at iteration [605]: 0.002404842303600177
Loss at iteration [606]: 0.002404279999224866
Loss at iteration [607]: 0.002404231893963337
Loss at iteration [608]: 0.0024041090631045725
Loss at iteration [609]: 0.0024040455789435147
Loss at iteration [610]: 0.0024039976041866783
Loss at iteration [611]: 0.002403949983097282
Loss at iteration [612]: 0.0024038921420384877
Loss at iteration [613]: 0.0024038132372749196
Loss at iteration [614]: 0.0024038132372749196
Loss at iteration [615]: 0.0024037930519173857
Loss at iteration [616]: 0.002403771011992752
Loss at iteration [617]: 0.0024037468181350996
Loss at iteration [618]: 0.002403712523545813
Loss at iteration [619]: 0.002403494904509229
Loss at iteration [620]: 0.002403477030991231
Loss at iteration [621]: 0.0024033235462391604
Loss at iteration [622]: 0.0024032952407647408
Loss at iteration [623]: 0.0024032653065288374
Loss at iteration [624]: 0.002403181394902071
Loss at iteration [625]: 0.002403181394902071
Loss at iteration [626]: 0.0024031641449214387
Loss at iteration [627]: 0.0024031515380224305
Loss at iteration [628]: 0.002403126910161125
Loss at iteration [629]: 0.0024031067008555623
Loss at iteration [630]: 0.002402891696736598
Loss at iteration [631]: 0.0024026959931294466
Loss at iteration [632]: 0.002402658640459256
Loss at iteration [633]: 0.0024013977347926594
Loss at iteration [634]: 0.0024013977347926594
Loss at iteration [635]: 0.002401336217345448
Loss at iteration [636]: 0.002401197277904559
Loss at iteration [637]: 0.002400484041056681
Loss at iteration [638]: 0.0024002793137549807
Loss at iteration [639]: 0.0024002502809708642
Loss at iteration [640]: 0.0023998623974570417
Loss at iteration [641]: 0.0023997641504941237
Loss at iteration [642]: 0.0023997096414317886
Loss at iteration [643]: 0.002399579137573343
Loss at iteration [644]: 0.002399579137573343
Loss at iteration [645]: 0.002399500181493606
Loss at iteration [646]: 0.0023994754053724633
Loss at iteration [647]: 0.0023994551835885636
Loss at iteration [648]: 0.0023994093719502552
Loss at iteration [649]: 0.002399241216118973
Loss at iteration [650]: 0.0023992021879702796
Loss at iteration [651]: 0.0023991606350203796
Loss at iteration [652]: 0.002398956148867282
Loss at iteration [653]: 0.0023987826656353
Loss at iteration [654]: 0.0023987089720760436
Loss at iteration [655]: 0.0023987089720760436
Loss at iteration [656]: 0.0023986619621968876
Loss at iteration [657]: 0.0023986098195773182
Loss at iteration [658]: 0.0023984891801695613
Loss at iteration [659]: 0.002398452298672996
Loss at iteration [660]: 0.0023984176576426542
Loss at iteration [661]: 0.002398334369602305
Loss at iteration [662]: 0.0023982916400855157
Loss at iteration [663]: 0.0023981980960350252
Loss at iteration [664]: 0.0023981171710362225
Loss at iteration [665]: 0.0023981171710362225
Loss at iteration [666]: 0.002398066871449106
Loss at iteration [667]: 0.0023980405042491677
Loss at iteration [668]: 0.0023979200382875532
Loss at iteration [669]: 0.0023978888412022658
Loss at iteration [670]: 0.002397742368709272
Loss at iteration [671]: 0.0023977277898736696
Loss at iteration [672]: 0.0023976956534121916
Loss at iteration [673]: 0.0023976720440632756
Loss at iteration [674]: 0.0023976447275158718
Loss at iteration [675]: 0.0023976447275158718
Loss at iteration [676]: 0.0023976157725744177
Loss at iteration [677]: 0.0023975793842135208
Loss at iteration [678]: 0.002397503706892203
Loss at iteration [679]: 0.00239746001471328
Loss at iteration [680]: 0.0023973728036015517
Loss at iteration [681]: 0.002397136647629622
Loss at iteration [682]: 0.0023970558330275974
Loss at iteration [683]: 0.0023969947403426387
Loss at iteration [684]: 0.002396838230446136
Loss at iteration [685]: 0.002396838230446136
Loss at iteration [686]: 0.002396796171061826
Loss at iteration [687]: 0.0023967425041814166
Loss at iteration [688]: 0.0023964440031967477
Loss at iteration [689]: 0.002396339216213863
Loss at iteration [690]: 0.0023963193927388302
Loss at iteration [691]: 0.0023962066242585876
Loss at iteration [692]: 0.002396109172748802
Loss at iteration [693]: 0.0023960789437873736
Loss at iteration [694]: 0.002396062043544149
Loss at iteration [695]: 0.002396025364063971
Loss at iteration [696]: 0.002396025364063971
Loss at iteration [697]: 0.0023960181154939076
Loss at iteration [698]: 0.002395937080790512
Loss at iteration [699]: 0.0023958129457288995
Loss at iteration [700]: 0.002395788549611693
Loss at iteration [701]: 0.0023952266988961565
Loss at iteration [702]: 0.002395036609572885
Loss at iteration [703]: 0.0023949593140523803
Loss at iteration [704]: 0.0023947095377408367
Loss at iteration [705]: 0.0023943873764350953
Loss at iteration [706]: 0.0023943873764350953
Loss at iteration [707]: 0.002394273159644003
Loss at iteration [708]: 0.0023940489519521982
Loss at iteration [709]: 0.0023939606112670857
Loss at iteration [710]: 0.0023938800727872577
Loss at iteration [711]: 0.00239382791386964
Loss at iteration [712]: 0.002393787519650891
Loss at iteration [713]: 0.00239374081637884
Loss at iteration [714]: 0.0023936775517197696
Loss at iteration [715]: 0.0023935899220403384
Loss at iteration [716]: 0.0023935899220403384
Loss at iteration [717]: 0.0023935410142148926
Loss at iteration [718]: 0.0023934635280128606
Loss at iteration [719]: 0.002393371803404714
Loss at iteration [720]: 0.002393339431735017
Loss at iteration [721]: 0.0023932998214094134
Loss at iteration [722]: 0.002393234248604659
Loss at iteration [723]: 0.002393228444373123
Loss at iteration [724]: 0.0023932080112047866
Loss at iteration [725]: 0.002393170103623992
Loss at iteration [726]: 0.002393170103623992
Loss at iteration [727]: 0.0023931613939618297
Loss at iteration [728]: 0.0023931294499632268
Loss at iteration [729]: 0.002393082752511558
Loss at iteration [730]: 0.0023930487783115676
Loss at iteration [731]: 0.002393023259087125
Loss at iteration [732]: 0.002392987884015155
Loss at iteration [733]: 0.002392938960302664
Loss at iteration [734]: 0.002392927676849561
Loss at iteration [735]: 0.0023928828486332702
Loss at iteration [736]: 0.0023928828486332702
Loss at iteration [737]: 0.0023928631751910555
Loss at iteration [738]: 0.0023928433384864463
Loss at iteration [739]: 0.002392787468548537
Loss at iteration [740]: 0.00239273965618594
Loss at iteration [741]: 0.0023926942649055956
Loss at iteration [742]: 0.002392574650147707
Loss at iteration [743]: 0.002391106612829541
Loss at iteration [744]: 0.0023909694371594685
Loss at iteration [745]: 0.0023905173345692824
Loss at iteration [746]: 0.0023905173345692824
Loss at iteration [747]: 0.0023904022703659994
Loss at iteration [748]: 0.002390261367676246
Loss at iteration [749]: 0.002389584931436353
Loss at iteration [750]: 0.0023895447386016256
Loss at iteration [751]: 0.0023894988312228626
Loss at iteration [752]: 0.0023893584901520525
Loss at iteration [753]: 0.002389244499612856
Loss at iteration [754]: 0.002389144671463685
Loss at iteration [755]: 0.002388937607557779
Loss at iteration [756]: 0.0023885451379856855
Loss at iteration [757]: 0.0023885451379856855
Loss at iteration [758]: 0.0023884781028321836
Loss at iteration [759]: 0.0023884275538794035
Loss at iteration [760]: 0.002388286295824928
Loss at iteration [761]: 0.002388213648541876
Loss at iteration [762]: 0.0023881550139531146
Loss at iteration [763]: 0.002387848345732175
Loss at iteration [764]: 0.0023878080804956895
Loss at iteration [765]: 0.0023875817252497896
Loss at iteration [766]: 0.0023875817252497896
Loss at iteration [767]: 0.0023874408837984873
Loss at iteration [768]: 0.002387401430052872
Loss at iteration [769]: 0.002387218226835231
Loss at iteration [770]: 0.00238719625997691
Loss at iteration [771]: 0.0023871281433223844
Loss at iteration [772]: 0.0023870924125920794
Loss at iteration [773]: 0.002387076850440713
Loss at iteration [774]: 0.002387054980431434
Loss at iteration [775]: 0.002386955014549474
Loss at iteration [776]: 0.002386955014549474
Loss at iteration [777]: 0.002386923649137439
Loss at iteration [778]: 0.0023869058704195797
Loss at iteration [779]: 0.002386855887653349
Loss at iteration [780]: 0.0023868196530614128
Loss at iteration [781]: 0.002386764616628833
Loss at iteration [782]: 0.0023867098826989316
Loss at iteration [783]: 0.00238648931485724
Loss at iteration [784]: 0.0023863907649170927
Loss at iteration [785]: 0.0023862927200769004
Loss at iteration [786]: 0.0023862927200769004
Loss at iteration [787]: 0.002386221576890508
Loss at iteration [788]: 0.002386194848543439
Loss at iteration [789]: 0.0023861212753507974
Loss at iteration [790]: 0.002386060396964511
Loss at iteration [791]: 0.002386032897400897
Loss at iteration [792]: 0.00238596339506435
Loss at iteration [793]: 0.0023859214257152313
Loss at iteration [794]: 0.002385829856410033
Loss at iteration [795]: 0.002385693737804833
Loss at iteration [796]: 0.002385673430486312
Loss at iteration [797]: 0.002385673430486312
Loss at iteration [798]: 0.0023856577009896277
Loss at iteration [799]: 0.002385519599077038
Loss at iteration [800]: 0.0023854838978568226
Loss at iteration [801]: 0.0023854607283631942
Loss at iteration [802]: 0.002385366679404491
Loss at iteration [803]: 0.0023853309813692387
Loss at iteration [804]: 0.0023853010585574443
Loss at iteration [805]: 0.002385225756827575
Loss at iteration [806]: 0.0023851897322720974
Loss at iteration [807]: 0.0023851897322720974
Loss at iteration [808]: 0.0023851691190371104
Loss at iteration [809]: 0.0023851386917708474
Loss at iteration [810]: 0.0023850905856061502
Loss at iteration [811]: 0.0023850011494572568
Loss at iteration [812]: 0.0023849336624167405
Loss at iteration [813]: 0.0023848584366928543
Loss at iteration [814]: 0.0023847306985711474
Loss at iteration [815]: 0.0023845139675899396
Loss at iteration [816]: 0.002384465888064559
Loss at iteration [817]: 0.002384465888064559
Loss at iteration [818]: 0.002384442136010238
Loss at iteration [819]: 0.002384325713159587
Loss at iteration [820]: 0.002384298394764116
Loss at iteration [821]: 0.002384189397226749
Loss at iteration [822]: 0.0023841301052536833
Loss at iteration [823]: 0.0023838914751253
Loss at iteration [824]: 0.002383848697644714
Loss at iteration [825]: 0.002383659606098689
Loss at iteration [826]: 0.002383659606098689
Loss at iteration [827]: 0.0023835238868298056
Loss at iteration [828]: 0.002383503921628449
Loss at iteration [829]: 0.0023833576096398434
Loss at iteration [830]: 0.0023833262720540666
Loss at iteration [831]: 0.0023833013792182935
Loss at iteration [832]: 0.0023832315167886904
Loss at iteration [833]: 0.0023831165897396367
Loss at iteration [834]: 0.0023830785556597472
Loss at iteration [835]: 0.0023830605104300196
Loss at iteration [836]: 0.0023829835880330758
Loss at iteration [837]: 0.0023829835880330758
Loss at iteration [838]: 0.0023829708109940223
Loss at iteration [839]: 0.0023829524441012087
Loss at iteration [840]: 0.0023829078692026735
Loss at iteration [841]: 0.0023828114185009573
Loss at iteration [842]: 0.0023827037438515133
Loss at iteration [843]: 0.002382569142550229
Loss at iteration [844]: 0.0023825070344417293
Loss at iteration [845]: 0.002382413589653721
Loss at iteration [846]: 0.002382340464198145
Loss at iteration [847]: 0.002382340464198145
Loss at iteration [848]: 0.002382319222331855
Loss at iteration [849]: 0.0023822650329813518
Loss at iteration [850]: 0.0023821860771852787
Loss at iteration [851]: 0.002382164585226595
Loss at iteration [852]: 0.0023820399680021948
Loss at iteration [853]: 0.002381975394832482
Loss at iteration [854]: 0.002381927031524498
Loss at iteration [855]: 0.002381854863369272
Loss at iteration [856]: 0.0023817154901341316
Loss at iteration [857]: 0.0023817154901341316
Loss at iteration [858]: 0.0023816898644376627
Loss at iteration [859]: 0.0023816500862950114
Loss at iteration [860]: 0.0023815715655009194
Loss at iteration [861]: 0.002381482297284092
Loss at iteration [862]: 0.002381431469145783
Loss at iteration [863]: 0.0023813715671922226
Loss at iteration [864]: 0.002381300814846143
Loss at iteration [865]: 0.002381082569611577
Loss at iteration [866]: 0.002381032460447377
Loss at iteration [867]: 0.002381032460447377
Loss at iteration [868]: 0.002380994895656692
Loss at iteration [869]: 0.0023809539470929972
Loss at iteration [870]: 0.002380858088999056
Loss at iteration [871]: 0.002380831488124749
Loss at iteration [872]: 0.002380797630429788
Loss at iteration [873]: 0.002380751537824823
Loss at iteration [874]: 0.002380688763821386
Loss at iteration [875]: 0.002380602738011564
Loss at iteration [876]: 0.002380505351934866
Loss at iteration [877]: 0.002380505351934866
Loss at iteration [878]: 0.0023804798981608764
Loss at iteration [879]: 0.0023804423491051527
Loss at iteration [880]: 0.002380322089305002
Loss at iteration [881]: 0.00238027573927295
Loss at iteration [882]: 0.002380241245450346
Loss at iteration [883]: 0.0023802065357769013
Loss at iteration [884]: 0.0023801555633601035
Loss at iteration [885]: 0.002380063190510108
Loss at iteration [886]: 0.002380063190510108
Loss at iteration [887]: 0.002380040073469135
Loss at iteration [888]: 0.002379980392649494
Loss at iteration [889]: 0.0023798928105860473
Loss at iteration [890]: 0.002379876793358419
Loss at iteration [891]: 0.0023797819921214714
Loss at iteration [892]: 0.0023797255095433676
Loss at iteration [893]: 0.0023796784090425608
Loss at iteration [894]: 0.002379563506829892
Loss at iteration [895]: 0.0023792327579142403
Loss at iteration [896]: 0.0023792327579142403
Loss at iteration [897]: 0.002379201882365734
Loss at iteration [898]: 0.002379012987937229
Loss at iteration [899]: 0.0023789653829664283
Loss at iteration [900]: 0.002378934945672432
Loss at iteration [901]: 0.0023788878275112896
Loss at iteration [902]: 0.0023787894481371026
Loss at iteration [903]: 0.002378767832499506
Loss at iteration [904]: 0.002378701476403494
Loss at iteration [905]: 0.002378626484189911
Loss at iteration [906]: 0.002378626484189911
Loss at iteration [907]: 0.0023785752458200665
Loss at iteration [908]: 0.0023785497279908206
Loss at iteration [909]: 0.0023784495801144425
Loss at iteration [910]: 0.002378408866162077
Loss at iteration [911]: 0.0023783835436564024
Loss at iteration [912]: 0.002378289411868427
Loss at iteration [913]: 0.0023782584693555326
Loss at iteration [914]: 0.002378211318432368
Loss at iteration [915]: 0.0023780244211759356
Loss at iteration [916]: 0.0023779460552269
Loss at iteration [917]: 0.0023779460552269
Loss at iteration [918]: 0.002377925560722817
Loss at iteration [919]: 0.0023777760929311106
Loss at iteration [920]: 0.0023776991295035646
Loss at iteration [921]: 0.002377668842707211
Loss at iteration [922]: 0.0023775797764645393
Loss at iteration [923]: 0.0023775532856197238
Loss at iteration [924]: 0.0023775103584718804
Loss at iteration [925]: 0.0023774869839597846
Loss at iteration [926]: 0.002377437600005676
Loss at iteration [927]: 0.002377437600005676
Loss at iteration [928]: 0.002377397174323544
Loss at iteration [929]: 0.0023773857522966053
Loss at iteration [930]: 0.0023773331111774255
Loss at iteration [931]: 0.002377314882478867
Loss at iteration [932]: 0.0023772568098394896
Loss at iteration [933]: 0.0023771624920644447
Loss at iteration [934]: 0.0023770393730924363
Loss at iteration [935]: 0.002376885406093831
Loss at iteration [936]: 0.0023768258679348637
Loss at iteration [937]: 0.0023768258679348637
Loss at iteration [938]: 0.0023767835019124555
Loss at iteration [939]: 0.0023767344243007843
Loss at iteration [940]: 0.0023765929465907835
Loss at iteration [941]: 0.0023764769973966295
Loss at iteration [942]: 0.0023764434093719185
Loss at iteration [943]: 0.0023763984060243422
Loss at iteration [944]: 0.002376345600351858
Loss at iteration [945]: 0.0023762217298496473
Loss at iteration [946]: 0.0023762217298496473
Loss at iteration [947]: 0.0023761197898459823
Loss at iteration [948]: 0.0023760804018601255
Loss at iteration [949]: 0.0023759532326516793
Loss at iteration [950]: 0.00237587317931113
Loss at iteration [951]: 0.002375826076493731
Loss at iteration [952]: 0.002375767544227927
Loss at iteration [953]: 0.0023757121809893683
Loss at iteration [954]: 0.0023756270632877417
Loss at iteration [955]: 0.0023756091686441866
Loss at iteration [956]: 0.0023754781997318017
Loss at iteration [957]: 0.0023754781997318017
Loss at iteration [958]: 0.002375456476128566
Loss at iteration [959]: 0.0023754227855850183
Loss at iteration [960]: 0.0023753946893985585
Loss at iteration [961]: 0.0023753784633400547
Loss at iteration [962]: 0.0023753168078085507
Loss at iteration [963]: 0.0023752969096613365
Loss at iteration [964]: 0.0023752205686318633
Loss at iteration [965]: 0.0023751618332701323
Loss at iteration [966]: 0.002375089775724336
Loss at iteration [967]: 0.002375089775724336
Loss at iteration [968]: 0.0023750667949232213
Loss at iteration [969]: 0.0023750379421907885
Loss at iteration [970]: 0.002374977721894609
Loss at iteration [971]: 0.002374950705692572
Loss at iteration [972]: 0.0023749283057818626
Loss at iteration [973]: 0.0023749036422147265
Loss at iteration [974]: 0.002374866117598887
Loss at iteration [975]: 0.0023748539053438903
Loss at iteration [976]: 0.002374843092625052
Loss at iteration [977]: 0.002374815600031777
Loss at iteration [978]: 0.002374815600031777
Loss at iteration [979]: 0.002374802721532642
Loss at iteration [980]: 0.002374766662563925
Loss at iteration [981]: 0.0023747540214901952
Loss at iteration [982]: 0.0023747066139750937
Loss at iteration [983]: 0.002374675192607099
Loss at iteration [984]: 0.0023746523652584447
Loss at iteration [985]: 0.002374586325773512
Loss at iteration [986]: 0.002374557074064701
Loss at iteration [987]: 0.002374517852746524
Loss at iteration [988]: 0.002374496341639876
Loss at iteration [989]: 0.002374496341639876
Loss at iteration [990]: 0.002374482999903334
Loss at iteration [991]: 0.0023744708966423186
Loss at iteration [992]: 0.002374461169691864
Loss at iteration [993]: 0.002374449690274465
Loss at iteration [994]: 0.002374429968237776
Loss at iteration [995]: 0.002374406760965994
Loss at iteration [996]: 0.002374377617984822
Loss at iteration [997]: 0.0023743592781705794
Loss at iteration [998]: 0.0023743462498087993
Loss at iteration [999]: 0.002374335508123959
Loss at iteration [1000]: 0.0023743224098451772
Loss at iteration [1001]: 0.0023743224098451772
Loss at iteration [1002]: 0.00237431369975643
Loss at iteration [1003]: 0.002374304000883467
Loss at iteration [1004]: 0.0023742896147770547
Loss at iteration [1005]: 0.0023742788160224914
Loss at iteration [1006]: 0.0023742383926496236
Loss at iteration [1007]: 0.0023741802510133104
Loss at iteration [1008]: 0.002374022767539718
Loss at iteration [1009]: 0.002372478074081391
Loss at iteration [1010]: 0.0023714815579250316
Loss at iteration [1011]: 0.0023712534320336196
Loss at iteration [1012]: 0.0023712534320336196
Loss at iteration [1013]: 0.0023711934668269497
Loss at iteration [1014]: 0.002370967395335139
Loss at iteration [1015]: 0.0023702500801673367
Loss at iteration [1016]: 0.0023701204008381204
Loss at iteration [1017]: 0.0023699842338767547
Loss at iteration [1018]: 0.002369950484853759
Loss at iteration [1019]: 0.002369810875984403
Loss at iteration [1020]: 0.0023697002731876376
Loss at iteration [1021]: 0.002369656369791743
Loss at iteration [1022]: 0.0023694320795421085
Loss at iteration [1023]: 0.0023694320795421085
Loss at iteration [1024]: 0.0023693770239395897
Loss at iteration [1025]: 0.0023693461798424604
Loss at iteration [1026]: 0.0023691786234119566
Loss at iteration [1027]: 0.002369154401641854
Loss at iteration [1028]: 0.0023689023879309547
Loss at iteration [1029]: 0.002368852324821273
Loss at iteration [1030]: 0.0023688278699539856
Loss at iteration [1031]: 0.002368764126156716
Loss at iteration [1032]: 0.0023687358252173843
Loss at iteration [1033]: 0.0023687358252173843
Loss at iteration [1034]: 0.00236871717310467
Loss at iteration [1035]: 0.0023686593670068676
Loss at iteration [1036]: 0.0023686184749650437
Loss at iteration [1037]: 0.002368596186088483
Loss at iteration [1038]: 0.002368322166448567
Loss at iteration [1039]: 0.002368223730278799
Loss at iteration [1040]: 0.002368137173475834
Loss at iteration [1041]: 0.002367837241898219
Loss at iteration [1042]: 0.002367837241898219
Loss at iteration [1043]: 0.002367724536221873
Loss at iteration [1044]: 0.00236769397485057
Loss at iteration [1045]: 0.0023676416082490173
Loss at iteration [1046]: 0.00236752448045931
Loss at iteration [1047]: 0.0023674599691083497
Loss at iteration [1048]: 0.0023674019524870665
Loss at iteration [1049]: 0.0023672621663795596
Loss at iteration [1050]: 0.0023672240327779763
Loss at iteration [1051]: 0.002367182698087314
Loss at iteration [1052]: 0.002367182698087314
Loss at iteration [1053]: 0.002367162978056018
Loss at iteration [1054]: 0.002367143052176019
Loss at iteration [1055]: 0.0023671029921384754
Loss at iteration [1056]: 0.002367080945449811
Loss at iteration [1057]: 0.0023670376611305334
Loss at iteration [1058]: 0.0023669559011676305
Loss at iteration [1059]: 0.002366803817660567
Loss at iteration [1060]: 0.002366759779896513
Loss at iteration [1061]: 0.0023667291432554827
Loss at iteration [1062]: 0.002366662092506578
Loss at iteration [1063]: 0.002366662092506578
Loss at iteration [1064]: 0.0023666404848630197
Loss at iteration [1065]: 0.0023665972196333603
Loss at iteration [1066]: 0.0023665756275150123
Loss at iteration [1067]: 0.002366507899755495
Loss at iteration [1068]: 0.002366455041315823
Loss at iteration [1069]: 0.0023663983432180746
Loss at iteration [1070]: 0.0023663319264016574
Loss at iteration [1071]: 0.00236613905787298
Loss at iteration [1072]: 0.0023660156551562252
Loss at iteration [1073]: 0.0023659047002127223
Loss at iteration [1074]: 0.0023659047002127223
Loss at iteration [1075]: 0.002365881061008776
Loss at iteration [1076]: 0.0023657598382987896
Loss at iteration [1077]: 0.0023657089150409794
Loss at iteration [1078]: 0.0023656591859364906
Loss at iteration [1079]: 0.0023654500571026146
Loss at iteration [1080]: 0.002365372685744733
Loss at iteration [1081]: 0.00236521256281524
Loss at iteration [1082]: 0.002365174124677922
Loss at iteration [1083]: 0.0023651229647449323
Loss at iteration [1084]: 0.0023651229647449323
Loss at iteration [1085]: 0.002365101290197606
Loss at iteration [1086]: 0.002365065948488907
Loss at iteration [1087]: 0.0023650156889117938
Loss at iteration [1088]: 0.0023649705691558354
Loss at iteration [1089]: 0.0023649091684833884
Loss at iteration [1090]: 0.002364859807611299
Loss at iteration [1091]: 0.0023647944173709246
Loss at iteration [1092]: 0.0023647219094424777
Loss at iteration [1093]: 0.002364670793043648
Loss at iteration [1094]: 0.00236452784535836
Loss at iteration [1095]: 0.00236452784535836
Loss at iteration [1096]: 0.002364483325847568
Loss at iteration [1097]: 0.002364459434247186
Loss at iteration [1098]: 0.0023644391318363126
Loss at iteration [1099]: 0.0023644092619821335
Loss at iteration [1100]: 0.002364337065491626
Loss at iteration [1101]: 0.0023643074694555967
Loss at iteration [1102]: 0.0023642704292627546
Loss at iteration [1103]: 0.002364202420410873
Loss at iteration [1104]: 0.002364190307913837
Loss at iteration [1105]: 0.002364068721125764
Loss at iteration [1106]: 0.002364068721125764
Loss at iteration [1107]: 0.0023640408918605654
Loss at iteration [1108]: 0.002364003878720111
Loss at iteration [1109]: 0.0023639538678383565
Loss at iteration [1110]: 0.002363926407536902
Loss at iteration [1111]: 0.002363893458187346
Loss at iteration [1112]: 0.002363864374336484
Loss at iteration [1113]: 0.002363833198936204
Loss at iteration [1114]: 0.002363807549343254
Loss at iteration [1115]: 0.00236379210554477
Loss at iteration [1116]: 0.00236379210554477
Loss at iteration [1117]: 0.0023637818535589324
Loss at iteration [1118]: 0.0023637701488383963
Loss at iteration [1119]: 0.002363744072255124
Loss at iteration [1120]: 0.002363706213233756
Loss at iteration [1121]: 0.002363643588812139
Loss at iteration [1122]: 0.0023634895352238144
Loss at iteration [1123]: 0.002363206740097116
Loss at iteration [1124]: 0.0023630475006881863
Loss at iteration [1125]: 0.002361501263766455
Loss at iteration [1126]: 0.002361501263766455
Loss at iteration [1127]: 0.0023611613378645585
Loss at iteration [1128]: 0.002361038895361826
Loss at iteration [1129]: 0.002360469647084695
Loss at iteration [1130]: 0.0023604026772299095
Loss at iteration [1131]: 0.0023602675760755927
Loss at iteration [1132]: 0.002360117195962914
Loss at iteration [1133]: 0.002360082278426551
Loss at iteration [1134]: 0.0023600026063203532
Loss at iteration [1135]: 0.0023599755001177873
Loss at iteration [1136]: 0.0023599755001177873
Loss at iteration [1137]: 0.0023599604091169126
Loss at iteration [1138]: 0.0023598838838307856
Loss at iteration [1139]: 0.0023598338102472968
Loss at iteration [1140]: 0.002359801004435898
Loss at iteration [1141]: 0.0023596705907583866
Loss at iteration [1142]: 0.0023596005151146934
Loss at iteration [1143]: 0.002359571796438607
Loss at iteration [1144]: 0.0023594988258944724
Loss at iteration [1145]: 0.002359460952572382
Loss at iteration [1146]: 0.002359460952572382
Loss at iteration [1147]: 0.0023594459167702265
Loss at iteration [1148]: 0.0023593913387565378
Loss at iteration [1149]: 0.0023593702438432966
Loss at iteration [1150]: 0.0023593417191966424
Loss at iteration [1151]: 0.002359278607946604
Loss at iteration [1152]: 0.002359234171472119
Loss at iteration [1153]: 0.0023591179841642622
Loss at iteration [1154]: 0.0023590020017070774
Loss at iteration [1155]: 0.002358824853787366
Loss at iteration [1156]: 0.002358824853787366
Loss at iteration [1157]: 0.0023587195112775286
Loss at iteration [1158]: 0.0023586559500071227
Loss at iteration [1159]: 0.002358408338080287
Loss at iteration [1160]: 0.002358372121814451
Loss at iteration [1161]: 0.0023582491776525686
Loss at iteration [1162]: 0.002358226241995075
Loss at iteration [1163]: 0.0023581943142540196
Loss at iteration [1164]: 0.00235812387623226
Loss at iteration [1165]: 0.0023580648261964814
Loss at iteration [1166]: 0.0023580258899399066
Loss at iteration [1167]: 0.0023580258899399066
Loss at iteration [1168]: 0.0023579900865669657
Loss at iteration [1169]: 0.00235797152099374
Loss at iteration [1170]: 0.0023579481452734473
Loss at iteration [1171]: 0.002357913800409969
Loss at iteration [1172]: 0.0023578866425286622
Loss at iteration [1173]: 0.002357859034013839
Loss at iteration [1174]: 0.0023578183941867017
Loss at iteration [1175]: 0.00235779165890375
Loss at iteration [1176]: 0.0023577654943330345
Loss at iteration [1177]: 0.0023577462803675365
Loss at iteration [1178]: 0.0023577462803675365
Loss at iteration [1179]: 0.002357736408754195
Loss at iteration [1180]: 0.002357721931104132
Loss at iteration [1181]: 0.0023576930206931222
Loss at iteration [1182]: 0.002357673322521649
Loss at iteration [1183]: 0.0023574787587064285
Loss at iteration [1184]: 0.0023573625632452733
Loss at iteration [1185]: 0.002356918194955472
Loss at iteration [1186]: 0.0023558157166787895
Loss at iteration [1187]: 0.0023549841663988652
Loss at iteration [1188]: 0.00235465756301037
