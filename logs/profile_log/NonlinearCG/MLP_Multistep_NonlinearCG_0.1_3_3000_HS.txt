Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.1
Beta type                             :HS
Total number of function evaluations  : 2468
Total number of iterations            : 694
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 7.070686340332031
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 49.97721666947331%
Percentage of parameters < 1e-7       : 49.97721666947331%
Percentage of parameters < 1e-6       : 49.97820724906143%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.20676434799515
Loss at iteration [2]: 1.2006788079285857
Loss at iteration [3]: 1.1970893881585498
Loss at iteration [4]: 1.1827370205270105
Loss at iteration [5]: 1.1768550069533414
Loss at iteration [6]: 1.1717875252910595
Loss at iteration [7]: 1.161581748797081
Loss at iteration [8]: 1.1480985544329
Loss at iteration [9]: 1.1403693696017863
Loss at iteration [10]: 1.123459543927496
Loss at iteration [11]: 1.115577792428466
Loss at iteration [12]: 1.0969601945498186
Loss at iteration [13]: 1.0895871224259022
Loss at iteration [14]: 1.0855736827163311
Loss at iteration [15]: 1.0855736827163311
Loss at iteration [16]: 1.0824031334274151
Loss at iteration [17]: 1.069248014461981
Loss at iteration [18]: 1.0588516826376562
Loss at iteration [19]: 1.0446825219481926
Loss at iteration [20]: 1.0295678746385037
Loss at iteration [21]: 1.027754548557613
Loss at iteration [22]: 1.0152343310027994
Loss at iteration [23]: 1.0106723189197806
Loss at iteration [24]: 1.0049358613267534
Loss at iteration [25]: 1.0024576345628187
Loss at iteration [26]: 0.998581993191872
Loss at iteration [27]: 0.992283313174058
Loss at iteration [28]: 0.9883691528424736
Loss at iteration [29]: 0.9883691528424736
Loss at iteration [30]: 0.9860634874882659
Loss at iteration [31]: 0.9768945293605953
Loss at iteration [32]: 0.9706068803601524
Loss at iteration [33]: 0.9615452027255479
Loss at iteration [34]: 0.9588777075849724
Loss at iteration [35]: 0.9489537510116216
Loss at iteration [36]: 0.9468223147035641
Loss at iteration [37]: 0.9419063634997205
Loss at iteration [38]: 0.938917706908807
Loss at iteration [39]: 0.9312040555620266
Loss at iteration [40]: 0.9304787702375656
Loss at iteration [41]: 0.920808242828462
Loss at iteration [42]: 0.9193161472922587
Loss at iteration [43]: 0.9193161472922587
Loss at iteration [44]: 0.9180376552728389
Loss at iteration [45]: 0.9081902006499339
Loss at iteration [46]: 0.9030437791393707
Loss at iteration [47]: 0.8971603129298972
Loss at iteration [48]: 0.8943221052707819
Loss at iteration [49]: 0.892572431188515
Loss at iteration [50]: 0.8884847409932487
Loss at iteration [51]: 0.8864574280288663
Loss at iteration [52]: 0.8828541410643478
Loss at iteration [53]: 0.8811555469919747
Loss at iteration [54]: 0.8769852572835097
Loss at iteration [55]: 0.8747435697316709
Loss at iteration [56]: 0.8707822574589701
Loss at iteration [57]: 0.8707822574589701
Loss at iteration [58]: 0.8695326203073034
Loss at iteration [59]: 0.8635800128604467
Loss at iteration [60]: 0.860411144938729
Loss at iteration [61]: 0.8590480613703281
Loss at iteration [62]: 0.857099745556942
Loss at iteration [63]: 0.8560583090850845
Loss at iteration [64]: 0.8539336940767642
Loss at iteration [65]: 0.8533061993659009
Loss at iteration [66]: 0.8502809397296215
Loss at iteration [67]: 0.8489626787833419
Loss at iteration [68]: 0.8432164644048599
Loss at iteration [69]: 0.8413191095676786
Loss at iteration [70]: 0.8413191095676786
Loss at iteration [71]: 0.8400226189123056
Loss at iteration [72]: 0.8318309072529898
Loss at iteration [73]: 0.8269469137757205
Loss at iteration [74]: 0.8256433234104169
Loss at iteration [75]: 0.8215150492759836
Loss at iteration [76]: 0.8211543290921425
Loss at iteration [77]: 0.8188383655226061
Loss at iteration [78]: 0.8179840719983595
Loss at iteration [79]: 0.8170936432387851
Loss at iteration [80]: 0.816445692445088
Loss at iteration [81]: 0.8157647996766921
Loss at iteration [82]: 0.8146743896425294
Loss at iteration [83]: 0.8146743896425294
Loss at iteration [84]: 0.8140661534646273
Loss at iteration [85]: 0.8106539018243766
Loss at iteration [86]: 0.8098404938835753
Loss at iteration [87]: 0.8074705660845609
Loss at iteration [88]: 0.806808141622069
Loss at iteration [89]: 0.8058046480139491
Loss at iteration [90]: 0.8056364856511252
Loss at iteration [91]: 0.80338790974722
Loss at iteration [92]: 0.8028613518862772
Loss at iteration [93]: 0.801402745894888
Loss at iteration [94]: 0.8008584556441168
Loss at iteration [95]: 0.7994127389951046
Loss at iteration [96]: 0.7994127389951046
Loss at iteration [97]: 0.7986081939489903
Loss at iteration [98]: 0.7959214788372264
Loss at iteration [99]: 0.7956690114289883
Loss at iteration [100]: 0.7945913663990927
Loss at iteration [101]: 0.7941999173536637
Loss at iteration [102]: 0.7920976317409862
Loss at iteration [103]: 0.7918325095526687
Loss at iteration [104]: 0.7899848342010942
Loss at iteration [105]: 0.7895515590401387
Loss at iteration [106]: 0.7885105664693937
Loss at iteration [107]: 0.7878593027803862
Loss at iteration [108]: 0.7869106536693091
Loss at iteration [109]: 0.7869106536693091
Loss at iteration [110]: 0.786581532560269
Loss at iteration [111]: 0.7839210466680332
Loss at iteration [112]: 0.7836340404558336
Loss at iteration [113]: 0.7823578375648458
Loss at iteration [114]: 0.7821787852888716
Loss at iteration [115]: 0.7812776767247593
Loss at iteration [116]: 0.7804450083725175
Loss at iteration [117]: 0.7801912868895174
Loss at iteration [118]: 0.7788246018656911
Loss at iteration [119]: 0.7774854152403875
Loss at iteration [120]: 0.7770805737755894
Loss at iteration [121]: 0.7758262332016845
Loss at iteration [122]: 0.7758262332016845
Loss at iteration [123]: 0.7753516320108742
Loss at iteration [124]: 0.7731641183589372
Loss at iteration [125]: 0.7726303464699279
Loss at iteration [126]: 0.7709215553843518
Loss at iteration [127]: 0.7699385113842616
Loss at iteration [128]: 0.7688788577814605
Loss at iteration [129]: 0.7681731595329261
Loss at iteration [130]: 0.7675571057541055
Loss at iteration [131]: 0.7671453343852932
Loss at iteration [132]: 0.7670071862527803
Loss at iteration [133]: 0.7666442085913127
Loss at iteration [134]: 0.7666442085913127
Loss at iteration [135]: 0.7661428804091238
Loss at iteration [136]: 0.7653080034883929
Loss at iteration [137]: 0.7649268830097853
Loss at iteration [138]: 0.7646232663055433
Loss at iteration [139]: 0.7644865420634199
Loss at iteration [140]: 0.7642649022997943
Loss at iteration [141]: 0.7641253691300692
Loss at iteration [142]: 0.7639802947889517
Loss at iteration [143]: 0.7638652279549276
Loss at iteration [144]: 0.7636571679833015
Loss at iteration [145]: 0.7636571679833015
Loss at iteration [146]: 0.7635118992719674
Loss at iteration [147]: 0.7633575736997069
Loss at iteration [148]: 0.7632173986250277
Loss at iteration [149]: 0.7630953836922769
Loss at iteration [150]: 0.7629585544630568
Loss at iteration [151]: 0.7626073353512474
Loss at iteration [152]: 0.7622166553076397
Loss at iteration [153]: 0.760847069101177
Loss at iteration [154]: 0.7605910664791299
Loss at iteration [155]: 0.7583563430655238
Loss at iteration [156]: 0.7583563430655238
Loss at iteration [157]: 0.7569533198025999
Loss at iteration [158]: 0.7560864715419162
Loss at iteration [159]: 0.7559530333553629
Loss at iteration [160]: 0.7557486738603787
Loss at iteration [161]: 0.7535000726622904
Loss at iteration [162]: 0.7509531960781465
Loss at iteration [163]: 0.7506341334465848
Loss at iteration [164]: 0.7493958710284195
Loss at iteration [165]: 0.7492635800573851
Loss at iteration [166]: 0.7486082817062385
Loss at iteration [167]: 0.7477598084490045
Loss at iteration [168]: 0.7467871693206652
Loss at iteration [169]: 0.7465618267385193
Loss at iteration [170]: 0.7465618267385193
Loss at iteration [171]: 0.7463353287139673
Loss at iteration [172]: 0.7458135917623322
Loss at iteration [173]: 0.7457023579497797
Loss at iteration [174]: 0.7452006623995301
Loss at iteration [175]: 0.7448143029919423
Loss at iteration [176]: 0.7446777401335404
Loss at iteration [177]: 0.7443664043956498
Loss at iteration [178]: 0.7441129440895736
Loss at iteration [179]: 0.7437026402044838
Loss at iteration [180]: 0.7435196227064532
Loss at iteration [181]: 0.7431266734426308
Loss at iteration [182]: 0.7431266734426308
Loss at iteration [183]: 0.7427799456210886
Loss at iteration [184]: 0.7426310915044755
Loss at iteration [185]: 0.7425718040226171
Loss at iteration [186]: 0.7423962266780009
Loss at iteration [187]: 0.7422160317157739
Loss at iteration [188]: 0.7421071487131552
Loss at iteration [189]: 0.7418889054441975
Loss at iteration [190]: 0.7417278799689473
Loss at iteration [191]: 0.7415232434923336
Loss at iteration [192]: 0.7415232434923336
Loss at iteration [193]: 0.7411049926420048
Loss at iteration [194]: 0.740906830200342
Loss at iteration [195]: 0.7408050012986712
Loss at iteration [196]: 0.7405954046868998
Loss at iteration [197]: 0.7404982073022549
Loss at iteration [198]: 0.7399583324855766
Loss at iteration [199]: 0.7396636456126335
Loss at iteration [200]: 0.7394715244985535
Loss at iteration [201]: 0.7387827553572391
Loss at iteration [202]: 0.738649366353219
Loss at iteration [203]: 0.7381469188293959
Loss at iteration [204]: 0.7381469188293959
Loss at iteration [205]: 0.7380171121239174
Loss at iteration [206]: 0.7376342093837569
Loss at iteration [207]: 0.7375710733886337
Loss at iteration [208]: 0.7374233370390919
Loss at iteration [209]: 0.7373504372206957
Loss at iteration [210]: 0.7372798540095702
Loss at iteration [211]: 0.7371729435006135
Loss at iteration [212]: 0.7370132685873182
Loss at iteration [213]: 0.7368316113059604
Loss at iteration [214]: 0.7368316113059604
Loss at iteration [215]: 0.736704042734881
Loss at iteration [216]: 0.7366251167210629
Loss at iteration [217]: 0.736551508921036
Loss at iteration [218]: 0.7364632523172987
Loss at iteration [219]: 0.7364019034303286
Loss at iteration [220]: 0.7363761159581037
Loss at iteration [221]: 0.7363165926371588
Loss at iteration [222]: 0.7362650106336068
Loss at iteration [223]: 0.7362350238876233
Loss at iteration [224]: 0.7362350238876233
Loss at iteration [225]: 0.7361870742120957
Loss at iteration [226]: 0.7361862371663258
Loss at iteration [227]: 0.7361582544858222
Loss at iteration [228]: 0.7361317343688671
Loss at iteration [229]: 0.7361246950298314
Loss at iteration [230]: 0.7360229853028821
Loss at iteration [231]: 0.7358835431573175
Loss at iteration [232]: 0.7358835431573175
Loss at iteration [233]: 0.7357906196191477
Loss at iteration [234]: 0.7357402798378646
Loss at iteration [235]: 0.7356911521141286
Loss at iteration [236]: 0.7356037266575994
Loss at iteration [237]: 0.7355450485173848
Loss at iteration [238]: 0.7354581965484038
Loss at iteration [239]: 0.7353550912870261
Loss at iteration [240]: 0.7350764310085286
Loss at iteration [241]: 0.7348883621013353
Loss at iteration [242]: 0.7348883621013353
Loss at iteration [243]: 0.7347636431689452
Loss at iteration [244]: 0.7342971263773227
Loss at iteration [245]: 0.7341353284883607
Loss at iteration [246]: 0.7340120644771221
Loss at iteration [247]: 0.7338326354482951
Loss at iteration [248]: 0.733734881888998
Loss at iteration [249]: 0.7335417449446084
Loss at iteration [250]: 0.7334504889694514
Loss at iteration [251]: 0.73320509438557
Loss at iteration [252]: 0.73320509438557
Loss at iteration [253]: 0.7331099664745754
Loss at iteration [254]: 0.7330342030259758
Loss at iteration [255]: 0.7329576232470839
Loss at iteration [256]: 0.7328938292449151
Loss at iteration [257]: 0.7328227238106303
Loss at iteration [258]: 0.7327505301891933
Loss at iteration [259]: 0.7326298098390619
Loss at iteration [260]: 0.7324494573824228
Loss at iteration [261]: 0.7324494573824228
Loss at iteration [262]: 0.7323561906353434
Loss at iteration [263]: 0.7322520565618871
Loss at iteration [264]: 0.7321890154871579
Loss at iteration [265]: 0.7321317901418398
Loss at iteration [266]: 0.7321233189640597
Loss at iteration [267]: 0.732043290615919
Loss at iteration [268]: 0.7319113339009607
Loss at iteration [269]: 0.7317630655720269
Loss at iteration [270]: 0.7315708637314786
Loss at iteration [271]: 0.7315708637314786
Loss at iteration [272]: 0.7313495847224133
Loss at iteration [273]: 0.7308921938138294
Loss at iteration [274]: 0.7308025186346185
Loss at iteration [275]: 0.7305703133154322
Loss at iteration [276]: 0.7303933922354717
Loss at iteration [277]: 0.7302855186220139
Loss at iteration [278]: 0.7300159438024422
Loss at iteration [279]: 0.7298489321522389
Loss at iteration [280]: 0.7295143172328006
Loss at iteration [281]: 0.7295143172328006
Loss at iteration [282]: 0.7293519295069909
Loss at iteration [283]: 0.7292313293257777
Loss at iteration [284]: 0.7291176636464799
Loss at iteration [285]: 0.7290576576493383
Loss at iteration [286]: 0.7289876807372276
Loss at iteration [287]: 0.728925299370371
Loss at iteration [288]: 0.7288118968521048
Loss at iteration [289]: 0.7287535505279816
Loss at iteration [290]: 0.7287535505279816
Loss at iteration [291]: 0.7286333378716305
Loss at iteration [292]: 0.7286004415505669
Loss at iteration [293]: 0.7285828182188063
Loss at iteration [294]: 0.7285563415962939
Loss at iteration [295]: 0.7285158560317925
Loss at iteration [296]: 0.7284598140049227
Loss at iteration [297]: 0.7282423939122573
Loss at iteration [298]: 0.7280814605663639
Loss at iteration [299]: 0.7280814605663639
Loss at iteration [300]: 0.7279423757845617
Loss at iteration [301]: 0.7277516909037185
Loss at iteration [302]: 0.7276600161829222
Loss at iteration [303]: 0.7275888219630905
Loss at iteration [304]: 0.727519664206374
Loss at iteration [305]: 0.7274474160720789
Loss at iteration [306]: 0.7273597195560383
Loss at iteration [307]: 0.7272536808498592
Loss at iteration [308]: 0.7271830935598593
Loss at iteration [309]: 0.7271490942328294
Loss at iteration [310]: 0.7271490942328294
Loss at iteration [311]: 0.7270589037274462
Loss at iteration [312]: 0.7270127951298353
Loss at iteration [313]: 0.7269815311082086
Loss at iteration [314]: 0.7269573778544687
Loss at iteration [315]: 0.7269560471177797
Loss at iteration [316]: 0.7269416026786573
Loss at iteration [317]: 0.7268503194264944
Loss at iteration [318]: 0.7268147271986171
Loss at iteration [319]: 0.7268147271986171
Loss at iteration [320]: 0.7266809274989378
Loss at iteration [321]: 0.7266135909914752
Loss at iteration [322]: 0.7265336182912546
Loss at iteration [323]: 0.7265221856514615
Loss at iteration [324]: 0.7264881900829903
Loss at iteration [325]: 0.7264478682038872
Loss at iteration [326]: 0.7263808205596592
Loss at iteration [327]: 0.7262103536720802
Loss at iteration [328]: 0.7262103536720802
Loss at iteration [329]: 0.7260316197409458
Loss at iteration [330]: 0.7259435027576702
Loss at iteration [331]: 0.7259019895321778
Loss at iteration [332]: 0.7258205797789418
Loss at iteration [333]: 0.7257786377134053
Loss at iteration [334]: 0.7257388348720394
Loss at iteration [335]: 0.7257338414970428
Loss at iteration [336]: 0.7257116025557209
Loss at iteration [337]: 0.7257116025557209
Loss at iteration [338]: 0.7255970669180373
Loss at iteration [339]: 0.725551513046006
Loss at iteration [340]: 0.7255220328374097
Loss at iteration [341]: 0.7254795167792579
Loss at iteration [342]: 0.725451546939737
Loss at iteration [343]: 0.7254422062403675
Loss at iteration [344]: 0.7254101982105075
Loss at iteration [345]: 0.7253818867515172
Loss at iteration [346]: 0.7253818867515172
Loss at iteration [347]: 0.7253297474600897
Loss at iteration [348]: 0.7253010166954122
Loss at iteration [349]: 0.7252781830390732
Loss at iteration [350]: 0.7252378321180812
Loss at iteration [351]: 0.7251901912625142
Loss at iteration [352]: 0.7251612521248518
Loss at iteration [353]: 0.7251285726701914
Loss at iteration [354]: 0.7249080621577504
Loss at iteration [355]: 0.7249080621577504
Loss at iteration [356]: 0.7247193776921795
Loss at iteration [357]: 0.7240233293502858
Loss at iteration [358]: 0.7238941484732659
Loss at iteration [359]: 0.7236613998119192
Loss at iteration [360]: 0.7235730481465469
Loss at iteration [361]: 0.7234258200529801
Loss at iteration [362]: 0.7233370706337405
Loss at iteration [363]: 0.7231680049435019
Loss at iteration [364]: 0.7231055827619299
Loss at iteration [365]: 0.7229841549086987
Loss at iteration [366]: 0.7229841549086987
Loss at iteration [367]: 0.7228601151754727
Loss at iteration [368]: 0.7228290908483419
Loss at iteration [369]: 0.7228070619061416
Loss at iteration [370]: 0.7227693303712959
Loss at iteration [371]: 0.7227392333980119
Loss at iteration [372]: 0.7226147443647589
Loss at iteration [373]: 0.7223814391230453
Loss at iteration [374]: 0.7222222149249261
Loss at iteration [375]: 0.7222222149249261
Loss at iteration [376]: 0.7221265969383664
Loss at iteration [377]: 0.7220610759320045
Loss at iteration [378]: 0.7219764426368305
Loss at iteration [379]: 0.7218862896152236
Loss at iteration [380]: 0.7218277581913113
Loss at iteration [381]: 0.7217791653004105
Loss at iteration [382]: 0.7217008807496669
Loss at iteration [383]: 0.7215969715686509
Loss at iteration [384]: 0.7215969715686509
Loss at iteration [385]: 0.7214958368965345
Loss at iteration [386]: 0.7214369228784042
Loss at iteration [387]: 0.7213960160862161
Loss at iteration [388]: 0.7213646747888831
Loss at iteration [389]: 0.7213026549566013
Loss at iteration [390]: 0.7212871805484558
Loss at iteration [391]: 0.7212558619728566
Loss at iteration [392]: 0.7211948274536012
Loss at iteration [393]: 0.7211948274536012
Loss at iteration [394]: 0.7211455756029259
Loss at iteration [395]: 0.7211262329665528
Loss at iteration [396]: 0.7211080013449508
Loss at iteration [397]: 0.7210917423798245
Loss at iteration [398]: 0.7210842219036305
Loss at iteration [399]: 0.7210653481969248
Loss at iteration [400]: 0.7210193145968465
Loss at iteration [401]: 0.7209211827742894
Loss at iteration [402]: 0.7209211827742894
Loss at iteration [403]: 0.7207667678507494
Loss at iteration [404]: 0.7207192107134834
Loss at iteration [405]: 0.7206643188132986
Loss at iteration [406]: 0.7206317463683763
Loss at iteration [407]: 0.7206048276503921
Loss at iteration [408]: 0.7206020754913524
Loss at iteration [409]: 0.7205558045761596
Loss at iteration [410]: 0.7204275336175141
Loss at iteration [411]: 0.7204275336175141
Loss at iteration [412]: 0.7199602420898322
Loss at iteration [413]: 0.7198946836150505
Loss at iteration [414]: 0.7198488312607582
Loss at iteration [415]: 0.7197585934633586
Loss at iteration [416]: 0.7196659625117592
Loss at iteration [417]: 0.719561833232531
Loss at iteration [418]: 0.7194728848811829
Loss at iteration [419]: 0.7193920602857459
Loss at iteration [420]: 0.7193920602857459
Loss at iteration [421]: 0.7192617069859523
Loss at iteration [422]: 0.719201526843483
Loss at iteration [423]: 0.7191722953669653
Loss at iteration [424]: 0.7191415825194593
Loss at iteration [425]: 0.7191010604854647
Loss at iteration [426]: 0.7190330893464777
Loss at iteration [427]: 0.7189459195571102
Loss at iteration [428]: 0.7188459685690238
Loss at iteration [429]: 0.7188459685690238
Loss at iteration [430]: 0.7187724949765639
Loss at iteration [431]: 0.7186555755028905
Loss at iteration [432]: 0.7185723391735233
Loss at iteration [433]: 0.7185394308071886
Loss at iteration [434]: 0.7184484859323187
Loss at iteration [435]: 0.7183640901760963
Loss at iteration [436]: 0.7183508989628073
Loss at iteration [437]: 0.71826998884114
Loss at iteration [438]: 0.71826998884114
Loss at iteration [439]: 0.7182033983321693
Loss at iteration [440]: 0.7181672002498256
Loss at iteration [441]: 0.7181236646811517
Loss at iteration [442]: 0.7180526112818525
Loss at iteration [443]: 0.7180172076840677
Loss at iteration [444]: 0.7179828593066696
Loss at iteration [445]: 0.7179276804982713
Loss at iteration [446]: 0.7178903221231748
Loss at iteration [447]: 0.7178903221231748
Loss at iteration [448]: 0.7178334591676121
Loss at iteration [449]: 0.717818034417992
Loss at iteration [450]: 0.7178036125543183
Loss at iteration [451]: 0.7177921754796491
Loss at iteration [452]: 0.717780042921093
Loss at iteration [453]: 0.7177152713736275
Loss at iteration [454]: 0.7176852278720205
Loss at iteration [455]: 0.7176666490720647
Loss at iteration [456]: 0.7176666490720647
Loss at iteration [457]: 0.7172551763887157
Loss at iteration [458]: 0.7166633601715355
Loss at iteration [459]: 0.7165974350932539
Loss at iteration [460]: 0.7164722312445821
Loss at iteration [461]: 0.7163889653142024
Loss at iteration [462]: 0.7162052546259629
Loss at iteration [463]: 0.7161222078084484
Loss at iteration [464]: 0.716060602968765
Loss at iteration [465]: 0.715969199729425
Loss at iteration [466]: 0.7158720386856271
Loss at iteration [467]: 0.7158720386856271
Loss at iteration [468]: 0.7157724141318745
Loss at iteration [469]: 0.7157136425345973
Loss at iteration [470]: 0.7156739236205432
Loss at iteration [471]: 0.715655438115158
Loss at iteration [472]: 0.715629700257069
Loss at iteration [473]: 0.7155946018337248
Loss at iteration [474]: 0.7155769614513654
Loss at iteration [475]: 0.7155769614513654
Loss at iteration [476]: 0.7155585155037193
Loss at iteration [477]: 0.7155459235740401
Loss at iteration [478]: 0.7155364018350441
Loss at iteration [479]: 0.7155243478455328
Loss at iteration [480]: 0.7155066795574142
Loss at iteration [481]: 0.7154961036904727
Loss at iteration [482]: 0.7154961036904727
Loss at iteration [483]: 0.7154481614767975
Loss at iteration [484]: 0.7154360213104388
Loss at iteration [485]: 0.715431218030789
Loss at iteration [486]: 0.7154209230498284
Loss at iteration [487]: 0.7154058251392051
Loss at iteration [488]: 0.7153753718437824
Loss at iteration [489]: 0.7152517070595582
Loss at iteration [490]: 0.7152517070595582
Loss at iteration [491]: 0.7151155263377093
Loss at iteration [492]: 0.7150885297662598
Loss at iteration [493]: 0.7150488483770694
Loss at iteration [494]: 0.7150240339118135
Loss at iteration [495]: 0.7150099068976727
Loss at iteration [496]: 0.7149833227416211
Loss at iteration [497]: 0.7149556695478456
Loss at iteration [498]: 0.7149556695478456
Loss at iteration [499]: 0.7149103310948263
Loss at iteration [500]: 0.7148978769686846
Loss at iteration [501]: 0.7148837495499001
Loss at iteration [502]: 0.7148677415225757
Loss at iteration [503]: 0.7148627647050677
Loss at iteration [504]: 0.7148478790596704
Loss at iteration [505]: 0.7148478790596704
Loss at iteration [506]: 0.7148162395285214
Loss at iteration [507]: 0.7148100359334126
Loss at iteration [508]: 0.7148054828236419
Loss at iteration [509]: 0.7147932010865699
Loss at iteration [510]: 0.7147843236518939
Loss at iteration [511]: 0.7147733786147897
Loss at iteration [512]: 0.7141583648843695
Loss at iteration [513]: 0.7141583648843695
Loss at iteration [514]: 0.7138371715344848
Loss at iteration [515]: 0.7132544103960144
Loss at iteration [516]: 0.7131612974056292
Loss at iteration [517]: 0.7126796290512319
Loss at iteration [518]: 0.712420023535707
Loss at iteration [519]: 0.7122761404253206
Loss at iteration [520]: 0.7120286780574052
Loss at iteration [521]: 0.7119112364892504
Loss at iteration [522]: 0.711798967136006
Loss at iteration [523]: 0.7116733254149902
Loss at iteration [524]: 0.7116733254149902
Loss at iteration [525]: 0.7114754731325811
Loss at iteration [526]: 0.7113541321159693
Loss at iteration [527]: 0.7112714631404344
Loss at iteration [528]: 0.711178275474936
Loss at iteration [529]: 0.7110749469778779
Loss at iteration [530]: 0.7110077597342637
Loss at iteration [531]: 0.7108405744902322
Loss at iteration [532]: 0.7107508989143257
Loss at iteration [533]: 0.7105856225194848
Loss at iteration [534]: 0.7105856225194848
Loss at iteration [535]: 0.7104178861849881
Loss at iteration [536]: 0.7103991956427376
Loss at iteration [537]: 0.7103357314550426
Loss at iteration [538]: 0.710303330456076
Loss at iteration [539]: 0.7102944090082062
Loss at iteration [540]: 0.7102338691554773
Loss at iteration [541]: 0.7101673261320718
Loss at iteration [542]: 0.7101221913955144
Loss at iteration [543]: 0.7101221913955144
Loss at iteration [544]: 0.7100153823456846
Loss at iteration [545]: 0.7099878026218595
Loss at iteration [546]: 0.7099455562250084
Loss at iteration [547]: 0.7099312191346293
Loss at iteration [548]: 0.7098927133615076
Loss at iteration [549]: 0.7098708344841275
Loss at iteration [550]: 0.7098406584651022
Loss at iteration [551]: 0.7098406584651022
Loss at iteration [552]: 0.7097902338110279
Loss at iteration [553]: 0.709777437692813
Loss at iteration [554]: 0.7097700636745236
Loss at iteration [555]: 0.7097644402988503
Loss at iteration [556]: 0.7097589609603685
Loss at iteration [557]: 0.709746954683334
Loss at iteration [558]: 0.709746954683334
Loss at iteration [559]: 0.7097173153595722
Loss at iteration [560]: 0.7097073853503667
Loss at iteration [561]: 0.7097049057615656
Loss at iteration [562]: 0.7096985199266652
Loss at iteration [563]: 0.7096801061899949
Loss at iteration [564]: 0.7096731418902118
Loss at iteration [565]: 0.7096731418902118
Loss at iteration [566]: 0.7096400799699536
Loss at iteration [567]: 0.7096279507048966
Loss at iteration [568]: 0.7096167273706854
Loss at iteration [569]: 0.7096098041541806
Loss at iteration [570]: 0.7096011371819679
Loss at iteration [571]: 0.7095969033298655
Loss at iteration [572]: 0.7095969033298655
Loss at iteration [573]: 0.7095561211355095
Loss at iteration [574]: 0.7095489572877467
Loss at iteration [575]: 0.7095482171749081
Loss at iteration [576]: 0.709534661769722
Loss at iteration [577]: 0.7095295291608017
Loss at iteration [578]: 0.7095172709127912
Loss at iteration [579]: 0.7095172709127912
Loss at iteration [580]: 0.7094916175055483
Loss at iteration [581]: 0.7094850660196362
Loss at iteration [582]: 0.7094805590423819
Loss at iteration [583]: 0.7094664521223593
Loss at iteration [584]: 0.7094618356530173
Loss at iteration [585]: 0.7094443296576155
Loss at iteration [586]: 0.7094443296576155
Loss at iteration [587]: 0.7094119595966891
Loss at iteration [588]: 0.7094065334712593
Loss at iteration [589]: 0.7094028624447286
Loss at iteration [590]: 0.7093862447830288
Loss at iteration [591]: 0.709372171686696
Loss at iteration [592]: 0.7093448291376478
Loss at iteration [593]: 0.7093448291376478
Loss at iteration [594]: 0.709326440128133
Loss at iteration [595]: 0.7093227276785985
Loss at iteration [596]: 0.7093207902693537
Loss at iteration [597]: 0.7093160455308464
Loss at iteration [598]: 0.7093075829712032
Loss at iteration [599]: 0.7093001410614641
Loss at iteration [600]: 0.7093001410614641
Loss at iteration [601]: 0.7092612181164808
Loss at iteration [602]: 0.7092525680211077
Loss at iteration [603]: 0.7092453591248414
Loss at iteration [604]: 0.7092372122443872
Loss at iteration [605]: 0.7092323019366067
Loss at iteration [606]: 0.7092306843077243
Loss at iteration [607]: 0.7092306843077243
Loss at iteration [608]: 0.7092079797600478
Loss at iteration [609]: 0.7092005578980021
Loss at iteration [610]: 0.7091972991502131
Loss at iteration [611]: 0.7091914402384156
Loss at iteration [612]: 0.7091806466824554
Loss at iteration [613]: 0.709163089147362
Loss at iteration [614]: 0.709163089147362
Loss at iteration [615]: 0.7091214141039897
Loss at iteration [616]: 0.7090994611239835
Loss at iteration [617]: 0.7090943683844847
Loss at iteration [618]: 0.7090862198053824
Loss at iteration [619]: 0.7090715402342557
Loss at iteration [620]: 0.7090497644842227
Loss at iteration [621]: 0.7090497644842227
Loss at iteration [622]: 0.7090278420263285
Loss at iteration [623]: 0.7090182055749774
Loss at iteration [624]: 0.7090044464784999
Loss at iteration [625]: 0.7089963205229083
Loss at iteration [626]: 0.7089912825387438
Loss at iteration [627]: 0.7089863964232855
Loss at iteration [628]: 0.7089863964232855
Loss at iteration [629]: 0.7089801141885439
Loss at iteration [630]: 0.708976847415138
Loss at iteration [631]: 0.7089714099468291
Loss at iteration [632]: 0.7089686730352344
Loss at iteration [633]: 0.7089647396973596
Loss at iteration [634]: 0.7089647396973596
Loss at iteration [635]: 0.7089583768624577
Loss at iteration [636]: 0.7089544336938919
Loss at iteration [637]: 0.7089535841019242
Loss at iteration [638]: 0.7089517326781598
Loss at iteration [639]: 0.7089499772697685
Loss at iteration [640]: 0.7089499772697685
Loss at iteration [641]: 0.7089399992604053
Loss at iteration [642]: 0.7089374605960325
Loss at iteration [643]: 0.7089349300184915
Loss at iteration [644]: 0.708928971522349
Loss at iteration [645]: 0.708924512996133
Loss at iteration [646]: 0.7089221017286591
Loss at iteration [647]: 0.7089221017286591
Loss at iteration [648]: 0.708910436661071
Loss at iteration [649]: 0.7089050772420004
Loss at iteration [650]: 0.7089013995878535
Loss at iteration [651]: 0.7088963554109712
Loss at iteration [652]: 0.7088915314083946
Loss at iteration [653]: 0.7088860900939397
Loss at iteration [654]: 0.7088860900939397
Loss at iteration [655]: 0.708880132028208
Loss at iteration [656]: 0.708875833149364
Loss at iteration [657]: 0.7088727173015062
Loss at iteration [658]: 0.7088712504297474
Loss at iteration [659]: 0.7088679942228814
Loss at iteration [660]: 0.7088679942228814
Loss at iteration [661]: 0.7088606470953541
Loss at iteration [662]: 0.70885979330203
Loss at iteration [663]: 0.7088571744135096
Loss at iteration [664]: 0.7088555600515359
Loss at iteration [665]: 0.7088542539348717
Loss at iteration [666]: 0.7088542539348717
Loss at iteration [667]: 0.7088526640106844
Loss at iteration [668]: 0.7088522728032067
Loss at iteration [669]: 0.708851555884268
Loss at iteration [670]: 0.7088499467188774
Loss at iteration [671]: 0.7088491848431997
Loss at iteration [672]: 0.7088491848431997
Loss at iteration [673]: 0.7088479942623326
Loss at iteration [674]: 0.708846845552133
Loss at iteration [675]: 0.7088453265152834
Loss at iteration [676]: 0.7088446500271967
Loss at iteration [677]: 0.7088429385069863
Loss at iteration [678]: 0.7088429385069863
Loss at iteration [679]: 0.7088389397056288
Loss at iteration [680]: 0.7088377428622195
Loss at iteration [681]: 0.7088361699685702
Loss at iteration [682]: 0.7088350436794426
Loss at iteration [683]: 0.7088334860106225
Loss at iteration [684]: 0.7088334860106225
Loss at iteration [685]: 0.7088319257462669
Loss at iteration [686]: 0.7088314275121393
Loss at iteration [687]: 0.7088311037065796
Loss at iteration [688]: 0.708830606753647
Loss at iteration [689]: 0.7088296782489174
Loss at iteration [690]: 0.7088296782489174
Loss at iteration [691]: 0.7088281351538743
Loss at iteration [692]: 0.7088274775566434
Loss at iteration [693]: 0.7088269635491492
Loss at iteration [694]: 0.708826330014632
Loss at iteration [695]: 0.7088259290378743
Loss at iteration [696]: 0.7088259290378743
Loss at iteration [697]: 0.708824960157812
Loss at iteration [698]: 0.7088246907203332
Loss at iteration [699]: 0.70882364241358
Loss at iteration [700]: 0.7088232771462659
Loss at iteration [701]: 0.7088232771462659
Loss at iteration [702]: 0.7088227203596081
Loss at iteration [703]: 0.7088223009654065
Loss at iteration [704]: 0.7088219150335604
Loss at iteration [705]: 0.7088213453248564
Loss at iteration [706]: 0.7088213453248564
Loss at iteration [707]: 0.7088211074380986
Loss at iteration [708]: 0.7088208851765407
Loss at iteration [709]: 0.7088206043447204
Loss at iteration [710]: 0.7088204331839663
Loss at iteration [711]: 0.7088204331839663
Loss at iteration [712]: 0.7088202599018352
Loss at iteration [713]: 0.7088200998584198
Loss at iteration [714]: 0.7088200753514098
Loss at iteration [715]: 0.7088199242662201
Loss at iteration [716]: 0.7088199242662201
Loss at iteration [717]: 0.7088194994513051
Loss at iteration [718]: 0.7088193625801079
Loss at iteration [719]: 0.7088193265026251
Loss at iteration [720]: 0.7088190612967624
Loss at iteration [721]: 0.7088190612967624
Loss at iteration [722]: 0.7088184021503771
Loss at iteration [723]: 0.7088181203180418
Loss at iteration [724]: 0.708817998241972
Loss at iteration [725]: 0.7088176419468776
Loss at iteration [726]: 0.7088176419468776
Loss at iteration [727]: 0.7088173650960348
Loss at iteration [728]: 0.7088172015069821
Loss at iteration [729]: 0.7088171103687828
Loss at iteration [730]: 0.7088170286836557
Loss at iteration [731]: 0.7088170286836557
Loss at iteration [732]: 0.7088165444722709
Loss at iteration [733]: 0.708816403737854
Loss at iteration [734]: 0.7088161359298167
Loss at iteration [735]: 0.708815975218344
Loss at iteration [736]: 0.708815975218344
Loss at iteration [737]: 0.7088156694868677
Loss at iteration [738]: 0.7088154350571331
Loss at iteration [739]: 0.708815408801035
Loss at iteration [740]: 0.7088153745034075
Loss at iteration [741]: 0.7088153745034075
Loss at iteration [742]: 0.708815052458191
Loss at iteration [743]: 0.7088149839444401
Loss at iteration [744]: 0.7088149464252893
Loss at iteration [745]: 0.708814848866633
Loss at iteration [746]: 0.708814848866633
Loss at iteration [747]: 0.7088146891593127
Loss at iteration [748]: 0.7088146420810102
Loss at iteration [749]: 0.7088145858219822
Loss at iteration [750]: 0.7088145847541962
Loss at iteration [751]: 0.7088145847541962
Loss at iteration [752]: 0.7088144766976637
Loss at iteration [753]: 0.7088144520923465
Loss at iteration [754]: 0.7088144293704365
Loss at iteration [755]: 0.7088143942645762
Loss at iteration [756]: 0.7088143942645762
Loss at iteration [757]: 0.7088143230677576
Loss at iteration [758]: 0.7088143104522294
Loss at iteration [759]: 0.7088143000075037
Loss at iteration [760]: 0.7088142875792525
Loss at iteration [761]: 0.7088142875792525
Loss at iteration [762]: 0.7088142346469165
Loss at iteration [763]: 0.7088142230771198
Loss at iteration [764]: 0.7088142111807426
Loss at iteration [765]: 0.7088141896987796
Loss at iteration [766]: 0.7088141896987796
Loss at iteration [767]: 0.7088141644803366
Loss at iteration [768]: 0.708814163470082
Loss at iteration [769]: 0.7088141513410164
Loss at iteration [770]: 0.7088141513410164
Loss at iteration [771]: 0.7088141423811289
Loss at iteration [772]: 0.7088141342464203
Loss at iteration [773]: 0.7088141279481655
Loss at iteration [774]: 0.7088141268021294
Loss at iteration [775]: 0.7088141268021294
Loss at iteration [776]: 0.7088141047206675
Loss at iteration [777]: 0.708814104042915
