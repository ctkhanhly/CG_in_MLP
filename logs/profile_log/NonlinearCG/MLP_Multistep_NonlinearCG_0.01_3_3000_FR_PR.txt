Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :FR_PR
Total number of function evaluations  : 1767
Total number of iterations            : 607
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 5.152330636978149
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 50.1575021545106%
Percentage of parameters < 1e-7       : 50.1575021545106%
Percentage of parameters < 1e-6       : 50.15898802389278%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.2284031748665656
Loss at iteration [2]: 1.2228719932241499
Loss at iteration [3]: 1.20626011171089
Loss at iteration [4]: 1.201374290015849
Loss at iteration [5]: 1.1956107534957872
Loss at iteration [6]: 1.1809162781746705
Loss at iteration [7]: 1.172149063172256
Loss at iteration [8]: 1.164438190486599
Loss at iteration [9]: 1.1530034500955335
Loss at iteration [10]: 1.1530034500955335
Loss at iteration [11]: 1.1497711307388738
Loss at iteration [12]: 1.1441897270832744
Loss at iteration [13]: 1.1385449892524393
Loss at iteration [14]: 1.1319754233166246
Loss at iteration [15]: 1.1248541217884869
Loss at iteration [16]: 1.1122380303042794
Loss at iteration [17]: 1.0939213014374802
Loss at iteration [18]: 1.0853581184884207
Loss at iteration [19]: 1.0750316180894697
Loss at iteration [20]: 1.0698146587749138
Loss at iteration [21]: 1.0698146587749138
Loss at iteration [22]: 1.0669671040997057
Loss at iteration [23]: 1.051925638952283
Loss at iteration [24]: 1.045024303134028
Loss at iteration [25]: 1.0377963155732082
Loss at iteration [26]: 1.0356617928713578
Loss at iteration [27]: 1.0292581795460054
Loss at iteration [28]: 1.0095032827899042
Loss at iteration [29]: 1.0057384612819698
Loss at iteration [30]: 0.9905326731897226
Loss at iteration [31]: 0.9822791770542246
Loss at iteration [32]: 0.9742507338573957
Loss at iteration [33]: 0.9720049778501629
Loss at iteration [34]: 0.9720049778501629
Loss at iteration [35]: 0.9706299781976762
Loss at iteration [36]: 0.96735156649405
Loss at iteration [37]: 0.9639145756335211
Loss at iteration [38]: 0.9613780666220471
Loss at iteration [39]: 0.9569302229408198
Loss at iteration [40]: 0.9537272918395548
Loss at iteration [41]: 0.9455511290845546
Loss at iteration [42]: 0.9384365279901615
Loss at iteration [43]: 0.9356301228013983
Loss at iteration [44]: 0.9336575608421407
Loss at iteration [45]: 0.9302307262877231
Loss at iteration [46]: 0.9302307262877231
Loss at iteration [47]: 0.9286668634222759
Loss at iteration [48]: 0.9263041680031519
Loss at iteration [49]: 0.9230857770871981
Loss at iteration [50]: 0.9213408855888495
Loss at iteration [51]: 0.9176372699227862
Loss at iteration [52]: 0.9156471841292002
Loss at iteration [53]: 0.911929978502379
Loss at iteration [54]: 0.9098875481216235
Loss at iteration [55]: 0.9070063302934817
Loss at iteration [56]: 0.9036256221029535
Loss at iteration [57]: 0.8973827620859114
Loss at iteration [58]: 0.8948859230942243
Loss at iteration [59]: 0.8801928596633314
Loss at iteration [60]: 0.8801928596633314
Loss at iteration [61]: 0.8787506181104832
Loss at iteration [62]: 0.8771258497842715
Loss at iteration [63]: 0.8746719510860013
Loss at iteration [64]: 0.8733878680535851
Loss at iteration [65]: 0.8710639705756923
Loss at iteration [66]: 0.8704664934214112
Loss at iteration [67]: 0.8675834022230888
Loss at iteration [68]: 0.8669173227190141
Loss at iteration [69]: 0.865973158191774
Loss at iteration [70]: 0.8653482550371825
Loss at iteration [71]: 0.8646344157149679
Loss at iteration [72]: 0.8638669580258957
Loss at iteration [73]: 0.8620907885756736
Loss at iteration [74]: 0.8607097555648201
Loss at iteration [75]: 0.858304558538083
Loss at iteration [76]: 0.8558129912594115
Loss at iteration [77]: 0.8558129912594115
Loss at iteration [78]: 0.8549811237835263
Loss at iteration [79]: 0.8540198970076588
Loss at iteration [80]: 0.853045213817294
Loss at iteration [81]: 0.8523072582122347
Loss at iteration [82]: 0.8511591748420783
Loss at iteration [83]: 0.8497528211202227
Loss at iteration [84]: 0.8483416797515719
Loss at iteration [85]: 0.8473850154056228
Loss at iteration [86]: 0.8457971050633125
Loss at iteration [87]: 0.8444263523041202
Loss at iteration [88]: 0.8437936413704962
Loss at iteration [89]: 0.8405953317775867
Loss at iteration [90]: 0.8395788625015383
Loss at iteration [91]: 0.838880670106913
Loss at iteration [92]: 0.838880670106913
Loss at iteration [93]: 0.838514977193544
Loss at iteration [94]: 0.8377014699922156
Loss at iteration [95]: 0.8373354411359086
Loss at iteration [96]: 0.8363568914865696
Loss at iteration [97]: 0.8343160305244587
Loss at iteration [98]: 0.8336261367503007
Loss at iteration [99]: 0.8331102994452899
Loss at iteration [100]: 0.832193096340939
Loss at iteration [101]: 0.8316770473856131
Loss at iteration [102]: 0.8311000463925899
Loss at iteration [103]: 0.8305853406433183
Loss at iteration [104]: 0.829356966101226
Loss at iteration [105]: 0.8288380692160063
Loss at iteration [106]: 0.827806055158783
Loss at iteration [107]: 0.8265978129202916
Loss at iteration [108]: 0.8257030875270223
Loss at iteration [109]: 0.8245449898603946
Loss at iteration [110]: 0.8223038247109742
Loss at iteration [111]: 0.8223038247109742
Loss at iteration [112]: 0.8214621025331533
Loss at iteration [113]: 0.8204930954339338
Loss at iteration [114]: 0.8204678249869962
Loss at iteration [115]: 0.820173955191463
Loss at iteration [116]: 0.8196182819238481
Loss at iteration [117]: 0.8185117268018598
Loss at iteration [118]: 0.817057031253218
Loss at iteration [119]: 0.8168802196938573
Loss at iteration [120]: 0.8166224015941356
Loss at iteration [121]: 0.8156133771378972
Loss at iteration [122]: 0.8149999000097536
Loss at iteration [123]: 0.8140351791964058
Loss at iteration [124]: 0.8133859572080316
Loss at iteration [125]: 0.8127451334406568
Loss at iteration [126]: 0.8123744987384561
Loss at iteration [127]: 0.8118697913775664
Loss at iteration [128]: 0.8114909459106032
Loss at iteration [129]: 0.8103954130861858
Loss at iteration [130]: 0.8103954130861858
Loss at iteration [131]: 0.8100737427605943
Loss at iteration [132]: 0.8094144211160823
Loss at iteration [133]: 0.8091027351992321
Loss at iteration [134]: 0.808674525028264
Loss at iteration [135]: 0.8084631422931841
Loss at iteration [136]: 0.8080033234570528
Loss at iteration [137]: 0.8075195394994703
Loss at iteration [138]: 0.8067705027016139
Loss at iteration [139]: 0.8055924123880811
Loss at iteration [140]: 0.8037704737395676
Loss at iteration [141]: 0.801683081875715
Loss at iteration [142]: 0.7995742996356967
Loss at iteration [143]: 0.7976562014539865
Loss at iteration [144]: 0.79515154546678
Loss at iteration [145]: 0.7936056092536727
Loss at iteration [146]: 0.7915243366030396
Loss at iteration [147]: 0.7894686650113696
Loss at iteration [148]: 0.7878348824863448
Loss at iteration [149]: 0.7864654802357898
Loss at iteration [150]: 0.7833576731287006
Loss at iteration [151]: 0.7833576731287006
Loss at iteration [152]: 0.7826034986089734
Loss at iteration [153]: 0.7815738275273034
Loss at iteration [154]: 0.7812480043188116
Loss at iteration [155]: 0.78055316877543
Loss at iteration [156]: 0.7802528497885505
Loss at iteration [157]: 0.7797865380448353
Loss at iteration [158]: 0.7794917052703739
Loss at iteration [159]: 0.7791580843108526
Loss at iteration [160]: 0.7788954774289146
Loss at iteration [161]: 0.7786466093340701
Loss at iteration [162]: 0.7783413732508945
Loss at iteration [163]: 0.7780077715342564
Loss at iteration [164]: 0.7776930394386401
Loss at iteration [165]: 0.7774204015187901
Loss at iteration [166]: 0.7772043209983802
Loss at iteration [167]: 0.777078026516578
Loss at iteration [168]: 0.7768169189135359
Loss at iteration [169]: 0.7768169189135359
Loss at iteration [170]: 0.7766182322841132
Loss at iteration [171]: 0.7765289387500475
Loss at iteration [172]: 0.7764309362278676
Loss at iteration [173]: 0.7762667063887712
Loss at iteration [174]: 0.7762388240855342
Loss at iteration [175]: 0.7761479203098599
Loss at iteration [176]: 0.7760447826298399
Loss at iteration [177]: 0.7758953767320701
Loss at iteration [178]: 0.7755279853400205
Loss at iteration [179]: 0.7752464055334211
Loss at iteration [180]: 0.7750127430056011
Loss at iteration [181]: 0.7746650129925623
Loss at iteration [182]: 0.7741857878716069
Loss at iteration [183]: 0.7737453869684054
Loss at iteration [184]: 0.7733240432423623
Loss at iteration [185]: 0.7733240432423623
Loss at iteration [186]: 0.7730976594381577
Loss at iteration [187]: 0.773029874776589
Loss at iteration [188]: 0.7729802559641745
Loss at iteration [189]: 0.7726704051327755
Loss at iteration [190]: 0.7724784240778787
Loss at iteration [191]: 0.7723936958157588
Loss at iteration [192]: 0.7722317690219537
Loss at iteration [193]: 0.7720878849454547
Loss at iteration [194]: 0.7719862247262588
Loss at iteration [195]: 0.7718468248667775
Loss at iteration [196]: 0.7715985344213283
Loss at iteration [197]: 0.7714163673264217
Loss at iteration [198]: 0.7709613750547653
Loss at iteration [199]: 0.7703797259079258
Loss at iteration [200]: 0.7695758605249633
Loss at iteration [201]: 0.7673862020873767
Loss at iteration [202]: 0.7665072489663146
Loss at iteration [203]: 0.7665072489663146
Loss at iteration [204]: 0.7657672626371069
Loss at iteration [205]: 0.7650814611116188
Loss at iteration [206]: 0.7647267736272243
Loss at iteration [207]: 0.7644542534569676
Loss at iteration [208]: 0.7641579066050954
Loss at iteration [209]: 0.7638662348892493
Loss at iteration [210]: 0.7637115575300665
Loss at iteration [211]: 0.7635690860130637
Loss at iteration [212]: 0.7633859529809672
Loss at iteration [213]: 0.7632072288675639
Loss at iteration [214]: 0.7629608860770438
Loss at iteration [215]: 0.7626888466274319
Loss at iteration [216]: 0.762338343492021
Loss at iteration [217]: 0.7621216514139679
Loss at iteration [218]: 0.7621216514139679
Loss at iteration [219]: 0.7619848779109251
Loss at iteration [220]: 0.7618263840565155
Loss at iteration [221]: 0.7617622580758678
Loss at iteration [222]: 0.7617219712172829
Loss at iteration [223]: 0.7616875628637801
Loss at iteration [224]: 0.7615390889608915
Loss at iteration [225]: 0.7612138533648642
Loss at iteration [226]: 0.7608363840320387
Loss at iteration [227]: 0.7606003304524281
Loss at iteration [228]: 0.7602918957777539
Loss at iteration [229]: 0.7598896754534982
Loss at iteration [230]: 0.7592236402971623
Loss at iteration [231]: 0.758309243177066
Loss at iteration [232]: 0.7552119448857786
Loss at iteration [233]: 0.7540337986542425
Loss at iteration [234]: 0.749588393247414
Loss at iteration [235]: 0.7475175448725725
Loss at iteration [236]: 0.7475175448725725
Loss at iteration [237]: 0.7467602945941655
Loss at iteration [238]: 0.744963936166974
Loss at iteration [239]: 0.744017874378692
Loss at iteration [240]: 0.7437193910188123
Loss at iteration [241]: 0.7432235547468468
Loss at iteration [242]: 0.7426479751101315
Loss at iteration [243]: 0.7419065953151514
Loss at iteration [244]: 0.74147395153764
Loss at iteration [245]: 0.7412053978649088
Loss at iteration [246]: 0.7409738831178768
Loss at iteration [247]: 0.7405041510260897
Loss at iteration [248]: 0.7399905972736777
Loss at iteration [249]: 0.7395815109179996
Loss at iteration [250]: 0.7393207127308966
Loss at iteration [251]: 0.7385494936632443
Loss at iteration [252]: 0.737610890280448
Loss at iteration [253]: 0.737610890280448
Loss at iteration [254]: 0.7372491652296718
Loss at iteration [255]: 0.7369832357288667
Loss at iteration [256]: 0.736717021949322
Loss at iteration [257]: 0.736637971280354
Loss at iteration [258]: 0.7364578143264817
Loss at iteration [259]: 0.7363034272560791
Loss at iteration [260]: 0.7360533247357406
Loss at iteration [261]: 0.7357079951746855
Loss at iteration [262]: 0.7352843672566962
Loss at iteration [263]: 0.7350079599971733
Loss at iteration [264]: 0.7348080833976027
Loss at iteration [265]: 0.7345744743435344
Loss at iteration [266]: 0.7340671332550973
Loss at iteration [267]: 0.7337640073484839
Loss at iteration [268]: 0.7331890910647503
Loss at iteration [269]: 0.7328926351523023
Loss at iteration [270]: 0.7328926351523023
Loss at iteration [271]: 0.7325523267600417
Loss at iteration [272]: 0.7321197991525431
Loss at iteration [273]: 0.7319531493938777
Loss at iteration [274]: 0.7318410546908984
Loss at iteration [275]: 0.7317393604949683
Loss at iteration [276]: 0.7316015722401012
Loss at iteration [277]: 0.7315039259824109
Loss at iteration [278]: 0.7314358576983967
Loss at iteration [279]: 0.7313157529054002
Loss at iteration [280]: 0.7311489217369233
Loss at iteration [281]: 0.7309854898069471
Loss at iteration [282]: 0.7306825961718983
Loss at iteration [283]: 0.7305348063163293
Loss at iteration [284]: 0.7305348063163293
Loss at iteration [285]: 0.730328188322931
Loss at iteration [286]: 0.7301945739615667
Loss at iteration [287]: 0.7300988831674168
Loss at iteration [288]: 0.7300502517117813
Loss at iteration [289]: 0.7299302737397001
Loss at iteration [290]: 0.7298509456682707
Loss at iteration [291]: 0.7298082112219638
Loss at iteration [292]: 0.7296972492530843
Loss at iteration [293]: 0.729621534609675
Loss at iteration [294]: 0.7294122864390574
Loss at iteration [295]: 0.7287688286232525
Loss at iteration [296]: 0.728371732930824
Loss at iteration [297]: 0.7277139416882326
Loss at iteration [298]: 0.7267481086231634
Loss at iteration [299]: 0.7267481086231634
Loss at iteration [300]: 0.7264185931969729
Loss at iteration [301]: 0.7258867312561345
Loss at iteration [302]: 0.7256982113736151
Loss at iteration [303]: 0.725439700829079
Loss at iteration [304]: 0.7253162739265652
Loss at iteration [305]: 0.7251628953823414
Loss at iteration [306]: 0.7250137820626213
Loss at iteration [307]: 0.7248632266483679
Loss at iteration [308]: 0.7247700389338804
Loss at iteration [309]: 0.724539583915242
Loss at iteration [310]: 0.7244332823478077
Loss at iteration [311]: 0.7242580825588304
Loss at iteration [312]: 0.7241180557012653
Loss at iteration [313]: 0.7241180557012653
Loss at iteration [314]: 0.7239814287573173
Loss at iteration [315]: 0.7239505123142745
Loss at iteration [316]: 0.7239231111341677
Loss at iteration [317]: 0.7238535169532279
Loss at iteration [318]: 0.7237927350668318
Loss at iteration [319]: 0.7237366468530612
Loss at iteration [320]: 0.7236738292267104
Loss at iteration [321]: 0.7235073902591205
Loss at iteration [322]: 0.7234460473456639
Loss at iteration [323]: 0.7233737800668056
Loss at iteration [324]: 0.7232257946803552
Loss at iteration [325]: 0.7231542166684736
Loss at iteration [326]: 0.7231542166684736
Loss at iteration [327]: 0.7229924315731752
Loss at iteration [328]: 0.7229410465582167
Loss at iteration [329]: 0.7229300001008641
Loss at iteration [330]: 0.7228841458570724
Loss at iteration [331]: 0.722845313702038
Loss at iteration [332]: 0.7227810212599689
Loss at iteration [333]: 0.7227300865155171
Loss at iteration [334]: 0.722671656512656
Loss at iteration [335]: 0.7226220956888132
Loss at iteration [336]: 0.7226048356472577
Loss at iteration [337]: 0.7223949643584939
Loss at iteration [338]: 0.7222980519584496
Loss at iteration [339]: 0.7222980519584496
Loss at iteration [340]: 0.7222264283584489
Loss at iteration [341]: 0.7221540749771277
Loss at iteration [342]: 0.7221157460549732
Loss at iteration [343]: 0.7220963605470073
Loss at iteration [344]: 0.7220708404203027
Loss at iteration [345]: 0.7220269292768459
Loss at iteration [346]: 0.722016537124
Loss at iteration [347]: 0.7219768374817775
Loss at iteration [348]: 0.72195264559632
Loss at iteration [349]: 0.7219404042084328
Loss at iteration [350]: 0.7219404042084328
Loss at iteration [351]: 0.7218536970916042
Loss at iteration [352]: 0.7218357600838751
Loss at iteration [353]: 0.7218154920013992
Loss at iteration [354]: 0.7218007859935133
Loss at iteration [355]: 0.7217861582750523
Loss at iteration [356]: 0.721762343754733
Loss at iteration [357]: 0.7217516851945756
Loss at iteration [358]: 0.7217387685296974
Loss at iteration [359]: 0.7217134548787851
Loss at iteration [360]: 0.7217134548787851
Loss at iteration [361]: 0.7216874211235258
Loss at iteration [362]: 0.7216767432952135
Loss at iteration [363]: 0.7216665003638169
Loss at iteration [364]: 0.7216609258335267
Loss at iteration [365]: 0.7216598981826866
Loss at iteration [366]: 0.7216456539773038
Loss at iteration [367]: 0.7216351677418665
Loss at iteration [368]: 0.7214184086238928
Loss at iteration [369]: 0.720954021886017
Loss at iteration [370]: 0.720188402599417
Loss at iteration [371]: 0.720188402599417
Loss at iteration [372]: 0.7200048328246345
Loss at iteration [373]: 0.7198146537336936
Loss at iteration [374]: 0.7196770139204531
Loss at iteration [375]: 0.7195878310563821
Loss at iteration [376]: 0.7195442509014037
Loss at iteration [377]: 0.7194114811970939
Loss at iteration [378]: 0.7193740129931875
Loss at iteration [379]: 0.7193408459833177
Loss at iteration [380]: 0.7193237480822373
Loss at iteration [381]: 0.7191635561884078
Loss at iteration [382]: 0.7190950189652564
Loss at iteration [383]: 0.7190674079474078
Loss at iteration [384]: 0.7190674079474078
Loss at iteration [385]: 0.7188363299166871
Loss at iteration [386]: 0.7188081491581757
Loss at iteration [387]: 0.718764260072814
Loss at iteration [388]: 0.7187539500791809
Loss at iteration [389]: 0.7187280036780136
Loss at iteration [390]: 0.7187143810875986
Loss at iteration [391]: 0.7186431677522883
Loss at iteration [392]: 0.7186246091484157
Loss at iteration [393]: 0.7185818283488028
Loss at iteration [394]: 0.7184374597467034
Loss at iteration [395]: 0.7183781140749306
Loss at iteration [396]: 0.7183781140749306
Loss at iteration [397]: 0.7182701806131547
Loss at iteration [398]: 0.7181803807099317
Loss at iteration [399]: 0.7181737995720946
Loss at iteration [400]: 0.7181485002907867
Loss at iteration [401]: 0.7181256985640927
Loss at iteration [402]: 0.7180842731174442
Loss at iteration [403]: 0.718061340567576
Loss at iteration [404]: 0.7180151101121336
Loss at iteration [405]: 0.7179633765127451
Loss at iteration [406]: 0.7179156470718991
Loss at iteration [407]: 0.7179156470718991
Loss at iteration [408]: 0.7179017303181146
Loss at iteration [409]: 0.7178674371573741
Loss at iteration [410]: 0.7178441804426087
Loss at iteration [411]: 0.7178196810595316
Loss at iteration [412]: 0.7177908216220655
Loss at iteration [413]: 0.7177720757622177
Loss at iteration [414]: 0.7177003272788084
Loss at iteration [415]: 0.7176837208215717
Loss at iteration [416]: 0.7176702398789986
Loss at iteration [417]: 0.7176702398789986
Loss at iteration [418]: 0.717640830473501
Loss at iteration [419]: 0.717634128589182
Loss at iteration [420]: 0.7176303152866945
Loss at iteration [421]: 0.7176288846696001
Loss at iteration [422]: 0.7176220018336128
Loss at iteration [423]: 0.7176115136189832
Loss at iteration [424]: 0.7176053548180917
Loss at iteration [425]: 0.7175812313206614
Loss at iteration [426]: 0.7175812313206614
Loss at iteration [427]: 0.7175639754655266
Loss at iteration [428]: 0.7175551456850568
Loss at iteration [429]: 0.7175469733249049
Loss at iteration [430]: 0.7175404015777332
Loss at iteration [431]: 0.7175290067164108
Loss at iteration [432]: 0.7175247170753207
Loss at iteration [433]: 0.7175173593892786
Loss at iteration [434]: 0.7175149125884075
Loss at iteration [435]: 0.7175149125884075
Loss at iteration [436]: 0.717495125205864
Loss at iteration [437]: 0.7174891004635214
Loss at iteration [438]: 0.7174831396137935
Loss at iteration [439]: 0.717480074097352
Loss at iteration [440]: 0.717479077042683
Loss at iteration [441]: 0.7174643003586872
Loss at iteration [442]: 0.7174592856222316
Loss at iteration [443]: 0.7174183706542101
Loss at iteration [444]: 0.7174183706542101
Loss at iteration [445]: 0.7173222589817276
Loss at iteration [446]: 0.7172848613499555
Loss at iteration [447]: 0.7172570274002312
Loss at iteration [448]: 0.717217277474049
Loss at iteration [449]: 0.7171847624876695
Loss at iteration [450]: 0.7171643843230557
Loss at iteration [451]: 0.7171479966811948
Loss at iteration [452]: 0.7171353035598053
Loss at iteration [453]: 0.7171242275274958
Loss at iteration [454]: 0.7171242275274958
Loss at iteration [455]: 0.7171056467932445
Loss at iteration [456]: 0.7170987567790318
Loss at iteration [457]: 0.7170942392967841
Loss at iteration [458]: 0.7170831406040041
Loss at iteration [459]: 0.7170784669424093
Loss at iteration [460]: 0.7170738031362859
Loss at iteration [461]: 0.7170700660566669
Loss at iteration [462]: 0.7170652766076202
Loss at iteration [463]: 0.7170652766076202
Loss at iteration [464]: 0.7170527806848319
Loss at iteration [465]: 0.7170464025708106
Loss at iteration [466]: 0.7170435757508138
Loss at iteration [467]: 0.7170400137073246
Loss at iteration [468]: 0.7170313510630374
Loss at iteration [469]: 0.7170231315312923
Loss at iteration [470]: 0.7170175863171704
Loss at iteration [471]: 0.7170175863171704
Loss at iteration [472]: 0.71700666580623
Loss at iteration [473]: 0.7170053076026971
Loss at iteration [474]: 0.7170040525191688
Loss at iteration [475]: 0.7170020565649241
Loss at iteration [476]: 0.7170008957999139
Loss at iteration [477]: 0.7169987822826894
Loss at iteration [478]: 0.7169987822826894
Loss at iteration [479]: 0.7169960490630084
Loss at iteration [480]: 0.7169944436199972
Loss at iteration [481]: 0.7169929524453148
Loss at iteration [482]: 0.7169918292147663
Loss at iteration [483]: 0.7169903650581687
Loss at iteration [484]: 0.7169891157783671
Loss at iteration [485]: 0.7169891157783671
Loss at iteration [486]: 0.7169874790444473
Loss at iteration [487]: 0.716986450154398
Loss at iteration [488]: 0.7169851458160716
Loss at iteration [489]: 0.7169848032885715
Loss at iteration [490]: 0.7169837591135971
Loss at iteration [491]: 0.7169797024500768
Loss at iteration [492]: 0.7169797024500768
Loss at iteration [493]: 0.7169785284517786
Loss at iteration [494]: 0.7169777591855918
Loss at iteration [495]: 0.716976417693646
Loss at iteration [496]: 0.7169758882119546
Loss at iteration [497]: 0.7169745214469742
Loss at iteration [498]: 0.7169736591324101
Loss at iteration [499]: 0.7169736591324101
Loss at iteration [500]: 0.7169735913508654
Loss at iteration [501]: 0.7169723836118066
Loss at iteration [502]: 0.7169715495167124
Loss at iteration [503]: 0.7169710385300753
Loss at iteration [504]: 0.7169707087152002
Loss at iteration [505]: 0.7169700540964055
Loss at iteration [506]: 0.7169700540964055
Loss at iteration [507]: 0.7169688719583732
Loss at iteration [508]: 0.7169681340753127
Loss at iteration [509]: 0.71696745962246
Loss at iteration [510]: 0.7169673622451224
Loss at iteration [511]: 0.7169665748451323
Loss at iteration [512]: 0.7169656209373994
Loss at iteration [513]: 0.7169656209373994
Loss at iteration [514]: 0.7169639826537273
Loss at iteration [515]: 0.7169624618305138
Loss at iteration [516]: 0.7169613391787786
Loss at iteration [517]: 0.7169609274459745
Loss at iteration [518]: 0.7169600431108296
Loss at iteration [519]: 0.716959638351132
Loss at iteration [520]: 0.716959638351132
Loss at iteration [521]: 0.7169575301132702
Loss at iteration [522]: 0.7169571944915016
Loss at iteration [523]: 0.7169569241739123
Loss at iteration [524]: 0.716956467301169
Loss at iteration [525]: 0.7169554315357124
Loss at iteration [526]: 0.716955319734805
Loss at iteration [527]: 0.716955319734805
Loss at iteration [528]: 0.7169529724775536
Loss at iteration [529]: 0.7169522843884462
Loss at iteration [530]: 0.716950575233437
Loss at iteration [531]: 0.7169503362129017
Loss at iteration [532]: 0.716949613628839
Loss at iteration [533]: 0.7169470235462839
Loss at iteration [534]: 0.7169470235462839
Loss at iteration [535]: 0.7169457339839396
Loss at iteration [536]: 0.7169454274227901
Loss at iteration [537]: 0.7169438400773693
Loss at iteration [538]: 0.7169431388525136
Loss at iteration [539]: 0.7169427200892395
Loss at iteration [540]: 0.7169410390023195
Loss at iteration [541]: 0.7169410390023195
Loss at iteration [542]: 0.7169407795516803
Loss at iteration [543]: 0.7169402089645921
Loss at iteration [544]: 0.7169383333363151
Loss at iteration [545]: 0.7169377754852462
Loss at iteration [546]: 0.7169376763638292
Loss at iteration [547]: 0.7169368052230886
Loss at iteration [548]: 0.7169368052230886
Loss at iteration [549]: 0.7169338988022366
Loss at iteration [550]: 0.7169325732693431
Loss at iteration [551]: 0.7169318003188782
Loss at iteration [552]: 0.7169309678984833
Loss at iteration [553]: 0.7169309447169534
Loss at iteration [554]: 0.7169298653276618
Loss at iteration [555]: 0.7169298653276618
Loss at iteration [556]: 0.7169252376336461
Loss at iteration [557]: 0.7169239345758526
Loss at iteration [558]: 0.7169234596317698
Loss at iteration [559]: 0.7169222512523034
Loss at iteration [560]: 0.7169220510218568
Loss at iteration [561]: 0.7169211645050722
Loss at iteration [562]: 0.7169211645050722
Loss at iteration [563]: 0.7169186495491866
Loss at iteration [564]: 0.7169177180468724
Loss at iteration [565]: 0.7169172841454035
Loss at iteration [566]: 0.7169164375014914
Loss at iteration [567]: 0.7169149814275364
Loss at iteration [568]: 0.7169139869497924
Loss at iteration [569]: 0.7169139869497924
Loss at iteration [570]: 0.7169132582903612
Loss at iteration [571]: 0.7169119087711829
Loss at iteration [572]: 0.7169109280804065
Loss at iteration [573]: 0.7169101242283838
Loss at iteration [574]: 0.7169095666772323
Loss at iteration [575]: 0.7169087545368246
Loss at iteration [576]: 0.7169087545368246
Loss at iteration [577]: 0.7169073649359508
Loss at iteration [578]: 0.7169060253008129
Loss at iteration [579]: 0.7169051750509075
Loss at iteration [580]: 0.7169048828535934
Loss at iteration [581]: 0.716904401744768
Loss at iteration [582]: 0.7169037590939493
Loss at iteration [583]: 0.7169037590939493
Loss at iteration [584]: 0.7169031484099891
Loss at iteration [585]: 0.7169025640599815
Loss at iteration [586]: 0.7169018807625234
Loss at iteration [587]: 0.7169007844360108
Loss at iteration [588]: 0.7169005046765696
Loss at iteration [589]: 0.716900262943468
Loss at iteration [590]: 0.716900262943468
Loss at iteration [591]: 0.7168997376612073
Loss at iteration [592]: 0.7168993788533342
Loss at iteration [593]: 0.7168972935555745
Loss at iteration [594]: 0.7168966841133916
Loss at iteration [595]: 0.7168964106104135
Loss at iteration [596]: 0.7168956674684266
Loss at iteration [597]: 0.7168956674684266
Loss at iteration [598]: 0.7168952218575415
Loss at iteration [599]: 0.7168950602784694
Loss at iteration [600]: 0.7168949386260769
Loss at iteration [601]: 0.7168946533403657
Loss at iteration [602]: 0.7168944124224939
Loss at iteration [603]: 0.7168944124224939
Loss at iteration [604]: 0.7168941509473136
Loss at iteration [605]: 0.7168939140372858
Loss at iteration [606]: 0.7168938301123023
Loss at iteration [607]: 0.7168937191418664
Loss at iteration [608]: 0.7168936866107554
Loss at iteration [609]: 0.7168936866107554
Loss at iteration [610]: 0.7168933756131942
Loss at iteration [611]: 0.7168933047628524
Loss at iteration [612]: 0.7168932516894265
Loss at iteration [613]: 0.7168932042229144
Loss at iteration [614]: 0.716893149547145
Loss at iteration [615]: 0.716893149547145
Loss at iteration [616]: 0.7168929943605982
Loss at iteration [617]: 0.7168929646581168
Loss at iteration [618]: 0.7168929012895687
Loss at iteration [619]: 0.7168928341835364
Loss at iteration [620]: 0.7168927844805008
Loss at iteration [621]: 0.7168927844805008
Loss at iteration [622]: 0.7168926915735881
Loss at iteration [623]: 0.7168926683161132
Loss at iteration [624]: 0.7168925967996808
Loss at iteration [625]: 0.7168925544602961
Loss at iteration [626]: 0.7168924484294965
Loss at iteration [627]: 0.7168924484294965
Loss at iteration [628]: 0.7168922801856392
Loss at iteration [629]: 0.7168922314643762
Loss at iteration [630]: 0.7168920397243214
Loss at iteration [631]: 0.7168919561006649
Loss at iteration [632]: 0.7168918907273921
Loss at iteration [633]: 0.7168918907273921
Loss at iteration [634]: 0.7168917901316474
Loss at iteration [635]: 0.716891734291168
Loss at iteration [636]: 0.7168916778604444
Loss at iteration [637]: 0.7168916552501293
Loss at iteration [638]: 0.7168916552501293
Loss at iteration [639]: 0.7168916112202048
Loss at iteration [640]: 0.7168916025664287
Loss at iteration [641]: 0.7168915810496636
Loss at iteration [642]: 0.7168915552259645
Loss at iteration [643]: 0.7168915552259645
Loss at iteration [644]: 0.7168915187794973
Loss at iteration [645]: 0.7168914903773694
Loss at iteration [646]: 0.7168914824653934
Loss at iteration [647]: 0.7168914820752692
Loss at iteration [648]: 0.7168914820752692
Loss at iteration [649]: 0.7168914484892686
Loss at iteration [650]: 0.716891442454801
Loss at iteration [651]: 0.7168914385424467
Loss at iteration [652]: 0.7168914326803578
Loss at iteration [653]: 0.7168914326803578
Loss at iteration [654]: 0.7168914184152594
Loss at iteration [655]: 0.7168914125623446
Loss at iteration [656]: 0.7168913987087524
Loss at iteration [657]: 0.7168913904796395
Loss at iteration [658]: 0.7168913904796395
Loss at iteration [659]: 0.7168913732995693
Loss at iteration [660]: 0.7168913680003647
Loss at iteration [661]: 0.7168913672561275
Loss at iteration [662]: 0.7168913672561275
Loss at iteration [663]: 0.7168913640846539
Loss at iteration [664]: 0.7168913640508977
Loss at iteration [665]: 0.7168913640508977
Loss at iteration [666]: 0.7168913611947529
Loss at iteration [667]: 0.7168913595071914
Loss at iteration [668]: 0.7168913587114965
