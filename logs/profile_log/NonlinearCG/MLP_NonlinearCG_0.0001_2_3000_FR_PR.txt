Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :FR_PR
Total number of function evaluations  : 3047
Total number of iterations            : 582
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 13.467781066894531
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.854951931169%
Percentage of parameters < 1e-7       : 49.854951931169%
Percentage of parameters < 1e-6       : 49.854951931169%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.003135576368922453
Loss at iteration [2]: 0.003041193750886519
Loss at iteration [3]: 0.002894969573740981
Loss at iteration [4]: 0.002848972652092518
Loss at iteration [5]: 0.002794025657007505
Loss at iteration [6]: 0.002670160275996328
Loss at iteration [7]: 0.002670160275996328
Loss at iteration [8]: 0.002653810988732378
Loss at iteration [9]: 0.0026389292501853147
Loss at iteration [10]: 0.0026224329515539585
Loss at iteration [11]: 0.0026135788823154088
Loss at iteration [12]: 0.0025664047112197333
Loss at iteration [13]: 0.0025664047112197333
Loss at iteration [14]: 0.0025492158683765196
Loss at iteration [15]: 0.0025462107938849475
Loss at iteration [16]: 0.0025330579672106952
Loss at iteration [17]: 0.002532212988814149
Loss at iteration [18]: 0.0025273360868574607
Loss at iteration [19]: 0.0025162784222179813
Loss at iteration [20]: 0.0025162784222179813
Loss at iteration [21]: 0.0025156977414085573
Loss at iteration [22]: 0.0025148262016516957
Loss at iteration [23]: 0.002512114551087536
Loss at iteration [24]: 0.0025114218223987136
Loss at iteration [25]: 0.0025100706502044477
Loss at iteration [26]: 0.0025100706502044477
Loss at iteration [27]: 0.0025097143325137185
Loss at iteration [28]: 0.0025086168393015453
Loss at iteration [29]: 0.0025070393431929757
Loss at iteration [30]: 0.00250622989558768
Loss at iteration [31]: 0.0025036956466595052
Loss at iteration [32]: 0.0025036956466595052
Loss at iteration [33]: 0.002502867603836405
Loss at iteration [34]: 0.002502663022036008
Loss at iteration [35]: 0.0025013683739584522
Loss at iteration [36]: 0.002501087136549839
Loss at iteration [37]: 0.002500804968825347
Loss at iteration [38]: 0.002500804968825347
Loss at iteration [39]: 0.00250055002326563
Loss at iteration [40]: 0.0025002651346817126
Loss at iteration [41]: 0.0024995650699552265
Loss at iteration [42]: 0.002499123808110372
Loss at iteration [43]: 0.002498195633622362
Loss at iteration [44]: 0.002498195633622362
Loss at iteration [45]: 0.002498041912632208
Loss at iteration [46]: 0.0024979230156880862
Loss at iteration [47]: 0.0024970427514552614
Loss at iteration [48]: 0.0024962860769158013
Loss at iteration [49]: 0.0024910669372503113
Loss at iteration [50]: 0.0024910669372503113
Loss at iteration [51]: 0.0024901845534277206
Loss at iteration [52]: 0.002490085285233868
Loss at iteration [53]: 0.002489946400435678
Loss at iteration [54]: 0.0024895947579338298
Loss at iteration [55]: 0.0024888719404576945
Loss at iteration [56]: 0.0024888719404576945
Loss at iteration [57]: 0.002488743374430637
Loss at iteration [58]: 0.00248828417609722
Loss at iteration [59]: 0.002487516850702044
Loss at iteration [60]: 0.0024856507941594477
Loss at iteration [61]: 0.002481614560367842
Loss at iteration [62]: 0.002481614560367842
Loss at iteration [63]: 0.0024806770847266655
Loss at iteration [64]: 0.00248054856679853
Loss at iteration [65]: 0.0024791486693622774
Loss at iteration [66]: 0.0024786929944893587
Loss at iteration [67]: 0.002478499284717551
Loss at iteration [68]: 0.002478499284717551
Loss at iteration [69]: 0.0024782589983384433
Loss at iteration [70]: 0.0024781755889180432
Loss at iteration [71]: 0.0024776413385870152
Loss at iteration [72]: 0.0024773712653273507
Loss at iteration [73]: 0.002476928655230339
Loss at iteration [74]: 0.002476928655230339
Loss at iteration [75]: 0.0024767633160673384
Loss at iteration [76]: 0.002476633540177679
Loss at iteration [77]: 0.0024762093989356337
Loss at iteration [78]: 0.0024761744916349843
Loss at iteration [79]: 0.00247603812156936
Loss at iteration [80]: 0.00247603812156936
Loss at iteration [81]: 0.0024759540949177198
Loss at iteration [82]: 0.0024758396149934282
Loss at iteration [83]: 0.002475279957968445
Loss at iteration [84]: 0.0024752412429135087
Loss at iteration [85]: 0.0024751816517510016
Loss at iteration [86]: 0.0024751816517510016
Loss at iteration [87]: 0.002475128190644841
Loss at iteration [88]: 0.0024750923431885257
Loss at iteration [89]: 0.0024749742212225407
Loss at iteration [90]: 0.002474642967109387
Loss at iteration [91]: 0.0024728184870721043
Loss at iteration [92]: 0.0024728184870721043
Loss at iteration [93]: 0.002472664189146241
Loss at iteration [94]: 0.0024725245458205904
Loss at iteration [95]: 0.002472297029616992
Loss at iteration [96]: 0.0024720700840591904
Loss at iteration [97]: 0.002471910142831973
Loss at iteration [98]: 0.002471910142831973
Loss at iteration [99]: 0.0024718278577685326
Loss at iteration [100]: 0.002471761727410053
Loss at iteration [101]: 0.002471676274632356
Loss at iteration [102]: 0.0024715975730057304
Loss at iteration [103]: 0.002471329570384979
Loss at iteration [104]: 0.002471235129871177
Loss at iteration [105]: 0.002471235129871177
Loss at iteration [106]: 0.002471187634161205
Loss at iteration [107]: 0.0024711356332302904
Loss at iteration [108]: 0.0024709978515269856
Loss at iteration [109]: 0.0024708388808092337
Loss at iteration [110]: 0.0024706170517795677
Loss at iteration [111]: 0.0024706170517795677
Loss at iteration [112]: 0.002470570283176033
Loss at iteration [113]: 0.002470405847294658
Loss at iteration [114]: 0.00247029131618593
Loss at iteration [115]: 0.0024701860447036996
Loss at iteration [116]: 0.0024701514418356734
Loss at iteration [117]: 0.0024701514418356734
Loss at iteration [118]: 0.0024701251344679495
Loss at iteration [119]: 0.002470059092678549
Loss at iteration [120]: 0.002469698535683044
Loss at iteration [121]: 0.002469432439338275
Loss at iteration [122]: 0.002468909608385906
Loss at iteration [123]: 0.002468909608385906
Loss at iteration [124]: 0.0024686216063244375
Loss at iteration [125]: 0.00246853692583111
Loss at iteration [126]: 0.002468481827592765
Loss at iteration [127]: 0.002468299592677073
Loss at iteration [128]: 0.0024681276131547428
Loss at iteration [129]: 0.0024680735954964476
Loss at iteration [130]: 0.0024680735954964476
Loss at iteration [131]: 0.002468032874827818
Loss at iteration [132]: 0.002467974994458708
Loss at iteration [133]: 0.0024678697848093062
Loss at iteration [134]: 0.002467769160833383
Loss at iteration [135]: 0.0024676838798021925
Loss at iteration [136]: 0.0024674470962089204
Loss at iteration [137]: 0.0024674470962089204
Loss at iteration [138]: 0.0024674193344108733
Loss at iteration [139]: 0.0024673957416475913
Loss at iteration [140]: 0.0024672744087810665
Loss at iteration [141]: 0.002466869405435305
Loss at iteration [142]: 0.0024666420450287644
Loss at iteration [143]: 0.0024666420450287644
Loss at iteration [144]: 0.002466533697242921
Loss at iteration [145]: 0.0024662380417388324
Loss at iteration [146]: 0.002466182670244416
Loss at iteration [147]: 0.002466050223892481
Loss at iteration [148]: 0.0024660040418649017
Loss at iteration [149]: 0.0024660040418649017
Loss at iteration [150]: 0.0024659136688723383
Loss at iteration [151]: 0.0024658872147931294
Loss at iteration [152]: 0.0024657952786844416
Loss at iteration [153]: 0.002465531761668574
Loss at iteration [154]: 0.0024650379084737726
Loss at iteration [155]: 0.0024650379084737726
Loss at iteration [156]: 0.0024649651511708635
Loss at iteration [157]: 0.0024648426898234107
Loss at iteration [158]: 0.002464785980969865
Loss at iteration [159]: 0.0024647480225381303
Loss at iteration [160]: 0.0024647114811170824
Loss at iteration [161]: 0.0024647114811170824
Loss at iteration [162]: 0.0024646825885942508
Loss at iteration [163]: 0.0024646406645300197
Loss at iteration [164]: 0.0024645713268772055
Loss at iteration [165]: 0.0024643262938735143
Loss at iteration [166]: 0.0024641567730991436
Loss at iteration [167]: 0.0024641567730991436
Loss at iteration [168]: 0.0024640536751781472
Loss at iteration [169]: 0.002464002515112115
Loss at iteration [170]: 0.0024639735826823828
Loss at iteration [171]: 0.0024638284830952634
Loss at iteration [172]: 0.002463211214649264
Loss at iteration [173]: 0.002463211214649264
Loss at iteration [174]: 0.002463068430654735
Loss at iteration [175]: 0.002463008562612314
Loss at iteration [176]: 0.0024629580989122867
Loss at iteration [177]: 0.0024626662670586066
Loss at iteration [178]: 0.00246246162952231
Loss at iteration [179]: 0.00246246162952231
Loss at iteration [180]: 0.002462407376734601
Loss at iteration [181]: 0.0024623800981562072
Loss at iteration [182]: 0.002462260264607708
Loss at iteration [183]: 0.0024621268162822478
Loss at iteration [184]: 0.002462010812885972
Loss at iteration [185]: 0.002462010812885972
Loss at iteration [186]: 0.0024618791339076087
Loss at iteration [187]: 0.002461840392482779
Loss at iteration [188]: 0.002461810739342595
Loss at iteration [189]: 0.0024616919095473877
Loss at iteration [190]: 0.0024615932594928523
Loss at iteration [191]: 0.0024614946007001114
Loss at iteration [192]: 0.0024614946007001114
Loss at iteration [193]: 0.0024614652369389298
Loss at iteration [194]: 0.002461423614698905
Loss at iteration [195]: 0.002461327869750431
Loss at iteration [196]: 0.0024612525611468375
Loss at iteration [197]: 0.0024611831582121322
Loss at iteration [198]: 0.0024611831582121322
Loss at iteration [199]: 0.0024611325536407157
Loss at iteration [200]: 0.0024610847296677855
Loss at iteration [201]: 0.002461052392664042
Loss at iteration [202]: 0.0024609600715625483
Loss at iteration [203]: 0.0024608775354187503
Loss at iteration [204]: 0.0024608775354187503
Loss at iteration [205]: 0.002460840188368419
Loss at iteration [206]: 0.0024608202383995578
Loss at iteration [207]: 0.0024607899572753884
Loss at iteration [208]: 0.0024607442405510053
Loss at iteration [209]: 0.002460714391815797
Loss at iteration [210]: 0.0024606674574405175
Loss at iteration [211]: 0.0024606674574405175
Loss at iteration [212]: 0.002460644728102118
Loss at iteration [213]: 0.002460618629769404
Loss at iteration [214]: 0.002459915677920402
Loss at iteration [215]: 0.0024592494641791376
Loss at iteration [216]: 0.00245761893714197
Loss at iteration [217]: 0.00245761893714197
Loss at iteration [218]: 0.0024569767909512733
Loss at iteration [219]: 0.0024569098421641893
Loss at iteration [220]: 0.002456237381940843
Loss at iteration [221]: 0.002455832430574592
Loss at iteration [222]: 0.0024554968145652158
Loss at iteration [223]: 0.0024554968145652158
Loss at iteration [224]: 0.0024553293011621903
Loss at iteration [225]: 0.0024552571134429514
Loss at iteration [226]: 0.0024549266096367717
Loss at iteration [227]: 0.002454814218274858
Loss at iteration [228]: 0.002454741503900285
Loss at iteration [229]: 0.002454538687394772
Loss at iteration [230]: 0.002454538687394772
Loss at iteration [231]: 0.0024544796934489435
Loss at iteration [232]: 0.0024544119990237245
Loss at iteration [233]: 0.0024543394772752742
Loss at iteration [234]: 0.002454265792866512
Loss at iteration [235]: 0.0024541397076032773
Loss at iteration [236]: 0.0024541397076032773
Loss at iteration [237]: 0.0024540857961578375
Loss at iteration [238]: 0.002454052355054331
Loss at iteration [239]: 0.002453994323972263
Loss at iteration [240]: 0.0024539119348001574
Loss at iteration [241]: 0.002453852327753048
Loss at iteration [242]: 0.0024535528483839936
Loss at iteration [243]: 0.0024535528483839936
Loss at iteration [244]: 0.0024533295458982036
Loss at iteration [245]: 0.002453164535183816
Loss at iteration [246]: 0.00245312429195963
Loss at iteration [247]: 0.002453076360993127
Loss at iteration [248]: 0.002453040234793268
Loss at iteration [249]: 0.002452996216269293
Loss at iteration [250]: 0.002452996216269293
Loss at iteration [251]: 0.002452971587989452
Loss at iteration [252]: 0.002452909770625373
Loss at iteration [253]: 0.0024527869330834766
Loss at iteration [254]: 0.002452690937558349
Loss at iteration [255]: 0.0024523940183147834
Loss at iteration [256]: 0.0024523940183147834
Loss at iteration [257]: 0.002452351706616688
Loss at iteration [258]: 0.0024522293459083473
Loss at iteration [259]: 0.002452202118714622
Loss at iteration [260]: 0.002452170662841969
Loss at iteration [261]: 0.002452074861540771
Loss at iteration [262]: 0.002452074861540771
Loss at iteration [263]: 0.0024520429482228876
Loss at iteration [264]: 0.0024520211343892744
Loss at iteration [265]: 0.0024519600693551163
Loss at iteration [266]: 0.002451930478112481
Loss at iteration [267]: 0.0024518087804612947
Loss at iteration [268]: 0.0024518087804612947
Loss at iteration [269]: 0.002451766849604494
Loss at iteration [270]: 0.002451748397836891
Loss at iteration [271]: 0.002451683648055505
Loss at iteration [272]: 0.0024516511185101305
Loss at iteration [273]: 0.002451573359141077
Loss at iteration [274]: 0.0024513342232068457
Loss at iteration [275]: 0.0024513342232068457
Loss at iteration [276]: 0.002451290730600923
Loss at iteration [277]: 0.00245123102118753
Loss at iteration [278]: 0.0024511751433541683
Loss at iteration [279]: 0.002451112153843741
Loss at iteration [280]: 0.002450960040288079
Loss at iteration [281]: 0.0024507538927689898
Loss at iteration [282]: 0.0024507538927689898
Loss at iteration [283]: 0.00245062614416743
Loss at iteration [284]: 0.002450593929377228
Loss at iteration [285]: 0.0024505259237797418
Loss at iteration [286]: 0.0024504670051712432
Loss at iteration [287]: 0.0024503206681283337
Loss at iteration [288]: 0.0024503206681283337
Loss at iteration [289]: 0.0024502708110869043
Loss at iteration [290]: 0.002450191716508438
Loss at iteration [291]: 0.0024501479856259014
Loss at iteration [292]: 0.0024501056941790243
Loss at iteration [293]: 0.002450056634763623
Loss at iteration [294]: 0.002450056634763623
Loss at iteration [295]: 0.002450029591762586
Loss at iteration [296]: 0.0024500011317164913
Loss at iteration [297]: 0.0024499564683627536
Loss at iteration [298]: 0.0024497688508367888
Loss at iteration [299]: 0.0024493369709155067
Loss at iteration [300]: 0.0024493369709155067
Loss at iteration [301]: 0.002449215254879043
Loss at iteration [302]: 0.0024490643238243804
Loss at iteration [303]: 0.002448974674531352
Loss at iteration [304]: 0.0024487884978539576
Loss at iteration [305]: 0.0024487242625015845
Loss at iteration [306]: 0.0024487242625015845
Loss at iteration [307]: 0.0024486949985077586
Loss at iteration [308]: 0.002448629725247575
Loss at iteration [309]: 0.0024485751126923846
Loss at iteration [310]: 0.0024485140486135695
Loss at iteration [311]: 0.002448481275114628
Loss at iteration [312]: 0.002448481275114628
Loss at iteration [313]: 0.0024484667223449988
Loss at iteration [314]: 0.002448416371483608
Loss at iteration [315]: 0.0024483924899788177
Loss at iteration [316]: 0.002448333938722067
Loss at iteration [317]: 0.002448136345765139
Loss at iteration [318]: 0.002448136345765139
Loss at iteration [319]: 0.0024479291881646975
Loss at iteration [320]: 0.002447873969192551
Loss at iteration [321]: 0.0024477031695144488
Loss at iteration [322]: 0.0024476674420428834
Loss at iteration [323]: 0.002447632140070828
Loss at iteration [324]: 0.002447632140070828
Loss at iteration [325]: 0.0024476169223216293
Loss at iteration [326]: 0.0024475920361392194
Loss at iteration [327]: 0.0024475091725756756
Loss at iteration [328]: 0.00244738382894115
Loss at iteration [329]: 0.0024469756270506446
Loss at iteration [330]: 0.0024469756270506446
Loss at iteration [331]: 0.0024467155295004
Loss at iteration [332]: 0.0024465745980422884
Loss at iteration [333]: 0.002446166610336867
Loss at iteration [334]: 0.0024460555784695573
Loss at iteration [335]: 0.0024459801503480556
Loss at iteration [336]: 0.0024459801503480556
Loss at iteration [337]: 0.002445917832779393
Loss at iteration [338]: 0.0024458837995318013
Loss at iteration [339]: 0.002445859539630378
Loss at iteration [340]: 0.0024458234361096334
Loss at iteration [341]: 0.0024457303637517715
Loss at iteration [342]: 0.0024457303637517715
Loss at iteration [343]: 0.0024456707246197544
Loss at iteration [344]: 0.002445629553451764
Loss at iteration [345]: 0.002445618276827861
Loss at iteration [346]: 0.0024454496527063067
Loss at iteration [347]: 0.0024453290926790597
Loss at iteration [348]: 0.0024453290926790597
Loss at iteration [349]: 0.0024452139107710965
Loss at iteration [350]: 0.002445156566327746
Loss at iteration [351]: 0.0024450934893972794
Loss at iteration [352]: 0.002445050324983876
Loss at iteration [353]: 0.0024450201049028984
Loss at iteration [354]: 0.0024450201049028984
Loss at iteration [355]: 0.0024450041381052067
Loss at iteration [356]: 0.0024449631622609816
Loss at iteration [357]: 0.002444835219535169
Loss at iteration [358]: 0.0024447335985888245
Loss at iteration [359]: 0.0024446630315840463
Loss at iteration [360]: 0.0024446630315840463
Loss at iteration [361]: 0.0024446307266464003
Loss at iteration [362]: 0.0024446001682730525
Loss at iteration [363]: 0.0024445604502696936
Loss at iteration [364]: 0.002444520550275719
Loss at iteration [365]: 0.00244446900913981
Loss at iteration [366]: 0.0024443586443216488
Loss at iteration [367]: 0.0024443586443216488
Loss at iteration [368]: 0.0024443298490206982
Loss at iteration [369]: 0.0024442294604990734
Loss at iteration [370]: 0.002444209944688858
Loss at iteration [371]: 0.0024440664160040337
Loss at iteration [372]: 0.00244391946952418
Loss at iteration [373]: 0.00244391946952418
Loss at iteration [374]: 0.002443867390867646
Loss at iteration [375]: 0.002443839568561028
Loss at iteration [376]: 0.0024438105603395224
Loss at iteration [377]: 0.002443743841970312
Loss at iteration [378]: 0.002443667221123836
Loss at iteration [379]: 0.0024436530658601156
Loss at iteration [380]: 0.0024436530658601156
Loss at iteration [381]: 0.0024436411524486135
Loss at iteration [382]: 0.002443587924629345
Loss at iteration [383]: 0.0024435185470264376
Loss at iteration [384]: 0.0024434790673925158
Loss at iteration [385]: 0.0024434521625172653
Loss at iteration [386]: 0.0024434521625172653
Loss at iteration [387]: 0.00244342718132224
Loss at iteration [388]: 0.002443396790912368
Loss at iteration [389]: 0.0024433525146226754
Loss at iteration [390]: 0.0024432521184543973
Loss at iteration [391]: 0.0024431164160447104
Loss at iteration [392]: 0.0024431164160447104
Loss at iteration [393]: 0.0024430627354044584
Loss at iteration [394]: 0.0024429934724115488
Loss at iteration [395]: 0.002442947368517856
Loss at iteration [396]: 0.0024429307754280745
Loss at iteration [397]: 0.0024429172250743174
Loss at iteration [398]: 0.0024429172250743174
Loss at iteration [399]: 0.002442902859887309
Loss at iteration [400]: 0.002442854136906857
Loss at iteration [401]: 0.0024424762932895366
Loss at iteration [402]: 0.0024416198372647665
Loss at iteration [403]: 0.002441295889583614
Loss at iteration [404]: 0.002441295889583614
Loss at iteration [405]: 0.00244120671543186
Loss at iteration [406]: 0.0024408563352745134
Loss at iteration [407]: 0.002440737243511533
Loss at iteration [408]: 0.002440559706000281
Loss at iteration [409]: 0.002440464148446979
Loss at iteration [410]: 0.002440464148446979
Loss at iteration [411]: 0.0024404290501045586
Loss at iteration [412]: 0.0024402636846131906
Loss at iteration [413]: 0.0024402182971603455
Loss at iteration [414]: 0.00244012214545141
Loss at iteration [415]: 0.0024400600693282462
Loss at iteration [416]: 0.002440024871254503
Loss at iteration [417]: 0.002440024871254503
Loss at iteration [418]: 0.0024399490789991725
Loss at iteration [419]: 0.0024397986283313145
Loss at iteration [420]: 0.002439741232234192
Loss at iteration [421]: 0.0024389811340029734
Loss at iteration [422]: 0.0024389811340029734
Loss at iteration [423]: 0.0024388073464630122
Loss at iteration [424]: 0.0024385781779631905
Loss at iteration [425]: 0.0024383170787374777
Loss at iteration [426]: 0.0024382849649620193
Loss at iteration [427]: 0.002438168185682513
Loss at iteration [428]: 0.002438168185682513
Loss at iteration [429]: 0.00243811114136616
Loss at iteration [430]: 0.0024380856762277092
Loss at iteration [431]: 0.0024380234288508373
Loss at iteration [432]: 0.002437929786845199
Loss at iteration [433]: 0.002437871078011293
Loss at iteration [434]: 0.002437871078011293
Loss at iteration [435]: 0.0024378428436125175
Loss at iteration [436]: 0.0024377679411455347
Loss at iteration [437]: 0.002437639147984924
Loss at iteration [438]: 0.0024375572937207703
Loss at iteration [439]: 0.0024373862578004096
Loss at iteration [440]: 0.0024373862578004096
Loss at iteration [441]: 0.0024372026659399103
Loss at iteration [442]: 0.002437152861621755
Loss at iteration [443]: 0.0024370814213574453
Loss at iteration [444]: 0.0024370374997374476
Loss at iteration [445]: 0.0024370229360557508
Loss at iteration [446]: 0.0024369042378767514
Loss at iteration [447]: 0.0024369042378767514
Loss at iteration [448]: 0.0024368191748020004
Loss at iteration [449]: 0.0024367975383497212
Loss at iteration [450]: 0.0024367611302014676
Loss at iteration [451]: 0.0024366590359493736
Loss at iteration [452]: 0.002436441892195942
Loss at iteration [453]: 0.002436441892195942
Loss at iteration [454]: 0.0024363591750662947
Loss at iteration [455]: 0.0024362907054606515
Loss at iteration [456]: 0.0024362646929092843
Loss at iteration [457]: 0.0024362193715683008
Loss at iteration [458]: 0.002436194248466827
Loss at iteration [459]: 0.002436194248466827
Loss at iteration [460]: 0.002436154217437678
Loss at iteration [461]: 0.0024361249800992912
Loss at iteration [462]: 0.0024360653449860123
Loss at iteration [463]: 0.002435861408550244
Loss at iteration [464]: 0.0024357179493229835
Loss at iteration [465]: 0.0024357179493229835
Loss at iteration [466]: 0.002435699397547204
Loss at iteration [467]: 0.0024356043926857794
Loss at iteration [468]: 0.002435563233147867
Loss at iteration [469]: 0.0024355346349894543
Loss at iteration [470]: 0.0024355162033319567
Loss at iteration [471]: 0.0024355162033319567
Loss at iteration [472]: 0.002435508148735935
Loss at iteration [473]: 0.0024354151792157917
Loss at iteration [474]: 0.0024351323433389085
Loss at iteration [475]: 0.002434761522644579
Loss at iteration [476]: 0.0024345846646953817
Loss at iteration [477]: 0.0024345846646953817
Loss at iteration [478]: 0.0024344449838621103
Loss at iteration [479]: 0.0024343435895848755
Loss at iteration [480]: 0.00243414785311445
Loss at iteration [481]: 0.002433965172287543
Loss at iteration [482]: 0.0024335900208289963
Loss at iteration [483]: 0.0024335900208289963
Loss at iteration [484]: 0.002433537127242522
Loss at iteration [485]: 0.002433428367823468
Loss at iteration [486]: 0.0024333258493537016
Loss at iteration [487]: 0.002433290903631109
Loss at iteration [488]: 0.002433098614163201
Loss at iteration [489]: 0.002433098614163201
Loss at iteration [490]: 0.0024330339574660936
Loss at iteration [491]: 0.0024329960767775615
Loss at iteration [492]: 0.0024329432885234537
Loss at iteration [493]: 0.0024328793058840275
Loss at iteration [494]: 0.0024328280544715073
Loss at iteration [495]: 0.0024328280544715073
Loss at iteration [496]: 0.002432796285884667
Loss at iteration [497]: 0.002432782352570201
Loss at iteration [498]: 0.002432708452341869
Loss at iteration [499]: 0.002432405932327934
Loss at iteration [500]: 0.0024322951908315378
Loss at iteration [501]: 0.0024322951908315378
Loss at iteration [502]: 0.002432223524506666
Loss at iteration [503]: 0.00243194459045612
Loss at iteration [504]: 0.002431915076643578
Loss at iteration [505]: 0.002431847153711573
Loss at iteration [506]: 0.0024316883249230555
Loss at iteration [507]: 0.0024316883249230555
Loss at iteration [508]: 0.002431668185599867
Loss at iteration [509]: 0.002431585152717562
Loss at iteration [510]: 0.0024314993110057543
Loss at iteration [511]: 0.0024312912620517084
Loss at iteration [512]: 0.0024312146315875844
Loss at iteration [513]: 0.0024312146315875844
Loss at iteration [514]: 0.002431139377176194
Loss at iteration [515]: 0.002431084517549785
Loss at iteration [516]: 0.002431056211378785
Loss at iteration [517]: 0.0024310094789872184
Loss at iteration [518]: 0.00243092128358039
Loss at iteration [519]: 0.00243092128358039
Loss at iteration [520]: 0.002430868142802764
Loss at iteration [521]: 0.0024308508352729793
Loss at iteration [522]: 0.0024308042672791536
Loss at iteration [523]: 0.002430701245013595
Loss at iteration [524]: 0.0024306194426378445
Loss at iteration [525]: 0.0024304482874294956
Loss at iteration [526]: 0.0024304482874294956
Loss at iteration [527]: 0.00243026705996484
Loss at iteration [528]: 0.0024301639366272098
Loss at iteration [529]: 0.0024301255964941223
Loss at iteration [530]: 0.002430017438939805
Loss at iteration [531]: 0.0024299868172337506
Loss at iteration [532]: 0.0024299868172337506
Loss at iteration [533]: 0.002429977868496599
Loss at iteration [534]: 0.0024299057100540776
Loss at iteration [535]: 0.0024298618480753575
Loss at iteration [536]: 0.0024298115848991634
Loss at iteration [537]: 0.0024297349557546796
Loss at iteration [538]: 0.0024297349557546796
Loss at iteration [539]: 0.002429706414133005
Loss at iteration [540]: 0.002429577040241788
Loss at iteration [541]: 0.0024295489646826355
Loss at iteration [542]: 0.0024295204394997284
Loss at iteration [543]: 0.00242945147551854
Loss at iteration [544]: 0.00242945147551854
Loss at iteration [545]: 0.002429331741149834
Loss at iteration [546]: 0.0024292753220940028
Loss at iteration [547]: 0.0024292598120555843
Loss at iteration [548]: 0.002429249409625713
Loss at iteration [549]: 0.002429215419704881
Loss at iteration [550]: 0.002429215419704881
Loss at iteration [551]: 0.0024292028494079547
Loss at iteration [552]: 0.002429154964168817
Loss at iteration [553]: 0.002429125038948375
Loss at iteration [554]: 0.0024290861366107483
Loss at iteration [555]: 0.0024290412978640115
Loss at iteration [556]: 0.0024290412978640115
Loss at iteration [557]: 0.002429030833475041
Loss at iteration [558]: 0.002429012912297393
Loss at iteration [559]: 0.0024289792167189396
Loss at iteration [560]: 0.0024289384650594097
Loss at iteration [561]: 0.0024288801060959745
Loss at iteration [562]: 0.0024288801060959745
Loss at iteration [563]: 0.0024288361226347363
Loss at iteration [564]: 0.0024287244067086916
Loss at iteration [565]: 0.0024287008387053237
Loss at iteration [566]: 0.0024286796039111277
Loss at iteration [567]: 0.0024286537992982064
Loss at iteration [568]: 0.0024286537992982064
Loss at iteration [569]: 0.002428622894858758
Loss at iteration [570]: 0.0024285968936763394
Loss at iteration [571]: 0.00242857864156563
Loss at iteration [572]: 0.0024285512162443164
Loss at iteration [573]: 0.0024285156588277576
Loss at iteration [574]: 0.0024285156588277576
Loss at iteration [575]: 0.0024284963900343855
Loss at iteration [576]: 0.0024284902243472974
Loss at iteration [577]: 0.0024284732343606416
Loss at iteration [578]: 0.0024284564965407606
Loss at iteration [579]: 0.0024284318714159945
Loss at iteration [580]: 0.0024284318714159945
Loss at iteration [581]: 0.0024284164143213035
Loss at iteration [582]: 0.0024284118289570433
Loss at iteration [583]: 0.0024283948819248044
Loss at iteration [584]: 0.002428363320600618
Loss at iteration [585]: 0.0024283339629981152
Loss at iteration [586]: 0.0024283027379236083
Loss at iteration [587]: 0.0024283027379236083
Loss at iteration [588]: 0.0024282670076823587
Loss at iteration [589]: 0.002428240860218834
Loss at iteration [590]: 0.0024282255796200677
Loss at iteration [591]: 0.002428222815239414
Loss at iteration [592]: 0.0024281979058789435
Loss at iteration [593]: 0.0024280933373373136
Loss at iteration [594]: 0.0024280933373373136
Loss at iteration [595]: 0.002428072608225839
Loss at iteration [596]: 0.002428046984932346
Loss at iteration [597]: 0.0024280196868646073
Loss at iteration [598]: 0.0024279621698194563
Loss at iteration [599]: 0.0024279142741597767
Loss at iteration [600]: 0.0024279142741597767
Loss at iteration [601]: 0.002427878909669934
Loss at iteration [602]: 0.0024278621736785293
Loss at iteration [603]: 0.002427802924032359
Loss at iteration [604]: 0.0024277077025021006
Loss at iteration [605]: 0.0024276025792872697
Loss at iteration [606]: 0.0024276025792872697
Loss at iteration [607]: 0.0024275653154006436
Loss at iteration [608]: 0.002427534990096964
Loss at iteration [609]: 0.002427486483937163
Loss at iteration [610]: 0.0024274261687047705
Loss at iteration [611]: 0.002427360024375834
Loss at iteration [612]: 0.002427360024375834
Loss at iteration [613]: 0.0024273239831903663
Loss at iteration [614]: 0.0024273010804303262
Loss at iteration [615]: 0.0024272763465249974
Loss at iteration [616]: 0.002427234064899371
Loss at iteration [617]: 0.002427156181473435
Loss at iteration [618]: 0.002427156181473435
Loss at iteration [619]: 0.002427127604905767
Loss at iteration [620]: 0.0024270747020815613
Loss at iteration [621]: 0.0024270352283178795
Loss at iteration [622]: 0.002426992408147861
Loss at iteration [623]: 0.0024269688845916224
Loss at iteration [624]: 0.0024269688845916224
Loss at iteration [625]: 0.002426929888947125
Loss at iteration [626]: 0.0024268858042923286
Loss at iteration [627]: 0.002426853781823268
Loss at iteration [628]: 0.002426816817460406
Loss at iteration [629]: 0.002426783495754846
Loss at iteration [630]: 0.002426783495754846
Loss at iteration [631]: 0.0024267791645516284
Loss at iteration [632]: 0.0024267698997470035
Loss at iteration [633]: 0.002426730123314002
Loss at iteration [634]: 0.002426699816592046
Loss at iteration [635]: 0.0024266188105315626
Loss at iteration [636]: 0.0024266188105315626
Loss at iteration [637]: 0.002426572018179885
Loss at iteration [638]: 0.002426547246839017
Loss at iteration [639]: 0.0024265043529876013
Loss at iteration [640]: 0.002426473331660246
Loss at iteration [641]: 0.0024264596736852297
Loss at iteration [642]: 0.0024264596736852297
Loss at iteration [643]: 0.002426438761701161
Loss at iteration [644]: 0.0024264367195518774
Loss at iteration [645]: 0.0024264258693259405
Loss at iteration [646]: 0.00242632437867079
Loss at iteration [647]: 0.0024262608399520276
Loss at iteration [648]: 0.0024262608399520276
Loss at iteration [649]: 0.0024262090201866864
Loss at iteration [650]: 0.0024261604074658108
Loss at iteration [651]: 0.0024261267067893213
Loss at iteration [652]: 0.0024261043074554345
Loss at iteration [653]: 0.002426090968774452
Loss at iteration [654]: 0.002426090968774452
Loss at iteration [655]: 0.002426086272736279
Loss at iteration [656]: 0.002426080850507603
Loss at iteration [657]: 0.0024260320664686863
Loss at iteration [658]: 0.0024259665646879374
Loss at iteration [659]: 0.0024258337662788217
Loss at iteration [660]: 0.0024258337662788217
Loss at iteration [661]: 0.0024257761293508956
Loss at iteration [662]: 0.0024257423762202943
Loss at iteration [663]: 0.0024256629377279585
Loss at iteration [664]: 0.002425652225921799
Loss at iteration [665]: 0.0024256370821981272
Loss at iteration [666]: 0.0024256370821981272
Loss at iteration [667]: 0.002425619900846304
Loss at iteration [668]: 0.0024255946084019504
Loss at iteration [669]: 0.0024254877892543863
Loss at iteration [670]: 0.0024253734899321367
Loss at iteration [671]: 0.00242523165246378
Loss at iteration [672]: 0.00242523165246378
Loss at iteration [673]: 0.002425214735314677
Loss at iteration [674]: 0.0024251748890962493
Loss at iteration [675]: 0.0024251088603985675
Loss at iteration [676]: 0.0024250438007415685
Loss at iteration [677]: 0.002424987751655906
Loss at iteration [678]: 0.002424987751655906
Loss at iteration [679]: 0.002424959591301305
Loss at iteration [680]: 0.0024248849282475604
Loss at iteration [681]: 0.002424809904470742
Loss at iteration [682]: 0.0024247655002043737
Loss at iteration [683]: 0.0024246890520294617
Loss at iteration [684]: 0.0024246890520294617
Loss at iteration [685]: 0.002424644549965671
Loss at iteration [686]: 0.002424630104085854
Loss at iteration [687]: 0.002424599604003157
Loss at iteration [688]: 0.0024245495804198236
Loss at iteration [689]: 0.0024245022488036607
