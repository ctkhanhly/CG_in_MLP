Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.001
Beta type                             :HS
Total number of function evaluations  : 3048
Total number of iterations            : 750
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 13.937350988388062
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.6915872120078%
Percentage of parameters < 1e-7       : 49.6915872120078%
Percentage of parameters < 1e-6       : 49.69257730121484%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.007104754780746617
Loss at iteration [2]: 0.0063142934383687
Loss at iteration [3]: 0.004742792002811141
Loss at iteration [4]: 0.003296692572563944
Loss at iteration [5]: 0.003163109988538872
Loss at iteration [6]: 0.0027769887070645343
Loss at iteration [7]: 0.0027428537768498535
Loss at iteration [8]: 0.0027020634711628555
Loss at iteration [9]: 0.0027020634711628555
Loss at iteration [10]: 0.002680791438160373
Loss at iteration [11]: 0.002671102956837766
Loss at iteration [12]: 0.0026108924060997004
Loss at iteration [13]: 0.002606261722800499
Loss at iteration [14]: 0.0025981294250558376
Loss at iteration [15]: 0.0025830801921621284
Loss at iteration [16]: 0.0025830801921621284
Loss at iteration [17]: 0.0025806848739775455
Loss at iteration [18]: 0.002577924077769344
Loss at iteration [19]: 0.0025700182539291905
Loss at iteration [20]: 0.0025662899335105625
Loss at iteration [21]: 0.0025554166238590764
Loss at iteration [22]: 0.0025514657393924545
Loss at iteration [23]: 0.0025514657393924545
Loss at iteration [24]: 0.00255030394865029
Loss at iteration [25]: 0.0025470994071212223
Loss at iteration [26]: 0.002544039064009265
Loss at iteration [27]: 0.002538526334442313
Loss at iteration [28]: 0.0025381543710151348
Loss at iteration [29]: 0.0025370981887236103
Loss at iteration [30]: 0.0025347383794879926
Loss at iteration [31]: 0.0025347383794879926
Loss at iteration [32]: 0.002534236226193582
Loss at iteration [33]: 0.002531591950877562
Loss at iteration [34]: 0.0025307166015012344
Loss at iteration [35]: 0.0025285935264629583
Loss at iteration [36]: 0.0025282191573302282
Loss at iteration [37]: 0.0025280458289032685
Loss at iteration [38]: 0.0025280458289032685
Loss at iteration [39]: 0.0025279294617001815
Loss at iteration [40]: 0.0025274726639906504
Loss at iteration [41]: 0.002526419375221302
Loss at iteration [42]: 0.0025261575234497096
Loss at iteration [43]: 0.0025252508276654385
Loss at iteration [44]: 0.002523442779023133
Loss at iteration [45]: 0.0025230819459283603
Loss at iteration [46]: 0.0025230819459283603
Loss at iteration [47]: 0.00252282400300594
Loss at iteration [48]: 0.0025222841471125796
Loss at iteration [49]: 0.002521807977309311
Loss at iteration [50]: 0.0025215792192865845
Loss at iteration [51]: 0.002521336218704194
Loss at iteration [52]: 0.002521275474442471
Loss at iteration [53]: 0.0025207912093483157
Loss at iteration [54]: 0.0025207912093483157
Loss at iteration [55]: 0.0025206999966691625
Loss at iteration [56]: 0.0025205177373214265
Loss at iteration [57]: 0.0025189280544627426
Loss at iteration [58]: 0.002518569692300332
Loss at iteration [59]: 0.0025170085149947346
Loss at iteration [60]: 0.002516419710875128
Loss at iteration [61]: 0.00251566856877229
Loss at iteration [62]: 0.00251566856877229
Loss at iteration [63]: 0.0025155135712217704
Loss at iteration [64]: 0.002515383315568815
Loss at iteration [65]: 0.00250512218627187
Loss at iteration [66]: 0.0025041239599213846
Loss at iteration [67]: 0.002499631894560158
Loss at iteration [68]: 0.002498910450473772
Loss at iteration [69]: 0.002498910450473772
Loss at iteration [70]: 0.0024984507244697664
Loss at iteration [71]: 0.0024972999455787344
Loss at iteration [72]: 0.002497054203853512
Loss at iteration [73]: 0.002496827615744406
Loss at iteration [74]: 0.0024965581707746855
Loss at iteration [75]: 0.0024956511313172654
Loss at iteration [76]: 0.002494646124656977
Loss at iteration [77]: 0.002494646124656977
Loss at iteration [78]: 0.002493935452793053
Loss at iteration [79]: 0.002492853311722807
Loss at iteration [80]: 0.0024924788214293384
Loss at iteration [81]: 0.002492289024148461
Loss at iteration [82]: 0.0024920022060528346
Loss at iteration [83]: 0.002491957154702442
Loss at iteration [84]: 0.0024918192195461726
Loss at iteration [85]: 0.0024918192195461726
Loss at iteration [86]: 0.0024916995818414036
Loss at iteration [87]: 0.002491647220593464
Loss at iteration [88]: 0.002489700930942326
Loss at iteration [89]: 0.002489647604949827
Loss at iteration [90]: 0.0024894661512286304
Loss at iteration [91]: 0.0024888376072798105
Loss at iteration [92]: 0.0024888376072798105
Loss at iteration [93]: 0.002488751558713874
Loss at iteration [94]: 0.0024884224479472406
Loss at iteration [95]: 0.0024882817490848256
Loss at iteration [96]: 0.0024882507507374882
Loss at iteration [97]: 0.0024862142739323395
Loss at iteration [98]: 0.002485681923445468
Loss at iteration [99]: 0.0024840073305857444
Loss at iteration [100]: 0.0024840073305857444
Loss at iteration [101]: 0.0024835230199557147
Loss at iteration [102]: 0.002483449643604633
Loss at iteration [103]: 0.0024821311992512525
Loss at iteration [104]: 0.002482008006370952
Loss at iteration [105]: 0.002481171781961888
Loss at iteration [106]: 0.0024806008291301104
Loss at iteration [107]: 0.0024806008291301104
Loss at iteration [108]: 0.002480481577674415
Loss at iteration [109]: 0.0024798415977095208
Loss at iteration [110]: 0.002479753251768158
Loss at iteration [111]: 0.0024796975143739353
Loss at iteration [112]: 0.0024789278115396013
Loss at iteration [113]: 0.0024788175783087224
Loss at iteration [114]: 0.0024787455625596595
Loss at iteration [115]: 0.0024787455625596595
Loss at iteration [116]: 0.002478680667455073
Loss at iteration [117]: 0.0024784056951961575
Loss at iteration [118]: 0.0024781741894939514
Loss at iteration [119]: 0.002477844342126303
Loss at iteration [120]: 0.002477642761817892
Loss at iteration [121]: 0.0024775319320392074
Loss at iteration [122]: 0.0024775319320392074
Loss at iteration [123]: 0.002477377520097967
Loss at iteration [124]: 0.002476673479704041
Loss at iteration [125]: 0.0024766071922383923
Loss at iteration [126]: 0.0024763862885193912
Loss at iteration [127]: 0.0024763545757540226
Loss at iteration [128]: 0.0024762421941080057
Loss at iteration [129]: 0.0024762421941080057
Loss at iteration [130]: 0.0024761469640832857
Loss at iteration [131]: 0.002476099915408248
Loss at iteration [132]: 0.002475484113072668
Loss at iteration [133]: 0.002475392488264342
Loss at iteration [134]: 0.0024753528075714217
Loss at iteration [135]: 0.002475312155081574
Loss at iteration [136]: 0.00247521269638913
Loss at iteration [137]: 0.00247521269638913
Loss at iteration [138]: 0.002475144535394887
Loss at iteration [139]: 0.002474645277143125
Loss at iteration [140]: 0.0024745886020981875
Loss at iteration [141]: 0.0024743453644862318
Loss at iteration [142]: 0.002474197454769137
Loss at iteration [143]: 0.0024741369010545487
Loss at iteration [144]: 0.0024741369010545487
Loss at iteration [145]: 0.002474081118615336
Loss at iteration [146]: 0.002473754682720255
Loss at iteration [147]: 0.0024732783590197477
Loss at iteration [148]: 0.0024732383925892597
Loss at iteration [149]: 0.0024732051045393885
Loss at iteration [150]: 0.00247302579847554
Loss at iteration [151]: 0.002472932865817076
Loss at iteration [152]: 0.002472932865817076
Loss at iteration [153]: 0.0024727908078009857
Loss at iteration [154]: 0.002472543007073446
Loss at iteration [155]: 0.002472425560614918
Loss at iteration [156]: 0.0024723759119559415
Loss at iteration [157]: 0.0024721286613720137
Loss at iteration [158]: 0.0024720553740440634
Loss at iteration [159]: 0.0024718943403280137
Loss at iteration [160]: 0.0024718943403280137
Loss at iteration [161]: 0.0024718028393932788
Loss at iteration [162]: 0.002471754958218719
Loss at iteration [163]: 0.00247154811420795
Loss at iteration [164]: 0.0024715072862722736
Loss at iteration [165]: 0.0024708283513976905
Loss at iteration [166]: 0.00247060506301295
Loss at iteration [167]: 0.00247060506301295
Loss at iteration [168]: 0.002470525997045601
Loss at iteration [169]: 0.002470182438896011
Loss at iteration [170]: 0.002469689368880542
Loss at iteration [171]: 0.0024695999704953146
Loss at iteration [172]: 0.002469443499168007
Loss at iteration [173]: 0.0024693290009556355
Loss at iteration [174]: 0.002468932970121936
Loss at iteration [175]: 0.002468932970121936
Loss at iteration [176]: 0.002468729823077478
Loss at iteration [177]: 0.0024684258631267587
Loss at iteration [178]: 0.002468386974852761
Loss at iteration [179]: 0.0024680895656544067
Loss at iteration [180]: 0.002468058933707859
Loss at iteration [181]: 0.0024678446789061313
Loss at iteration [182]: 0.0024673876091957543
Loss at iteration [183]: 0.0024673876091957543
Loss at iteration [184]: 0.002467354062191171
Loss at iteration [185]: 0.0024671883126293217
Loss at iteration [186]: 0.002467083082437685
Loss at iteration [187]: 0.0024670677899718894
Loss at iteration [188]: 0.002466977975386903
Loss at iteration [189]: 0.0024669206079105045
Loss at iteration [190]: 0.0024668804894190373
Loss at iteration [191]: 0.0024668804894190373
Loss at iteration [192]: 0.002466846875276568
Loss at iteration [193]: 0.002466812994580538
Loss at iteration [194]: 0.0024666980562855025
Loss at iteration [195]: 0.002466668883558624
Loss at iteration [196]: 0.0024665454793754223
Loss at iteration [197]: 0.002466521603385699
Loss at iteration [198]: 0.002466521603385699
Loss at iteration [199]: 0.002466498592368813
Loss at iteration [200]: 0.0024663747936033344
Loss at iteration [201]: 0.002466348092334639
Loss at iteration [202]: 0.0024662692567284997
Loss at iteration [203]: 0.0024658824153216325
Loss at iteration [204]: 0.0024655517031420423
Loss at iteration [205]: 0.0024655517031420423
Loss at iteration [206]: 0.002465327543713749
Loss at iteration [207]: 0.002465208965750349
Loss at iteration [208]: 0.002464851641092179
Loss at iteration [209]: 0.0024648199182101486
Loss at iteration [210]: 0.002464781134570593
Loss at iteration [211]: 0.0024643643432674563
Loss at iteration [212]: 0.002464228393418564
Loss at iteration [213]: 0.002464228393418564
Loss at iteration [214]: 0.0024641709831146386
Loss at iteration [215]: 0.0024638842986241557
Loss at iteration [216]: 0.002463854736599595
Loss at iteration [217]: 0.002463706666177519
Loss at iteration [218]: 0.0024632491591409303
Loss at iteration [219]: 0.002463166553513435
Loss at iteration [220]: 0.0024628956003053455
Loss at iteration [221]: 0.0024628956003053455
Loss at iteration [222]: 0.0024627581283486327
Loss at iteration [223]: 0.002462730901391988
Loss at iteration [224]: 0.002462326627202639
Loss at iteration [225]: 0.002462289928997892
Loss at iteration [226]: 0.0024618314307990174
Loss at iteration [227]: 0.0024616404916352162
Loss at iteration [228]: 0.0024616404916352162
Loss at iteration [229]: 0.002461530093989194
Loss at iteration [230]: 0.0024613550599673155
Loss at iteration [231]: 0.0024613252454942815
Loss at iteration [232]: 0.0024612599452001832
Loss at iteration [233]: 0.0024604888082557065
Loss at iteration [234]: 0.00246041179268361
Loss at iteration [235]: 0.00246041179268361
Loss at iteration [236]: 0.0024603723584133395
Loss at iteration [237]: 0.0024602697768750727
Loss at iteration [238]: 0.002459950706169406
Loss at iteration [239]: 0.0024599024269032943
Loss at iteration [240]: 0.0024596549344213744
Loss at iteration [241]: 0.0024596074083481087
Loss at iteration [242]: 0.0024594982082028027
Loss at iteration [243]: 0.0024594982082028027
Loss at iteration [244]: 0.00245947034487891
Loss at iteration [245]: 0.002459270196767224
Loss at iteration [246]: 0.0024592359396454584
Loss at iteration [247]: 0.0024586658450203576
Loss at iteration [248]: 0.0024585479738659616
Loss at iteration [249]: 0.002458083710094772
Loss at iteration [250]: 0.002458083710094772
Loss at iteration [251]: 0.002457959068328064
Loss at iteration [252]: 0.0024576119722491334
Loss at iteration [253]: 0.0024573865673297563
Loss at iteration [254]: 0.002457335462318797
Loss at iteration [255]: 0.0024571242817359976
Loss at iteration [256]: 0.0024568854229104855
Loss at iteration [257]: 0.0024567851310172677
Loss at iteration [258]: 0.0024567851310172677
Loss at iteration [259]: 0.0024567661340336015
Loss at iteration [260]: 0.0024564400934520875
Loss at iteration [261]: 0.0024563697169853915
Loss at iteration [262]: 0.0024562258627617794
Loss at iteration [263]: 0.0024550833559864272
Loss at iteration [264]: 0.0024550833559864272
Loss at iteration [265]: 0.0024550256930864363
Loss at iteration [266]: 0.0024549935015050366
Loss at iteration [267]: 0.002454608617011425
Loss at iteration [268]: 0.002454428771881507
Loss at iteration [269]: 0.0024543583512616654
Loss at iteration [270]: 0.002454250095037815
Loss at iteration [271]: 0.0024540057646503834
Loss at iteration [272]: 0.0024540057646503834
Loss at iteration [273]: 0.0024538480654442757
Loss at iteration [274]: 0.0024538243429726224
Loss at iteration [275]: 0.0024534380708600544
Loss at iteration [276]: 0.0024534080903386396
Loss at iteration [277]: 0.0024532926065864644
Loss at iteration [278]: 0.0024531163872856714
Loss at iteration [279]: 0.002453031147818577
Loss at iteration [280]: 0.002453031147818577
Loss at iteration [281]: 0.002452975339367052
Loss at iteration [282]: 0.0024529630900633538
Loss at iteration [283]: 0.0024527769932317666
Loss at iteration [284]: 0.002452663690019801
Loss at iteration [285]: 0.002452259076065046
Loss at iteration [286]: 0.002451628233106616
Loss at iteration [287]: 0.002451628233106616
Loss at iteration [288]: 0.0024515106048625668
Loss at iteration [289]: 0.0024512242028686087
Loss at iteration [290]: 0.002450880417083549
Loss at iteration [291]: 0.0024504309373280775
Loss at iteration [292]: 0.002450286435081838
Loss at iteration [293]: 0.0024502489950650338
Loss at iteration [294]: 0.0024501225846544675
Loss at iteration [295]: 0.0024501225846544675
Loss at iteration [296]: 0.0024499725206351113
Loss at iteration [297]: 0.002449951922230743
Loss at iteration [298]: 0.0024497919997103317
Loss at iteration [299]: 0.002449764486966613
Loss at iteration [300]: 0.002449722217492574
Loss at iteration [301]: 0.0024492952123384122
Loss at iteration [302]: 0.002449265865007844
Loss at iteration [303]: 0.002449265865007844
Loss at iteration [304]: 0.002449181301328714
Loss at iteration [305]: 0.0024485617936162722
Loss at iteration [306]: 0.0024484995561577117
Loss at iteration [307]: 0.0024484589623862893
Loss at iteration [308]: 0.0024484242085233855
Loss at iteration [309]: 0.002448386113854676
Loss at iteration [310]: 0.0024483532375634918
Loss at iteration [311]: 0.0024483532375634918
Loss at iteration [312]: 0.002448334536871653
Loss at iteration [313]: 0.002448315870969639
Loss at iteration [314]: 0.0024482470795988607
Loss at iteration [315]: 0.0024481838836089
Loss at iteration [316]: 0.0024481200612376814
Loss at iteration [317]: 0.0024478880820134863
Loss at iteration [318]: 0.0024478880820134863
Loss at iteration [319]: 0.002447790111413109
Loss at iteration [320]: 0.0024475896776378916
Loss at iteration [321]: 0.002447518735919793
Loss at iteration [322]: 0.002447337571863641
Loss at iteration [323]: 0.0024471880316812504
Loss at iteration [324]: 0.002447139257058449
Loss at iteration [325]: 0.002447139257058449
Loss at iteration [326]: 0.0024471176225791228
Loss at iteration [327]: 0.0024470357474012058
Loss at iteration [328]: 0.0024469677107660116
Loss at iteration [329]: 0.0024468833666455125
Loss at iteration [330]: 0.0024468302280126445
Loss at iteration [331]: 0.0024468121664346252
Loss at iteration [332]: 0.0024467546980120243
Loss at iteration [333]: 0.0024467546980120243
Loss at iteration [334]: 0.002446722244068698
Loss at iteration [335]: 0.002446613266250177
Loss at iteration [336]: 0.0024465996775358285
Loss at iteration [337]: 0.0024465070464051446
Loss at iteration [338]: 0.0024463974352825057
Loss at iteration [339]: 0.00244586730871075
Loss at iteration [340]: 0.00244586730871075
Loss at iteration [341]: 0.0024452572201286563
Loss at iteration [342]: 0.0024451100063665
Loss at iteration [343]: 0.0024445350287235607
Loss at iteration [344]: 0.0024445073597350385
Loss at iteration [345]: 0.0024443169110700672
Loss at iteration [346]: 0.0024440045134635074
Loss at iteration [347]: 0.0024439754164187876
Loss at iteration [348]: 0.0024439754164187876
Loss at iteration [349]: 0.0024439539680393097
Loss at iteration [350]: 0.0024438937205523586
Loss at iteration [351]: 0.002443752279515973
Loss at iteration [352]: 0.002443716151861762
Loss at iteration [353]: 0.002443526290220648
Loss at iteration [354]: 0.002443465414943502
Loss at iteration [355]: 0.0024434153596469433
Loss at iteration [356]: 0.0024434153596469433
Loss at iteration [357]: 0.0024433894977891947
Loss at iteration [358]: 0.002443366551800198
Loss at iteration [359]: 0.0024432927540370673
Loss at iteration [360]: 0.00244322478481467
Loss at iteration [361]: 0.0024431955322773233
Loss at iteration [362]: 0.002443135604886156
Loss at iteration [363]: 0.0024430682526064285
Loss at iteration [364]: 0.0024430682526064285
Loss at iteration [365]: 0.002443041090714582
Loss at iteration [366]: 0.002443012696431795
Loss at iteration [367]: 0.0024429130066811835
Loss at iteration [368]: 0.0024429048490013884
Loss at iteration [369]: 0.0024428757318099782
Loss at iteration [370]: 0.0024428154504865405
Loss at iteration [371]: 0.0024428154504865405
Loss at iteration [372]: 0.0024427951690830572
Loss at iteration [373]: 0.002442741202650123
Loss at iteration [374]: 0.0024426996147125977
Loss at iteration [375]: 0.0024426728515189473
Loss at iteration [376]: 0.002442608535212134
Loss at iteration [377]: 0.0024425522475565857
Loss at iteration [378]: 0.0024425191051998985
Loss at iteration [379]: 0.0024425191051998985
Loss at iteration [380]: 0.0024425003437570187
Loss at iteration [381]: 0.0024424466748597736
Loss at iteration [382]: 0.0024423926587440643
Loss at iteration [383]: 0.002442358988026575
Loss at iteration [384]: 0.0024423248377740394
Loss at iteration [385]: 0.0024421789252956506
Loss at iteration [386]: 0.0024421789252956506
Loss at iteration [387]: 0.0024421159751063697
Loss at iteration [388]: 0.002442084222193634
Loss at iteration [389]: 0.0024420382708270905
Loss at iteration [390]: 0.0024420102165515186
Loss at iteration [391]: 0.002441925166260689
Loss at iteration [392]: 0.0024419113049105544
Loss at iteration [393]: 0.0024418477515173874
Loss at iteration [394]: 0.0024418477515173874
Loss at iteration [395]: 0.0024418355759578854
Loss at iteration [396]: 0.0024418240144123944
Loss at iteration [397]: 0.0024417843605117014
Loss at iteration [398]: 0.002441740724143461
Loss at iteration [399]: 0.0024416966068297238
Loss at iteration [400]: 0.0024416088221953
Loss at iteration [401]: 0.002439466481597212
Loss at iteration [402]: 0.002439466481597212
Loss at iteration [403]: 0.0024388468633410316
Loss at iteration [404]: 0.002438797137448242
Loss at iteration [405]: 0.002438473370762323
Loss at iteration [406]: 0.00243836505424377
Loss at iteration [407]: 0.0024381542373254077
Loss at iteration [408]: 0.002437813508292169
Loss at iteration [409]: 0.002437248194167037
Loss at iteration [410]: 0.002437248194167037
Loss at iteration [411]: 0.002437169540684814
Loss at iteration [412]: 0.002436845107258761
Loss at iteration [413]: 0.002436771376912391
Loss at iteration [414]: 0.002436653483904049
Loss at iteration [415]: 0.002436592114135428
Loss at iteration [416]: 0.002436562083164262
Loss at iteration [417]: 0.002436444261467765
Loss at iteration [418]: 0.002436444261467765
Loss at iteration [419]: 0.002436410517426843
Loss at iteration [420]: 0.0024362145176166487
Loss at iteration [421]: 0.0024361260213604193
Loss at iteration [422]: 0.002436045131674266
Loss at iteration [423]: 0.0024360288350871975
Loss at iteration [424]: 0.002436003014623175
Loss at iteration [425]: 0.002435916689878446
Loss at iteration [426]: 0.002435916689878446
Loss at iteration [427]: 0.0024358902674649473
Loss at iteration [428]: 0.002435864492419443
Loss at iteration [429]: 0.0024357591064528487
Loss at iteration [430]: 0.0024357382050507145
Loss at iteration [431]: 0.0024356847589602935
Loss at iteration [432]: 0.0024355428703211834
Loss at iteration [433]: 0.0024355428703211834
Loss at iteration [434]: 0.0024355143773727088
Loss at iteration [435]: 0.0024354143599987794
Loss at iteration [436]: 0.002435404712805846
Loss at iteration [437]: 0.0024351323207323773
Loss at iteration [438]: 0.0024350895692616037
Loss at iteration [439]: 0.002434966963411976
Loss at iteration [440]: 0.002434588021251532
Loss at iteration [441]: 0.002434588021251532
Loss at iteration [442]: 0.0024345670210992906
Loss at iteration [443]: 0.002434469067143715
Loss at iteration [444]: 0.002434445507286022
Loss at iteration [445]: 0.0024344244452498332
Loss at iteration [446]: 0.002434175518951415
Loss at iteration [447]: 0.00243406079831575
Loss at iteration [448]: 0.0024340442473164926
Loss at iteration [449]: 0.0024340442473164926
Loss at iteration [450]: 0.0024339430286062945
Loss at iteration [451]: 0.002433908995240967
Loss at iteration [452]: 0.002433865181140127
Loss at iteration [453]: 0.00243372798938392
Loss at iteration [454]: 0.0024336750624770545
Loss at iteration [455]: 0.002433495932449058
Loss at iteration [456]: 0.002433495932449058
Loss at iteration [457]: 0.002433454248387768
Loss at iteration [458]: 0.002433432778266786
Loss at iteration [459]: 0.0024331943107430763
Loss at iteration [460]: 0.002433134334153237
Loss at iteration [461]: 0.0024330806087464185
Loss at iteration [462]: 0.0024330539610984197
Loss at iteration [463]: 0.002433026173080667
Loss at iteration [464]: 0.002433026173080667
Loss at iteration [465]: 0.0024330107591832343
Loss at iteration [466]: 0.0024329585674202027
Loss at iteration [467]: 0.0024328269982954085
Loss at iteration [468]: 0.0024327985150213944
Loss at iteration [469]: 0.002432674850797164
Loss at iteration [470]: 0.0024325567016591867
Loss at iteration [471]: 0.0024325567016591867
Loss at iteration [472]: 0.0024325165724172347
Loss at iteration [473]: 0.002432364460364401
Loss at iteration [474]: 0.0024321779939303393
Loss at iteration [475]: 0.0024321328321289587
Loss at iteration [476]: 0.0024320292551448536
Loss at iteration [477]: 0.0024320053772219435
Loss at iteration [478]: 0.0024320053772219435
Loss at iteration [479]: 0.002431988596222519
Loss at iteration [480]: 0.002431924348239024
Loss at iteration [481]: 0.002431878009484555
Loss at iteration [482]: 0.0024318591281132485
Loss at iteration [483]: 0.0024317436306448524
Loss at iteration [484]: 0.002431603710458587
Loss at iteration [485]: 0.002431603710458587
Loss at iteration [486]: 0.002431582657575599
Loss at iteration [487]: 0.0024315219362845454
Loss at iteration [488]: 0.0024314387499421142
Loss at iteration [489]: 0.002431329017981211
Loss at iteration [490]: 0.0024313132188372505
Loss at iteration [491]: 0.0024311447040995748
Loss at iteration [492]: 0.0024311447040995748
Loss at iteration [493]: 0.002431072754468105
Loss at iteration [494]: 0.0024310071790223193
Loss at iteration [495]: 0.002430986619139947
Loss at iteration [496]: 0.0024308853975459027
Loss at iteration [497]: 0.0024308764444688816
Loss at iteration [498]: 0.0024308159112257353
Loss at iteration [499]: 0.0024308159112257353
Loss at iteration [500]: 0.002430792110689221
Loss at iteration [501]: 0.0024307804897539366
Loss at iteration [502]: 0.002430695890774941
Loss at iteration [503]: 0.0024306762738808365
Loss at iteration [504]: 0.002430599534844409
Loss at iteration [505]: 0.0024303657102416635
Loss at iteration [506]: 0.0024303657102416635
Loss at iteration [507]: 0.002430277488882717
Loss at iteration [508]: 0.002430135361284237
Loss at iteration [509]: 0.002429995642535249
Loss at iteration [510]: 0.0024299325076904445
Loss at iteration [511]: 0.0024298392211017936
Loss at iteration [512]: 0.002429821240908818
Loss at iteration [513]: 0.002429479408169007
Loss at iteration [514]: 0.002429479408169007
Loss at iteration [515]: 0.002429446506400656
Loss at iteration [516]: 0.0024294218085924445
Loss at iteration [517]: 0.0024293285829128358
Loss at iteration [518]: 0.0024293170366392895
Loss at iteration [519]: 0.0024292263168184925
Loss at iteration [520]: 0.002429072778793125
Loss at iteration [521]: 0.0024290567084194873
Loss at iteration [522]: 0.0024290567084194873
Loss at iteration [523]: 0.002429045773998047
Loss at iteration [524]: 0.0024289707087002672
Loss at iteration [525]: 0.0024289375637505163
Loss at iteration [526]: 0.002428897927984821
Loss at iteration [527]: 0.0024286006818294596
Loss at iteration [528]: 0.0024285838194184365
Loss at iteration [529]: 0.0024285838194184365
Loss at iteration [530]: 0.0024284049952884162
Loss at iteration [531]: 0.0024283437540229633
Loss at iteration [532]: 0.0024283219681742185
Loss at iteration [533]: 0.00242824229101668
Loss at iteration [534]: 0.0024281027940941014
Loss at iteration [535]: 0.0024280725780052616
Loss at iteration [536]: 0.002427990661434414
Loss at iteration [537]: 0.002427990661434414
Loss at iteration [538]: 0.002427969253260642
Loss at iteration [539]: 0.002427901363931977
Loss at iteration [540]: 0.0024278579520565923
Loss at iteration [541]: 0.0024278392261554665
Loss at iteration [542]: 0.0024278245807449425
Loss at iteration [543]: 0.002427816287074873
Loss at iteration [544]: 0.002427795556652304
Loss at iteration [545]: 0.002427795556652304
Loss at iteration [546]: 0.0024277883724316894
Loss at iteration [547]: 0.0024277275364385834
Loss at iteration [548]: 0.0024276570250826375
Loss at iteration [549]: 0.002427520916618248
Loss at iteration [550]: 0.0024274676411356866
Loss at iteration [551]: 0.002427407915908486
Loss at iteration [552]: 0.0024273374757324885
Loss at iteration [553]: 0.0024273374757324885
Loss at iteration [554]: 0.0024273303885020267
Loss at iteration [555]: 0.002427295754357825
Loss at iteration [556]: 0.0024272509155432056
Loss at iteration [557]: 0.0024272310941362125
Loss at iteration [558]: 0.0024272010481381865
Loss at iteration [559]: 0.0024271553694955894
Loss at iteration [560]: 0.0024270664901832545
Loss at iteration [561]: 0.0024270664901832545
Loss at iteration [562]: 0.002427023267780961
Loss at iteration [563]: 0.0024270134034505893
Loss at iteration [564]: 0.0024269452294044284
Loss at iteration [565]: 0.0024269337984887036
Loss at iteration [566]: 0.00242670113124832
Loss at iteration [567]: 0.002426672930449969
Loss at iteration [568]: 0.002426672930449969
Loss at iteration [569]: 0.002426646981233399
Loss at iteration [570]: 0.002426515361547661
Loss at iteration [571]: 0.0024264913030883637
Loss at iteration [572]: 0.002426355933369139
Loss at iteration [573]: 0.002426299111967338
Loss at iteration [574]: 0.0024262700029729755
Loss at iteration [575]: 0.0024262700029729755
Loss at iteration [576]: 0.002426238195000098
Loss at iteration [577]: 0.00242620468791673
Loss at iteration [578]: 0.002426135214136158
Loss at iteration [579]: 0.0024261260946506105
Loss at iteration [580]: 0.002426080463483271
Loss at iteration [581]: 0.0024260338177160465
Loss at iteration [582]: 0.0024260179430507017
Loss at iteration [583]: 0.0024260179430507017
Loss at iteration [584]: 0.0024260060299892213
Loss at iteration [585]: 0.0024259768099443788
Loss at iteration [586]: 0.002425933259864631
Loss at iteration [587]: 0.0024259243492081146
Loss at iteration [588]: 0.002425736471349865
Loss at iteration [589]: 0.002425398188058833
Loss at iteration [590]: 0.002425398188058833
Loss at iteration [591]: 0.0024253114423385086
Loss at iteration [592]: 0.002425234887845183
Loss at iteration [593]: 0.0024247374767384987
Loss at iteration [594]: 0.0024247014339903055
Loss at iteration [595]: 0.0024246545790003586
Loss at iteration [596]: 0.0024246131121286123
Loss at iteration [597]: 0.0024246131121286123
Loss at iteration [598]: 0.002424590485047395
Loss at iteration [599]: 0.002424566549144102
Loss at iteration [600]: 0.0024242963544825464
Loss at iteration [601]: 0.002424287523777709
Loss at iteration [602]: 0.0024242561534002067
Loss at iteration [603]: 0.002424166686129023
Loss at iteration [604]: 0.0024241501232133253
Loss at iteration [605]: 0.0024241501232133253
Loss at iteration [606]: 0.0024241412548880923
Loss at iteration [607]: 0.002424083199652165
Loss at iteration [608]: 0.002424041650977275
Loss at iteration [609]: 0.0024240281296610687
Loss at iteration [610]: 0.0024240142237446347
Loss at iteration [611]: 0.002423980547476264
Loss at iteration [612]: 0.002423958725309748
Loss at iteration [613]: 0.002423958725309748
Loss at iteration [614]: 0.002423948864851995
Loss at iteration [615]: 0.0024239372948664764
Loss at iteration [616]: 0.0024238839494366967
Loss at iteration [617]: 0.002423845743803662
Loss at iteration [618]: 0.002423749209281337
Loss at iteration [619]: 0.0024237009217823384
Loss at iteration [620]: 0.0024237009217823384
Loss at iteration [621]: 0.0024236592903244974
Loss at iteration [622]: 0.002423484737669946
Loss at iteration [623]: 0.002423459367430341
Loss at iteration [624]: 0.0024233594347178975
Loss at iteration [625]: 0.002423297873202414
Loss at iteration [626]: 0.002423162116883903
Loss at iteration [627]: 0.002423162116883903
Loss at iteration [628]: 0.0024231289472556416
Loss at iteration [629]: 0.0024231143286660446
Loss at iteration [630]: 0.0024230243698584977
Loss at iteration [631]: 0.002422991386069113
Loss at iteration [632]: 0.0024229649648454285
Loss at iteration [633]: 0.0024228095055096152
Loss at iteration [634]: 0.0024227873633514306
Loss at iteration [635]: 0.0024227873633514306
Loss at iteration [636]: 0.002422768023406938
Loss at iteration [637]: 0.002422706302075008
Loss at iteration [638]: 0.002422696542899893
Loss at iteration [639]: 0.0024226714254314582
Loss at iteration [640]: 0.0024226641197096473
Loss at iteration [641]: 0.00242265219681097
Loss at iteration [642]: 0.00242265219681097
Loss at iteration [643]: 0.0024226384139723464
Loss at iteration [644]: 0.0024225797294230786
Loss at iteration [645]: 0.002422508684331166
Loss at iteration [646]: 0.00242233727028808
Loss at iteration [647]: 0.002422284660139671
Loss at iteration [648]: 0.002422231869694615
Loss at iteration [649]: 0.0024220276130237414
Loss at iteration [650]: 0.0024220276130237414
Loss at iteration [651]: 0.0024220013611206746
Loss at iteration [652]: 0.0024219298023096655
Loss at iteration [653]: 0.0024217222189879912
Loss at iteration [654]: 0.0024216545710682696
Loss at iteration [655]: 0.0024215912600630348
Loss at iteration [656]: 0.00242157322388534
Loss at iteration [657]: 0.002421553276648567
Loss at iteration [658]: 0.002421553276648567
Loss at iteration [659]: 0.0024215299394451917
Loss at iteration [660]: 0.0024215212680131055
Loss at iteration [661]: 0.0024214074164079207
Loss at iteration [662]: 0.00242137899804364
Loss at iteration [663]: 0.0024213461811538276
Loss at iteration [664]: 0.002421253709373497
Loss at iteration [665]: 0.0024211562327648645
Loss at iteration [666]: 0.0024211562327648645
Loss at iteration [667]: 0.0024211307646263224
Loss at iteration [668]: 0.0024211113287827906
Loss at iteration [669]: 0.002420990339457608
Loss at iteration [670]: 0.0024209497003886875
Loss at iteration [671]: 0.0024209246894130966
Loss at iteration [672]: 0.0024208901986547847
Loss at iteration [673]: 0.002420770711572966
Loss at iteration [674]: 0.002420770711572966
Loss at iteration [675]: 0.0024207537327999365
Loss at iteration [676]: 0.002420733788596267
Loss at iteration [677]: 0.0024206490188922523
Loss at iteration [678]: 0.002420634798565954
Loss at iteration [679]: 0.002420589239408837
Loss at iteration [680]: 0.0024204884484488438
Loss at iteration [681]: 0.0024204325203491827
Loss at iteration [682]: 0.0024204325203491827
Loss at iteration [683]: 0.0024204169654446672
Loss at iteration [684]: 0.0024203982763599573
Loss at iteration [685]: 0.0024203418268232257
Loss at iteration [686]: 0.0024202907112026166
Loss at iteration [687]: 0.002420223106423896
Loss at iteration [688]: 0.0024201992761607727
Loss at iteration [689]: 0.0024201767418508416
Loss at iteration [690]: 0.0024201767418508416
Loss at iteration [691]: 0.002420164840162983
Loss at iteration [692]: 0.002420148530956238
Loss at iteration [693]: 0.0024201132007148326
Loss at iteration [694]: 0.002420081717418326
Loss at iteration [695]: 0.002419996152395255
Loss at iteration [696]: 0.0024199871700862088
Loss at iteration [697]: 0.002419949615197489
Loss at iteration [698]: 0.002419949615197489
Loss at iteration [699]: 0.0024199390500312316
Loss at iteration [700]: 0.002419927276711148
Loss at iteration [701]: 0.0024199129496126934
Loss at iteration [702]: 0.0024198958273474462
Loss at iteration [703]: 0.0024198046505041677
Loss at iteration [704]: 0.0024197823820676844
Loss at iteration [705]: 0.002419445715847887
Loss at iteration [706]: 0.002419445715847887
Loss at iteration [707]: 0.002419393078323987
Loss at iteration [708]: 0.0024193357122262874
Loss at iteration [709]: 0.0024192134232879237
Loss at iteration [710]: 0.0024191998796166125
Loss at iteration [711]: 0.0024191713817740164
Loss at iteration [712]: 0.002419079799275668
Loss at iteration [713]: 0.002419070635338454
Loss at iteration [714]: 0.002419070635338454
Loss at iteration [715]: 0.002419056025323667
Loss at iteration [716]: 0.002418970923761504
Loss at iteration [717]: 0.0024189589269255213
Loss at iteration [718]: 0.0024189261172418886
Loss at iteration [719]: 0.002418862236045536
Loss at iteration [720]: 0.002418842069731643
Loss at iteration [721]: 0.002418801850184341
Loss at iteration [722]: 0.002418801850184341
Loss at iteration [723]: 0.002418772695792391
Loss at iteration [724]: 0.002418704868303827
Loss at iteration [725]: 0.0024186895336012646
Loss at iteration [726]: 0.002418668197079051
Loss at iteration [727]: 0.0024186438933747135
Loss at iteration [728]: 0.002418602666272153
Loss at iteration [729]: 0.002418463797064855
Loss at iteration [730]: 0.002418463797064855
Loss at iteration [731]: 0.0024184451232035233
Loss at iteration [732]: 0.002418422390395265
Loss at iteration [733]: 0.0024183786732115227
Loss at iteration [734]: 0.002418334694662163
Loss at iteration [735]: 0.0024183149829547245
Loss at iteration [736]: 0.002418302167253148
Loss at iteration [737]: 0.0024182689547527086
Loss at iteration [738]: 0.0024182689547527086
Loss at iteration [739]: 0.002418254478502674
Loss at iteration [740]: 0.0024182452046432874
Loss at iteration [741]: 0.0024182190319323653
Loss at iteration [742]: 0.0024181685038993624
Loss at iteration [743]: 0.00241814212849704
Loss at iteration [744]: 0.0024181024996304435
Loss at iteration [745]: 0.002418054544548602
Loss at iteration [746]: 0.002418054544548602
Loss at iteration [747]: 0.0024180191596476193
Loss at iteration [748]: 0.0024179944873510763
Loss at iteration [749]: 0.002417914517641341
Loss at iteration [750]: 0.0024178186871376146
Loss at iteration [751]: 0.002417770881773762
Loss at iteration [752]: 0.0024177433681602306
Loss at iteration [753]: 0.0024177433681602306
Loss at iteration [754]: 0.0024177321679364257
Loss at iteration [755]: 0.00241770151377089
Loss at iteration [756]: 0.002417615522181733
Loss at iteration [757]: 0.002417590149630453
Loss at iteration [758]: 0.002417554909288707
Loss at iteration [759]: 0.0024175251648244902
Loss at iteration [760]: 0.002417456323610352
Loss at iteration [761]: 0.002417456323610352
Loss at iteration [762]: 0.0024174201563665976
Loss at iteration [763]: 0.0024174074849408556
Loss at iteration [764]: 0.0024173638602839314
Loss at iteration [765]: 0.0024173448240047968
Loss at iteration [766]: 0.0024173079333754234
Loss at iteration [767]: 0.002417283109856556
Loss at iteration [768]: 0.002417228057984507
Loss at iteration [769]: 0.002417228057984507
Loss at iteration [770]: 0.0024171924617604097
Loss at iteration [771]: 0.0024171583771939596
Loss at iteration [772]: 0.002417099252220201
Loss at iteration [773]: 0.0024170941957834965
Loss at iteration [774]: 0.0024170546987013312
Loss at iteration [775]: 0.0024170421842881437
Loss at iteration [776]: 0.002417023566755967
Loss at iteration [777]: 0.002417023566755967
Loss at iteration [778]: 0.00241700035983567
Loss at iteration [779]: 0.0024169924538526484
Loss at iteration [780]: 0.002416962565528976
Loss at iteration [781]: 0.0024169351907518486
Loss at iteration [782]: 0.002416879349318705
Loss at iteration [783]: 0.00241682089502986
Loss at iteration [784]: 0.00241682089502986
Loss at iteration [785]: 0.002416809619345568
Loss at iteration [786]: 0.002416762456903253
Loss at iteration [787]: 0.002416702244278064
Loss at iteration [788]: 0.002416681056879298
Loss at iteration [789]: 0.0024166463849231005
Loss at iteration [790]: 0.0024165798625189043
Loss at iteration [791]: 0.0024165798625189043
Loss at iteration [792]: 0.002416561552596542
Loss at iteration [793]: 0.0024165275214014514
Loss at iteration [794]: 0.0024164455558399666
Loss at iteration [795]: 0.002416434810079775
Loss at iteration [796]: 0.0024163711005375575
Loss at iteration [797]: 0.002416230727985799
Loss at iteration [798]: 0.002416230727985799
Loss at iteration [799]: 0.002416190401590966
Loss at iteration [800]: 0.0024161502840826894
Loss at iteration [801]: 0.0024161102633017567
Loss at iteration [802]: 0.0024160606582383265
Loss at iteration [803]: 0.002416045946995398
Loss at iteration [804]: 0.002415948471975938
Loss at iteration [805]: 0.0024159228113666926
Loss at iteration [806]: 0.0024159228113666926
Loss at iteration [807]: 0.0024159141255993962
Loss at iteration [808]: 0.0024158590273841672
Loss at iteration [809]: 0.0024158397879236434
Loss at iteration [810]: 0.0024158180883982645
Loss at iteration [811]: 0.0024156553611588954
Loss at iteration [812]: 0.0024156553611588954
Loss at iteration [813]: 0.0024156211052124956
Loss at iteration [814]: 0.002415524606445285
Loss at iteration [815]: 0.002415504960631501
Loss at iteration [816]: 0.0024154733634199708
Loss at iteration [817]: 0.002415434969954151
Loss at iteration [818]: 0.0024154190995864227
Loss at iteration [819]: 0.002415323073169138
Loss at iteration [820]: 0.002415323073169138
Loss at iteration [821]: 0.0024152961254593538
Loss at iteration [822]: 0.0024152872265690802
Loss at iteration [823]: 0.0024152468461432625
Loss at iteration [824]: 0.002415209352293525
Loss at iteration [825]: 0.0024151824066121464
Loss at iteration [826]: 0.0024151488663200224
Loss at iteration [827]: 0.002415123790635808
Loss at iteration [828]: 0.002415123790635808
Loss at iteration [829]: 0.0024151164563246505
Loss at iteration [830]: 0.002415084500053858
Loss at iteration [831]: 0.002415025800301159
Loss at iteration [832]: 0.002414973261157596
Loss at iteration [833]: 0.0024148905481508405
Loss at iteration [834]: 0.0024148260681920446
Loss at iteration [835]: 0.0024148260681920446
Loss at iteration [836]: 0.0024147902189008944
Loss at iteration [837]: 0.002414717703232481
Loss at iteration [838]: 0.0024146304206498297
Loss at iteration [839]: 0.0024145860805486774
Loss at iteration [840]: 0.0024145366555775602
Loss at iteration [841]: 0.002414489843875046
Loss at iteration [842]: 0.002414489843875046
Loss at iteration [843]: 0.0024144578685515268
Loss at iteration [844]: 0.002414446706236268
Loss at iteration [845]: 0.002414385029774599
Loss at iteration [846]: 0.002414365366836431
Loss at iteration [847]: 0.0024143477390235653
Loss at iteration [848]: 0.002414316177746992
Loss at iteration [849]: 0.002414316177746992
Loss at iteration [850]: 0.0024143026215250702
Loss at iteration [851]: 0.0024142814693573064
Loss at iteration [852]: 0.0024142603549067413
Loss at iteration [853]: 0.002414250082174666
Loss at iteration [854]: 0.002414223522322521
Loss at iteration [855]: 0.002414169404684794
Loss at iteration [856]: 0.0024140761186293175
