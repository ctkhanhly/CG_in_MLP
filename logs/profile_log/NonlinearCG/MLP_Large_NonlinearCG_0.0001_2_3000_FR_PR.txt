Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :FR_PR
Total number of function evaluations  : 3048
Total number of iterations            : 591
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 41.72246241569519
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.99912935539961%
Percentage of parameters < 1e-7       : 49.99912935539961%
Percentage of parameters < 1e-6       : 49.999875622199944%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.004121680728030831
Loss at iteration [2]: 0.003963555252896683
Loss at iteration [3]: 0.0038298021183997217
Loss at iteration [4]: 0.003303654520308497
Loss at iteration [5]: 0.0031978750288014497
Loss at iteration [6]: 0.0030612584627697738
Loss at iteration [7]: 0.0030612584627697738
Loss at iteration [8]: 0.0029898452622606213
Loss at iteration [9]: 0.0029761090280541014
Loss at iteration [10]: 0.002758855213687668
Loss at iteration [11]: 0.0027435452555523714
Loss at iteration [12]: 0.002627188330109241
Loss at iteration [13]: 0.002627188330109241
Loss at iteration [14]: 0.0026081398005583854
Loss at iteration [15]: 0.0026051704214922326
Loss at iteration [16]: 0.002580235707671704
Loss at iteration [17]: 0.0025702054360662624
Loss at iteration [18]: 0.0025690984423959676
Loss at iteration [19]: 0.0025676493225591335
Loss at iteration [20]: 0.0025676493225591335
Loss at iteration [21]: 0.002566560473911342
Loss at iteration [22]: 0.002564978096900753
Loss at iteration [23]: 0.0025530268057751583
Loss at iteration [24]: 0.002550525745276042
Loss at iteration [25]: 0.002541603682637004
Loss at iteration [26]: 0.002541603682637004
Loss at iteration [27]: 0.0025384239880756774
Loss at iteration [28]: 0.002537527728368766
Loss at iteration [29]: 0.0025302280481195967
Loss at iteration [30]: 0.0025296127532304963
Loss at iteration [31]: 0.0025289412218172953
Loss at iteration [32]: 0.002524913698970851
Loss at iteration [33]: 0.002524913698970851
Loss at iteration [34]: 0.0025238916725218454
Loss at iteration [35]: 0.0025204377497790704
Loss at iteration [36]: 0.0025201645241390578
Loss at iteration [37]: 0.002516494234871225
Loss at iteration [38]: 0.0025159715432164585
Loss at iteration [39]: 0.0025159715432164585
Loss at iteration [40]: 0.0025143779276955835
Loss at iteration [41]: 0.0025118680286817206
Loss at iteration [42]: 0.0025110011575474826
Loss at iteration [43]: 0.0025106187784095773
Loss at iteration [44]: 0.0025083766874823984
Loss at iteration [45]: 0.0025083766874823984
Loss at iteration [46]: 0.0025078481923266516
Loss at iteration [47]: 0.0025077562345797643
Loss at iteration [48]: 0.0025039385918377228
Loss at iteration [49]: 0.002503271606858426
Loss at iteration [50]: 0.0024988814382401075
Loss at iteration [51]: 0.0024988814382401075
Loss at iteration [52]: 0.0024969302918497398
Loss at iteration [53]: 0.002496710884401123
Loss at iteration [54]: 0.002496480856642379
Loss at iteration [55]: 0.002494938167640095
Loss at iteration [56]: 0.0024946940319774086
Loss at iteration [57]: 0.0024946940319774086
Loss at iteration [58]: 0.0024945904211372176
Loss at iteration [59]: 0.002493877757384359
Loss at iteration [60]: 0.0024937918292647202
Loss at iteration [61]: 0.002493649906721335
Loss at iteration [62]: 0.002492221318656838
Loss at iteration [63]: 0.002492221318656838
Loss at iteration [64]: 0.0024912249464797043
Loss at iteration [65]: 0.002491147471417813
Loss at iteration [66]: 0.0024907122437248597
Loss at iteration [67]: 0.0024885341221776705
Loss at iteration [68]: 0.002487516609891948
Loss at iteration [69]: 0.002487516609891948
Loss at iteration [70]: 0.0024873266618958612
Loss at iteration [71]: 0.002486092815466373
Loss at iteration [72]: 0.0024850746532684978
Loss at iteration [73]: 0.0024839072257065646
Loss at iteration [74]: 0.002483811612411808
Loss at iteration [75]: 0.002483811612411808
Loss at iteration [76]: 0.0024837550158225974
Loss at iteration [77]: 0.002483526476547893
Loss at iteration [78]: 0.0024830463185206387
Loss at iteration [79]: 0.002482424299500749
Loss at iteration [80]: 0.0024821057914405113
Loss at iteration [81]: 0.0024818197635968872
Loss at iteration [82]: 0.0024818197635968872
Loss at iteration [83]: 0.002481597932273119
Loss at iteration [84]: 0.0024814700368399954
Loss at iteration [85]: 0.0024810518880739245
Loss at iteration [86]: 0.0024807866513679518
Loss at iteration [87]: 0.0024807360968401213
Loss at iteration [88]: 0.0024806192086272933
Loss at iteration [89]: 0.0024806192086272933
Loss at iteration [90]: 0.0024805104301049963
Loss at iteration [91]: 0.002480463148920506
Loss at iteration [92]: 0.002480037006780596
Loss at iteration [93]: 0.0024766616944585946
Loss at iteration [94]: 0.0024750011618080566
Loss at iteration [95]: 0.0024750011618080566
Loss at iteration [96]: 0.0024741545038711897
Loss at iteration [97]: 0.0024701159057878307
Loss at iteration [98]: 0.0024695645074145497
Loss at iteration [99]: 0.002469424777141077
Loss at iteration [100]: 0.0024689138875145684
Loss at iteration [101]: 0.0024686298826877865
Loss at iteration [102]: 0.0024686298826877865
Loss at iteration [103]: 0.0024681914045263788
Loss at iteration [104]: 0.0024680790716445395
Loss at iteration [105]: 0.002467452528737924
Loss at iteration [106]: 0.0024672009473694725
Loss at iteration [107]: 0.002465568318150295
Loss at iteration [108]: 0.002465568318150295
Loss at iteration [109]: 0.0024650370790779033
Loss at iteration [110]: 0.0024649610776230546
Loss at iteration [111]: 0.0024645623164920856
Loss at iteration [112]: 0.0024643602328239035
Loss at iteration [113]: 0.002464314413977742
Loss at iteration [114]: 0.002463501633096594
Loss at iteration [115]: 0.002463501633096594
Loss at iteration [116]: 0.002463419766117063
Loss at iteration [117]: 0.002463237371930318
Loss at iteration [118]: 0.002463090060014813
Loss at iteration [119]: 0.0024628925203391532
Loss at iteration [120]: 0.0024627428905957057
Loss at iteration [121]: 0.0024627428905957057
Loss at iteration [122]: 0.002462678317036188
Loss at iteration [123]: 0.0024625814204550815
Loss at iteration [124]: 0.002462447883573403
Loss at iteration [125]: 0.0024624221870125983
Loss at iteration [126]: 0.0024621711304978694
Loss at iteration [127]: 0.0024621071650419582
Loss at iteration [128]: 0.0024621071650419582
Loss at iteration [129]: 0.0024620544699813485
Loss at iteration [130]: 0.002462017548730804
Loss at iteration [131]: 0.00246186758485026
Loss at iteration [132]: 0.0024616834564314375
Loss at iteration [133]: 0.0024615855964419805
Loss at iteration [134]: 0.002461302857887045
Loss at iteration [135]: 0.002461302857887045
Loss at iteration [136]: 0.002461207971282751
Loss at iteration [137]: 0.0024611678983716837
Loss at iteration [138]: 0.0024609736653948716
Loss at iteration [139]: 0.002460878681321487
Loss at iteration [140]: 0.0024607136670080853
Loss at iteration [141]: 0.0024607136670080853
Loss at iteration [142]: 0.00246045303865519
Loss at iteration [143]: 0.0024604267520395196
Loss at iteration [144]: 0.002459871958017148
Loss at iteration [145]: 0.0024591513717790794
Loss at iteration [146]: 0.0024589661052345268
Loss at iteration [147]: 0.0024589661052345268
Loss at iteration [148]: 0.002458755343730521
Loss at iteration [149]: 0.0024582143903055026
Loss at iteration [150]: 0.002458195903398761
Loss at iteration [151]: 0.002458062597660232
Loss at iteration [152]: 0.002457582761683083
Loss at iteration [153]: 0.002457582761683083
Loss at iteration [154]: 0.002457268945453805
Loss at iteration [155]: 0.0024567693426414874
Loss at iteration [156]: 0.0024566661939755437
Loss at iteration [157]: 0.0024564932097487753
Loss at iteration [158]: 0.0024564160924813385
Loss at iteration [159]: 0.0024564160924813385
Loss at iteration [160]: 0.002456387168980346
Loss at iteration [161]: 0.002456325505457668
Loss at iteration [162]: 0.002455726088392575
Loss at iteration [163]: 0.0024556790741245326
Loss at iteration [164]: 0.0024556205431900766
Loss at iteration [165]: 0.0024556205431900766
Loss at iteration [166]: 0.0024555715871861167
Loss at iteration [167]: 0.002455529141335172
Loss at iteration [168]: 0.0024550491178299176
Loss at iteration [169]: 0.002454970339031481
Loss at iteration [170]: 0.0024548999067451767
Loss at iteration [171]: 0.0024548999067451767
Loss at iteration [172]: 0.002454847308431463
Loss at iteration [173]: 0.002454759733017377
Loss at iteration [174]: 0.002454470522109568
Loss at iteration [175]: 0.0024544349069414583
Loss at iteration [176]: 0.0024542273205535848
Loss at iteration [177]: 0.0024542273205535848
Loss at iteration [178]: 0.0024541369690515512
Loss at iteration [179]: 0.002454093374917666
Loss at iteration [180]: 0.0024539674949415145
Loss at iteration [181]: 0.0024538853911668743
Loss at iteration [182]: 0.0024538012305592495
Loss at iteration [183]: 0.0024537045906189063
Loss at iteration [184]: 0.0024537045906189063
Loss at iteration [185]: 0.0024536776549922827
Loss at iteration [186]: 0.0024534890379551365
Loss at iteration [187]: 0.0024530379324283016
Loss at iteration [188]: 0.0024528398542417837
Loss at iteration [189]: 0.0024526134751567353
Loss at iteration [190]: 0.0024526134751567353
Loss at iteration [191]: 0.002452472447548801
Loss at iteration [192]: 0.0024523034689563046
Loss at iteration [193]: 0.0024521841449421894
Loss at iteration [194]: 0.00245206895458301
Loss at iteration [195]: 0.0024518475618272935
Loss at iteration [196]: 0.0024518475618272935
Loss at iteration [197]: 0.0024517875527621984
Loss at iteration [198]: 0.0024517367985926303
Loss at iteration [199]: 0.002451618570354109
Loss at iteration [200]: 0.0024515375518540995
Loss at iteration [201]: 0.002451512857168944
Loss at iteration [202]: 0.0024514795022696012
Loss at iteration [203]: 0.0024514795022696012
Loss at iteration [204]: 0.0024514549325260035
Loss at iteration [205]: 0.002451370937721004
Loss at iteration [206]: 0.00245126242486619
Loss at iteration [207]: 0.002451130390568139
Loss at iteration [208]: 0.002450901751036561
Loss at iteration [209]: 0.002450901751036561
Loss at iteration [210]: 0.002450861859642727
Loss at iteration [211]: 0.002450833733401773
Loss at iteration [212]: 0.002450720959214304
Loss at iteration [213]: 0.002450623347584536
Loss at iteration [214]: 0.002450575549823736
Loss at iteration [215]: 0.002450575549823736
Loss at iteration [216]: 0.0024505422892395078
Loss at iteration [217]: 0.0024504183750545007
Loss at iteration [218]: 0.002450367994316244
Loss at iteration [219]: 0.002450337351097639
Loss at iteration [220]: 0.002450244921452237
Loss at iteration [221]: 0.002450244921452237
Loss at iteration [222]: 0.002450192415302159
Loss at iteration [223]: 0.002450150926130807
Loss at iteration [224]: 0.0024500655609023293
Loss at iteration [225]: 0.002449816161682713
Loss at iteration [226]: 0.002449575664036531
Loss at iteration [227]: 0.002449575664036531
Loss at iteration [228]: 0.0024495309471226543
Loss at iteration [229]: 0.0024494649699902268
Loss at iteration [230]: 0.0024493598814266265
Loss at iteration [231]: 0.0024492746432641438
Loss at iteration [232]: 0.002449179783774509
Loss at iteration [233]: 0.002449179783774509
Loss at iteration [234]: 0.002449132640152932
Loss at iteration [235]: 0.002449114929552645
Loss at iteration [236]: 0.0024487880051645874
Loss at iteration [237]: 0.002448136806266778
Loss at iteration [238]: 0.002447506399685181
Loss at iteration [239]: 0.002447506399685181
Loss at iteration [240]: 0.0024470964863777156
Loss at iteration [241]: 0.002447032832055714
Loss at iteration [242]: 0.0024466673396999847
Loss at iteration [243]: 0.0024466140471810743
Loss at iteration [244]: 0.0024464545531451896
Loss at iteration [245]: 0.0024464545531451896
Loss at iteration [246]: 0.0024463633740384426
Loss at iteration [247]: 0.00244634739839527
Loss at iteration [248]: 0.002446139845060197
Loss at iteration [249]: 0.0024461111622061382
Loss at iteration [250]: 0.0024457340451065623
Loss at iteration [251]: 0.0024457340451065623
Loss at iteration [252]: 0.0024454094458155476
Loss at iteration [253]: 0.0024453702271269515
Loss at iteration [254]: 0.0024451514159858326
Loss at iteration [255]: 0.002445078278416423
Loss at iteration [256]: 0.002445037129542337
Loss at iteration [257]: 0.002445037129542337
Loss at iteration [258]: 0.0024449893321202443
Loss at iteration [259]: 0.0024449607215339293
Loss at iteration [260]: 0.0024447782526370457
Loss at iteration [261]: 0.0024447414945935756
Loss at iteration [262]: 0.002444384464124385
Loss at iteration [263]: 0.002444384464124385
Loss at iteration [264]: 0.0024442038537481115
Loss at iteration [265]: 0.002444012182423826
Loss at iteration [266]: 0.002443834806178706
Loss at iteration [267]: 0.0024437416991178553
Loss at iteration [268]: 0.0024436913874379463
Loss at iteration [269]: 0.0024436913874379463
Loss at iteration [270]: 0.0024436782276971766
Loss at iteration [271]: 0.002443591768200025
Loss at iteration [272]: 0.00244336893051168
Loss at iteration [273]: 0.002443314914365625
Loss at iteration [274]: 0.0024431338189146615
Loss at iteration [275]: 0.0024431338189146615
Loss at iteration [276]: 0.0024430674673997423
Loss at iteration [277]: 0.002443037129387638
Loss at iteration [278]: 0.002442938950926271
Loss at iteration [279]: 0.002442859832813987
Loss at iteration [280]: 0.002442835956012127
Loss at iteration [281]: 0.0024428007602208457
Loss at iteration [282]: 0.0024428007602208457
Loss at iteration [283]: 0.002442775678409928
Loss at iteration [284]: 0.0024427380218693144
Loss at iteration [285]: 0.0024426664053039
Loss at iteration [286]: 0.0024425787869390256
Loss at iteration [287]: 0.002442509529409002
Loss at iteration [288]: 0.002442509529409002
Loss at iteration [289]: 0.002442426000861712
Loss at iteration [290]: 0.0024423456421855468
Loss at iteration [291]: 0.0024423125485722287
Loss at iteration [292]: 0.002442237116552645
Loss at iteration [293]: 0.002442094685195888
Loss at iteration [294]: 0.002442094685195888
Loss at iteration [295]: 0.002442052365860474
Loss at iteration [296]: 0.0024419921830233305
Loss at iteration [297]: 0.0024419059515848477
Loss at iteration [298]: 0.0024418508039135227
Loss at iteration [299]: 0.0024418193195159038
Loss at iteration [300]: 0.0024416859411615457
Loss at iteration [301]: 0.0024416859411615457
Loss at iteration [302]: 0.002441676098710449
Loss at iteration [303]: 0.002441613907295153
Loss at iteration [304]: 0.0024414948980343385
Loss at iteration [305]: 0.0024414665672636213
Loss at iteration [306]: 0.002441380320764801
Loss at iteration [307]: 0.002441380320764801
Loss at iteration [308]: 0.002441310213460836
Loss at iteration [309]: 0.002441295683716399
Loss at iteration [310]: 0.002441121362587886
Loss at iteration [311]: 0.0024410584692751043
Loss at iteration [312]: 0.0024410216801704363
Loss at iteration [313]: 0.0024410216801704363
Loss at iteration [314]: 0.002440998083610039
Loss at iteration [315]: 0.002440957704841895
Loss at iteration [316]: 0.0024408075048517846
Loss at iteration [317]: 0.002440744900640745
Loss at iteration [318]: 0.0024407146372123394
Loss at iteration [319]: 0.0024407146372123394
Loss at iteration [320]: 0.0024406969745883887
Loss at iteration [321]: 0.0024406145163762805
Loss at iteration [322]: 0.002440453337170936
Loss at iteration [323]: 0.0024404293129861596
Loss at iteration [324]: 0.0024402811689309034
Loss at iteration [325]: 0.0024402811689309034
Loss at iteration [326]: 0.002440228514112362
Loss at iteration [327]: 0.0024402038218175885
Loss at iteration [328]: 0.0024400337957587734
Loss at iteration [329]: 0.0024400101560874197
Loss at iteration [330]: 0.0024399806755441584
Loss at iteration [331]: 0.0024399806755441584
Loss at iteration [332]: 0.0024399582967227435
Loss at iteration [333]: 0.0024399340903494414
Loss at iteration [334]: 0.0024398513835990772
Loss at iteration [335]: 0.002439754032573128
Loss at iteration [336]: 0.002439709878823601
Loss at iteration [337]: 0.002439709878823601
Loss at iteration [338]: 0.0024396630058197526
Loss at iteration [339]: 0.002439601353807569
Loss at iteration [340]: 0.002439483537156281
Loss at iteration [341]: 0.0024394731197417167
Loss at iteration [342]: 0.0024394481947384934
Loss at iteration [343]: 0.0024394481947384934
Loss at iteration [344]: 0.002439434317330701
Loss at iteration [345]: 0.002439403295820679
Loss at iteration [346]: 0.0024393049138800578
Loss at iteration [347]: 0.002439215358733295
Loss at iteration [348]: 0.0024388921929613112
Loss at iteration [349]: 0.0024388921929613112
Loss at iteration [350]: 0.00243867277778604
Loss at iteration [351]: 0.002438645622347254
Loss at iteration [352]: 0.0024383988844057084
Loss at iteration [353]: 0.0024383789611366995
Loss at iteration [354]: 0.0024379740386574313
Loss at iteration [355]: 0.0024379740386574313
Loss at iteration [356]: 0.0024379492785995523
Loss at iteration [357]: 0.0024379063169285647
Loss at iteration [358]: 0.002437702169906173
Loss at iteration [359]: 0.002437625975397432
Loss at iteration [360]: 0.0024375455626589946
Loss at iteration [361]: 0.0024375455626589946
Loss at iteration [362]: 0.0024374744242853194
Loss at iteration [363]: 0.0024374199066526676
Loss at iteration [364]: 0.002437290755968025
Loss at iteration [365]: 0.0024372530879728724
Loss at iteration [366]: 0.0024371259931763966
Loss at iteration [367]: 0.0024371259931763966
Loss at iteration [368]: 0.0024370993475420623
Loss at iteration [369]: 0.002437064345099038
Loss at iteration [370]: 0.002437014291994451
Loss at iteration [371]: 0.0024369507380857533
Loss at iteration [372]: 0.0024368671859016358
Loss at iteration [373]: 0.0024368443640365033
Loss at iteration [374]: 0.0024368443640365033
Loss at iteration [375]: 0.002436825578143764
Loss at iteration [376]: 0.0024367789015103843
Loss at iteration [377]: 0.0024366845145003166
Loss at iteration [378]: 0.0024366738644541375
Loss at iteration [379]: 0.00243664499263647
Loss at iteration [380]: 0.00243664499263647
Loss at iteration [381]: 0.0024366254855042293
Loss at iteration [382]: 0.0024365978742314984
Loss at iteration [383]: 0.002436503610593032
Loss at iteration [384]: 0.00243643773814631
Loss at iteration [385]: 0.002436306998129136
Loss at iteration [386]: 0.002436306998129136
Loss at iteration [387]: 0.002436241466309036
Loss at iteration [388]: 0.002436197856820468
Loss at iteration [389]: 0.0024361384991101686
Loss at iteration [390]: 0.002436047209529357
Loss at iteration [391]: 0.0024360211436712373
Loss at iteration [392]: 0.0024360211436712373
Loss at iteration [393]: 0.002436006350392365
Loss at iteration [394]: 0.0024359464271362383
Loss at iteration [395]: 0.002435854489649131
Loss at iteration [396]: 0.002435837746004896
Loss at iteration [397]: 0.002435793256315745
Loss at iteration [398]: 0.002435793256315745
Loss at iteration [399]: 0.002435751409846272
Loss at iteration [400]: 0.002435739979558675
Loss at iteration [401]: 0.002435614215612067
Loss at iteration [402]: 0.0024355371848822204
Loss at iteration [403]: 0.0024354849398342117
Loss at iteration [404]: 0.0024354849398342117
Loss at iteration [405]: 0.0024354505492307993
Loss at iteration [406]: 0.002435385066871141
Loss at iteration [407]: 0.0024352317860471804
Loss at iteration [408]: 0.0024352047324355925
Loss at iteration [409]: 0.002434908988420228
Loss at iteration [410]: 0.002434908988420228
Loss at iteration [411]: 0.002434777408840201
Loss at iteration [412]: 0.0024347548991695014
Loss at iteration [413]: 0.0024346647050599722
Loss at iteration [414]: 0.002434606979018775
Loss at iteration [415]: 0.002434582860394802
Loss at iteration [416]: 0.0024344427276170397
Loss at iteration [417]: 0.0024344427276170397
Loss at iteration [418]: 0.0024343825175974017
Loss at iteration [419]: 0.0024342867560697046
Loss at iteration [420]: 0.0024342039256229973
Loss at iteration [421]: 0.0024341703529370184
Loss at iteration [422]: 0.0024341516545097106
Loss at iteration [423]: 0.002434120810496797
Loss at iteration [424]: 0.002434120810496797
Loss at iteration [425]: 0.002434101911780864
Loss at iteration [426]: 0.0024340788509341024
Loss at iteration [427]: 0.0024340095534076595
Loss at iteration [428]: 0.0024338301819603115
Loss at iteration [429]: 0.002433632119741724
Loss at iteration [430]: 0.002433632119741724
Loss at iteration [431]: 0.002433565643733497
Loss at iteration [432]: 0.0024335328264355935
Loss at iteration [433]: 0.0024333979068861854
Loss at iteration [434]: 0.0024333741820061336
Loss at iteration [435]: 0.0024333174498388185
Loss at iteration [436]: 0.0024331167210738004
Loss at iteration [437]: 0.0024331167210738004
Loss at iteration [438]: 0.0024330958316019775
Loss at iteration [439]: 0.0024330584368907468
Loss at iteration [440]: 0.002432914966258189
Loss at iteration [441]: 0.002432895012540205
Loss at iteration [442]: 0.002432849479169475
Loss at iteration [443]: 0.002432849479169475
Loss at iteration [444]: 0.0024328334025542405
Loss at iteration [445]: 0.0024328124108103045
Loss at iteration [446]: 0.00243276139500359
Loss at iteration [447]: 0.002432739303404307
Loss at iteration [448]: 0.00243266205936378
Loss at iteration [449]: 0.00243266205936378
Loss at iteration [450]: 0.0024326277723969586
Loss at iteration [451]: 0.002432575010409904
Loss at iteration [452]: 0.0024325571259780957
Loss at iteration [453]: 0.0024324444549261716
Loss at iteration [454]: 0.0024320782450923673
Loss at iteration [455]: 0.0024320782450923673
Loss at iteration [456]: 0.0024319379261586766
Loss at iteration [457]: 0.0024316791425726546
Loss at iteration [458]: 0.0024316462205439507
Loss at iteration [459]: 0.0024314981179121924
Loss at iteration [460]: 0.0024314677347340944
Loss at iteration [461]: 0.0024314677347340944
Loss at iteration [462]: 0.0024314541717584525
Loss at iteration [463]: 0.0024313762312097764
Loss at iteration [464]: 0.002431360989576229
Loss at iteration [465]: 0.002431324311846831
Loss at iteration [466]: 0.002431225030662476
Loss at iteration [467]: 0.002431225030662476
Loss at iteration [468]: 0.0024311836574577826
Loss at iteration [469]: 0.0024311497215342057
Loss at iteration [470]: 0.0024310598909949984
Loss at iteration [471]: 0.002431008861065922
Loss at iteration [472]: 0.0024309497766169677
Loss at iteration [473]: 0.002430928476732038
Loss at iteration [474]: 0.002430928476732038
Loss at iteration [475]: 0.002430907713224903
Loss at iteration [476]: 0.0024308315423653176
Loss at iteration [477]: 0.00243071736105897
Loss at iteration [478]: 0.0024307049186798667
Loss at iteration [479]: 0.002430685782898308
Loss at iteration [480]: 0.002430685782898308
Loss at iteration [481]: 0.002430672597712362
Loss at iteration [482]: 0.002430653865914884
Loss at iteration [483]: 0.0024305377999756816
Loss at iteration [484]: 0.002430494091437319
Loss at iteration [485]: 0.0024303786939809725
Loss at iteration [486]: 0.0024303786939809725
Loss at iteration [487]: 0.002430279104470498
Loss at iteration [488]: 0.0024302562340097816
Loss at iteration [489]: 0.0024301459710773876
Loss at iteration [490]: 0.0024300902566953666
Loss at iteration [491]: 0.002430063699289974
Loss at iteration [492]: 0.0024298934836306687
Loss at iteration [493]: 0.0024298934836306687
Loss at iteration [494]: 0.002429861759964364
Loss at iteration [495]: 0.0024297299459057147
Loss at iteration [496]: 0.002429672447298975
Loss at iteration [497]: 0.002429633697527813
Loss at iteration [498]: 0.0024293002727150495
Loss at iteration [499]: 0.0024293002727150495
Loss at iteration [500]: 0.0024292594285553176
Loss at iteration [501]: 0.0024291964607653727
Loss at iteration [502]: 0.002428952000809109
Loss at iteration [503]: 0.002428929374364331
Loss at iteration [504]: 0.002428856942796035
Loss at iteration [505]: 0.002428856942796035
Loss at iteration [506]: 0.0024288251111484528
Loss at iteration [507]: 0.002428807375503018
Loss at iteration [508]: 0.0024287197688705126
Loss at iteration [509]: 0.002428690968614134
Loss at iteration [510]: 0.0024285164725380043
Loss at iteration [511]: 0.0024285164725380043
Loss at iteration [512]: 0.0024284416423025357
Loss at iteration [513]: 0.002428411375460245
Loss at iteration [514]: 0.002428362057536314
Loss at iteration [515]: 0.0024283451388450767
Loss at iteration [516]: 0.002428314302204672
Loss at iteration [517]: 0.002428314302204672
Loss at iteration [518]: 0.002428303169606223
Loss at iteration [519]: 0.00242828196276237
Loss at iteration [520]: 0.0024282645522985557
Loss at iteration [521]: 0.002428243618275357
Loss at iteration [522]: 0.0024281754482562024
Loss at iteration [523]: 0.0024281754482562024
Loss at iteration [524]: 0.0024281484643853125
Loss at iteration [525]: 0.0024280947622607224
Loss at iteration [526]: 0.0024280818057877927
Loss at iteration [527]: 0.002428051110992959
Loss at iteration [528]: 0.00242799391902454
Loss at iteration [529]: 0.00242799391902454
Loss at iteration [530]: 0.0024279776388648026
Loss at iteration [531]: 0.0024279091873636587
Loss at iteration [532]: 0.0024278872381484993
Loss at iteration [533]: 0.0024278280304746614
Loss at iteration [534]: 0.0024277188327414922
Loss at iteration [535]: 0.0024277188327414922
Loss at iteration [536]: 0.0024277066964695847
Loss at iteration [537]: 0.002427656795180806
Loss at iteration [538]: 0.002427640401121699
Loss at iteration [539]: 0.0024275969964353663
Loss at iteration [540]: 0.0024275814898506807
Loss at iteration [541]: 0.0024275814898506807
Loss at iteration [542]: 0.002427568341351454
Loss at iteration [543]: 0.0024275280550027714
Loss at iteration [544]: 0.002427518390986829
Loss at iteration [545]: 0.0024274674262158907
Loss at iteration [546]: 0.0024274304204738775
Loss at iteration [547]: 0.0024274304204738775
Loss at iteration [548]: 0.0024274091634901873
Loss at iteration [549]: 0.0024273630028351805
Loss at iteration [550]: 0.0024273431744814278
Loss at iteration [551]: 0.0024273054783584295
Loss at iteration [552]: 0.0024272665675516775
Loss at iteration [553]: 0.0024272665675516775
Loss at iteration [554]: 0.002427250424672259
Loss at iteration [555]: 0.0024272164085305708
Loss at iteration [556]: 0.0024271948890968497
Loss at iteration [557]: 0.002427016510042482
Loss at iteration [558]: 0.0024268946345036902
Loss at iteration [559]: 0.0024268946345036902
Loss at iteration [560]: 0.0024268596175321534
Loss at iteration [561]: 0.002426799689276513
Loss at iteration [562]: 0.002426721643013825
Loss at iteration [563]: 0.0024266711454891867
Loss at iteration [564]: 0.0024265751417953587
Loss at iteration [565]: 0.0024265751417953587
Loss at iteration [566]: 0.0024264900763185725
Loss at iteration [567]: 0.0024264708300274634
Loss at iteration [568]: 0.0024264038149614223
Loss at iteration [569]: 0.0024263656494726534
Loss at iteration [570]: 0.002426344546767156
Loss at iteration [571]: 0.0024262963764362577
Loss at iteration [572]: 0.0024262963764362577
Loss at iteration [573]: 0.002426283212487073
Loss at iteration [574]: 0.0024262434199579194
Loss at iteration [575]: 0.002426220980798082
Loss at iteration [576]: 0.0024261138973656567
Loss at iteration [577]: 0.0024260229442459947
Loss at iteration [578]: 0.00242597191561681
Loss at iteration [579]: 0.00242597191561681
Loss at iteration [580]: 0.002425946505346528
Loss at iteration [581]: 0.0024259103856178743
Loss at iteration [582]: 0.002425827649285279
Loss at iteration [583]: 0.002425810467192368
Loss at iteration [584]: 0.0024257628649133767
Loss at iteration [585]: 0.0024257628649133767
Loss at iteration [586]: 0.0024257513042480663
Loss at iteration [587]: 0.0024257231892727605
Loss at iteration [588]: 0.002425684343206609
Loss at iteration [589]: 0.002425661032182658
Loss at iteration [590]: 0.0024255583666278517
Loss at iteration [591]: 0.0024255583666278517
Loss at iteration [592]: 0.00242549299934044
Loss at iteration [593]: 0.0024253510515816258
Loss at iteration [594]: 0.002425268578823075
Loss at iteration [595]: 0.0024252492551785788
Loss at iteration [596]: 0.0024251954678810705
Loss at iteration [597]: 0.0024251954678810705
Loss at iteration [598]: 0.00242515775238392
Loss at iteration [599]: 0.0024251408087602997
Loss at iteration [600]: 0.0024250799491394087
Loss at iteration [601]: 0.0024250561639025127
Loss at iteration [602]: 0.002425023087974228
Loss at iteration [603]: 0.002425023087974228
Loss at iteration [604]: 0.0024250051989150373
Loss at iteration [605]: 0.002424967179591449
Loss at iteration [606]: 0.0024249389751890447
Loss at iteration [607]: 0.0024249119519329566
Loss at iteration [608]: 0.0024248663308722987
Loss at iteration [609]: 0.0024248663308722987
Loss at iteration [610]: 0.002424831751147547
Loss at iteration [611]: 0.0024247588193487585
Loss at iteration [612]: 0.0024247430860853173
Loss at iteration [613]: 0.0024247247880488506
Loss at iteration [614]: 0.002424708236335338
Loss at iteration [615]: 0.002424692230228022
Loss at iteration [616]: 0.002424692230228022
Loss at iteration [617]: 0.0024246793912568705
Loss at iteration [618]: 0.002424660526117178
Loss at iteration [619]: 0.0024246257369024762
Loss at iteration [620]: 0.0024245584881220296
Loss at iteration [621]: 0.0024245067265117938
Loss at iteration [622]: 0.0024245067265117938
Loss at iteration [623]: 0.002424475233482969
Loss at iteration [624]: 0.0024243012263653724
Loss at iteration [625]: 0.0024242453841069093
Loss at iteration [626]: 0.002424202554977935
Loss at iteration [627]: 0.0024241596255093053
Loss at iteration [628]: 0.0024241596255093053
Loss at iteration [629]: 0.0024241461100000406
Loss at iteration [630]: 0.0024240763283201994
Loss at iteration [631]: 0.0024240025581167275
Loss at iteration [632]: 0.002423965274533345
Loss at iteration [633]: 0.002423930655525232
Loss at iteration [634]: 0.002423930655525232
Loss at iteration [635]: 0.002423904049822243
Loss at iteration [636]: 0.00242385908440225
Loss at iteration [637]: 0.002423810397120462
Loss at iteration [638]: 0.00242379013126917
Loss at iteration [639]: 0.002423740857450782
Loss at iteration [640]: 0.002423740857450782
Loss at iteration [641]: 0.002423702152825205
Loss at iteration [642]: 0.00242369079459689
Loss at iteration [643]: 0.0024236577951470516
Loss at iteration [644]: 0.002423637532223457
Loss at iteration [645]: 0.0024235691896645145
Loss at iteration [646]: 0.0024235691896645145
Loss at iteration [647]: 0.0024235513238632803
Loss at iteration [648]: 0.0024235061322175977
Loss at iteration [649]: 0.0024234626895980435
Loss at iteration [650]: 0.0024234448396723497
Loss at iteration [651]: 0.0024234234881109246
Loss at iteration [652]: 0.0024234234881109246
Loss at iteration [653]: 0.0024234087884223996
Loss at iteration [654]: 0.0024233946572769787
Loss at iteration [655]: 0.002423359183587274
Loss at iteration [656]: 0.0024233237931587286
Loss at iteration [657]: 0.0024233015190939953
Loss at iteration [658]: 0.0024233015190939953
Loss at iteration [659]: 0.002423283862170359
Loss at iteration [660]: 0.0024232679383851493
Loss at iteration [661]: 0.002423231559265866
Loss at iteration [662]: 0.0024232079459517437
Loss at iteration [663]: 0.0024230854565480027
Loss at iteration [664]: 0.0024230854565480027
Loss at iteration [665]: 0.0024230054528362605
Loss at iteration [666]: 0.002422989179570016
Loss at iteration [667]: 0.0024228840075282278
Loss at iteration [668]: 0.0024228641834276024
Loss at iteration [669]: 0.0024227613169697784
Loss at iteration [670]: 0.0024227613169697784
Loss at iteration [671]: 0.002422725671315203
Loss at iteration [672]: 0.0024227094738492787
Loss at iteration [673]: 0.0024226665520304534
Loss at iteration [674]: 0.002422632125863674
Loss at iteration [675]: 0.0024225904929856532
Loss at iteration [676]: 0.0024225904929856532
Loss at iteration [677]: 0.0024225613517601633
Loss at iteration [678]: 0.0024225090596516102
Loss at iteration [679]: 0.002422480627284139
Loss at iteration [680]: 0.0024224472479540357
Loss at iteration [681]: 0.0024223522087929543
Loss at iteration [682]: 0.0024223522087929543
Loss at iteration [683]: 0.002422331652314617
Loss at iteration [684]: 0.002422259110564462
Loss at iteration [685]: 0.002422227797318777
Loss at iteration [686]: 0.002422198686241745
Loss at iteration [687]: 0.002422181182871607
Loss at iteration [688]: 0.002422181182871607
Loss at iteration [689]: 0.002422167568505998
Loss at iteration [690]: 0.002422129487600911
Loss at iteration [691]: 0.0024221052087273704
Loss at iteration [692]: 0.002421946494054058
Loss at iteration [693]: 0.0024218316672549737
Loss at iteration [694]: 0.0024218316672549737
Loss at iteration [695]: 0.002421759868504533
Loss at iteration [696]: 0.002421684898014301
Loss at iteration [697]: 0.0024216486015850944
Loss at iteration [698]: 0.002421595397818423
Loss at iteration [699]: 0.00242147544311309
