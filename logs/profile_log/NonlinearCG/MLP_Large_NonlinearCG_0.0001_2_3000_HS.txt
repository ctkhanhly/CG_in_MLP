Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.0001
Beta type                             :HS
Total number of function evaluations  : 3050
Total number of iterations            : 598
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 41.745654821395874
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 50.10435297424634%
Percentage of parameters < 1e-7       : 50.10460172984644%
Percentage of parameters < 1e-6       : 50.10534799664678%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.010446211058619863
Loss at iteration [2]: 0.008620884623822189
Loss at iteration [3]: 0.007883505210272352
Loss at iteration [4]: 0.004683253888339212
Loss at iteration [5]: 0.004251505236754971
Loss at iteration [6]: 0.0040984577473268426
Loss at iteration [7]: 0.003379979524687927
Loss at iteration [8]: 0.003379979524687927
Loss at iteration [9]: 0.0033286848504004186
Loss at iteration [10]: 0.0032283414717133078
Loss at iteration [11]: 0.0030504386293521346
Loss at iteration [12]: 0.003013436223074914
Loss at iteration [13]: 0.002982265467906316
Loss at iteration [14]: 0.0028433089830811374
Loss at iteration [15]: 0.0028433089830811374
Loss at iteration [16]: 0.0027991298864935084
Loss at iteration [17]: 0.002794904788569605
Loss at iteration [18]: 0.0025899835312086584
Loss at iteration [19]: 0.002573195091373568
Loss at iteration [20]: 0.002567851341357881
Loss at iteration [21]: 0.002567851341357881
Loss at iteration [22]: 0.002562946398122044
Loss at iteration [23]: 0.0025557349577107194
Loss at iteration [24]: 0.0025412220445615876
Loss at iteration [25]: 0.00254049205656682
Loss at iteration [26]: 0.0025377434864032366
Loss at iteration [27]: 0.0025335964424716586
Loss at iteration [28]: 0.0025335964424716586
Loss at iteration [29]: 0.0025324962866531712
Loss at iteration [30]: 0.0025311382403547696
Loss at iteration [31]: 0.00252746723961355
Loss at iteration [32]: 0.0025269256997322645
Loss at iteration [33]: 0.0025244689354902514
Loss at iteration [34]: 0.0025244689354902514
Loss at iteration [35]: 0.0025231382645981254
Loss at iteration [36]: 0.0025228465728016187
Loss at iteration [37]: 0.0025060818165256486
Loss at iteration [38]: 0.002505298502465247
Loss at iteration [39]: 0.002504455279103625
Loss at iteration [40]: 0.002504455279103625
Loss at iteration [41]: 0.0025037578148360533
Loss at iteration [42]: 0.0025035556089657986
Loss at iteration [43]: 0.0025032086520810934
Loss at iteration [44]: 0.0025028573021378857
Loss at iteration [45]: 0.002501959619965238
Loss at iteration [46]: 0.0025014419770897447
Loss at iteration [47]: 0.0025014419770897447
Loss at iteration [48]: 0.0025012210691121267
Loss at iteration [49]: 0.002501062202443093
Loss at iteration [50]: 0.0024987952788630583
Loss at iteration [51]: 0.002498387107159829
Loss at iteration [52]: 0.0024966350105968095
Loss at iteration [53]: 0.002491239082239853
Loss at iteration [54]: 0.002491239082239853
Loss at iteration [55]: 0.002490659410050451
Loss at iteration [56]: 0.0024905752980883505
Loss at iteration [57]: 0.0024885931485977195
Loss at iteration [58]: 0.002487895555856102
Loss at iteration [59]: 0.0024877162985035583
Loss at iteration [60]: 0.0024877162985035583
Loss at iteration [61]: 0.0024876013760838484
Loss at iteration [62]: 0.002486725136878481
Loss at iteration [63]: 0.0024863696668952365
Loss at iteration [64]: 0.0024836749620972365
Loss at iteration [65]: 0.002481831030287019
Loss at iteration [66]: 0.002481831030287019
Loss at iteration [67]: 0.0024816091257297878
Loss at iteration [68]: 0.0024799583545761874
Loss at iteration [69]: 0.002479739677245604
Loss at iteration [70]: 0.0024795667133245343
Loss at iteration [71]: 0.0024786549659848176
Loss at iteration [72]: 0.0024786549659848176
Loss at iteration [73]: 0.0024785583110348117
Loss at iteration [74]: 0.002478416267951345
Loss at iteration [75]: 0.0024780503315413463
Loss at iteration [76]: 0.0024779692088834028
Loss at iteration [77]: 0.002477888993591412
Loss at iteration [78]: 0.00247713868324247
Loss at iteration [79]: 0.00247713868324247
Loss at iteration [80]: 0.002477107449782975
Loss at iteration [81]: 0.0024767938906984435
Loss at iteration [82]: 0.002476487277045876
Loss at iteration [83]: 0.00247593171515528
Loss at iteration [84]: 0.002474129867402818
Loss at iteration [85]: 0.002474129867402818
Loss at iteration [86]: 0.0024736081174760935
Loss at iteration [87]: 0.0024733797407784286
Loss at iteration [88]: 0.002472445832069582
Loss at iteration [89]: 0.002472400522789688
Loss at iteration [90]: 0.00247195247163137
Loss at iteration [91]: 0.00247195247163137
Loss at iteration [92]: 0.002471789685948122
Loss at iteration [93]: 0.0024716583196878576
Loss at iteration [94]: 0.0024714136355805587
Loss at iteration [95]: 0.0024713198916318656
Loss at iteration [96]: 0.002470071875764354
Loss at iteration [97]: 0.002470071875764354
Loss at iteration [98]: 0.0024698203956276497
Loss at iteration [99]: 0.0024693660357231442
Loss at iteration [100]: 0.0024691301807862973
Loss at iteration [101]: 0.002468671031569181
Loss at iteration [102]: 0.002468391253697768
Loss at iteration [103]: 0.002468391253697768
Loss at iteration [104]: 0.0024682965833213825
Loss at iteration [105]: 0.002468062518798934
Loss at iteration [106]: 0.002467997060554844
Loss at iteration [107]: 0.0024674630582346433
Loss at iteration [108]: 0.002466227422408975
Loss at iteration [109]: 0.002466227422408975
Loss at iteration [110]: 0.0024661306440422735
Loss at iteration [111]: 0.0024659787765533487
Loss at iteration [112]: 0.0024656084970551043
Loss at iteration [113]: 0.002465548193971206
Loss at iteration [114]: 0.002465219986233793
Loss at iteration [115]: 0.002465219986233793
Loss at iteration [116]: 0.002465142110654709
Loss at iteration [117]: 0.002465112548070109
Loss at iteration [118]: 0.0024647639735362646
Loss at iteration [119]: 0.00246470856146623
Loss at iteration [120]: 0.0024646189401868294
Loss at iteration [121]: 0.0024641101826108295
Loss at iteration [122]: 0.0024641101826108295
Loss at iteration [123]: 0.002464027503443762
Loss at iteration [124]: 0.0024640008390451533
Loss at iteration [125]: 0.0024637932324091216
Loss at iteration [126]: 0.002463772322933021
Loss at iteration [127]: 0.0024635944103875798
Loss at iteration [128]: 0.0024633631573775625
Loss at iteration [129]: 0.0024633631573775625
Loss at iteration [130]: 0.0024632948572344063
Loss at iteration [131]: 0.0024632495777088454
Loss at iteration [132]: 0.002463016874096607
Loss at iteration [133]: 0.00246298744722271
Loss at iteration [134]: 0.0024629508944543433
Loss at iteration [135]: 0.0024626351188772153
Loss at iteration [136]: 0.0024626351188772153
Loss at iteration [137]: 0.0024624663773847046
Loss at iteration [138]: 0.0024624387664468426
Loss at iteration [139]: 0.0024623747675372567
Loss at iteration [140]: 0.0024622025064046154
Loss at iteration [141]: 0.0024621337804608527
Loss at iteration [142]: 0.00246191692314758
Loss at iteration [143]: 0.00246191692314758
Loss at iteration [144]: 0.0024618496094935428
Loss at iteration [145]: 0.002461793707787975
Loss at iteration [146]: 0.0024615655987449267
Loss at iteration [147]: 0.0024615026636453157
Loss at iteration [148]: 0.002461267812410769
Loss at iteration [149]: 0.002461267812410769
Loss at iteration [150]: 0.0024611579356908753
Loss at iteration [151]: 0.0024611180988816654
Loss at iteration [152]: 0.0024608181216454797
Loss at iteration [153]: 0.0024607916230925883
Loss at iteration [154]: 0.0024607155028968856
Loss at iteration [155]: 0.0024607155028968856
Loss at iteration [156]: 0.002460660982587835
Loss at iteration [157]: 0.0024606421623480797
Loss at iteration [158]: 0.0024604495759337607
Loss at iteration [159]: 0.0024603165616888887
Loss at iteration [160]: 0.00246019312509239
Loss at iteration [161]: 0.00246019312509239
Loss at iteration [162]: 0.0024601392077410315
Loss at iteration [163]: 0.002460048675495105
Loss at iteration [164]: 0.002459943309037126
Loss at iteration [165]: 0.002459917328541357
Loss at iteration [166]: 0.0024598153826718133
Loss at iteration [167]: 0.0024598153826718133
Loss at iteration [168]: 0.0024597899590382308
Loss at iteration [169]: 0.0024597347224357244
Loss at iteration [170]: 0.0024596397088901775
Loss at iteration [171]: 0.0024595120048887067
Loss at iteration [172]: 0.0024588509833139037
Loss at iteration [173]: 0.0024588509833139037
Loss at iteration [174]: 0.0024582677092798693
Loss at iteration [175]: 0.0024581876931254647
Loss at iteration [176]: 0.002457527463063618
Loss at iteration [177]: 0.002456868903814188
Loss at iteration [178]: 0.002456794152129811
Loss at iteration [179]: 0.002456794152129811
Loss at iteration [180]: 0.0024567080784872247
Loss at iteration [181]: 0.0024565880020183013
Loss at iteration [182]: 0.0024562963976775018
Loss at iteration [183]: 0.0024562486615706896
Loss at iteration [184]: 0.0024562094860550755
Loss at iteration [185]: 0.002456089168597539
Loss at iteration [186]: 0.002456089168597539
Loss at iteration [187]: 0.0024559575344567145
Loss at iteration [188]: 0.0024559305124762583
Loss at iteration [189]: 0.0024554533578180584
Loss at iteration [190]: 0.00245530105712678
Loss at iteration [191]: 0.0024551583482613276
Loss at iteration [192]: 0.0024551583482613276
Loss at iteration [193]: 0.0024551076792664055
Loss at iteration [194]: 0.002454962394113549
Loss at iteration [195]: 0.00245478223296758
Loss at iteration [196]: 0.002454705895333983
Loss at iteration [197]: 0.0024546205129932796
Loss at iteration [198]: 0.0024546205129932796
Loss at iteration [199]: 0.00245455149973844
Loss at iteration [200]: 0.0024544591129981035
Loss at iteration [201]: 0.002454320305199339
Loss at iteration [202]: 0.002454296085628299
Loss at iteration [203]: 0.002454227921429581
Loss at iteration [204]: 0.0024541680009237663
Loss at iteration [205]: 0.0024541680009237663
Loss at iteration [206]: 0.002454123767894834
Loss at iteration [207]: 0.002454076733098337
Loss at iteration [208]: 0.002454022745877442
Loss at iteration [209]: 0.002453887974554062
Loss at iteration [210]: 0.0024538111869543276
Loss at iteration [211]: 0.0024538111869543276
Loss at iteration [212]: 0.002453734438109128
Loss at iteration [213]: 0.0024534493594696623
Loss at iteration [214]: 0.0024534138065072193
Loss at iteration [215]: 0.002453378616426534
Loss at iteration [216]: 0.002453354803078651
Loss at iteration [217]: 0.002453354803078651
Loss at iteration [218]: 0.002453333481114952
Loss at iteration [219]: 0.002453255031370861
Loss at iteration [220]: 0.002453181767745893
Loss at iteration [221]: 0.0024531319276012995
Loss at iteration [222]: 0.0024524916140575117
Loss at iteration [223]: 0.0024524916140575117
Loss at iteration [224]: 0.0024522991184149135
Loss at iteration [225]: 0.002452193289631091
Loss at iteration [226]: 0.0024517100872172913
Loss at iteration [227]: 0.0024516602933414766
Loss at iteration [228]: 0.0024516093323996525
Loss at iteration [229]: 0.002451163827546603
Loss at iteration [230]: 0.002451163827546603
Loss at iteration [231]: 0.002451122653237199
Loss at iteration [232]: 0.002450922436321751
Loss at iteration [233]: 0.002450814111834198
Loss at iteration [234]: 0.0024507897275283315
Loss at iteration [235]: 0.0024505889316596683
Loss at iteration [236]: 0.0024505889316596683
Loss at iteration [237]: 0.0024505473385356804
Loss at iteration [238]: 0.0024504850065749768
Loss at iteration [239]: 0.002450375727298221
Loss at iteration [240]: 0.002450345990944889
Loss at iteration [241]: 0.002450326013657293
Loss at iteration [242]: 0.0024503047790826087
Loss at iteration [243]: 0.0024503047790826087
Loss at iteration [244]: 0.002450289284506958
Loss at iteration [245]: 0.00245017767778793
Loss at iteration [246]: 0.002450152734027783
Loss at iteration [247]: 0.0024491657619495123
Loss at iteration [248]: 0.002448901598571855
Loss at iteration [249]: 0.002448901598571855
Loss at iteration [250]: 0.002448826605151074
Loss at iteration [251]: 0.002448639748337542
Loss at iteration [252]: 0.002448536588304053
Loss at iteration [253]: 0.0024484530157645587
Loss at iteration [254]: 0.0024483661146580175
Loss at iteration [255]: 0.0024482886760551184
Loss at iteration [256]: 0.0024482886760551184
Loss at iteration [257]: 0.002448225759381355
Loss at iteration [258]: 0.0024481548389663882
Loss at iteration [259]: 0.002448024183352196
Loss at iteration [260]: 0.002447965677307613
Loss at iteration [261]: 0.002447553358721822
Loss at iteration [262]: 0.002447553358721822
Loss at iteration [263]: 0.0024474424927306182
Loss at iteration [264]: 0.002447372962644066
Loss at iteration [265]: 0.0024470834033752335
Loss at iteration [266]: 0.002446957226217894
Loss at iteration [267]: 0.0024469079784523273
Loss at iteration [268]: 0.0024468514013453197
Loss at iteration [269]: 0.0024468514013453197
Loss at iteration [270]: 0.0024468197232277882
Loss at iteration [271]: 0.0024467850194449473
Loss at iteration [272]: 0.002446632195855035
Loss at iteration [273]: 0.002446613552845199
Loss at iteration [274]: 0.0024465888342070797
Loss at iteration [275]: 0.0024465888342070797
Loss at iteration [276]: 0.0024465702983877415
Loss at iteration [277]: 0.0024465349249705127
Loss at iteration [278]: 0.0024464333369581585
Loss at iteration [279]: 0.002446305106449904
Loss at iteration [280]: 0.0024461039218025378
Loss at iteration [281]: 0.0024461039218025378
Loss at iteration [282]: 0.0024459546150306767
Loss at iteration [283]: 0.002445859192154026
Loss at iteration [284]: 0.0024455463368840895
Loss at iteration [285]: 0.0024455158954595027
Loss at iteration [286]: 0.002445223674152521
Loss at iteration [287]: 0.002445223674152521
Loss at iteration [288]: 0.0024450615727254245
Loss at iteration [289]: 0.00244501877869916
Loss at iteration [290]: 0.0024447678929364644
Loss at iteration [291]: 0.0024447460513578
Loss at iteration [292]: 0.0024444925601490373
Loss at iteration [293]: 0.0024444925601490373
Loss at iteration [294]: 0.0024444333191454685
Loss at iteration [295]: 0.002444406183489684
Loss at iteration [296]: 0.00244428673278307
Loss at iteration [297]: 0.0024442596068502843
Loss at iteration [298]: 0.0024442103443950156
Loss at iteration [299]: 0.0024442103443950156
Loss at iteration [300]: 0.0024441723634359394
Loss at iteration [301]: 0.002444123520287666
Loss at iteration [302]: 0.002444047131536824
Loss at iteration [303]: 0.0024440331295204117
Loss at iteration [304]: 0.002443939303340697
Loss at iteration [305]: 0.0024438615600767617
Loss at iteration [306]: 0.0024438615600767617
Loss at iteration [307]: 0.0024438402873845303
Loss at iteration [308]: 0.0024436143142524567
Loss at iteration [309]: 0.0024435934886727416
Loss at iteration [310]: 0.002443510626395654
Loss at iteration [311]: 0.0024433702342886512
Loss at iteration [312]: 0.0024433702342886512
Loss at iteration [313]: 0.0024433498265074355
Loss at iteration [314]: 0.0024432980564621557
Loss at iteration [315]: 0.0024432221778094916
Loss at iteration [316]: 0.0024431873213257528
Loss at iteration [317]: 0.002443106062226626
Loss at iteration [318]: 0.002443106062226626
Loss at iteration [319]: 0.0024430719746109586
Loss at iteration [320]: 0.002442887569548936
Loss at iteration [321]: 0.0024428681497612077
Loss at iteration [322]: 0.002442759398120338
Loss at iteration [323]: 0.002442694734996554
Loss at iteration [324]: 0.002442694734996554
Loss at iteration [325]: 0.002442645629578861
Loss at iteration [326]: 0.0024424628087096074
Loss at iteration [327]: 0.002442385485323262
Loss at iteration [328]: 0.0024423398615472285
Loss at iteration [329]: 0.0024422682177653032
Loss at iteration [330]: 0.0024422682177653032
Loss at iteration [331]: 0.002442222767503842
Loss at iteration [332]: 0.002442169884215511
Loss at iteration [333]: 0.0024421318954225587
Loss at iteration [334]: 0.0024418701745079255
Loss at iteration [335]: 0.002441780387381978
Loss at iteration [336]: 0.002441780387381978
Loss at iteration [337]: 0.0024417558652340006
Loss at iteration [338]: 0.002441641150017557
Loss at iteration [339]: 0.0024415713148248338
Loss at iteration [340]: 0.0024415470489092895
Loss at iteration [341]: 0.0024414246018697673
Loss at iteration [342]: 0.0024414246018697673
Loss at iteration [343]: 0.0024413661387599566
Loss at iteration [344]: 0.002441344362273472
Loss at iteration [345]: 0.0024411888370758222
Loss at iteration [346]: 0.0024411746004442928
Loss at iteration [347]: 0.002441060079837021
Loss at iteration [348]: 0.002440953052160966
Loss at iteration [349]: 0.002440953052160966
Loss at iteration [350]: 0.002440928675286629
Loss at iteration [351]: 0.002440802818493635
Loss at iteration [352]: 0.0024407850506520876
Loss at iteration [353]: 0.002440764501906282
Loss at iteration [354]: 0.0024405854148003225
Loss at iteration [355]: 0.0024405854148003225
Loss at iteration [356]: 0.0024405547169887108
Loss at iteration [357]: 0.002440493790748732
Loss at iteration [358]: 0.0024404096291991634
Loss at iteration [359]: 0.0024403962355019743
Loss at iteration [360]: 0.0024403908210818004
Loss at iteration [361]: 0.0024403231743059567
Loss at iteration [362]: 0.0024403231743059567
Loss at iteration [363]: 0.002440286918110852
Loss at iteration [364]: 0.002440198404202609
Loss at iteration [365]: 0.0024401837472677527
Loss at iteration [366]: 0.0024401083572223436
Loss at iteration [367]: 0.0024400651481113985
Loss at iteration [368]: 0.0024400651481113985
Loss at iteration [369]: 0.002440047545201277
Loss at iteration [370]: 0.0024399752459168932
Loss at iteration [371]: 0.002439931828865626
Loss at iteration [372]: 0.0024399072544915488
Loss at iteration [373]: 0.0024398340696505315
Loss at iteration [374]: 0.0024398340696505315
Loss at iteration [375]: 0.002439813682351962
Loss at iteration [376]: 0.0024397057426406112
Loss at iteration [377]: 0.0024396876066255786
Loss at iteration [378]: 0.002439664194732346
Loss at iteration [379]: 0.0024396012543533227
Loss at iteration [380]: 0.0024395572124032198
Loss at iteration [381]: 0.0024395572124032198
Loss at iteration [382]: 0.002439536305149271
Loss at iteration [383]: 0.002439512098472595
Loss at iteration [384]: 0.0024394488902894904
Loss at iteration [385]: 0.002439435166982936
Loss at iteration [386]: 0.002439328646109242
Loss at iteration [387]: 0.0024390684268835116
Loss at iteration [388]: 0.0024390684268835116
Loss at iteration [389]: 0.0024390412690656295
Loss at iteration [390]: 0.0024390115330401134
Loss at iteration [391]: 0.002438945500382555
Loss at iteration [392]: 0.0024389279434027535
Loss at iteration [393]: 0.0024388965065436203
Loss at iteration [394]: 0.0024388965065436203
Loss at iteration [395]: 0.002438872120801386
Loss at iteration [396]: 0.0024388595713006524
Loss at iteration [397]: 0.0024388493011530082
Loss at iteration [398]: 0.00243883003972543
Loss at iteration [399]: 0.00243880104418291
Loss at iteration [400]: 0.0024387562813128994
Loss at iteration [401]: 0.0024387562813128994
Loss at iteration [402]: 0.0024387408464870103
Loss at iteration [403]: 0.0024387222842550604
Loss at iteration [404]: 0.0024387018243255047
Loss at iteration [405]: 0.0024386671219871195
Loss at iteration [406]: 0.0024385846287990945
Loss at iteration [407]: 0.0024385846287990945
Loss at iteration [408]: 0.002438538078154057
Loss at iteration [409]: 0.002438521287995142
Loss at iteration [410]: 0.0024385050260227682
Loss at iteration [411]: 0.002438486803532284
Loss at iteration [412]: 0.0024384486624615897
Loss at iteration [413]: 0.0024384486624615897
Loss at iteration [414]: 0.002438425019036431
Loss at iteration [415]: 0.0024383857060397804
Loss at iteration [416]: 0.002438360526695338
Loss at iteration [417]: 0.002438343811710414
Loss at iteration [418]: 0.002437899573136153
Loss at iteration [419]: 0.002437899573136153
Loss at iteration [420]: 0.0024375130726674824
Loss at iteration [421]: 0.002437491444066364
Loss at iteration [422]: 0.002437166918467286
Loss at iteration [423]: 0.002437141493004498
Loss at iteration [424]: 0.0024370522027363233
Loss at iteration [425]: 0.0024370522027363233
Loss at iteration [426]: 0.00243700130212685
Loss at iteration [427]: 0.0024369687616316743
Loss at iteration [428]: 0.002436828238748383
Loss at iteration [429]: 0.002436808139706934
Loss at iteration [430]: 0.0024367783267778375
Loss at iteration [431]: 0.002436744966916038
Loss at iteration [432]: 0.002436744966916038
Loss at iteration [433]: 0.002436722605152639
Loss at iteration [434]: 0.002436677364462067
Loss at iteration [435]: 0.002436641170838196
Loss at iteration [436]: 0.0024366201073660453
Loss at iteration [437]: 0.0024366048333783266
Loss at iteration [438]: 0.0024366048333783266
Loss at iteration [439]: 0.002436586942956321
Loss at iteration [440]: 0.0024365399365923112
Loss at iteration [441]: 0.002436495225649418
Loss at iteration [442]: 0.002436461554427788
Loss at iteration [443]: 0.00243631016059204
Loss at iteration [444]: 0.00243631016059204
Loss at iteration [445]: 0.002436276285915959
Loss at iteration [446]: 0.0024362470713067833
Loss at iteration [447]: 0.002436159469055062
Loss at iteration [448]: 0.002436123204922158
Loss at iteration [449]: 0.0024360945719029145
Loss at iteration [450]: 0.002435919661579075
Loss at iteration [451]: 0.002435919661579075
Loss at iteration [452]: 0.0024358580777405956
Loss at iteration [453]: 0.002435722068189414
Loss at iteration [454]: 0.002435639951607087
Loss at iteration [455]: 0.0024355824856815643
Loss at iteration [456]: 0.002435536114816803
Loss at iteration [457]: 0.002435536114816803
Loss at iteration [458]: 0.002435510481606992
Loss at iteration [459]: 0.002435455890732604
Loss at iteration [460]: 0.0024353925423797507
Loss at iteration [461]: 0.0024353750381984063
Loss at iteration [462]: 0.0024352296096622063
Loss at iteration [463]: 0.0024352296096622063
Loss at iteration [464]: 0.002435173678460858
Loss at iteration [465]: 0.0024349480459881488
Loss at iteration [466]: 0.002434913632381658
Loss at iteration [467]: 0.002434899494970608
Loss at iteration [468]: 0.002434853203080101
Loss at iteration [469]: 0.0024348234823784076
Loss at iteration [470]: 0.0024348234823784076
Loss at iteration [471]: 0.0024347957903586414
Loss at iteration [472]: 0.0024347745076827845
Loss at iteration [473]: 0.0024347233371631214
Loss at iteration [474]: 0.0024346741718980418
Loss at iteration [475]: 0.0024345065142224497
Loss at iteration [476]: 0.0024345065142224497
Loss at iteration [477]: 0.0024344737113953106
Loss at iteration [478]: 0.0024342701559609157
Loss at iteration [479]: 0.0024342482040591315
Loss at iteration [480]: 0.0024341911653965316
Loss at iteration [481]: 0.002434114567104673
Loss at iteration [482]: 0.002434114567104673
Loss at iteration [483]: 0.002434089388370998
Loss at iteration [484]: 0.002434056315924743
Loss at iteration [485]: 0.0024339676095039816
Loss at iteration [486]: 0.0024339343070248582
Loss at iteration [487]: 0.002433890725677863
Loss at iteration [488]: 0.002433890725677863
Loss at iteration [489]: 0.0024338686033328867
Loss at iteration [490]: 0.002433838987452136
Loss at iteration [491]: 0.002433829270605471
Loss at iteration [492]: 0.0024338021302385154
Loss at iteration [493]: 0.0024337792195007514
Loss at iteration [494]: 0.0024337792195007514
Loss at iteration [495]: 0.0024337640388040396
Loss at iteration [496]: 0.002433741069044434
Loss at iteration [497]: 0.002433729991586129
Loss at iteration [498]: 0.0024337021528413397
Loss at iteration [499]: 0.0024335571610538407
Loss at iteration [500]: 0.0024335571610538407
Loss at iteration [501]: 0.0024335181517361132
Loss at iteration [502]: 0.0024334645671523335
Loss at iteration [503]: 0.0024334381344668925
Loss at iteration [504]: 0.0024334229912453854
Loss at iteration [505]: 0.0024333883339358515
Loss at iteration [506]: 0.0024333486502403612
Loss at iteration [507]: 0.0024333486502403612
Loss at iteration [508]: 0.0024333360157681584
Loss at iteration [509]: 0.002433301895216085
Loss at iteration [510]: 0.002433281319262939
Loss at iteration [511]: 0.00243326140494401
Loss at iteration [512]: 0.0024324288982978315
Loss at iteration [513]: 0.0024324288982978315
Loss at iteration [514]: 0.002431984377327942
Loss at iteration [515]: 0.002431907838523443
Loss at iteration [516]: 0.002431287129912681
Loss at iteration [517]: 0.0024312512959621985
Loss at iteration [518]: 0.0024307778977816566
Loss at iteration [519]: 0.0024307778977816566
Loss at iteration [520]: 0.0024307074022270675
Loss at iteration [521]: 0.002430641007418076
Loss at iteration [522]: 0.0024304939948111798
Loss at iteration [523]: 0.002430463285254705
Loss at iteration [524]: 0.002430413919798114
Loss at iteration [525]: 0.002430413919798114
Loss at iteration [526]: 0.0024303666410804767
Loss at iteration [527]: 0.002430334799983632
Loss at iteration [528]: 0.002430269860971651
Loss at iteration [529]: 0.0024302023154695726
Loss at iteration [530]: 0.0024300545212142728
Loss at iteration [531]: 0.0024300545212142728
Loss at iteration [532]: 0.0024300101641636447
Loss at iteration [533]: 0.002429932560570557
Loss at iteration [534]: 0.0024296164192570873
Loss at iteration [535]: 0.0024295943242620788
Loss at iteration [536]: 0.00242953157729517
Loss at iteration [537]: 0.00242953157729517
Loss at iteration [538]: 0.00242948720836692
Loss at iteration [539]: 0.002429465647475813
Loss at iteration [540]: 0.002429372524491169
Loss at iteration [541]: 0.002429341675745748
Loss at iteration [542]: 0.0024293306595383583
Loss at iteration [543]: 0.0024293306595383583
Loss at iteration [544]: 0.002429315206751338
Loss at iteration [545]: 0.002429282578461053
Loss at iteration [546]: 0.002429245133919274
Loss at iteration [547]: 0.002429195520785313
Loss at iteration [548]: 0.002429107923352963
Loss at iteration [549]: 0.002429107923352963
Loss at iteration [550]: 0.0024290623598841083
Loss at iteration [551]: 0.002429024757754801
Loss at iteration [552]: 0.002428942012509773
Loss at iteration [553]: 0.002428855547325924
Loss at iteration [554]: 0.0024287600439584413
Loss at iteration [555]: 0.0024287600439584413
Loss at iteration [556]: 0.0024287346989346993
Loss at iteration [557]: 0.0024285526345295344
Loss at iteration [558]: 0.0024285140070936656
Loss at iteration [559]: 0.0024283815936265965
Loss at iteration [560]: 0.0024283574189749788
Loss at iteration [561]: 0.0024283574189749788
Loss at iteration [562]: 0.0024283435456203327
Loss at iteration [563]: 0.002428307382029132
Loss at iteration [564]: 0.0024282912116049887
Loss at iteration [565]: 0.002428248057338926
Loss at iteration [566]: 0.002428202831719464
Loss at iteration [567]: 0.002428072772783513
Loss at iteration [568]: 0.002428072772783513
Loss at iteration [569]: 0.002428023707741007
Loss at iteration [570]: 0.002427924579074445
Loss at iteration [571]: 0.002427807266386579
Loss at iteration [572]: 0.0024277896249186607
Loss at iteration [573]: 0.002427754801760927
Loss at iteration [574]: 0.002427754801760927
Loss at iteration [575]: 0.0024277289506004157
Loss at iteration [576]: 0.0024276965722088957
Loss at iteration [577]: 0.0024276439469570433
Loss at iteration [578]: 0.002427614040203857
Loss at iteration [579]: 0.0024275914650425485
Loss at iteration [580]: 0.0024275914650425485
Loss at iteration [581]: 0.0024275750899873773
Loss at iteration [582]: 0.0024275363831551527
Loss at iteration [583]: 0.002427485485213209
Loss at iteration [584]: 0.002427463824087679
Loss at iteration [585]: 0.00242743995846701
Loss at iteration [586]: 0.00242743995846701
Loss at iteration [587]: 0.002427423384541009
Loss at iteration [588]: 0.00242740777107824
Loss at iteration [589]: 0.0024273719276454706
Loss at iteration [590]: 0.0024273316489666673
Loss at iteration [591]: 0.002427299062522626
Loss at iteration [592]: 0.002427299062522626
Loss at iteration [593]: 0.002427279837544628
Loss at iteration [594]: 0.002427248385245404
Loss at iteration [595]: 0.0024272239090614095
Loss at iteration [596]: 0.0024271985012733043
Loss at iteration [597]: 0.0024271799089476873
Loss at iteration [598]: 0.0024271665724077884
Loss at iteration [599]: 0.0024271665724077884
Loss at iteration [600]: 0.002427138576749941
Loss at iteration [601]: 0.002427104164001853
Loss at iteration [602]: 0.002427090792118666
Loss at iteration [603]: 0.0024270794371227377
Loss at iteration [604]: 0.0024270673471350327
Loss at iteration [605]: 0.0024270673471350327
Loss at iteration [606]: 0.00242705974101623
Loss at iteration [607]: 0.002427053561473808
Loss at iteration [608]: 0.0024270376314120662
Loss at iteration [609]: 0.0024269899366628107
Loss at iteration [610]: 0.002426792446780722
Loss at iteration [611]: 0.002426792446780722
Loss at iteration [612]: 0.0024266642606579794
Loss at iteration [613]: 0.0024266277805802105
Loss at iteration [614]: 0.0024265819221040135
Loss at iteration [615]: 0.0024265506650442614
Loss at iteration [616]: 0.0024265089062448873
Loss at iteration [617]: 0.0024265089062448873
Loss at iteration [618]: 0.0024264721198248265
Loss at iteration [619]: 0.0024264161504126114
Loss at iteration [620]: 0.00242639974570363
Loss at iteration [621]: 0.00242636345512479
Loss at iteration [622]: 0.0024263038760719037
Loss at iteration [623]: 0.0024263038760719037
Loss at iteration [624]: 0.002426262975921051
Loss at iteration [625]: 0.002426202833174039
Loss at iteration [626]: 0.0024261901093307766
Loss at iteration [627]: 0.0024261719092866026
Loss at iteration [628]: 0.0024261446933084322
Loss at iteration [629]: 0.0024261446933084322
Loss at iteration [630]: 0.002426131520558495
Loss at iteration [631]: 0.0024261030629414296
Loss at iteration [632]: 0.0024260910363486142
Loss at iteration [633]: 0.0024260703866769012
Loss at iteration [634]: 0.0024260343864466764
Loss at iteration [635]: 0.0024260052541177635
Loss at iteration [636]: 0.0024260052541177635
Loss at iteration [637]: 0.002425986134286095
Loss at iteration [638]: 0.002425961239331576
Loss at iteration [639]: 0.0024259263745997775
Loss at iteration [640]: 0.0024259116326882184
Loss at iteration [641]: 0.0024258852325115244
Loss at iteration [642]: 0.0024258852325115244
Loss at iteration [643]: 0.0024258732445321687
Loss at iteration [644]: 0.00242586223576878
Loss at iteration [645]: 0.0024258358898244896
Loss at iteration [646]: 0.002425770341409659
Loss at iteration [647]: 0.002425592135865984
Loss at iteration [648]: 0.002425592135865984
Loss at iteration [649]: 0.0024254660748387895
Loss at iteration [650]: 0.002425446586674193
Loss at iteration [651]: 0.00242513149041012
Loss at iteration [652]: 0.0024251138412302157
Loss at iteration [653]: 0.0024250628848198584
Loss at iteration [654]: 0.0024250628848198584
Loss at iteration [655]: 0.0024250163210959427
Loss at iteration [656]: 0.0024250018071460024
Loss at iteration [657]: 0.0024248392431274117
Loss at iteration [658]: 0.002424818124255624
Loss at iteration [659]: 0.0024246551512954397
Loss at iteration [660]: 0.0024246551512954397
Loss at iteration [661]: 0.00242463158899725
Loss at iteration [662]: 0.002424600734583473
Loss at iteration [663]: 0.002424570089123465
Loss at iteration [664]: 0.0024245435684015697
Loss at iteration [665]: 0.0024245245123746
Loss at iteration [666]: 0.0024244792218437667
Loss at iteration [667]: 0.0024244792218437667
Loss at iteration [668]: 0.002424465792116335
Loss at iteration [669]: 0.002424440133643111
Loss at iteration [670]: 0.002424431174853177
Loss at iteration [671]: 0.002424412758329438
Loss at iteration [672]: 0.0024243973386960656
Loss at iteration [673]: 0.002424311710009666
Loss at iteration [674]: 0.002424311710009666
Loss at iteration [675]: 0.002424234925802944
Loss at iteration [676]: 0.0024242134575320103
Loss at iteration [677]: 0.002424120289376778
Loss at iteration [678]: 0.0024241044880937033
Loss at iteration [679]: 0.002424018482933683
Loss at iteration [680]: 0.002423993189542239
Loss at iteration [681]: 0.002423993189542239
Loss at iteration [682]: 0.002423984099448557
Loss at iteration [683]: 0.002423871256845419
Loss at iteration [684]: 0.0024238352721405415
Loss at iteration [685]: 0.0024238011727697405
Loss at iteration [686]: 0.00242358934370162
Loss at iteration [687]: 0.00242358934370162
Loss at iteration [688]: 0.0024235395859596902
Loss at iteration [689]: 0.0024235189506261806
Loss at iteration [690]: 0.002423466609374142
Loss at iteration [691]: 0.002423449205658958
Loss at iteration [692]: 0.0024234114771857964
Loss at iteration [693]: 0.0024233827327583774
Loss at iteration [694]: 0.0024233827327583774
Loss at iteration [695]: 0.002423360218866429
Loss at iteration [696]: 0.0024233306610587082
Loss at iteration [697]: 0.002423284967326196
Loss at iteration [698]: 0.002423127364015182
Loss at iteration [699]: 0.002422985847934707
Loss at iteration [700]: 0.002422985847934707
Loss at iteration [701]: 0.0024229028359700243
Loss at iteration [702]: 0.00242258025659746
Loss at iteration [703]: 0.002422529837604943
Loss at iteration [704]: 0.0024224564812761642
Loss at iteration [705]: 0.002422344508174825
