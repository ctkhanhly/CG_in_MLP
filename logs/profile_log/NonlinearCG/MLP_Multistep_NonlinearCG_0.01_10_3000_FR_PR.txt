Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :FR_PR
Total number of function evaluations  : 1010
Total number of iterations            : 494
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 1.9895069599151611
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 49.83177735585484%
Percentage of parameters < 1e-7       : 49.83177735585484%
Percentage of parameters < 1e-6       : 49.83374487216063%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.4645237393468495
Loss at iteration [2]: 0.43992742671275503
Loss at iteration [3]: 0.40139657195518585
Loss at iteration [4]: 0.37221698050031754
Loss at iteration [5]: 0.3580234783365002
Loss at iteration [6]: 0.3401217082386093
Loss at iteration [7]: 0.321921335201873
Loss at iteration [8]: 0.296896321855617
Loss at iteration [9]: 0.2860836974687852
Loss at iteration [10]: 0.27618701264108914
Loss at iteration [11]: 0.27252362584907147
Loss at iteration [12]: 0.2660921751755709
Loss at iteration [13]: 0.2660921751755709
Loss at iteration [14]: 0.26446317431147415
Loss at iteration [15]: 0.2482884210032324
Loss at iteration [16]: 0.24653095214414858
Loss at iteration [17]: 0.23919984628938556
Loss at iteration [18]: 0.2374459626947969
Loss at iteration [19]: 0.23445426474470224
Loss at iteration [20]: 0.23210519467557134
Loss at iteration [21]: 0.2301981844293968
Loss at iteration [22]: 0.2262630528133991
Loss at iteration [23]: 0.2234205132764391
Loss at iteration [24]: 0.22172555772073602
Loss at iteration [25]: 0.22069323654430467
Loss at iteration [26]: 0.22069323654430467
Loss at iteration [27]: 0.2201449288165148
Loss at iteration [28]: 0.21928766697731225
Loss at iteration [29]: 0.21901198460165652
Loss at iteration [30]: 0.2178717203033383
Loss at iteration [31]: 0.21764705208850474
Loss at iteration [32]: 0.21675815275466795
Loss at iteration [33]: 0.21670563247370792
Loss at iteration [34]: 0.21624962779724852
Loss at iteration [35]: 0.21598059062076747
Loss at iteration [36]: 0.21557433265661677
Loss at iteration [37]: 0.21541539183400338
Loss at iteration [38]: 0.21530369851144315
Loss at iteration [39]: 0.21527884313935208
Loss at iteration [40]: 0.21510125345011236
Loss at iteration [41]: 0.21510125345011236
Loss at iteration [42]: 0.21508814619679356
Loss at iteration [43]: 0.21504966891523583
Loss at iteration [44]: 0.21503606595654437
Loss at iteration [45]: 0.21502189007359543
Loss at iteration [46]: 0.21501677632600447
Loss at iteration [47]: 0.2149827314197896
Loss at iteration [48]: 0.21497761595725484
Loss at iteration [49]: 0.2149407451992482
Loss at iteration [50]: 0.21494005727244403
Loss at iteration [51]: 0.21493816548003292
Loss at iteration [52]: 0.2149378291759779
Loss at iteration [53]: 0.21493177348837572
Loss at iteration [54]: 0.2149298559001313
Loss at iteration [55]: 0.2149298559001313
Loss at iteration [56]: 0.21492323176216152
Loss at iteration [57]: 0.21492164170626504
Loss at iteration [58]: 0.2149203004852122
Loss at iteration [59]: 0.2149082447119247
Loss at iteration [60]: 0.21489475131033736
Loss at iteration [61]: 0.21489153991471047
Loss at iteration [62]: 0.21488804856141536
Loss at iteration [63]: 0.21488756792255223
Loss at iteration [64]: 0.21488323947697702
Loss at iteration [65]: 0.2148820477627117
Loss at iteration [66]: 0.21487107464160973
Loss at iteration [67]: 0.21487004918434702
Loss at iteration [68]: 0.21485390177297808
Loss at iteration [69]: 0.21485390177297808
Loss at iteration [70]: 0.21485142439726454
Loss at iteration [71]: 0.21484470555633792
Loss at iteration [72]: 0.21484406080930377
Loss at iteration [73]: 0.21484188107497265
Loss at iteration [74]: 0.21483975369626265
Loss at iteration [75]: 0.2148372518070658
Loss at iteration [76]: 0.21483085104882627
Loss at iteration [77]: 0.214829790148529
Loss at iteration [78]: 0.21482719272797812
Loss at iteration [79]: 0.21482672118599921
Loss at iteration [80]: 0.21482237557882336
Loss at iteration [81]: 0.21481556901455792
Loss at iteration [82]: 0.21481433810910527
Loss at iteration [83]: 0.21481433810910527
Loss at iteration [84]: 0.21481340842338545
Loss at iteration [85]: 0.2148074697005669
Loss at iteration [86]: 0.21480721440222764
Loss at iteration [87]: 0.214805580821307
Loss at iteration [88]: 0.21480516199773003
Loss at iteration [89]: 0.2148023367259364
Loss at iteration [90]: 0.21480109034211944
Loss at iteration [91]: 0.2147976404321859
Loss at iteration [92]: 0.21479700703488552
Loss at iteration [93]: 0.21479487188210217
Loss at iteration [94]: 0.21479293672265126
Loss at iteration [95]: 0.21479293672265126
Loss at iteration [96]: 0.21479145038686312
Loss at iteration [97]: 0.21478978143848854
Loss at iteration [98]: 0.21477914933276507
Loss at iteration [99]: 0.21477232165012936
Loss at iteration [100]: 0.21476162359854953
Loss at iteration [101]: 0.21475919534824278
Loss at iteration [102]: 0.2147590267137646
Loss at iteration [103]: 0.21475606745073056
Loss at iteration [104]: 0.2147544583187482
Loss at iteration [105]: 0.21475369793707158
Loss at iteration [106]: 0.21475326598916167
Loss at iteration [107]: 0.21475295618778553
Loss at iteration [108]: 0.21475295618778553
Loss at iteration [109]: 0.21475270406066005
Loss at iteration [110]: 0.214752273959507
Loss at iteration [111]: 0.21475179324132057
Loss at iteration [112]: 0.21475121951376694
Loss at iteration [113]: 0.21475086038233798
Loss at iteration [114]: 0.2147496673336783
Loss at iteration [115]: 0.21474751600611125
Loss at iteration [116]: 0.21474611232835253
Loss at iteration [117]: 0.2147431682706833
Loss at iteration [118]: 0.2147270975255971
Loss at iteration [119]: 0.21472147033100128
Loss at iteration [120]: 0.2146991160774058
Loss at iteration [121]: 0.21468220799941418
Loss at iteration [122]: 0.21468220799941418
Loss at iteration [123]: 0.2146800401773617
Loss at iteration [124]: 0.2146784796771593
Loss at iteration [125]: 0.21467725591987896
Loss at iteration [126]: 0.2146748292413401
Loss at iteration [127]: 0.21467286122324505
Loss at iteration [128]: 0.21467225068531637
Loss at iteration [129]: 0.21467149531026752
Loss at iteration [130]: 0.21467073344574122
Loss at iteration [131]: 0.21467026167904568
Loss at iteration [132]: 0.2146701781200458
Loss at iteration [133]: 0.21466961073757784
Loss at iteration [134]: 0.2146694103394265
Loss at iteration [135]: 0.21466782398950016
Loss at iteration [136]: 0.21466782398950016
Loss at iteration [137]: 0.21466764940635907
Loss at iteration [138]: 0.21466694146885115
Loss at iteration [139]: 0.2146657445935516
Loss at iteration [140]: 0.2146655533936415
Loss at iteration [141]: 0.21466327540477956
Loss at iteration [142]: 0.21466174169767444
Loss at iteration [143]: 0.2146601725219024
Loss at iteration [144]: 0.21465701379076574
Loss at iteration [145]: 0.21465565466463069
Loss at iteration [146]: 0.21465380999022932
Loss at iteration [147]: 0.21465075145382467
Loss at iteration [148]: 0.21464960269600836
Loss at iteration [149]: 0.2146492255009081
Loss at iteration [150]: 0.2146492255009081
Loss at iteration [151]: 0.21464907473753342
Loss at iteration [152]: 0.21464875843198236
Loss at iteration [153]: 0.21464837634178907
Loss at iteration [154]: 0.2146483165250236
Loss at iteration [155]: 0.21464766879501224
Loss at iteration [156]: 0.2146472944761678
Loss at iteration [157]: 0.21464683924613379
Loss at iteration [158]: 0.2146466905686507
Loss at iteration [159]: 0.21464655213692416
Loss at iteration [160]: 0.21464568464649203
Loss at iteration [161]: 0.2146450240439099
Loss at iteration [162]: 0.2146446518320752
Loss at iteration [163]: 0.2146433488816518
Loss at iteration [164]: 0.2146433488816518
Loss at iteration [165]: 0.21464289427771271
Loss at iteration [166]: 0.21464257784779253
Loss at iteration [167]: 0.2146423532515481
Loss at iteration [168]: 0.21464196542815023
Loss at iteration [169]: 0.21464166642973606
Loss at iteration [170]: 0.2146415828951303
Loss at iteration [171]: 0.21464144208446898
Loss at iteration [172]: 0.21464118328734938
Loss at iteration [173]: 0.21464108451340166
Loss at iteration [174]: 0.214640758729351
Loss at iteration [175]: 0.2146401092592566
Loss at iteration [176]: 0.21463992817106814
Loss at iteration [177]: 0.2146390996884452
Loss at iteration [178]: 0.2146390996884452
Loss at iteration [179]: 0.21463892068881757
Loss at iteration [180]: 0.21463843147808814
Loss at iteration [181]: 0.21463779206785571
Loss at iteration [182]: 0.21463734377536486
Loss at iteration [183]: 0.21463714214716376
Loss at iteration [184]: 0.21463703273415558
Loss at iteration [185]: 0.2146364375745336
Loss at iteration [186]: 0.2146362756138641
Loss at iteration [187]: 0.2146349835504602
Loss at iteration [188]: 0.2146289636764497
Loss at iteration [189]: 0.2146285383426461
Loss at iteration [190]: 0.2146239523073522
Loss at iteration [191]: 0.2146239523073522
Loss at iteration [192]: 0.2146221777959211
Loss at iteration [193]: 0.21462115373845542
Loss at iteration [194]: 0.2146204390233022
Loss at iteration [195]: 0.21462033215345033
Loss at iteration [196]: 0.21462013363384064
Loss at iteration [197]: 0.21461997813800454
Loss at iteration [198]: 0.2146199108078809
Loss at iteration [199]: 0.2146196134868493
Loss at iteration [200]: 0.21461800218836233
Loss at iteration [201]: 0.21461724072553107
Loss at iteration [202]: 0.21461375090155496
Loss at iteration [203]: 0.21461375090155496
Loss at iteration [204]: 0.21461169733854485
Loss at iteration [205]: 0.21460922162452453
Loss at iteration [206]: 0.21460899715842569
Loss at iteration [207]: 0.21460856850504856
Loss at iteration [208]: 0.21460848851106482
Loss at iteration [209]: 0.2146080824918927
Loss at iteration [210]: 0.214607858992817
Loss at iteration [211]: 0.21460682828402117
Loss at iteration [212]: 0.21460670199582318
Loss at iteration [213]: 0.2146049037385913
Loss at iteration [214]: 0.21460463692292253
Loss at iteration [215]: 0.21460463692292253
Loss at iteration [216]: 0.21460439674117143
Loss at iteration [217]: 0.21460327669462664
Loss at iteration [218]: 0.2146026525391157
Loss at iteration [219]: 0.21460228024179545
Loss at iteration [220]: 0.21460193440821956
Loss at iteration [221]: 0.21460157458162024
Loss at iteration [222]: 0.21460155809181156
Loss at iteration [223]: 0.2146014781313173
Loss at iteration [224]: 0.21460145638270695
Loss at iteration [225]: 0.214601263741695
Loss at iteration [226]: 0.21460113849181897
Loss at iteration [227]: 0.21460113849181897
Loss at iteration [228]: 0.21460112385917676
Loss at iteration [229]: 0.21460093084884144
Loss at iteration [230]: 0.21460089796446513
Loss at iteration [231]: 0.2146007148141274
Loss at iteration [232]: 0.2146006987514517
Loss at iteration [233]: 0.2146002927686773
Loss at iteration [234]: 0.21460011332523304
Loss at iteration [235]: 0.21459956799524008
Loss at iteration [236]: 0.21459928484168014
Loss at iteration [237]: 0.21459923362693561
Loss at iteration [238]: 0.21459920859495216
Loss at iteration [239]: 0.21459905306445456
Loss at iteration [240]: 0.21459903300775365
Loss at iteration [241]: 0.21459888850103018
Loss at iteration [242]: 0.21459888850103018
Loss at iteration [243]: 0.21459887192813856
Loss at iteration [244]: 0.21459882627385068
Loss at iteration [245]: 0.21459875169691128
Loss at iteration [246]: 0.21459870846634788
Loss at iteration [247]: 0.2145986579377504
Loss at iteration [248]: 0.21459851378540562
Loss at iteration [249]: 0.2145981553104569
Loss at iteration [250]: 0.2145976984717845
Loss at iteration [251]: 0.2145970852199825
Loss at iteration [252]: 0.21459534243273573
Loss at iteration [253]: 0.21459040305505328
Loss at iteration [254]: 0.21458528828089293
Loss at iteration [255]: 0.21458207059212991
Loss at iteration [256]: 0.21458207059212991
Loss at iteration [257]: 0.21458127237436772
Loss at iteration [258]: 0.21457985750333075
Loss at iteration [259]: 0.21457903364734182
Loss at iteration [260]: 0.21457844165605205
Loss at iteration [261]: 0.21457840564807926
Loss at iteration [262]: 0.21457835933872865
Loss at iteration [263]: 0.21457833677097152
Loss at iteration [264]: 0.21457822869211776
Loss at iteration [265]: 0.21457821280220246
Loss at iteration [266]: 0.2145780480470983
Loss at iteration [267]: 0.2145779782381125
Loss at iteration [268]: 0.21457791024142014
Loss at iteration [269]: 0.21457788919369
Loss at iteration [270]: 0.21457788919369
Loss at iteration [271]: 0.21457788379237683
Loss at iteration [272]: 0.21457785384709727
Loss at iteration [273]: 0.21457781854575872
Loss at iteration [274]: 0.21457772383284232
Loss at iteration [275]: 0.21457770628487122
Loss at iteration [276]: 0.2145776532881102
Loss at iteration [277]: 0.2145776469065661
Loss at iteration [278]: 0.2145776160187307
Loss at iteration [279]: 0.2145776028499374
Loss at iteration [280]: 0.21457756143573362
Loss at iteration [281]: 0.21457754265754264
Loss at iteration [282]: 0.21457746781124282
Loss at iteration [283]: 0.21457740136758816
Loss at iteration [284]: 0.21457740136758816
Loss at iteration [285]: 0.21457738501207815
Loss at iteration [286]: 0.21457723319980238
Loss at iteration [287]: 0.21457714873111544
Loss at iteration [288]: 0.2145771411315152
Loss at iteration [289]: 0.21457712821004105
Loss at iteration [290]: 0.21457712463819684
Loss at iteration [291]: 0.21457711246036196
Loss at iteration [292]: 0.21457710079200076
Loss at iteration [293]: 0.21457705362328303
Loss at iteration [294]: 0.21457699052071535
Loss at iteration [295]: 0.21457691699692677
Loss at iteration [296]: 0.21457680265002896
Loss at iteration [297]: 0.21457670237486276
Loss at iteration [298]: 0.21457670237486276
Loss at iteration [299]: 0.2145766378202055
Loss at iteration [300]: 0.21457636678523456
Loss at iteration [301]: 0.21457632829821618
Loss at iteration [302]: 0.21457628233916118
Loss at iteration [303]: 0.2145762357201621
Loss at iteration [304]: 0.21457622847424584
Loss at iteration [305]: 0.21457615424713383
Loss at iteration [306]: 0.2145761408233953
Loss at iteration [307]: 0.21457611982512892
Loss at iteration [308]: 0.2145760671772733
Loss at iteration [309]: 0.21457605839896848
Loss at iteration [310]: 0.21457599037579675
Loss at iteration [311]: 0.21457598135821554
Loss at iteration [312]: 0.21457589546465422
Loss at iteration [313]: 0.21457589546465422
Loss at iteration [314]: 0.21457585902983817
Loss at iteration [315]: 0.21457584663080972
Loss at iteration [316]: 0.21457582947750073
Loss at iteration [317]: 0.21457581175893953
Loss at iteration [318]: 0.21457578372850233
Loss at iteration [319]: 0.21457568620308884
Loss at iteration [320]: 0.2145755612869525
Loss at iteration [321]: 0.214575172707795
Loss at iteration [322]: 0.2145750005343658
Loss at iteration [323]: 0.2145746850306936
Loss at iteration [324]: 0.2145742206771042
Loss at iteration [325]: 0.2145740104612568
Loss at iteration [326]: 0.2145740104612568
Loss at iteration [327]: 0.21457391699504444
Loss at iteration [328]: 0.2145729068791033
Loss at iteration [329]: 0.21457237196916015
Loss at iteration [330]: 0.2145722913379709
Loss at iteration [331]: 0.21457224088686433
Loss at iteration [332]: 0.21457196132275486
Loss at iteration [333]: 0.2145719203933266
Loss at iteration [334]: 0.21457184158771875
Loss at iteration [335]: 0.21457180193390993
Loss at iteration [336]: 0.2145717710975378
Loss at iteration [337]: 0.21457176041891757
Loss at iteration [338]: 0.2145717391363753
Loss at iteration [339]: 0.21457173466299279
Loss at iteration [340]: 0.21457168378991823
Loss at iteration [341]: 0.21457168378991823
Loss at iteration [342]: 0.2145716707783011
Loss at iteration [343]: 0.21457164374593504
Loss at iteration [344]: 0.21457163938666804
Loss at iteration [345]: 0.21457163225593603
Loss at iteration [346]: 0.2145716021402483
Loss at iteration [347]: 0.21457159685942936
Loss at iteration [348]: 0.2145715693857114
Loss at iteration [349]: 0.2145715639086435
Loss at iteration [350]: 0.2145715068200699
Loss at iteration [351]: 0.21457149445845275
Loss at iteration [352]: 0.21457130785586362
Loss at iteration [353]: 0.21457125632374038
Loss at iteration [354]: 0.2145711332580073
Loss at iteration [355]: 0.2145711332580073
Loss at iteration [356]: 0.21457110850756364
Loss at iteration [357]: 0.21457103402485458
Loss at iteration [358]: 0.21457103005124484
Loss at iteration [359]: 0.21457098980498834
Loss at iteration [360]: 0.21457098686661824
Loss at iteration [361]: 0.21457097213985774
Loss at iteration [362]: 0.2145709542842979
Loss at iteration [363]: 0.2145709439119193
Loss at iteration [364]: 0.21457088189742704
Loss at iteration [365]: 0.2145708596507338
Loss at iteration [366]: 0.2145707462987301
Loss at iteration [367]: 0.2145707379049704
Loss at iteration [368]: 0.21457069683597463
Loss at iteration [369]: 0.21457069683597463
Loss at iteration [370]: 0.21457067665522
Loss at iteration [371]: 0.2145706731910803
Loss at iteration [372]: 0.21457065232721312
Loss at iteration [373]: 0.21456859253139873
Loss at iteration [374]: 0.21456854819710536
Loss at iteration [375]: 0.21456747512173954
Loss at iteration [376]: 0.21456710506580062
Loss at iteration [377]: 0.21456670380980566
Loss at iteration [378]: 0.21456664793464583
Loss at iteration [379]: 0.21456614752716338
Loss at iteration [380]: 0.21456611668033376
Loss at iteration [381]: 0.2145660751045499
Loss at iteration [382]: 0.2145660751045499
Loss at iteration [383]: 0.2145660559423987
Loss at iteration [384]: 0.21456601666440167
Loss at iteration [385]: 0.2145659770958346
Loss at iteration [386]: 0.2145659639281677
Loss at iteration [387]: 0.21456595204749876
Loss at iteration [388]: 0.21456593099348897
Loss at iteration [389]: 0.2145659160977745
Loss at iteration [390]: 0.21456589983611687
Loss at iteration [391]: 0.21456589232821105
Loss at iteration [392]: 0.21456586062122965
Loss at iteration [393]: 0.2145658266496061
Loss at iteration [394]: 0.2145658212691319
Loss at iteration [395]: 0.21456579581313998
Loss at iteration [396]: 0.21456579581313998
Loss at iteration [397]: 0.21456579047919971
Loss at iteration [398]: 0.2145657814073666
Loss at iteration [399]: 0.21456577880280622
Loss at iteration [400]: 0.21456576699259605
Loss at iteration [401]: 0.2145657448976363
Loss at iteration [402]: 0.21456571107828742
Loss at iteration [403]: 0.21456567367796214
Loss at iteration [404]: 0.21456567167098364
Loss at iteration [405]: 0.21456565247547063
Loss at iteration [406]: 0.21456556753364991
Loss at iteration [407]: 0.21456554600755334
Loss at iteration [408]: 0.2145655006124723
Loss at iteration [409]: 0.21456525624878484
Loss at iteration [410]: 0.21456525624878484
Loss at iteration [411]: 0.21456520328805856
Loss at iteration [412]: 0.21456514426806625
Loss at iteration [413]: 0.21456509961555867
Loss at iteration [414]: 0.21456507204247735
Loss at iteration [415]: 0.21456505256659966
Loss at iteration [416]: 0.21456504428688247
Loss at iteration [417]: 0.21456504021664743
Loss at iteration [418]: 0.2145650358423117
Loss at iteration [419]: 0.21456503116788497
Loss at iteration [420]: 0.2145650271846047
Loss at iteration [421]: 0.21456502089441917
Loss at iteration [422]: 0.2145650073159319
Loss at iteration [423]: 0.21456499097832304
Loss at iteration [424]: 0.21456499097832304
Loss at iteration [425]: 0.21456498638219007
Loss at iteration [426]: 0.21456496852548773
Loss at iteration [427]: 0.21456496004029313
Loss at iteration [428]: 0.214564938842461
Loss at iteration [429]: 0.21456492882529477
Loss at iteration [430]: 0.21456491342574208
Loss at iteration [431]: 0.21456490748645807
Loss at iteration [432]: 0.21456489984194388
Loss at iteration [433]: 0.21456489555309832
Loss at iteration [434]: 0.21456489087715666
Loss at iteration [435]: 0.21456488704531917
Loss at iteration [436]: 0.21456487469899696
Loss at iteration [437]: 0.21456485148994198
Loss at iteration [438]: 0.2145648332008416
Loss at iteration [439]: 0.2145648332008416
Loss at iteration [440]: 0.21456482409059358
Loss at iteration [441]: 0.21456476995667353
Loss at iteration [442]: 0.2145647644597608
Loss at iteration [443]: 0.21456474405511822
Loss at iteration [444]: 0.2145647415662822
Loss at iteration [445]: 0.21456473442819415
Loss at iteration [446]: 0.21456472682351282
Loss at iteration [447]: 0.2145647093068084
Loss at iteration [448]: 0.21456466241721417
Loss at iteration [449]: 0.21456464347415422
Loss at iteration [450]: 0.2145644895893025
Loss at iteration [451]: 0.21456447039023313
Loss at iteration [452]: 0.21456447039023313
Loss at iteration [453]: 0.21456445511221856
Loss at iteration [454]: 0.2145644303976555
Loss at iteration [455]: 0.2145644211172129
Loss at iteration [456]: 0.21456438713888798
Loss at iteration [457]: 0.21456437513103746
Loss at iteration [458]: 0.21456436133355034
Loss at iteration [459]: 0.2145643556947555
Loss at iteration [460]: 0.21456434543763195
Loss at iteration [461]: 0.21456434045648073
Loss at iteration [462]: 0.21456433741336042
Loss at iteration [463]: 0.21456432311150841
Loss at iteration [464]: 0.2145642944450887
Loss at iteration [465]: 0.21456428015547974
Loss at iteration [466]: 0.21456428015547974
Loss at iteration [467]: 0.21456427408507192
Loss at iteration [468]: 0.21456423726670243
Loss at iteration [469]: 0.2145642193726814
Loss at iteration [470]: 0.21456421379744078
Loss at iteration [471]: 0.21456419445049893
Loss at iteration [472]: 0.21456414968171156
Loss at iteration [473]: 0.2145641256678325
Loss at iteration [474]: 0.21456405870295675
Loss at iteration [475]: 0.21456403438349358
Loss at iteration [476]: 0.21456402111948955
Loss at iteration [477]: 0.21456399912219834
Loss at iteration [478]: 0.2145639917469897
Loss at iteration [479]: 0.2145639917469897
Loss at iteration [480]: 0.21456398844418015
Loss at iteration [481]: 0.2145639773611816
Loss at iteration [482]: 0.21456396450568252
Loss at iteration [483]: 0.21456393840830415
Loss at iteration [484]: 0.21456378098219922
Loss at iteration [485]: 0.21456376510768188
Loss at iteration [486]: 0.2145627694560885
Loss at iteration [487]: 0.2145620933698776
Loss at iteration [488]: 0.21456202785927012
Loss at iteration [489]: 0.21456170238807334
Loss at iteration [490]: 0.2145615731076318
Loss at iteration [491]: 0.2145614545110431
Loss at iteration [492]: 0.2145613892420079
Loss at iteration [493]: 0.2145613892420079
Loss at iteration [494]: 0.2145613859705114
Loss at iteration [495]: 0.21456135347118227
Loss at iteration [496]: 0.21456134752826045
Loss at iteration [497]: 0.21456134102225416
Loss at iteration [498]: 0.21456130238727267
Loss at iteration [499]: 0.21456128775782066
Loss at iteration [500]: 0.21456125838073684
Loss at iteration [501]: 0.21456125117950373
Loss at iteration [502]: 0.21456117907419817
Loss at iteration [503]: 0.21456116619069127
Loss at iteration [504]: 0.21456096125486432
Loss at iteration [505]: 0.21456091875846925
Loss at iteration [506]: 0.2145605127525938
Loss at iteration [507]: 0.2145605127525938
Loss at iteration [508]: 0.214560451773464
Loss at iteration [509]: 0.2145604116657488
Loss at iteration [510]: 0.214560406415534
Loss at iteration [511]: 0.2145603980185917
Loss at iteration [512]: 0.21456039300547933
Loss at iteration [513]: 0.21456038101946956
Loss at iteration [514]: 0.21456037362936045
Loss at iteration [515]: 0.21456036298983455
Loss at iteration [516]: 0.21456035855051286
Loss at iteration [517]: 0.21456034755229436
Loss at iteration [518]: 0.21456034565092874
Loss at iteration [519]: 0.21456033708629513
Loss at iteration [520]: 0.2145603347578206
Loss at iteration [521]: 0.2145602918048202
Loss at iteration [522]: 0.2145602918048202
Loss at iteration [523]: 0.21456027090395205
Loss at iteration [524]: 0.21456025646749183
Loss at iteration [525]: 0.21456025590400699
