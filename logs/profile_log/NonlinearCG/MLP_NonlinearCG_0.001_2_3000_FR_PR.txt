Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.001
Beta type                             :FR_PR
Total number of function evaluations  : 3032
Total number of iterations            : 774
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 13.707183122634888
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 50.153958871694336%
Percentage of parameters < 1e-7       : 50.153958871694336%
Percentage of parameters < 1e-6       : 50.153958871694336%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.00426754994807486
Loss at iteration [2]: 0.004135065075039864
Loss at iteration [3]: 0.0032069500634024947
Loss at iteration [4]: 0.003013808180148762
Loss at iteration [5]: 0.002951319727280223
Loss at iteration [6]: 0.0029298438682932937
Loss at iteration [7]: 0.002909464809571079
Loss at iteration [8]: 0.0028865177021907096
Loss at iteration [9]: 0.0028865177021907096
Loss at iteration [10]: 0.0028781277943590675
Loss at iteration [11]: 0.002862953751068963
Loss at iteration [12]: 0.002820787185493194
Loss at iteration [13]: 0.002808594666127606
Loss at iteration [14]: 0.002791790902000932
Loss at iteration [15]: 0.0026558348755507874
Loss at iteration [16]: 0.002564717866847378
Loss at iteration [17]: 0.002564717866847378
Loss at iteration [18]: 0.0025631857959147806
Loss at iteration [19]: 0.0025586640562967966
Loss at iteration [20]: 0.002552988630890527
Loss at iteration [21]: 0.0025526910873160647
Loss at iteration [22]: 0.0025523032353353637
Loss at iteration [23]: 0.002549572837517705
Loss at iteration [24]: 0.0025439818570951243
Loss at iteration [25]: 0.0025439818570951243
Loss at iteration [26]: 0.0025416267787560644
Loss at iteration [27]: 0.0025400327513688172
Loss at iteration [28]: 0.0025363423511881016
Loss at iteration [29]: 0.0025293754372227952
Loss at iteration [30]: 0.002527726107563429
Loss at iteration [31]: 0.0025272209121678353
Loss at iteration [32]: 0.0025229679002121775
Loss at iteration [33]: 0.0025229679002121775
Loss at iteration [34]: 0.0025203118485982775
Loss at iteration [35]: 0.0025199371109278167
Loss at iteration [36]: 0.0025172190030865104
Loss at iteration [37]: 0.002515248760813562
Loss at iteration [38]: 0.0025150643688853142
Loss at iteration [39]: 0.002513278629647312
Loss at iteration [40]: 0.002513074563218444
Loss at iteration [41]: 0.0025120776145649017
Loss at iteration [42]: 0.0025120776145649017
Loss at iteration [43]: 0.0025110012733216397
Loss at iteration [44]: 0.0025108805396648623
Loss at iteration [45]: 0.002509535858672388
Loss at iteration [46]: 0.0025086504100819995
Loss at iteration [47]: 0.0025085071993389326
Loss at iteration [48]: 0.0025078524531495948
Loss at iteration [49]: 0.002507451503703062
Loss at iteration [50]: 0.002507451503703062
Loss at iteration [51]: 0.0025073146621539997
Loss at iteration [52]: 0.0025070492430903065
Loss at iteration [53]: 0.002506670657386746
Loss at iteration [54]: 0.0025064256826012802
Loss at iteration [55]: 0.0025063153833403714
Loss at iteration [56]: 0.002504625637457006
Loss at iteration [57]: 0.002504160764864235
Loss at iteration [58]: 0.002504160764864235
Loss at iteration [59]: 0.002503970396161233
Loss at iteration [60]: 0.002503124606337816
Loss at iteration [61]: 0.00250297778298289
Loss at iteration [62]: 0.00250056240698811
Loss at iteration [63]: 0.00249744250606946
Loss at iteration [64]: 0.0024911617814999862
Loss at iteration [65]: 0.0024911617814999862
Loss at iteration [66]: 0.0024890211710889504
Loss at iteration [67]: 0.0024887521213057522
Loss at iteration [68]: 0.002487894999526854
Loss at iteration [69]: 0.0024875614564550015
Loss at iteration [70]: 0.0024864406986809732
Loss at iteration [71]: 0.002485778519882985
Loss at iteration [72]: 0.002485778519882985
Loss at iteration [73]: 0.002485591321503273
Loss at iteration [74]: 0.0024855247174656978
Loss at iteration [75]: 0.002484515120879676
Loss at iteration [76]: 0.00248379723708876
Loss at iteration [77]: 0.002483088455831267
Loss at iteration [78]: 0.0024828800036329353
Loss at iteration [79]: 0.0024826260883574554
Loss at iteration [80]: 0.0024826260883574554
Loss at iteration [81]: 0.002482383488812314
Loss at iteration [82]: 0.0024820172798999426
Loss at iteration [83]: 0.002481518940872647
Loss at iteration [84]: 0.0024811053337961933
Loss at iteration [85]: 0.002480950845903336
Loss at iteration [86]: 0.0024808053379472327
Loss at iteration [87]: 0.002480010631527104
Loss at iteration [88]: 0.002480010631527104
Loss at iteration [89]: 0.0024798894800529985
Loss at iteration [90]: 0.0024796966319125834
Loss at iteration [91]: 0.0024789448366551834
Loss at iteration [92]: 0.0024789001774293694
Loss at iteration [93]: 0.0024785234564273684
Loss at iteration [94]: 0.0024782727441103676
Loss at iteration [95]: 0.002478117212636439
Loss at iteration [96]: 0.002478117212636439
Loss at iteration [97]: 0.0024780416232595176
Loss at iteration [98]: 0.002477988210518383
Loss at iteration [99]: 0.0024775222003147234
Loss at iteration [100]: 0.0024774470490016047
Loss at iteration [101]: 0.0024771463325301527
Loss at iteration [102]: 0.002476914224868688
Loss at iteration [103]: 0.0024768599626135064
Loss at iteration [104]: 0.0024768599626135064
Loss at iteration [105]: 0.00247683146549511
Loss at iteration [106]: 0.0024765824539848863
Loss at iteration [107]: 0.0024765041811390842
Loss at iteration [108]: 0.0024764470709770403
Loss at iteration [109]: 0.002474236998368448
Loss at iteration [110]: 0.0024716624261931085
Loss at iteration [111]: 0.0024671036380818794
Loss at iteration [112]: 0.0024671036380818794
Loss at iteration [113]: 0.0024668012500802506
Loss at iteration [114]: 0.002465118519656746
Loss at iteration [115]: 0.0024647904996368502
Loss at iteration [116]: 0.0024646977255940333
Loss at iteration [117]: 0.0024642766809187013
Loss at iteration [118]: 0.0024633850649923078
Loss at iteration [119]: 0.0024632543249648187
Loss at iteration [120]: 0.0024632543249648187
Loss at iteration [121]: 0.002463159480917145
Loss at iteration [122]: 0.00246260737357494
Loss at iteration [123]: 0.0024623773569612756
Loss at iteration [124]: 0.002462294033378116
Loss at iteration [125]: 0.0024621880364986038
Loss at iteration [126]: 0.0024613384173239406
Loss at iteration [127]: 0.00246116695023752
Loss at iteration [128]: 0.00246116695023752
Loss at iteration [129]: 0.0024610120900080924
Loss at iteration [130]: 0.002459994048354203
Loss at iteration [131]: 0.0024599486965741386
Loss at iteration [132]: 0.0024593011256201338
Loss at iteration [133]: 0.0024591250673548534
Loss at iteration [134]: 0.0024590470374847716
Loss at iteration [135]: 0.002458608525822555
Loss at iteration [136]: 0.002458608525822555
Loss at iteration [137]: 0.0024585898720560235
Loss at iteration [138]: 0.002458484630398253
Loss at iteration [139]: 0.002458074685142635
Loss at iteration [140]: 0.002457957045973439
Loss at iteration [141]: 0.002457785794174568
Loss at iteration [142]: 0.0024575195772360485
Loss at iteration [143]: 0.0024575079559526754
Loss at iteration [144]: 0.0024575079559526754
Loss at iteration [145]: 0.002457493453846409
Loss at iteration [146]: 0.0024568236797835037
Loss at iteration [147]: 0.0024564517387817776
Loss at iteration [148]: 0.002456388695652386
Loss at iteration [149]: 0.0024544431622874974
Loss at iteration [150]: 0.002453546800687511
Loss at iteration [151]: 0.002453546800687511
Loss at iteration [152]: 0.002453434098637172
Loss at iteration [153]: 0.0024532968382452807
Loss at iteration [154]: 0.0024529348988429653
Loss at iteration [155]: 0.002452777456318933
Loss at iteration [156]: 0.002452723538277814
Loss at iteration [157]: 0.00245240692001516
Loss at iteration [158]: 0.0024517467921563426
Loss at iteration [159]: 0.0024517467921563426
Loss at iteration [160]: 0.0024515952735635616
Loss at iteration [161]: 0.002451425115946387
Loss at iteration [162]: 0.002450963675482731
Loss at iteration [163]: 0.0024509396524295284
Loss at iteration [164]: 0.002450858834231644
Loss at iteration [165]: 0.002450006576408261
Loss at iteration [166]: 0.0024498328070805364
Loss at iteration [167]: 0.0024498328070805364
Loss at iteration [168]: 0.0024497966396142333
Loss at iteration [169]: 0.002449545849214857
Loss at iteration [170]: 0.0024495127457048595
Loss at iteration [171]: 0.0024494816659990154
Loss at iteration [172]: 0.0024492244372034326
Loss at iteration [173]: 0.0024490664074265757
Loss at iteration [174]: 0.0024490086470793323
Loss at iteration [175]: 0.0024490086470793323
Loss at iteration [176]: 0.0024489769161013
Loss at iteration [177]: 0.002448811309580398
Loss at iteration [178]: 0.0024487167290226284
Loss at iteration [179]: 0.0024486781764142874
Loss at iteration [180]: 0.0024485907607957923
Loss at iteration [181]: 0.0024482849210253673
Loss at iteration [182]: 0.0024481602276519502
Loss at iteration [183]: 0.0024481602276519502
Loss at iteration [184]: 0.0024481166654360875
Loss at iteration [185]: 0.002448070716383197
Loss at iteration [186]: 0.0024479512614864374
Loss at iteration [187]: 0.0024479121921521293
Loss at iteration [188]: 0.0024477702544372167
Loss at iteration [189]: 0.0024477111524030776
Loss at iteration [190]: 0.002447676610944421
Loss at iteration [191]: 0.0024474008124623988
Loss at iteration [192]: 0.0024474008124623988
Loss at iteration [193]: 0.0024472929584235946
Loss at iteration [194]: 0.002447186492187865
Loss at iteration [195]: 0.002446984333767452
Loss at iteration [196]: 0.0024469670702563966
Loss at iteration [197]: 0.0024468923790419246
Loss at iteration [198]: 0.002446867267674947
Loss at iteration [199]: 0.002446867267674947
Loss at iteration [200]: 0.002446851415816445
Loss at iteration [201]: 0.0024467483148643892
Loss at iteration [202]: 0.002446719214985809
Loss at iteration [203]: 0.0024467015549955637
Loss at iteration [204]: 0.002446342076670306
Loss at iteration [205]: 0.0024461437691735816
Loss at iteration [206]: 0.0024460143235306734
Loss at iteration [207]: 0.0024460143235306734
Loss at iteration [208]: 0.002445946076474723
Loss at iteration [209]: 0.0024458459016771803
Loss at iteration [210]: 0.0024455597650848227
Loss at iteration [211]: 0.0024455088445293695
Loss at iteration [212]: 0.0024453779774419033
Loss at iteration [213]: 0.002445299052806291
Loss at iteration [214]: 0.0024452399351802493
Loss at iteration [215]: 0.0024452399351802493
Loss at iteration [216]: 0.0024451991932867923
Loss at iteration [217]: 0.0024448913346306715
Loss at iteration [218]: 0.0024448635381682164
Loss at iteration [219]: 0.0024447938859546805
Loss at iteration [220]: 0.002444608320827729
Loss at iteration [221]: 0.002444572623412782
Loss at iteration [222]: 0.002444380891160311
Loss at iteration [223]: 0.002444380891160311
Loss at iteration [224]: 0.0024442737226928256
Loss at iteration [225]: 0.0024442540968241363
Loss at iteration [226]: 0.002444136072793073
Loss at iteration [227]: 0.0024440715664035256
Loss at iteration [228]: 0.0024439991773275616
Loss at iteration [229]: 0.0024439376030427787
Loss at iteration [230]: 0.00244377911966471
Loss at iteration [231]: 0.00244377911966471
Loss at iteration [232]: 0.002443761729574897
Loss at iteration [233]: 0.002443722501181429
Loss at iteration [234]: 0.0024436366142281713
Loss at iteration [235]: 0.002443611428731648
Loss at iteration [236]: 0.0024435898037284964
Loss at iteration [237]: 0.00244355908109991
Loss at iteration [238]: 0.0024435169036295707
Loss at iteration [239]: 0.0024435169036295707
Loss at iteration [240]: 0.002443501009208252
Loss at iteration [241]: 0.0024434893632596117
Loss at iteration [242]: 0.0024432985089339193
Loss at iteration [243]: 0.002443203019009086
Loss at iteration [244]: 0.002443165270100434
Loss at iteration [245]: 0.002442641968019859
Loss at iteration [246]: 0.0024421515387154803
Loss at iteration [247]: 0.0024421515387154803
Loss at iteration [248]: 0.0024420659906451646
Loss at iteration [249]: 0.002441998400639422
Loss at iteration [250]: 0.002441857941335718
Loss at iteration [251]: 0.002441727152379492
Loss at iteration [252]: 0.0024416989451887137
Loss at iteration [253]: 0.0024416040414563054
Loss at iteration [254]: 0.0024415176486839597
Loss at iteration [255]: 0.0024415176486839597
Loss at iteration [256]: 0.0024414828428925618
Loss at iteration [257]: 0.002441414208215686
Loss at iteration [258]: 0.002441311130022129
Loss at iteration [259]: 0.0024410567314807367
Loss at iteration [260]: 0.002440885314170111
Loss at iteration [261]: 0.0024408559528411337
Loss at iteration [262]: 0.0024407959070632435
Loss at iteration [263]: 0.0024407959070632435
Loss at iteration [264]: 0.00244074934918286
Loss at iteration [265]: 0.0024407351510468945
Loss at iteration [266]: 0.0024405383559614868
Loss at iteration [267]: 0.002440401978265868
Loss at iteration [268]: 0.0024403770191153357
Loss at iteration [269]: 0.002440355935630316
Loss at iteration [270]: 0.0024402352533144543
Loss at iteration [271]: 0.0024402352533144543
Loss at iteration [272]: 0.0024402182943749303
Loss at iteration [273]: 0.002440181070407391
Loss at iteration [274]: 0.0024400873057109558
Loss at iteration [275]: 0.0024400644117762347
Loss at iteration [276]: 0.0024400249247180923
Loss at iteration [277]: 0.002439876331839815
Loss at iteration [278]: 0.002439801391459167
Loss at iteration [279]: 0.002439801391459167
Loss at iteration [280]: 0.0024397541003958402
Loss at iteration [281]: 0.0024396822904327303
Loss at iteration [282]: 0.0024394955360361394
Loss at iteration [283]: 0.002439434907387646
Loss at iteration [284]: 0.002439379078897875
Loss at iteration [285]: 0.0024393063482057932
Loss at iteration [286]: 0.002439286938947996
Loss at iteration [287]: 0.002439286938947996
Loss at iteration [288]: 0.0024392685335753272
Loss at iteration [289]: 0.002439235417489885
Loss at iteration [290]: 0.0024391444201881394
Loss at iteration [291]: 0.0024391334946213565
Loss at iteration [292]: 0.0024389590787085233
Loss at iteration [293]: 0.002438882818122185
Loss at iteration [294]: 0.002438882818122185
Loss at iteration [295]: 0.0024388544722052227
Loss at iteration [296]: 0.0024386569346018976
Loss at iteration [297]: 0.002438635031220417
Loss at iteration [298]: 0.002438538498992418
Loss at iteration [299]: 0.0024384471167032034
Loss at iteration [300]: 0.0024384091766936253
Loss at iteration [301]: 0.0024383652061325304
Loss at iteration [302]: 0.0024383652061325304
Loss at iteration [303]: 0.002438354431487011
Loss at iteration [304]: 0.0024381718054077637
Loss at iteration [305]: 0.0024380936598327697
Loss at iteration [306]: 0.002437983425908084
Loss at iteration [307]: 0.002437534738008155
Loss at iteration [308]: 0.0024375027373014046
Loss at iteration [309]: 0.002437392721817162
Loss at iteration [310]: 0.002437392721817162
Loss at iteration [311]: 0.0024373383944998233
Loss at iteration [312]: 0.0024373185447354256
Loss at iteration [313]: 0.0024369881994253345
Loss at iteration [314]: 0.002436802956426144
Loss at iteration [315]: 0.0024367602558790016
Loss at iteration [316]: 0.002436745698844756
Loss at iteration [317]: 0.0024366932951998977
Loss at iteration [318]: 0.0024366932951998977
Loss at iteration [319]: 0.0024366672394092294
Loss at iteration [320]: 0.0024366427078808726
Loss at iteration [321]: 0.002436613518360206
Loss at iteration [322]: 0.0024365324936040916
Loss at iteration [323]: 0.0024364458626332593
Loss at iteration [324]: 0.0024364268026913155
Loss at iteration [325]: 0.002436371904606474
Loss at iteration [326]: 0.002436371904606474
Loss at iteration [327]: 0.0024363401110015824
Loss at iteration [328]: 0.0024363270940816247
Loss at iteration [329]: 0.0024362512460372205
Loss at iteration [330]: 0.0024362414155434074
Loss at iteration [331]: 0.002436213073556698
Loss at iteration [332]: 0.0024360229770577545
Loss at iteration [333]: 0.0024356962257322166
Loss at iteration [334]: 0.0024356962257322166
Loss at iteration [335]: 0.0024356491675041362
Loss at iteration [336]: 0.002435365150934654
Loss at iteration [337]: 0.0024351499314040313
Loss at iteration [338]: 0.0024350657112767013
Loss at iteration [339]: 0.0024349664395453157
Loss at iteration [340]: 0.002434923236525137
Loss at iteration [341]: 0.0024348932783481493
Loss at iteration [342]: 0.0024348932783481493
Loss at iteration [343]: 0.002434874638687416
Loss at iteration [344]: 0.0024348373116572907
Loss at iteration [345]: 0.0024347812006616826
Loss at iteration [346]: 0.0024347392969511048
Loss at iteration [347]: 0.002434613336553564
Loss at iteration [348]: 0.0024343433027957367
Loss at iteration [349]: 0.0024342886206811373
Loss at iteration [350]: 0.0024342886206811373
Loss at iteration [351]: 0.0024342653999302293
Loss at iteration [352]: 0.0024341243562815613
Loss at iteration [353]: 0.002433978446816376
Loss at iteration [354]: 0.002433941659988572
Loss at iteration [355]: 0.00243366567528595
Loss at iteration [356]: 0.0024336403226507203
Loss at iteration [357]: 0.0024336403226507203
Loss at iteration [358]: 0.0024336225457557326
Loss at iteration [359]: 0.0024334891579053104
Loss at iteration [360]: 0.0024334666322568727
Loss at iteration [361]: 0.0024334484904788827
Loss at iteration [362]: 0.0024333264465531738
Loss at iteration [363]: 0.002433295709436349
Loss at iteration [364]: 0.0024332522997858193
Loss at iteration [365]: 0.0024332522997858193
Loss at iteration [366]: 0.0024331898751689435
Loss at iteration [367]: 0.0024331699479283515
Loss at iteration [368]: 0.002433066461260556
Loss at iteration [369]: 0.0024330453036016807
Loss at iteration [370]: 0.0024330288100860315
Loss at iteration [371]: 0.002433000086563398
Loss at iteration [372]: 0.002432933659298953
Loss at iteration [373]: 0.002432933659298953
Loss at iteration [374]: 0.002432901474558482
Loss at iteration [375]: 0.002432872304111512
Loss at iteration [376]: 0.002432831194581285
Loss at iteration [377]: 0.0024328082504953027
Loss at iteration [378]: 0.0024327369611913197
Loss at iteration [379]: 0.0024315948468054543
Loss at iteration [380]: 0.0024315948468054543
Loss at iteration [381]: 0.002430963869302177
Loss at iteration [382]: 0.00243088666836834
Loss at iteration [383]: 0.002430679086475562
Loss at iteration [384]: 0.0024306622018894237
Loss at iteration [385]: 0.002430640820103209
Loss at iteration [386]: 0.0024305288149354095
Loss at iteration [387]: 0.0024304412798231024
Loss at iteration [388]: 0.0024304412798231024
Loss at iteration [389]: 0.0024304028598805217
Loss at iteration [390]: 0.0024303171602987187
Loss at iteration [391]: 0.002430193996886051
Loss at iteration [392]: 0.0024301407544925764
Loss at iteration [393]: 0.002429689184007425
Loss at iteration [394]: 0.002429559028886512
Loss at iteration [395]: 0.00242951564026856
Loss at iteration [396]: 0.00242951564026856
Loss at iteration [397]: 0.002429477756795108
Loss at iteration [398]: 0.002429395052075343
Loss at iteration [399]: 0.002429309738394971
Loss at iteration [400]: 0.002429303564909617
Loss at iteration [401]: 0.002429283110223669
Loss at iteration [402]: 0.0024292427376275665
Loss at iteration [403]: 0.0024292135879400603
Loss at iteration [404]: 0.0024292135879400603
Loss at iteration [405]: 0.0024292003865355343
Loss at iteration [406]: 0.002429118476431299
Loss at iteration [407]: 0.002429065859455404
Loss at iteration [408]: 0.002429050312774991
Loss at iteration [409]: 0.0024290098461439923
Loss at iteration [410]: 0.0024289606376971524
Loss at iteration [411]: 0.0024289606376971524
Loss at iteration [412]: 0.0024289482851370687
Loss at iteration [413]: 0.002428929431892556
Loss at iteration [414]: 0.00242888314756781
Loss at iteration [415]: 0.002428834806978716
Loss at iteration [416]: 0.002428780244983495
Loss at iteration [417]: 0.0024285750884970934
Loss at iteration [418]: 0.0024285224973900578
Loss at iteration [419]: 0.0024285224973900578
Loss at iteration [420]: 0.0024284899206986044
Loss at iteration [421]: 0.0024283326233741345
Loss at iteration [422]: 0.0024282138696482534
Loss at iteration [423]: 0.0024281656975207906
Loss at iteration [424]: 0.0024281435580709087
Loss at iteration [425]: 0.0024281010764849887
Loss at iteration [426]: 0.0024280674077666077
Loss at iteration [427]: 0.0024280674077666077
Loss at iteration [428]: 0.0024280491640421794
Loss at iteration [429]: 0.0024280392001235925
Loss at iteration [430]: 0.0024279979310471113
Loss at iteration [431]: 0.0024278707803371703
Loss at iteration [432]: 0.002427737089268488
Loss at iteration [433]: 0.002427686045741104
Loss at iteration [434]: 0.002427686045741104
Loss at iteration [435]: 0.002427654518869806
Loss at iteration [436]: 0.0024276190725001073
Loss at iteration [437]: 0.0024275303977643733
Loss at iteration [438]: 0.002427441644122678
Loss at iteration [439]: 0.0024274126224344793
Loss at iteration [440]: 0.0024272734667652907
Loss at iteration [441]: 0.002427247711614014
Loss at iteration [442]: 0.002427247711614014
Loss at iteration [443]: 0.002427232496844707
Loss at iteration [444]: 0.0024270022924937186
Loss at iteration [445]: 0.002426969181178032
Loss at iteration [446]: 0.0024269404815361255
Loss at iteration [447]: 0.0024268609629919382
Loss at iteration [448]: 0.0024268450187749703
Loss at iteration [449]: 0.0024267491996378316
Loss at iteration [450]: 0.0024267491996378316
Loss at iteration [451]: 0.0024267352798357507
Loss at iteration [452]: 0.002426721892674777
Loss at iteration [453]: 0.0024266722105208745
Loss at iteration [454]: 0.002426642445277631
Loss at iteration [455]: 0.0024265485907470617
Loss at iteration [456]: 0.002426488675115056
Loss at iteration [457]: 0.002426488675115056
Loss at iteration [458]: 0.0024264723515722516
Loss at iteration [459]: 0.002426406047443659
Loss at iteration [460]: 0.002426351306164552
Loss at iteration [461]: 0.002426321185456386
Loss at iteration [462]: 0.00242628662008728
Loss at iteration [463]: 0.0024262639073344538
Loss at iteration [464]: 0.002426232501317777
Loss at iteration [465]: 0.002426232501317777
Loss at iteration [466]: 0.002426214192884866
Loss at iteration [467]: 0.0024262034098057237
Loss at iteration [468]: 0.0024261588638297854
Loss at iteration [469]: 0.002426108535655423
Loss at iteration [470]: 0.00242609124299712
Loss at iteration [471]: 0.0024257223020714798
Loss at iteration [472]: 0.002425476185319645
Loss at iteration [473]: 0.002425476185319645
Loss at iteration [474]: 0.00242543319616344
Loss at iteration [475]: 0.0024252908568999296
Loss at iteration [476]: 0.0024250774994735787
Loss at iteration [477]: 0.002425035961427278
Loss at iteration [478]: 0.002424898967160647
Loss at iteration [479]: 0.0024247504916794617
Loss at iteration [480]: 0.0024247161962154978
Loss at iteration [481]: 0.0024247161962154978
Loss at iteration [482]: 0.002424696441241478
Loss at iteration [483]: 0.002424647810650092
Loss at iteration [484]: 0.0024245791049005857
Loss at iteration [485]: 0.002424545025247791
Loss at iteration [486]: 0.002424523513020509
Loss at iteration [487]: 0.002424487303552896
Loss at iteration [488]: 0.002424487303552896
Loss at iteration [489]: 0.0024244638410917682
Loss at iteration [490]: 0.002424432316840128
Loss at iteration [491]: 0.0024243984631605806
Loss at iteration [492]: 0.002424370348113836
Loss at iteration [493]: 0.00242432383657081
Loss at iteration [494]: 0.002424267080858213
Loss at iteration [495]: 0.002424267080858213
Loss at iteration [496]: 0.0024242424414469394
Loss at iteration [497]: 0.0024241387643647245
Loss at iteration [498]: 0.0024241139654482134
Loss at iteration [499]: 0.0024241003577868975
Loss at iteration [500]: 0.0024240773508646747
Loss at iteration [501]: 0.002424066877358822
Loss at iteration [502]: 0.00242404381713567
Loss at iteration [503]: 0.00242404381713567
Loss at iteration [504]: 0.0024240330625284454
Loss at iteration [505]: 0.0024240247079642917
Loss at iteration [506]: 0.002423964161613853
Loss at iteration [507]: 0.0024239152411873644
Loss at iteration [508]: 0.0024238296534119844
Loss at iteration [509]: 0.0024233435806013085
Loss at iteration [510]: 0.002422613111690516
Loss at iteration [511]: 0.002422613111690516
Loss at iteration [512]: 0.0024223880542829463
Loss at iteration [513]: 0.0024223441594839984
Loss at iteration [514]: 0.0024220920630136987
Loss at iteration [515]: 0.0024220714615615395
Loss at iteration [516]: 0.0024219337177331995
Loss at iteration [517]: 0.0024217009750344518
Loss at iteration [518]: 0.0024216279678745253
Loss at iteration [519]: 0.0024216279678745253
Loss at iteration [520]: 0.00242156946350043
Loss at iteration [521]: 0.00242154837677523
Loss at iteration [522]: 0.002421398116466619
Loss at iteration [523]: 0.0024213885018216845
Loss at iteration [524]: 0.0024213642948010417
Loss at iteration [525]: 0.002421236706573693
Loss at iteration [526]: 0.0024212032935329014
Loss at iteration [527]: 0.0024212032935329014
Loss at iteration [528]: 0.0024211816224837092
Loss at iteration [529]: 0.0024211149249366644
Loss at iteration [530]: 0.0024210660878416297
Loss at iteration [531]: 0.0024210155291380943
Loss at iteration [532]: 0.002420967306440856
Loss at iteration [533]: 0.0024209434754104598
Loss at iteration [534]: 0.002420913024198213
Loss at iteration [535]: 0.002420913024198213
Loss at iteration [536]: 0.002420893418633144
Loss at iteration [537]: 0.002420876844348602
Loss at iteration [538]: 0.0024208201149488974
Loss at iteration [539]: 0.0024207855215542845
Loss at iteration [540]: 0.002420681292028095
Loss at iteration [541]: 0.0024205782024211606
Loss at iteration [542]: 0.0024203688705150576
Loss at iteration [543]: 0.0024203688705150576
Loss at iteration [544]: 0.002420257204098579
Loss at iteration [545]: 0.0024202288776008257
Loss at iteration [546]: 0.0024201410652487437
Loss at iteration [547]: 0.002420126460744405
Loss at iteration [548]: 0.0024200912300065113
Loss at iteration [549]: 0.0024200480357606737
Loss at iteration [550]: 0.0024200480357606737
Loss at iteration [551]: 0.002420032074336826
Loss at iteration [552]: 0.0024199844030601677
Loss at iteration [553]: 0.0024199425703609213
Loss at iteration [554]: 0.002419920056626497
Loss at iteration [555]: 0.0024198839736546958
Loss at iteration [556]: 0.00241975089381971
Loss at iteration [557]: 0.0024196719496914995
Loss at iteration [558]: 0.0024196719496914995
Loss at iteration [559]: 0.0024196542296499113
Loss at iteration [560]: 0.0024195036875956193
Loss at iteration [561]: 0.002419472050813388
Loss at iteration [562]: 0.0024194405245259545
Loss at iteration [563]: 0.0024193652656671117
Loss at iteration [564]: 0.0024193103959585832
Loss at iteration [565]: 0.0024193103959585832
Loss at iteration [566]: 0.0024192711567246512
Loss at iteration [567]: 0.002419252829031837
Loss at iteration [568]: 0.0024191842557206695
Loss at iteration [569]: 0.0024191706841071824
Loss at iteration [570]: 0.002419147002121584
Loss at iteration [571]: 0.0024190633322701428
Loss at iteration [572]: 0.002419048364027544
Loss at iteration [573]: 0.002419048364027544
Loss at iteration [574]: 0.002419030165414764
Loss at iteration [575]: 0.00241898451385368
Loss at iteration [576]: 0.0024189685386263684
Loss at iteration [577]: 0.002418939620789683
Loss at iteration [578]: 0.0024188530078961905
Loss at iteration [579]: 0.0024187649539845557
Loss at iteration [580]: 0.0024187649539845557
Loss at iteration [581]: 0.002418749042121556
Loss at iteration [582]: 0.0024186562648428516
Loss at iteration [583]: 0.0024186461580176168
Loss at iteration [584]: 0.002418630277642882
Loss at iteration [585]: 0.002418619637244269
Loss at iteration [586]: 0.0024185943919724983
Loss at iteration [587]: 0.0024185693959648635
Loss at iteration [588]: 0.0024185693959648635
Loss at iteration [589]: 0.0024185529103830294
Loss at iteration [590]: 0.002418504722686444
Loss at iteration [591]: 0.0024184923053296617
Loss at iteration [592]: 0.00241844588249267
Loss at iteration [593]: 0.0024183638017528162
Loss at iteration [594]: 0.0024183146680861107
Loss at iteration [595]: 0.0024183146680861107
Loss at iteration [596]: 0.0024182775509608346
Loss at iteration [597]: 0.0024182101788170225
Loss at iteration [598]: 0.0024181631316880095
Loss at iteration [599]: 0.0024181253489959508
Loss at iteration [600]: 0.0024180555518008616
Loss at iteration [601]: 0.002418029324585097
Loss at iteration [602]: 0.002417994080324657
Loss at iteration [603]: 0.002417994080324657
Loss at iteration [604]: 0.0024179586048653
Loss at iteration [605]: 0.002417949162618705
Loss at iteration [606]: 0.0024178599784701925
Loss at iteration [607]: 0.0024178413103562594
Loss at iteration [608]: 0.002417811874290376
Loss at iteration [609]: 0.0024177434683721024
Loss at iteration [610]: 0.0024176373683894603
Loss at iteration [611]: 0.0024176373683894603
Loss at iteration [612]: 0.002417580005963381
Loss at iteration [613]: 0.0024175326461792837
Loss at iteration [614]: 0.002417426256012911
Loss at iteration [615]: 0.0024174032140009977
Loss at iteration [616]: 0.0024173258337637834
Loss at iteration [617]: 0.002417258009007495
Loss at iteration [618]: 0.002417258009007495
Loss at iteration [619]: 0.002417241645124043
Loss at iteration [620]: 0.0024171813819631347
Loss at iteration [621]: 0.002417093414294328
Loss at iteration [622]: 0.0024170671089749148
Loss at iteration [623]: 0.002417053875023654
Loss at iteration [624]: 0.0024170151388921675
Loss at iteration [625]: 0.002416994842163332
Loss at iteration [626]: 0.002416994842163332
Loss at iteration [627]: 0.002416981906025097
Loss at iteration [628]: 0.002416936995660179
Loss at iteration [629]: 0.002416914706557786
Loss at iteration [630]: 0.0024168919404695395
Loss at iteration [631]: 0.002416385179138111
Loss at iteration [632]: 0.002416385179138111
Loss at iteration [633]: 0.0024161068326254477
Loss at iteration [634]: 0.00241589182817511
Loss at iteration [635]: 0.0024156917653736765
Loss at iteration [636]: 0.0024156430860617587
Loss at iteration [637]: 0.0024155765115820797
Loss at iteration [638]: 0.0024154002432332177
Loss at iteration [639]: 0.0024154002432332177
Loss at iteration [640]: 0.0024153448392534973
Loss at iteration [641]: 0.00241530237077388
Loss at iteration [642]: 0.002415190405566237
Loss at iteration [643]: 0.0024151762482734936
Loss at iteration [644]: 0.0024151097108608384
Loss at iteration [645]: 0.002415005446931302
Loss at iteration [646]: 0.002415005446931302
Loss at iteration [647]: 0.002414992278367256
Loss at iteration [648]: 0.0024149074436585717
Loss at iteration [649]: 0.0024149006581643943
Loss at iteration [650]: 0.0024148573982126632
Loss at iteration [651]: 0.002414842709878365
Loss at iteration [652]: 0.0024148215595566156
Loss at iteration [653]: 0.00241477270918212
Loss at iteration [654]: 0.00241477270918212
Loss at iteration [655]: 0.002414756369201983
Loss at iteration [656]: 0.002414735742337589
Loss at iteration [657]: 0.0024147033773399247
Loss at iteration [658]: 0.0024146785712411325
Loss at iteration [659]: 0.0024146186176201116
Loss at iteration [660]: 0.002414351844595806
Loss at iteration [661]: 0.002414351844595806
Loss at iteration [662]: 0.0024142730134530882
Loss at iteration [663]: 0.002414237195928827
Loss at iteration [664]: 0.002414109758564477
Loss at iteration [665]: 0.0024140964244067442
Loss at iteration [666]: 0.0024140460522611065
Loss at iteration [667]: 0.002414012991992698
Loss at iteration [668]: 0.002414012991992698
Loss at iteration [669]: 0.0024139956318511837
Loss at iteration [670]: 0.0024139347144607657
Loss at iteration [671]: 0.0024139020765039146
Loss at iteration [672]: 0.00241389220494678
Loss at iteration [673]: 0.002413870119989013
Loss at iteration [674]: 0.0024138532436441098
Loss at iteration [675]: 0.0024138327562730614
Loss at iteration [676]: 0.0024138327562730614
Loss at iteration [677]: 0.002413823512670576
Loss at iteration [678]: 0.002413797214669804
Loss at iteration [679]: 0.0024137754068766786
Loss at iteration [680]: 0.0024137656946873953
Loss at iteration [681]: 0.0024137416870946542
Loss at iteration [682]: 0.002413722630330299
Loss at iteration [683]: 0.002413722630330299
Loss at iteration [684]: 0.002413711621102633
Loss at iteration [685]: 0.0024136958950269688
Loss at iteration [686]: 0.0024136825663103128
Loss at iteration [687]: 0.002413675472048077
Loss at iteration [688]: 0.0024136651164653137
Loss at iteration [689]: 0.0024136523570334944
Loss at iteration [690]: 0.0024136276345904997
Loss at iteration [691]: 0.0024136276345904997
Loss at iteration [692]: 0.0024135953964034697
Loss at iteration [693]: 0.0024135841411184306
Loss at iteration [694]: 0.0024135650854925153
Loss at iteration [695]: 0.0024135472218404217
Loss at iteration [696]: 0.002413527612271969
Loss at iteration [697]: 0.0024133788316754467
Loss at iteration [698]: 0.0024133788316754467
Loss at iteration [699]: 0.002413315017602114
Loss at iteration [700]: 0.002413302231296583
Loss at iteration [701]: 0.0024131506427404774
Loss at iteration [702]: 0.002413134794555161
Loss at iteration [703]: 0.0024130830900061253
Loss at iteration [704]: 0.0024130373263245575
Loss at iteration [705]: 0.0024128638836922603
Loss at iteration [706]: 0.0024128638836922603
Loss at iteration [707]: 0.002412817071136801
Loss at iteration [708]: 0.002412786457958722
Loss at iteration [709]: 0.0024126851821318347
Loss at iteration [710]: 0.0024126694047890587
Loss at iteration [711]: 0.002412563493946675
Loss at iteration [712]: 0.002412545412154225
Loss at iteration [713]: 0.0024124498133778222
Loss at iteration [714]: 0.0024124498133778222
Loss at iteration [715]: 0.0024123992631775427
Loss at iteration [716]: 0.002412387718840606
Loss at iteration [717]: 0.0024122963524446863
Loss at iteration [718]: 0.002412283824059161
Loss at iteration [719]: 0.0024122333111524278
Loss at iteration [720]: 0.0024121135647245323
Loss at iteration [721]: 0.0024121135647245323
Loss at iteration [722]: 0.0024121014082495555
Loss at iteration [723]: 0.002412027854272198
Loss at iteration [724]: 0.0024119660090601226
Loss at iteration [725]: 0.002411953758700909
Loss at iteration [726]: 0.0024119191038110346
Loss at iteration [727]: 0.0024118789783196285
Loss at iteration [728]: 0.0024118714148763706
Loss at iteration [729]: 0.0024118714148763706
Loss at iteration [730]: 0.002411857278193138
Loss at iteration [731]: 0.0024118195485616787
Loss at iteration [732]: 0.0024117885845717094
Loss at iteration [733]: 0.0024116853482988644
Loss at iteration [734]: 0.0024114647627557443
Loss at iteration [735]: 0.0024114384196793413
Loss at iteration [736]: 0.0024114384196793413
Loss at iteration [737]: 0.002411417365314106
Loss at iteration [738]: 0.0024113598008080493
Loss at iteration [739]: 0.0024111678160357846
Loss at iteration [740]: 0.0024111505808643284
Loss at iteration [741]: 0.002411067327254597
Loss at iteration [742]: 0.0024110305616475416
Loss at iteration [743]: 0.0024110116570038817
Loss at iteration [744]: 0.0024110116570038817
Loss at iteration [745]: 0.0024110000904649935
Loss at iteration [746]: 0.0024109717541713174
Loss at iteration [747]: 0.0024109264523245128
Loss at iteration [748]: 0.0024109081680001765
Loss at iteration [749]: 0.002410891838234696
Loss at iteration [750]: 0.002410854955766239
Loss at iteration [751]: 0.0024108346738415305
Loss at iteration [752]: 0.0024108346738415305
Loss at iteration [753]: 0.0024108243287694646
Loss at iteration [754]: 0.0024108005823504476
Loss at iteration [755]: 0.0024107799799758856
Loss at iteration [756]: 0.002410749736283656
Loss at iteration [757]: 0.002410668530975975
Loss at iteration [758]: 0.002410600425336751
Loss at iteration [759]: 0.0024104721821166988
Loss at iteration [760]: 0.0024104721821166988
Loss at iteration [761]: 0.0024103242623052666
Loss at iteration [762]: 0.002410309954669843
Loss at iteration [763]: 0.002410230880788471
Loss at iteration [764]: 0.0024101996819886015
Loss at iteration [765]: 0.002410117372164709
Loss at iteration [766]: 0.0024100974437404423
Loss at iteration [767]: 0.0024100974437404423
Loss at iteration [768]: 0.00241008034902127
Loss at iteration [769]: 0.0024100466320215617
Loss at iteration [770]: 0.002410001358632127
Loss at iteration [771]: 0.00240996525986701
Loss at iteration [772]: 0.0024099488580198213
Loss at iteration [773]: 0.0024099167897830977
Loss at iteration [774]: 0.0024099167897830977
Loss at iteration [775]: 0.0024098565243490986
Loss at iteration [776]: 0.0024098442447823246
Loss at iteration [777]: 0.0024097798894729882
Loss at iteration [778]: 0.0024097728654271645
Loss at iteration [779]: 0.0024097446980826743
Loss at iteration [780]: 0.002409711631538097
Loss at iteration [781]: 0.002409699149065376
Loss at iteration [782]: 0.002409699149065376
Loss at iteration [783]: 0.0024096902043758968
Loss at iteration [784]: 0.0024096619704138673
Loss at iteration [785]: 0.002409630753441197
Loss at iteration [786]: 0.0024096191212672996
Loss at iteration [787]: 0.002409569117244115
Loss at iteration [788]: 0.002409291542587499
Loss at iteration [789]: 0.002409291542587499
Loss at iteration [790]: 0.002409096173023325
Loss at iteration [791]: 0.0024090562398341275
Loss at iteration [792]: 0.002408937491997928
Loss at iteration [793]: 0.002408925437662182
Loss at iteration [794]: 0.002408877807627268
Loss at iteration [795]: 0.0024088539910700852
Loss at iteration [796]: 0.002408809703524393
Loss at iteration [797]: 0.002408809703524393
Loss at iteration [798]: 0.0024088009514590747
Loss at iteration [799]: 0.002408738806602156
Loss at iteration [800]: 0.0024086796335620613
Loss at iteration [801]: 0.0024085746127595095
Loss at iteration [802]: 0.0024084620593586223
Loss at iteration [803]: 0.0024083093566358723
Loss at iteration [804]: 0.0024083093566358723
Loss at iteration [805]: 0.0024082915172605355
Loss at iteration [806]: 0.002408145976925199
Loss at iteration [807]: 0.0024080850259068586
Loss at iteration [808]: 0.0024080488850315492
Loss at iteration [809]: 0.0024079147132242025
Loss at iteration [810]: 0.002407881730285553
Loss at iteration [811]: 0.002407852893361508
Loss at iteration [812]: 0.002407852893361508
Loss at iteration [813]: 0.002407832554477718
Loss at iteration [814]: 0.002407818378024189
Loss at iteration [815]: 0.002407765204556122
Loss at iteration [816]: 0.0024077480416796917
Loss at iteration [817]: 0.0024077324995056323
Loss at iteration [818]: 0.0024076932281788285
Loss at iteration [819]: 0.002407670823899968
Loss at iteration [820]: 0.002407670823899968
Loss at iteration [821]: 0.0024076584098749903
Loss at iteration [822]: 0.0024076293877796817
Loss at iteration [823]: 0.002407591481452045
Loss at iteration [824]: 0.0024075671170840575
Loss at iteration [825]: 0.0024075483932963397
Loss at iteration [826]: 0.0024075340980799207
Loss at iteration [827]: 0.002407514886814807
Loss at iteration [828]: 0.002407514886814807
Loss at iteration [829]: 0.0024075092200973045
Loss at iteration [830]: 0.00240750152931619
Loss at iteration [831]: 0.0024074758121516715
Loss at iteration [832]: 0.002407460132561522
Loss at iteration [833]: 0.002407422893748169
Loss at iteration [834]: 0.002407388998862348
Loss at iteration [835]: 0.0024073803476434203
Loss at iteration [836]: 0.0024073543500846323
Loss at iteration [837]: 0.0024073543500846323
Loss at iteration [838]: 0.0024073436204311988
Loss at iteration [839]: 0.002407334691078801
Loss at iteration [840]: 0.002407306829487513
Loss at iteration [841]: 0.0024072911603120794
Loss at iteration [842]: 0.0024071171369184725
Loss at iteration [843]: 0.002407062541309917
Loss at iteration [844]: 0.002407062541309917
Loss at iteration [845]: 0.0024070483462654575
Loss at iteration [846]: 0.0024068854094607124
Loss at iteration [847]: 0.0024068731185134863
Loss at iteration [848]: 0.002406824931738279
Loss at iteration [849]: 0.0024067541610821113
Loss at iteration [850]: 0.002406737003003305
Loss at iteration [851]: 0.002406737003003305
Loss at iteration [852]: 0.002406727162148467
Loss at iteration [853]: 0.002406681416448626
Loss at iteration [854]: 0.0024066445943990585
Loss at iteration [855]: 0.00240663652006213
Loss at iteration [856]: 0.0024066275681725873
Loss at iteration [857]: 0.0024066171678204865
Loss at iteration [858]: 0.0024065999390955415
Loss at iteration [859]: 0.0024065999390955415
Loss at iteration [860]: 0.002406587768026745
Loss at iteration [861]: 0.0024065800364533275
Loss at iteration [862]: 0.00240656353432745
Loss at iteration [863]: 0.0024065394896261006
Loss at iteration [864]: 0.0024065227478010758
Loss at iteration [865]: 0.0024064963997399713
Loss at iteration [866]: 0.0024064523360352243
Loss at iteration [867]: 0.0024064523360352243
Loss at iteration [868]: 0.0024064440139954183
Loss at iteration [869]: 0.0024064281833429957
Loss at iteration [870]: 0.0024063944942995085
Loss at iteration [871]: 0.0024063811778265998
Loss at iteration [872]: 0.0024063143250763817
Loss at iteration [873]: 0.0024062889740411207
Loss at iteration [874]: 0.0024062665132436596
Loss at iteration [875]: 0.0024062665132436596
Loss at iteration [876]: 0.0024062535388029984
Loss at iteration [877]: 0.002406241736670613
Loss at iteration [878]: 0.0024062042485269307
Loss at iteration [879]: 0.0024061826903093247
Loss at iteration [880]: 0.0024061715288635805
Loss at iteration [881]: 0.0024061606594882904
Loss at iteration [882]: 0.00240613062447382
