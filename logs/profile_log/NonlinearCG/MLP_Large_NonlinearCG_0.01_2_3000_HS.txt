Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : NonlinearCG
Learning rate                         : 0.01
Beta type                             :HS
Total number of function evaluations  : 3025
Total number of iterations            : 1043
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 42.77730083465576
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.99241295419663%
Percentage of parameters < 1e-7       : 49.99266170979674%
Percentage of parameters < 1e-6       : 49.99365673219718%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.0028109687833258474
Loss at iteration [2]: 0.0027837076487947994
Loss at iteration [3]: 0.0027445684940014792
Loss at iteration [4]: 0.0026782989699829667
Loss at iteration [5]: 0.0026642319348736164
Loss at iteration [6]: 0.002644723544669484
Loss at iteration [7]: 0.0026410964694977054
Loss at iteration [8]: 0.0026360753681997262
Loss at iteration [9]: 0.0026184130583149245
Loss at iteration [10]: 0.0026158456666332363
Loss at iteration [11]: 0.0026089773666683027
Loss at iteration [12]: 0.002572537286164835
Loss at iteration [13]: 0.002572537286164835
Loss at iteration [14]: 0.0025643573195720582
Loss at iteration [15]: 0.002563327476301682
Loss at iteration [16]: 0.002553931738012047
Loss at iteration [17]: 0.0025524803944774804
Loss at iteration [18]: 0.0025456117515224304
Loss at iteration [19]: 0.002540256751129722
Loss at iteration [20]: 0.00253932266500949
Loss at iteration [21]: 0.002538298265294005
Loss at iteration [22]: 0.0025379443174537364
Loss at iteration [23]: 0.0025358348115014284
Loss at iteration [24]: 0.0025358348115014284
Loss at iteration [25]: 0.0025347134592893497
Loss at iteration [26]: 0.002534415517666429
Loss at iteration [27]: 0.0025249971860964895
Loss at iteration [28]: 0.0025239630257067034
Loss at iteration [29]: 0.0025187532015331013
Loss at iteration [30]: 0.002510886902862045
Loss at iteration [31]: 0.002510259833092008
Loss at iteration [32]: 0.0025099983812632335
Loss at iteration [33]: 0.0025099983812632335
Loss at iteration [34]: 0.002509778818193266
Loss at iteration [35]: 0.0025093750171681743
Loss at iteration [36]: 0.00250752568943424
Loss at iteration [37]: 0.002507191551627868
Loss at iteration [38]: 0.0025067679620862653
Loss at iteration [39]: 0.0025064866240313965
Loss at iteration [40]: 0.0025060158277470123
Loss at iteration [41]: 0.0025041738518742133
Loss at iteration [42]: 0.0025040777082512827
Loss at iteration [43]: 0.0024995545529175774
Loss at iteration [44]: 0.0024995545529175774
Loss at iteration [45]: 0.0024992291219635577
Loss at iteration [46]: 0.0024990768788322832
Loss at iteration [47]: 0.002498208287023702
Loss at iteration [48]: 0.00249510017329252
Loss at iteration [49]: 0.002491829370573396
Loss at iteration [50]: 0.0024905094336089358
Loss at iteration [51]: 0.002490281107973217
Loss at iteration [52]: 0.0024896719406203725
Loss at iteration [53]: 0.0024888138957071147
Loss at iteration [54]: 0.002488030507940097
Loss at iteration [55]: 0.002488030507940097
Loss at iteration [56]: 0.002487673546483543
Loss at iteration [57]: 0.002487586619607174
Loss at iteration [58]: 0.0024860266702206573
Loss at iteration [59]: 0.0024858501801059466
Loss at iteration [60]: 0.0024856108283108033
Loss at iteration [61]: 0.002484013894952241
Loss at iteration [62]: 0.0024838067215277497
Loss at iteration [63]: 0.002483661524159854
Loss at iteration [64]: 0.002483371249911584
Loss at iteration [65]: 0.002483371249911584
Loss at iteration [66]: 0.0024831469851927363
Loss at iteration [67]: 0.0024824017588146235
Loss at iteration [68]: 0.0024822359571377555
Loss at iteration [69]: 0.0024817137680412395
Loss at iteration [70]: 0.002480842732469639
Loss at iteration [71]: 0.002480594220211123
Loss at iteration [72]: 0.0024802354657792384
Loss at iteration [73]: 0.0024797028479164023
Loss at iteration [74]: 0.0024797028479164023
Loss at iteration [75]: 0.0024795482173907576
Loss at iteration [76]: 0.002478211573750607
Loss at iteration [77]: 0.002477621739864854
Loss at iteration [78]: 0.002477455104917912
Loss at iteration [79]: 0.002477100179784929
Loss at iteration [80]: 0.002476887574730909
Loss at iteration [81]: 0.0024767311501378026
Loss at iteration [82]: 0.002476538551764209
Loss at iteration [83]: 0.0024764587380917864
Loss at iteration [84]: 0.0024763577435319073
Loss at iteration [85]: 0.0024763577435319073
Loss at iteration [86]: 0.0024762928304377643
Loss at iteration [87]: 0.002476124865906566
Loss at iteration [88]: 0.0024753214859369282
Loss at iteration [89]: 0.002475270790588757
Loss at iteration [90]: 0.0024749255119357206
Loss at iteration [91]: 0.0024748358647191542
Loss at iteration [92]: 0.002473806547590365
Loss at iteration [93]: 0.0024736464156059922
Loss at iteration [94]: 0.002473574169305564
Loss at iteration [95]: 0.002473574169305564
Loss at iteration [96]: 0.002473552620041056
Loss at iteration [97]: 0.0024734961903777925
Loss at iteration [98]: 0.0024726766044289032
Loss at iteration [99]: 0.0024724787969126307
Loss at iteration [100]: 0.002472359703524302
Loss at iteration [101]: 0.002472014824113107
Loss at iteration [102]: 0.00247178066674357
Loss at iteration [103]: 0.002471192322941904
Loss at iteration [104]: 0.0024707770613211968
Loss at iteration [105]: 0.0024707770613211968
Loss at iteration [106]: 0.0024704699156652965
Loss at iteration [107]: 0.0024700803641405306
Loss at iteration [108]: 0.002468618177820534
Loss at iteration [109]: 0.0024678608072579546
Loss at iteration [110]: 0.0024678139778687856
Loss at iteration [111]: 0.0024660623083760857
Loss at iteration [112]: 0.0024656660444473044
Loss at iteration [113]: 0.0024656346474024255
Loss at iteration [114]: 0.0024656346474024255
Loss at iteration [115]: 0.002465598932187625
Loss at iteration [116]: 0.0024654239491662365
Loss at iteration [117]: 0.002465162300831459
Loss at iteration [118]: 0.002465092325931751
Loss at iteration [119]: 0.002465034488981983
Loss at iteration [120]: 0.0024649276955371353
Loss at iteration [121]: 0.0024645729265952592
Loss at iteration [122]: 0.002464487689460661
Loss at iteration [123]: 0.002464318012713496
Loss at iteration [124]: 0.002464318012713496
Loss at iteration [125]: 0.0024642310228242714
Loss at iteration [126]: 0.002464092097880334
Loss at iteration [127]: 0.002463753236212501
Loss at iteration [128]: 0.0024636131805518195
Loss at iteration [129]: 0.0024633883167193248
Loss at iteration [130]: 0.0024633085273840236
Loss at iteration [131]: 0.0024632591911459196
Loss at iteration [132]: 0.0024630951466682706
Loss at iteration [133]: 0.002463017869376941
Loss at iteration [134]: 0.002463017869376941
Loss at iteration [135]: 0.002463002901225669
Loss at iteration [136]: 0.0024629732786474374
Loss at iteration [137]: 0.0024625237743857893
Loss at iteration [138]: 0.0024623166683125143
Loss at iteration [139]: 0.0024620082708748563
Loss at iteration [140]: 0.002461546586964667
Loss at iteration [141]: 0.0024609191271290657
Loss at iteration [142]: 0.0024609191271290657
Loss at iteration [143]: 0.002460418901717312
Loss at iteration [144]: 0.0024583068636391944
Loss at iteration [145]: 0.002458227147008661
Loss at iteration [146]: 0.002457563175261831
Loss at iteration [147]: 0.002457437781929595
Loss at iteration [148]: 0.002457198377355345
Loss at iteration [149]: 0.0024565754820417693
Loss at iteration [150]: 0.002456031957177637
Loss at iteration [151]: 0.002456031957177637
Loss at iteration [152]: 0.00245569087542473
Loss at iteration [153]: 0.002455641341817541
Loss at iteration [154]: 0.0024551545707582733
Loss at iteration [155]: 0.002455039383462491
Loss at iteration [156]: 0.0024547820144123975
Loss at iteration [157]: 0.002454230756618223
Loss at iteration [158]: 0.0024541746232274787
Loss at iteration [159]: 0.002453991956737587
Loss at iteration [160]: 0.002453672602820295
Loss at iteration [161]: 0.002453672602820295
Loss at iteration [162]: 0.0024531809353668652
Loss at iteration [163]: 0.00245302131783052
Loss at iteration [164]: 0.002452972611650897
Loss at iteration [165]: 0.002452446181046011
Loss at iteration [166]: 0.0024523896311712348
Loss at iteration [167]: 0.002452276735233664
Loss at iteration [168]: 0.0024520712490397984
Loss at iteration [169]: 0.00245202686382678
Loss at iteration [170]: 0.002451857988718897
Loss at iteration [171]: 0.002451857988718897
Loss at iteration [172]: 0.0024517674377035037
Loss at iteration [173]: 0.0024517444638388767
Loss at iteration [174]: 0.002451667425168184
Loss at iteration [175]: 0.002451624435895601
Loss at iteration [176]: 0.0024515852078172327
Loss at iteration [177]: 0.0024513782693803164
Loss at iteration [178]: 0.0024509990461434096
Loss at iteration [179]: 0.0024501190338656435
Loss at iteration [180]: 0.002449820606496341
Loss at iteration [181]: 0.0024496803892396484
Loss at iteration [182]: 0.002449606661029482
Loss at iteration [183]: 0.002449606661029482
Loss at iteration [184]: 0.002449533790766072
Loss at iteration [185]: 0.002449432716740515
Loss at iteration [186]: 0.0024481519880751763
Loss at iteration [187]: 0.00244806080052097
Loss at iteration [188]: 0.0024477510360395097
Loss at iteration [189]: 0.002447614802865284
Loss at iteration [190]: 0.0024474166159566164
Loss at iteration [191]: 0.0024469153678686587
Loss at iteration [192]: 0.002446750021332946
Loss at iteration [193]: 0.002446750021332946
Loss at iteration [194]: 0.0024466809869058822
Loss at iteration [195]: 0.0024464093559170806
Loss at iteration [196]: 0.0024462038956913528
Loss at iteration [197]: 0.002446145447192599
Loss at iteration [198]: 0.002446088540589459
Loss at iteration [199]: 0.002445999056851252
Loss at iteration [200]: 0.0024458553295692714
Loss at iteration [201]: 0.00244577170928275
Loss at iteration [202]: 0.0024456052800410177
Loss at iteration [203]: 0.0024456052800410177
Loss at iteration [204]: 0.0024455428028924498
Loss at iteration [205]: 0.002445484175927794
Loss at iteration [206]: 0.002444987591085931
Loss at iteration [207]: 0.002444949642795931
Loss at iteration [208]: 0.0024448646477378145
Loss at iteration [209]: 0.0024447549656797336
Loss at iteration [210]: 0.0024447418904253635
Loss at iteration [211]: 0.0024446675152985714
Loss at iteration [212]: 0.0024446675152985714
Loss at iteration [213]: 0.002444649409957583
Loss at iteration [214]: 0.0024445823155098503
Loss at iteration [215]: 0.0024445027043197435
Loss at iteration [216]: 0.0024444716339204462
Loss at iteration [217]: 0.002444427562743951
Loss at iteration [218]: 0.0024443489426994457
Loss at iteration [219]: 0.0024436924751263644
Loss at iteration [220]: 0.0024435817831373256
Loss at iteration [221]: 0.0024420918034111262
Loss at iteration [222]: 0.0024420918034111262
Loss at iteration [223]: 0.0024411118753325635
Loss at iteration [224]: 0.0024410427674206774
Loss at iteration [225]: 0.0024403003847809994
Loss at iteration [226]: 0.002440219772660059
Loss at iteration [227]: 0.0024399993381062995
Loss at iteration [228]: 0.002439682241378322
Loss at iteration [229]: 0.0024396802205028027
Loss at iteration [230]: 0.0024396308758741097
Loss at iteration [231]: 0.002439439456316601
Loss at iteration [232]: 0.002439439456316601
Loss at iteration [233]: 0.0024393433616397373
Loss at iteration [234]: 0.0024392335836919735
Loss at iteration [235]: 0.0024390087523782837
Loss at iteration [236]: 0.0024389732032171645
Loss at iteration [237]: 0.0024388692913492205
Loss at iteration [238]: 0.0024388168169629295
Loss at iteration [239]: 0.002438358063861979
Loss at iteration [240]: 0.0024381420143258937
Loss at iteration [241]: 0.0024381420143258937
Loss at iteration [242]: 0.0024380934475642384
Loss at iteration [243]: 0.0024380574123654703
Loss at iteration [244]: 0.0024379183734567367
Loss at iteration [245]: 0.00243786846207207
Loss at iteration [246]: 0.0024378066786804376
Loss at iteration [247]: 0.0024377501376156036
Loss at iteration [248]: 0.0024376936695760985
Loss at iteration [249]: 0.002437474808152777
Loss at iteration [250]: 0.0024372860591139367
Loss at iteration [251]: 0.00243705549378905
Loss at iteration [252]: 0.00243705549378905
Loss at iteration [253]: 0.002436996064291465
Loss at iteration [254]: 0.0024369209938178726
Loss at iteration [255]: 0.0024365827752096934
Loss at iteration [256]: 0.002436435976560372
Loss at iteration [257]: 0.0024363330592387907
Loss at iteration [258]: 0.0024362995267219354
Loss at iteration [259]: 0.002436219712703349
Loss at iteration [260]: 0.0024359503103316797
Loss at iteration [261]: 0.002435917676694947
Loss at iteration [262]: 0.0024358468077265664
Loss at iteration [263]: 0.0024358468077265664
Loss at iteration [264]: 0.002435807680725296
Loss at iteration [265]: 0.0024357728884488792
Loss at iteration [266]: 0.0024356522987060785
Loss at iteration [267]: 0.002435640493361911
Loss at iteration [268]: 0.0024355417758362397
Loss at iteration [269]: 0.0024354414595914816
Loss at iteration [270]: 0.0024354143139504944
Loss at iteration [271]: 0.002435181463077756
Loss at iteration [272]: 0.002434944891364885
Loss at iteration [273]: 0.002434944891364885
Loss at iteration [274]: 0.002434912002801071
Loss at iteration [275]: 0.0024348246452206785
Loss at iteration [276]: 0.002434550429310641
Loss at iteration [277]: 0.0024345156698471785
Loss at iteration [278]: 0.0024344261356547647
Loss at iteration [279]: 0.002434389289292445
Loss at iteration [280]: 0.002434363724487251
Loss at iteration [281]: 0.002434224538486039
Loss at iteration [282]: 0.002434148855300028
Loss at iteration [283]: 0.002434148855300028
Loss at iteration [284]: 0.0024341344879209004
Loss at iteration [285]: 0.0024340198434347465
Loss at iteration [286]: 0.0024339989954887996
Loss at iteration [287]: 0.0024338531884837166
Loss at iteration [288]: 0.0024337624820066433
Loss at iteration [289]: 0.0024336165656063413
Loss at iteration [290]: 0.0024335474749260636
Loss at iteration [291]: 0.0024329176819779218
Loss at iteration [292]: 0.0024329176819779218
Loss at iteration [293]: 0.002432685292280928
Loss at iteration [294]: 0.0024326208831735763
Loss at iteration [295]: 0.0024319314172720078
Loss at iteration [296]: 0.002431848174024185
Loss at iteration [297]: 0.0024317490227685206
Loss at iteration [298]: 0.002431497966911523
Loss at iteration [299]: 0.0024314429449319164
Loss at iteration [300]: 0.002431361475464905
Loss at iteration [301]: 0.002431126076948817
Loss at iteration [302]: 0.0024310940963908198
Loss at iteration [303]: 0.0024310940963908198
Loss at iteration [304]: 0.0024310729409967088
Loss at iteration [305]: 0.002430939663226552
Loss at iteration [306]: 0.0024308644232399553
Loss at iteration [307]: 0.002430823791125186
Loss at iteration [308]: 0.0024306888882911935
Loss at iteration [309]: 0.002430648944773159
Loss at iteration [310]: 0.002430628076067644
Loss at iteration [311]: 0.002430480808880304
Loss at iteration [312]: 0.002430443110375571
Loss at iteration [313]: 0.002430443110375571
Loss at iteration [314]: 0.002430425585289851
Loss at iteration [315]: 0.002430348086273245
Loss at iteration [316]: 0.0024302922798925793
Loss at iteration [317]: 0.0024302050894343175
Loss at iteration [318]: 0.002429963463322432
Loss at iteration [319]: 0.0024296579234389807
Loss at iteration [320]: 0.0024295428473991337
Loss at iteration [321]: 0.00242946154713465
Loss at iteration [322]: 0.0024288378283538773
Loss at iteration [323]: 0.0024288378283538773
Loss at iteration [324]: 0.0024287522910010156
Loss at iteration [325]: 0.002428705646260325
Loss at iteration [326]: 0.0024283859439688414
Loss at iteration [327]: 0.0024283494148993733
Loss at iteration [328]: 0.0024280382185748193
Loss at iteration [329]: 0.0024280045743272733
Loss at iteration [330]: 0.0024279279080806513
Loss at iteration [331]: 0.0024277691961457955
Loss at iteration [332]: 0.0024277605112942875
Loss at iteration [333]: 0.0024277343431302373
Loss at iteration [334]: 0.0024277343431302373
Loss at iteration [335]: 0.0024277162500471444
Loss at iteration [336]: 0.0024277030388121863
Loss at iteration [337]: 0.0024274780729973207
Loss at iteration [338]: 0.002427340764660994
Loss at iteration [339]: 0.002427253047504902
Loss at iteration [340]: 0.002427172761431711
Loss at iteration [341]: 0.002426937047364844
Loss at iteration [342]: 0.0024264610383741758
Loss at iteration [343]: 0.00242639683692239
Loss at iteration [344]: 0.00242639683692239
Loss at iteration [345]: 0.002426348018930823
Loss at iteration [346]: 0.0024261669762967327
Loss at iteration [347]: 0.002426040208411009
Loss at iteration [348]: 0.002426002820228823
Loss at iteration [349]: 0.0024258611433229
Loss at iteration [350]: 0.0024258061077604013
Loss at iteration [351]: 0.002425733953509879
Loss at iteration [352]: 0.002425548439504058
Loss at iteration [353]: 0.002425351893756097
Loss at iteration [354]: 0.002425351893756097
Loss at iteration [355]: 0.002425290774013902
Loss at iteration [356]: 0.0024252633067836826
Loss at iteration [357]: 0.0024251654924696085
Loss at iteration [358]: 0.0024251138570728103
Loss at iteration [359]: 0.002425040914566117
Loss at iteration [360]: 0.0024249836972841674
Loss at iteration [361]: 0.0024249100888141337
Loss at iteration [362]: 0.002424791300610169
Loss at iteration [363]: 0.0024247113464028486
Loss at iteration [364]: 0.0024247113464028486
Loss at iteration [365]: 0.002424650980883353
Loss at iteration [366]: 0.002424614840629908
Loss at iteration [367]: 0.0024244914322462145
Loss at iteration [368]: 0.002424458174141821
Loss at iteration [369]: 0.0024243865212365593
Loss at iteration [370]: 0.0024243316032717633
Loss at iteration [371]: 0.0024241971415093822
Loss at iteration [372]: 0.002424127622797514
Loss at iteration [373]: 0.002424004888177277
Loss at iteration [374]: 0.002424004888177277
Loss at iteration [375]: 0.0024239530699764656
Loss at iteration [376]: 0.002423927956438688
Loss at iteration [377]: 0.002423792542298096
Loss at iteration [378]: 0.002423750763949792
Loss at iteration [379]: 0.002423696895485779
Loss at iteration [380]: 0.0024235084173878534
Loss at iteration [381]: 0.0024234101290574357
Loss at iteration [382]: 0.002423345522770052
Loss at iteration [383]: 0.002423345522770052
Loss at iteration [384]: 0.002423330859978669
Loss at iteration [385]: 0.002423300082424798
Loss at iteration [386]: 0.002423238599099116
Loss at iteration [387]: 0.0024232069198725477
Loss at iteration [388]: 0.0024231717004879628
Loss at iteration [389]: 0.002423150128770558
Loss at iteration [390]: 0.002423103249267015
Loss at iteration [391]: 0.002422267096989952
Loss at iteration [392]: 0.002422267096989952
Loss at iteration [393]: 0.0024218761796100403
Loss at iteration [394]: 0.0024218329685443195
Loss at iteration [395]: 0.0024213742088567973
Loss at iteration [396]: 0.0024212974080778963
Loss at iteration [397]: 0.002421166718511811
Loss at iteration [398]: 0.0024209913190854594
Loss at iteration [399]: 0.0024209423374656107
Loss at iteration [400]: 0.0024208227028695135
Loss at iteration [401]: 0.0024208227028695135
Loss at iteration [402]: 0.0024207691972354334
Loss at iteration [403]: 0.0024207532331215298
Loss at iteration [404]: 0.0024205819634064555
Loss at iteration [405]: 0.002420558616247607
Loss at iteration [406]: 0.0024204765213933165
Loss at iteration [407]: 0.0024203826265632375
Loss at iteration [408]: 0.0024203499127283734
Loss at iteration [409]: 0.002420152130993892
Loss at iteration [410]: 0.002420152130993892
Loss at iteration [411]: 0.002420109418837819
Loss at iteration [412]: 0.002420072814365734
Loss at iteration [413]: 0.0024200031356107787
Loss at iteration [414]: 0.002419964116931343
Loss at iteration [415]: 0.002419939326571555
Loss at iteration [416]: 0.0024199027958060663
Loss at iteration [417]: 0.0024198942563596402
Loss at iteration [418]: 0.0024198590608419477
Loss at iteration [419]: 0.0024198336866872033
Loss at iteration [420]: 0.0024198336866872033
Loss at iteration [421]: 0.002419809535097645
Loss at iteration [422]: 0.002419769878551056
Loss at iteration [423]: 0.00241974455425668
Loss at iteration [424]: 0.0024197327464954054
Loss at iteration [425]: 0.0024196990709038657
Loss at iteration [426]: 0.0024196424616808776
Loss at iteration [427]: 0.0024196146381982152
Loss at iteration [428]: 0.002419509652615197
Loss at iteration [429]: 0.00241934845236156
Loss at iteration [430]: 0.00241934845236156
Loss at iteration [431]: 0.002419309437745312
Loss at iteration [432]: 0.0024192245374377217
Loss at iteration [433]: 0.002419014518222827
Loss at iteration [434]: 0.002418999483650008
Loss at iteration [435]: 0.0024189791159168446
Loss at iteration [436]: 0.002418914536162983
Loss at iteration [437]: 0.002418901185142677
Loss at iteration [438]: 0.0024187861905192332
Loss at iteration [439]: 0.0024186990659486665
Loss at iteration [440]: 0.0024186990659486665
Loss at iteration [441]: 0.002418674620558268
Loss at iteration [442]: 0.002418657703746394
Loss at iteration [443]: 0.002418574056800796
Loss at iteration [444]: 0.0024185646451299185
Loss at iteration [445]: 0.0024185534370274155
Loss at iteration [446]: 0.0024185343463664793
Loss at iteration [447]: 0.0024185077455434814
Loss at iteration [448]: 0.0024184254288526414
Loss at iteration [449]: 0.0024184254288526414
Loss at iteration [450]: 0.0024183984047529013
Loss at iteration [451]: 0.0024183728447727693
Loss at iteration [452]: 0.002418317952376882
Loss at iteration [453]: 0.002418277484090998
Loss at iteration [454]: 0.002418232561464418
Loss at iteration [455]: 0.0024181847870720255
Loss at iteration [456]: 0.0024180703716667946
Loss at iteration [457]: 0.0024174480128393377
Loss at iteration [458]: 0.0024166744367337635
Loss at iteration [459]: 0.0024166744367337635
Loss at iteration [460]: 0.0024166331161227734
Loss at iteration [461]: 0.002416026679546717
Loss at iteration [462]: 0.002415972373134875
Loss at iteration [463]: 0.0024155077017268154
Loss at iteration [464]: 0.00241539702891775
Loss at iteration [465]: 0.002415109204885324
Loss at iteration [466]: 0.002415075464459956
Loss at iteration [467]: 0.0024147508628175325
Loss at iteration [468]: 0.0024147508628175325
Loss at iteration [469]: 0.0024146879642242378
Loss at iteration [470]: 0.002414377341254941
Loss at iteration [471]: 0.002414340191095192
Loss at iteration [472]: 0.0024142243562731837
Loss at iteration [473]: 0.002414076375528486
Loss at iteration [474]: 0.0024140534437523888
Loss at iteration [475]: 0.002413798976828544
Loss at iteration [476]: 0.0024137105177165453
Loss at iteration [477]: 0.0024137105177165453
Loss at iteration [478]: 0.0024136965604197418
Loss at iteration [479]: 0.0024136307534889075
Loss at iteration [480]: 0.0024135473637767043
Loss at iteration [481]: 0.0024135360175945113
Loss at iteration [482]: 0.0024134164338408116
Loss at iteration [483]: 0.0024132922984103724
Loss at iteration [484]: 0.0024132440895575846
Loss at iteration [485]: 0.0024128622021692895
Loss at iteration [486]: 0.0024128622021692895
Loss at iteration [487]: 0.002412735982700124
Loss at iteration [488]: 0.0024126818364807842
Loss at iteration [489]: 0.002412430842500422
Loss at iteration [490]: 0.0024123963782695336
Loss at iteration [491]: 0.002412365844465976
Loss at iteration [492]: 0.0024122559437605483
Loss at iteration [493]: 0.002412137615182478
Loss at iteration [494]: 0.002412042188247849
Loss at iteration [495]: 0.0024119478432254766
Loss at iteration [496]: 0.0024118929368082665
Loss at iteration [497]: 0.0024118929368082665
Loss at iteration [498]: 0.002411847915595338
Loss at iteration [499]: 0.002411725533840595
Loss at iteration [500]: 0.0024116971626291068
Loss at iteration [501]: 0.0024115452505874277
Loss at iteration [502]: 0.0024115023182524173
Loss at iteration [503]: 0.0024114707980839537
Loss at iteration [504]: 0.0024113060701060245
Loss at iteration [505]: 0.0024112651392263875
Loss at iteration [506]: 0.0024110409862451846
Loss at iteration [507]: 0.0024110409862451846
Loss at iteration [508]: 0.0024109532974064696
Loss at iteration [509]: 0.002410906323145951
Loss at iteration [510]: 0.002410832587258172
Loss at iteration [511]: 0.0024108121828887146
Loss at iteration [512]: 0.0024107283322603347
Loss at iteration [513]: 0.002410699425023768
Loss at iteration [514]: 0.002410669891524524
Loss at iteration [515]: 0.002410535173338622
Loss at iteration [516]: 0.0024105064616086296
Loss at iteration [517]: 0.0024105064616086296
Loss at iteration [518]: 0.002410477858573072
Loss at iteration [519]: 0.0024104465698373315
Loss at iteration [520]: 0.002410408846809986
Loss at iteration [521]: 0.00241033669322833
Loss at iteration [522]: 0.002410313097073213
Loss at iteration [523]: 0.0024102796048304682
Loss at iteration [524]: 0.002410227937642832
Loss at iteration [525]: 0.0024102124493103325
Loss at iteration [526]: 0.0024101754043035477
Loss at iteration [527]: 0.0024101754043035477
Loss at iteration [528]: 0.0024101514065727006
Loss at iteration [529]: 0.0024101354215607515
Loss at iteration [530]: 0.0024100769699471193
Loss at iteration [531]: 0.0024100445653335136
Loss at iteration [532]: 0.002410032032032744
Loss at iteration [533]: 0.002410010585971268
Loss at iteration [534]: 0.0024099915514623972
Loss at iteration [535]: 0.002409955457378329
Loss at iteration [536]: 0.0024099173611525676
Loss at iteration [537]: 0.002409861033009066
Loss at iteration [538]: 0.002409861033009066
Loss at iteration [539]: 0.002409832936249144
Loss at iteration [540]: 0.002409807709614853
Loss at iteration [541]: 0.0024097499218229327
Loss at iteration [542]: 0.0024097296191326147
Loss at iteration [543]: 0.0024097024136383103
Loss at iteration [544]: 0.0024096450660015685
Loss at iteration [545]: 0.002409537093794754
Loss at iteration [546]: 0.0024092055786020653
Loss at iteration [547]: 0.0024081755966111873
Loss at iteration [548]: 0.0024081755966111873
Loss at iteration [549]: 0.002407695818898087
Loss at iteration [550]: 0.0024076546396958757
Loss at iteration [551]: 0.002406914083640856
Loss at iteration [552]: 0.0024063965521881293
Loss at iteration [553]: 0.0024059861706724334
Loss at iteration [554]: 0.002405875236031747
Loss at iteration [555]: 0.002405710451894006
Loss at iteration [556]: 0.002405407748588349
Loss at iteration [557]: 0.0024052899458535454
Loss at iteration [558]: 0.0024052899458535454
Loss at iteration [559]: 0.002405256724201386
Loss at iteration [560]: 0.002405187531529509
Loss at iteration [561]: 0.0024049394481739277
Loss at iteration [562]: 0.002404873228317
Loss at iteration [563]: 0.0024048328922188352
Loss at iteration [564]: 0.00240471230212445
Loss at iteration [565]: 0.002404661130781085
Loss at iteration [566]: 0.002404661130781085
Loss at iteration [567]: 0.002404631993176428
Loss at iteration [568]: 0.0024045292789177937
Loss at iteration [569]: 0.002404194848681268
Loss at iteration [570]: 0.002404171828895123
Loss at iteration [571]: 0.0024040841762761186
Loss at iteration [572]: 0.0024039702650063234
Loss at iteration [573]: 0.002403951928517948
Loss at iteration [574]: 0.0024038724154680593
Loss at iteration [575]: 0.0024037096373141515
Loss at iteration [576]: 0.0024037096373141515
Loss at iteration [577]: 0.002403675205839226
Loss at iteration [578]: 0.0024036150994895737
Loss at iteration [579]: 0.0024035159015144223
Loss at iteration [580]: 0.0024035073415520298
Loss at iteration [581]: 0.0024034831728067156
Loss at iteration [582]: 0.0024034623850227475
Loss at iteration [583]: 0.0024034503677354204
Loss at iteration [584]: 0.0024033976318875525
Loss at iteration [585]: 0.0024033976318875525
Loss at iteration [586]: 0.0024033798165527713
Loss at iteration [587]: 0.0024032501902150837
Loss at iteration [588]: 0.0024032329396680746
Loss at iteration [589]: 0.002403203894096955
Loss at iteration [590]: 0.002403146181587562
Loss at iteration [591]: 0.0024030385916176946
Loss at iteration [592]: 0.002402969794081553
Loss at iteration [593]: 0.0024028512589493382
Loss at iteration [594]: 0.002402795591157424
Loss at iteration [595]: 0.002402795591157424
Loss at iteration [596]: 0.0024027434625896516
Loss at iteration [597]: 0.002402616834787351
Loss at iteration [598]: 0.0024023896606899797
Loss at iteration [599]: 0.0024022899130879643
Loss at iteration [600]: 0.0024022371879732817
Loss at iteration [601]: 0.0024022090238054175
Loss at iteration [602]: 0.0024021871136506137
Loss at iteration [603]: 0.00240212450499213
Loss at iteration [604]: 0.00240209834626064
Loss at iteration [605]: 0.00240209834626064
Loss at iteration [606]: 0.002402083148330668
Loss at iteration [607]: 0.0024020686181165426
Loss at iteration [608]: 0.0024019010354807643
Loss at iteration [609]: 0.002401858903861711
Loss at iteration [610]: 0.002401594683582734
Loss at iteration [611]: 0.002401504329346788
Loss at iteration [612]: 0.002401388712102288
Loss at iteration [613]: 0.002401057640859803
Loss at iteration [614]: 0.002400915820206986
Loss at iteration [615]: 0.00240085610353588
Loss at iteration [616]: 0.00240085610353588
Loss at iteration [617]: 0.0024008067320483596
Loss at iteration [618]: 0.0024007463672071775
Loss at iteration [619]: 0.0024004452857301238
Loss at iteration [620]: 0.0024004048711033722
Loss at iteration [621]: 0.0023999517038486365
Loss at iteration [622]: 0.0023998344985817098
Loss at iteration [623]: 0.002399737010665442
Loss at iteration [624]: 0.0023996232796349858
Loss at iteration [625]: 0.0023996033080913018
Loss at iteration [626]: 0.0023996033080913018
Loss at iteration [627]: 0.0023995872610821168
Loss at iteration [628]: 0.002399490370174493
Loss at iteration [629]: 0.0023992672438636784
Loss at iteration [630]: 0.0023992468918279
Loss at iteration [631]: 0.0023991889423663907
Loss at iteration [632]: 0.0023991025124080216
Loss at iteration [633]: 0.0023989764930393296
Loss at iteration [634]: 0.002398927221311978
Loss at iteration [635]: 0.002398901452271193
Loss at iteration [636]: 0.002398901452271193
Loss at iteration [637]: 0.0023988727050918437
Loss at iteration [638]: 0.002398847348208722
Loss at iteration [639]: 0.002398686929970295
Loss at iteration [640]: 0.0023986230783942125
Loss at iteration [641]: 0.0023985712125385813
Loss at iteration [642]: 0.002398548025444033
Loss at iteration [643]: 0.0023985358475190855
Loss at iteration [644]: 0.002398490152454747
Loss at iteration [645]: 0.0023984715174584664
Loss at iteration [646]: 0.0023984715174584664
Loss at iteration [647]: 0.002398460430325946
Loss at iteration [648]: 0.0023984282875001033
Loss at iteration [649]: 0.0023983930391826827
Loss at iteration [650]: 0.0023983740393803735
Loss at iteration [651]: 0.002398202847634871
Loss at iteration [652]: 0.002398163775793347
Loss at iteration [653]: 0.002398084121970989
Loss at iteration [654]: 0.002397973810465028
Loss at iteration [655]: 0.0023974587376302753
Loss at iteration [656]: 0.0023974587376302753
Loss at iteration [657]: 0.00239735173773915
Loss at iteration [658]: 0.0023972840786597666
Loss at iteration [659]: 0.0023971614802245525
Loss at iteration [660]: 0.002397122459181124
Loss at iteration [661]: 0.00239709040692679
Loss at iteration [662]: 0.0023970429553229955
Loss at iteration [663]: 0.002396991069062957
Loss at iteration [664]: 0.0023968299583130848
Loss at iteration [665]: 0.0023968299583130848
Loss at iteration [666]: 0.002396768099246383
Loss at iteration [667]: 0.0023967468160987363
Loss at iteration [668]: 0.0023966562439754506
Loss at iteration [669]: 0.0023966378527363864
Loss at iteration [670]: 0.0023964444646793564
Loss at iteration [671]: 0.002396324400895249
Loss at iteration [672]: 0.00239627481556713
Loss at iteration [673]: 0.0023962303366025063
Loss at iteration [674]: 0.0023958433883249255
Loss at iteration [675]: 0.0023958433883249255
Loss at iteration [676]: 0.0023958012428575565
Loss at iteration [677]: 0.0023955522593753463
Loss at iteration [678]: 0.0023953902835154393
Loss at iteration [679]: 0.0023952490147767925
Loss at iteration [680]: 0.0023951231445652277
Loss at iteration [681]: 0.0023950542652854514
Loss at iteration [682]: 0.0023950295103825387
Loss at iteration [683]: 0.0023949072704879584
Loss at iteration [684]: 0.0023949072704879584
Loss at iteration [685]: 0.00239487998661016
Loss at iteration [686]: 0.002394855561891073
Loss at iteration [687]: 0.0023947075071587697
Loss at iteration [688]: 0.002394535359255577
Loss at iteration [689]: 0.0023944268435670507
Loss at iteration [690]: 0.002394377210922559
Loss at iteration [691]: 0.002394324371716442
Loss at iteration [692]: 0.0023942911657164755
Loss at iteration [693]: 0.0023942608206571764
Loss at iteration [694]: 0.0023942608206571764
Loss at iteration [695]: 0.002394250387055861
Loss at iteration [696]: 0.0023942215395178525
Loss at iteration [697]: 0.002394154278968721
Loss at iteration [698]: 0.0023941294312967588
Loss at iteration [699]: 0.0023940944520996392
Loss at iteration [700]: 0.00239370147876877
Loss at iteration [701]: 0.002393526483536664
Loss at iteration [702]: 0.0023935093461861672
Loss at iteration [703]: 0.0023934280887161117
Loss at iteration [704]: 0.0023934280887161117
Loss at iteration [705]: 0.002393367600853319
Loss at iteration [706]: 0.0023933343638104974
Loss at iteration [707]: 0.0023931817386999865
Loss at iteration [708]: 0.0023930991542766324
Loss at iteration [709]: 0.0023930067294952563
Loss at iteration [710]: 0.0023929891337146287
Loss at iteration [711]: 0.002392960466433669
Loss at iteration [712]: 0.0023929197207626677
Loss at iteration [713]: 0.0023929197207626677
Loss at iteration [714]: 0.0023928974454992917
Loss at iteration [715]: 0.002392653982679735
Loss at iteration [716]: 0.0023926397471885695
Loss at iteration [717]: 0.002392498031503561
Loss at iteration [718]: 0.0023924657558643753
Loss at iteration [719]: 0.002392449757485887
Loss at iteration [720]: 0.002392424138331699
Loss at iteration [721]: 0.0023923980997709814
Loss at iteration [722]: 0.0023923511911456688
Loss at iteration [723]: 0.0023923511911456688
Loss at iteration [724]: 0.002392341886635944
Loss at iteration [725]: 0.002392323982602433
Loss at iteration [726]: 0.0023922401409825727
Loss at iteration [727]: 0.002392220803795526
Loss at iteration [728]: 0.00239216727436007
Loss at iteration [729]: 0.0023920164585387736
Loss at iteration [730]: 0.0023917792179204405
Loss at iteration [731]: 0.002391136416291185
Loss at iteration [732]: 0.002391136416291185
Loss at iteration [733]: 0.00239104683647005
Loss at iteration [734]: 0.0023903261099581887
Loss at iteration [735]: 0.002390255153450271
Loss at iteration [736]: 0.002390095769749841
Loss at iteration [737]: 0.002389750817287886
Loss at iteration [738]: 0.0023897180321517156
Loss at iteration [739]: 0.0023894561862902414
Loss at iteration [740]: 0.00238937554694501
Loss at iteration [741]: 0.00238937554694501
Loss at iteration [742]: 0.0023893505525872543
Loss at iteration [743]: 0.002389238427305602
Loss at iteration [744]: 0.0023891711401874856
Loss at iteration [745]: 0.002389145595901964
Loss at iteration [746]: 0.002389087976958229
Loss at iteration [747]: 0.0023890548733332016
Loss at iteration [748]: 0.002388915257560703
Loss at iteration [749]: 0.0023888207555871135
Loss at iteration [750]: 0.002388781778023474
Loss at iteration [751]: 0.002388729160545015
Loss at iteration [752]: 0.002388729160545015
Loss at iteration [753]: 0.002388683399451993
Loss at iteration [754]: 0.0023886670694997377
Loss at iteration [755]: 0.002388575922644027
Loss at iteration [756]: 0.0023885505309444656
Loss at iteration [757]: 0.0023885211873998827
Loss at iteration [758]: 0.0023884291320810415
Loss at iteration [759]: 0.002388389706366004
Loss at iteration [760]: 0.002388358392940021
Loss at iteration [761]: 0.0023881658948020977
Loss at iteration [762]: 0.0023881658948020977
Loss at iteration [763]: 0.002388112918751318
Loss at iteration [764]: 0.0023880058382937854
Loss at iteration [765]: 0.0023878012984020398
Loss at iteration [766]: 0.0023877398431556033
Loss at iteration [767]: 0.002387682199223832
Loss at iteration [768]: 0.0023875787314238937
Loss at iteration [769]: 0.002387547475479587
Loss at iteration [770]: 0.0023874850580450115
Loss at iteration [771]: 0.0023874850580450115
Loss at iteration [772]: 0.0023874438974518563
Loss at iteration [773]: 0.0023874030368578017
Loss at iteration [774]: 0.00238729608707274
Loss at iteration [775]: 0.0023872787242002133
Loss at iteration [776]: 0.0023872458056550306
Loss at iteration [777]: 0.0023871955806359104
Loss at iteration [778]: 0.0023871804481327215
Loss at iteration [779]: 0.0023871501191516336
Loss at iteration [780]: 0.002387088550668528
Loss at iteration [781]: 0.0023870430251461804
Loss at iteration [782]: 0.0023870430251461804
Loss at iteration [783]: 0.002387019608046583
Loss at iteration [784]: 0.0023869322018278583
Loss at iteration [785]: 0.0023868808881578434
Loss at iteration [786]: 0.002386867819225895
Loss at iteration [787]: 0.002386811409833411
Loss at iteration [788]: 0.002386772283058851
Loss at iteration [789]: 0.0023867218024838754
Loss at iteration [790]: 0.0023866724055496467
Loss at iteration [791]: 0.0023866506949692177
Loss at iteration [792]: 0.0023866506949692177
Loss at iteration [793]: 0.002386641675418518
Loss at iteration [794]: 0.002386618670592815
Loss at iteration [795]: 0.0023865995645723052
Loss at iteration [796]: 0.002386581571520692
Loss at iteration [797]: 0.0023863763314600638
Loss at iteration [798]: 0.0023859598685328724
Loss at iteration [799]: 0.0023858540895613178
Loss at iteration [800]: 0.002385401536575954
Loss at iteration [801]: 0.002385294583772182
Loss at iteration [802]: 0.002385294583772182
Loss at iteration [803]: 0.002385274029323804
Loss at iteration [804]: 0.0023850966265148914
Loss at iteration [805]: 0.002385013143026368
Loss at iteration [806]: 0.002384997132705139
Loss at iteration [807]: 0.002384847238645121
Loss at iteration [808]: 0.0023846860010363255
Loss at iteration [809]: 0.0023846523459807928
Loss at iteration [810]: 0.002384541203219472
Loss at iteration [811]: 0.002384541203219472
Loss at iteration [812]: 0.0023844878332010314
Loss at iteration [813]: 0.0023843635676166343
Loss at iteration [814]: 0.00238432587433572
Loss at iteration [815]: 0.002384206778741346
Loss at iteration [816]: 0.002384095786207462
Loss at iteration [817]: 0.0023840758245520324
Loss at iteration [818]: 0.002384058712394468
Loss at iteration [819]: 0.002384033289269512
Loss at iteration [820]: 0.002384033289269512
Loss at iteration [821]: 0.0023840144699823836
Loss at iteration [822]: 0.002383998118970467
Loss at iteration [823]: 0.002383973008250664
Loss at iteration [824]: 0.0023839001535754293
Loss at iteration [825]: 0.002383869502521702
Loss at iteration [826]: 0.0023838234898300534
Loss at iteration [827]: 0.002383788834019827
Loss at iteration [828]: 0.0023837375060895773
Loss at iteration [829]: 0.002383675771358349
Loss at iteration [830]: 0.0023836096331402348
Loss at iteration [831]: 0.0023836096331402348
Loss at iteration [832]: 0.002383581050468153
Loss at iteration [833]: 0.0023835638528488805
Loss at iteration [834]: 0.002383471284798159
Loss at iteration [835]: 0.0023834479907262045
Loss at iteration [836]: 0.0023834172669809986
Loss at iteration [837]: 0.002383380733383519
Loss at iteration [838]: 0.002383363815804099
Loss at iteration [839]: 0.0023833068293776048
Loss at iteration [840]: 0.002383277086072685
Loss at iteration [841]: 0.0023832495002512363
Loss at iteration [842]: 0.0023832495002512363
Loss at iteration [843]: 0.0023832320705541627
Loss at iteration [844]: 0.0023832184377926575
Loss at iteration [845]: 0.002383199888230001
Loss at iteration [846]: 0.0023831822877328095
Loss at iteration [847]: 0.0023831510768707047
Loss at iteration [848]: 0.002383136351379612
Loss at iteration [849]: 0.0023829663075181696
Loss at iteration [850]: 0.0023824326954275067
Loss at iteration [851]: 0.002381565480982014
Loss at iteration [852]: 0.002381565480982014
Loss at iteration [853]: 0.0023814850884486248
Loss at iteration [854]: 0.002380941221858621
Loss at iteration [855]: 0.0023804544668708373
Loss at iteration [856]: 0.0023801670211506267
Loss at iteration [857]: 0.0023800450307544987
Loss at iteration [858]: 0.0023799401702334975
Loss at iteration [859]: 0.0023798799614545376
Loss at iteration [860]: 0.0023792814314492605
Loss at iteration [861]: 0.0023790693072273553
Loss at iteration [862]: 0.0023789856207164666
Loss at iteration [863]: 0.0023789856207164666
Loss at iteration [864]: 0.0023789196288318756
Loss at iteration [865]: 0.0023788275035832303
Loss at iteration [866]: 0.0023784809411114168
Loss at iteration [867]: 0.002378393805424173
Loss at iteration [868]: 0.0023783220839115908
Loss at iteration [869]: 0.002378241440496464
Loss at iteration [870]: 0.002378092705525275
Loss at iteration [871]: 0.002378006964664358
Loss at iteration [872]: 0.0023779532733867
Loss at iteration [873]: 0.0023779532733867
Loss at iteration [874]: 0.0023779246015185117
Loss at iteration [875]: 0.0023778755389376187
Loss at iteration [876]: 0.0023777502727955835
Loss at iteration [877]: 0.002377736107299244
Loss at iteration [878]: 0.0023777259891222436
Loss at iteration [879]: 0.002377704872980601
Loss at iteration [880]: 0.0023776854585677077
Loss at iteration [881]: 0.0023776460926541276
Loss at iteration [882]: 0.002377593080470405
Loss at iteration [883]: 0.0023774838314388528
Loss at iteration [884]: 0.0023774838314388528
Loss at iteration [885]: 0.0023773899724197412
Loss at iteration [886]: 0.002377361252042209
Loss at iteration [887]: 0.0023771381179304696
Loss at iteration [888]: 0.0023771169269621514
Loss at iteration [889]: 0.002377054739180413
Loss at iteration [890]: 0.0023769169609118702
Loss at iteration [891]: 0.002376875248153447
Loss at iteration [892]: 0.0023768323520975275
Loss at iteration [893]: 0.002376523587453018
Loss at iteration [894]: 0.002376523587453018
Loss at iteration [895]: 0.0023761676967960716
Loss at iteration [896]: 0.0023760963127292226
Loss at iteration [897]: 0.0023757242401789094
Loss at iteration [898]: 0.002375684097726382
Loss at iteration [899]: 0.0023756264259437625
Loss at iteration [900]: 0.002375482992380542
Loss at iteration [901]: 0.0023754569384399973
Loss at iteration [902]: 0.002375420313496699
Loss at iteration [903]: 0.0023753933125261997
Loss at iteration [904]: 0.002375263795134829
Loss at iteration [905]: 0.002375263795134829
Loss at iteration [906]: 0.002375243216757238
Loss at iteration [907]: 0.002375230149708448
Loss at iteration [908]: 0.002375171957456831
Loss at iteration [909]: 0.002375124113736642
Loss at iteration [910]: 0.0023750831665464403
Loss at iteration [911]: 0.0023749708193662074
Loss at iteration [912]: 0.0023748370714546368
Loss at iteration [913]: 0.0023747556097933184
Loss at iteration [914]: 0.0023743886830667935
Loss at iteration [915]: 0.0023743886830667935
Loss at iteration [916]: 0.002374267195245902
Loss at iteration [917]: 0.002374215931270176
Loss at iteration [918]: 0.0023737956167891325
Loss at iteration [919]: 0.002373752490154025
Loss at iteration [920]: 0.0023737125358272125
Loss at iteration [921]: 0.0023734363693267246
Loss at iteration [922]: 0.0023734246283184384
Loss at iteration [923]: 0.002373314019278931
Loss at iteration [924]: 0.002372961942928651
Loss at iteration [925]: 0.002372961942928651
Loss at iteration [926]: 0.0023729255477729946
Loss at iteration [927]: 0.002372868797988379
Loss at iteration [928]: 0.0023727113926477403
Loss at iteration [929]: 0.002372692971663226
Loss at iteration [930]: 0.002372649831213069
Loss at iteration [931]: 0.00237249202572081
Loss at iteration [932]: 0.0023724560416738187
Loss at iteration [933]: 0.0023724119541605393
Loss at iteration [934]: 0.0023724119541605393
Loss at iteration [935]: 0.002372371395960126
Loss at iteration [936]: 0.0023723207157346615
Loss at iteration [937]: 0.0023721849936617495
Loss at iteration [938]: 0.002372173568325006
Loss at iteration [939]: 0.002372106265794255
Loss at iteration [940]: 0.002372042647489808
Loss at iteration [941]: 0.002372019295949549
Loss at iteration [942]: 0.002371968140927994
Loss at iteration [943]: 0.00237194815123529
Loss at iteration [944]: 0.002371892401739387
Loss at iteration [945]: 0.002371892401739387
Loss at iteration [946]: 0.0023718612037461934
Loss at iteration [947]: 0.002371832968935784
Loss at iteration [948]: 0.002371771037625674
Loss at iteration [949]: 0.002371761069682634
Loss at iteration [950]: 0.002371722135109251
Loss at iteration [951]: 0.0023716941982622804
Loss at iteration [952]: 0.0023716416172183414
Loss at iteration [953]: 0.0023714948726814965
Loss at iteration [954]: 0.0023714948726814965
Loss at iteration [955]: 0.0023714597852528773
Loss at iteration [956]: 0.0023713704326897614
Loss at iteration [957]: 0.0023712830185390384
Loss at iteration [958]: 0.002371270985540192
Loss at iteration [959]: 0.0023712189455609763
Loss at iteration [960]: 0.0023711842592991335
Loss at iteration [961]: 0.002371156187068232
Loss at iteration [962]: 0.0023710608766425484
Loss at iteration [963]: 0.0023709888456742273
Loss at iteration [964]: 0.0023709888456742273
Loss at iteration [965]: 0.002370953488141092
Loss at iteration [966]: 0.002370909866481765
Loss at iteration [967]: 0.002370867513760725
Loss at iteration [968]: 0.0023708536762512894
Loss at iteration [969]: 0.0023706799697774065
Loss at iteration [970]: 0.0023706524533942206
Loss at iteration [971]: 0.002370518117868813
Loss at iteration [972]: 0.0023699723069015716
Loss at iteration [973]: 0.0023697884976176496
Loss at iteration [974]: 0.0023697884976176496
Loss at iteration [975]: 0.002369719955573635
Loss at iteration [976]: 0.0023695963370530095
Loss at iteration [977]: 0.002369433285778289
Loss at iteration [978]: 0.002369380456174077
Loss at iteration [979]: 0.002369110541479484
Loss at iteration [980]: 0.0023690660046468636
Loss at iteration [981]: 0.002368980966209602
Loss at iteration [982]: 0.0023688003290231246
Loss at iteration [983]: 0.002368744834740081
Loss at iteration [984]: 0.002368744834740081
Loss at iteration [985]: 0.0023687192580916436
Loss at iteration [986]: 0.0023686373455175376
Loss at iteration [987]: 0.0023684701807538213
Loss at iteration [988]: 0.0023684422147812806
Loss at iteration [989]: 0.0023684086764343467
Loss at iteration [990]: 0.0023683495464087894
Loss at iteration [991]: 0.0023682677237028553
Loss at iteration [992]: 0.0023682348112122843
Loss at iteration [993]: 0.0023682070334610653
Loss at iteration [994]: 0.002368132327113018
Loss at iteration [995]: 0.002368132327113018
Loss at iteration [996]: 0.002368112110710101
Loss at iteration [997]: 0.002368062334157214
Loss at iteration [998]: 0.0023680429306020614
Loss at iteration [999]: 0.002367990672640773
Loss at iteration [1000]: 0.002367978307479257
Loss at iteration [1001]: 0.0023679617232971626
Loss at iteration [1002]: 0.0023678045680609475
Loss at iteration [1003]: 0.0023677766464235304
Loss at iteration [1004]: 0.0023675866501285158
Loss at iteration [1005]: 0.0023675866501285158
Loss at iteration [1006]: 0.002367515807789206
Loss at iteration [1007]: 0.0023674506424911845
Loss at iteration [1008]: 0.0023673303392731317
Loss at iteration [1009]: 0.0023673192229181606
Loss at iteration [1010]: 0.0023673035514319225
Loss at iteration [1011]: 0.00236728905106722
Loss at iteration [1012]: 0.0023672656501213247
Loss at iteration [1013]: 0.002367207569941277
Loss at iteration [1014]: 0.00236718752631491
Loss at iteration [1015]: 0.0023670170940039547
Loss at iteration [1016]: 0.0023670170940039547
Loss at iteration [1017]: 0.002366938887138078
Loss at iteration [1018]: 0.002366906170673684
Loss at iteration [1019]: 0.0023667768051099398
Loss at iteration [1020]: 0.002366753547360898
Loss at iteration [1021]: 0.002366507813937045
Loss at iteration [1022]: 0.002366478811471873
Loss at iteration [1023]: 0.0023664386360232217
Loss at iteration [1024]: 0.0023662792812563886
Loss at iteration [1025]: 0.0023662561874678756
Loss at iteration [1026]: 0.0023662561874678756
Loss at iteration [1027]: 0.0023662222200175323
Loss at iteration [1028]: 0.00236593054491421
Loss at iteration [1029]: 0.002365887121832822
Loss at iteration [1030]: 0.0023658611021012614
Loss at iteration [1031]: 0.002365760860383547
Loss at iteration [1032]: 0.002365650203617129
Loss at iteration [1033]: 0.0023656097669780096
Loss at iteration [1034]: 0.0023655353994961855
Loss at iteration [1035]: 0.0023654821778559126
Loss at iteration [1036]: 0.0023654821778559126
Loss at iteration [1037]: 0.0023654687031281424
Loss at iteration [1038]: 0.0023654192144274013
Loss at iteration [1039]: 0.002365389298441745
Loss at iteration [1040]: 0.0023653710574908548
Loss at iteration [1041]: 0.002365242224904278
Loss at iteration [1042]: 0.002365208388642988
Loss at iteration [1043]: 0.0023651209957952237
Loss at iteration [1044]: 0.0023650790157719722
Loss at iteration [1045]: 0.002365026049044974
Loss at iteration [1046]: 0.0023647990273095274
Loss at iteration [1047]: 0.0023647990273095274
Loss at iteration [1048]: 0.0023647754402423083
Loss at iteration [1049]: 0.0023647393337847015
Loss at iteration [1050]: 0.002364677228393522
Loss at iteration [1051]: 0.0023646404418168885
Loss at iteration [1052]: 0.002364613084876112
Loss at iteration [1053]: 0.002364569722147984
Loss at iteration [1054]: 0.0023642928289384036
Loss at iteration [1055]: 0.0023642048728760383
Loss at iteration [1056]: 0.0023642048728760383
Loss at iteration [1057]: 0.002364169736568053
Loss at iteration [1058]: 0.0023639970999546974
Loss at iteration [1059]: 0.0023638499147013536
Loss at iteration [1060]: 0.0023638315106837247
Loss at iteration [1061]: 0.00236381122995215
Loss at iteration [1062]: 0.002363740999696957
Loss at iteration [1063]: 0.00236368961474889
Loss at iteration [1064]: 0.002363634618834446
Loss at iteration [1065]: 0.002363612272531251
Loss at iteration [1066]: 0.0023635926021532087
Loss at iteration [1067]: 0.0023635926021532087
Loss at iteration [1068]: 0.0023635706202813633
Loss at iteration [1069]: 0.002363552107843983
Loss at iteration [1070]: 0.002363528766695494
Loss at iteration [1071]: 0.0023634992123550067
Loss at iteration [1072]: 0.002363465763241773
Loss at iteration [1073]: 0.0023634450849805136
Loss at iteration [1074]: 0.002363433325889929
Loss at iteration [1075]: 0.002363400890411472
Loss at iteration [1076]: 0.0023633286579548595
Loss at iteration [1077]: 0.0023633286579548595
Loss at iteration [1078]: 0.002363308359970979
Loss at iteration [1079]: 0.002363170237609675
Loss at iteration [1080]: 0.0023631593774743726
Loss at iteration [1081]: 0.0023630915195960657
Loss at iteration [1082]: 0.00236303476865688
Loss at iteration [1083]: 0.0023629880319056802
Loss at iteration [1084]: 0.002362943539897501
Loss at iteration [1085]: 0.0023629150071365645
Loss at iteration [1086]: 0.0023627974480632443
Loss at iteration [1087]: 0.0023627974480632443
Loss at iteration [1088]: 0.0023627230424759283
Loss at iteration [1089]: 0.002362680424966794
Loss at iteration [1090]: 0.002362666389442262
Loss at iteration [1091]: 0.0023626019284528643
Loss at iteration [1092]: 0.0023625813136001774
Loss at iteration [1093]: 0.0023625498766059647
Loss at iteration [1094]: 0.0023625026268499104
Loss at iteration [1095]: 0.002362485458611456
Loss at iteration [1096]: 0.0023623783350827923
Loss at iteration [1097]: 0.002362333957124093
Loss at iteration [1098]: 0.002362333957124093
Loss at iteration [1099]: 0.002362313871926724
Loss at iteration [1100]: 0.002362298635355384
Loss at iteration [1101]: 0.002362282675754382
Loss at iteration [1102]: 0.0023622682960905
Loss at iteration [1103]: 0.0023622240374322006
Loss at iteration [1104]: 0.0023621944037260398
Loss at iteration [1105]: 0.0023621564506389847
Loss at iteration [1106]: 0.0023620748312486437
Loss at iteration [1107]: 0.002361968483559457
Loss at iteration [1108]: 0.0023613848011367748
Loss at iteration [1109]: 0.0023613848011367748
Loss at iteration [1110]: 0.002361303530665505
Loss at iteration [1111]: 0.0023612372189766267
Loss at iteration [1112]: 0.002360642562585329
Loss at iteration [1113]: 0.0023606013756510714
Loss at iteration [1114]: 0.0023605724985336597
Loss at iteration [1115]: 0.002360397914222525
Loss at iteration [1116]: 0.002360195716396482
Loss at iteration [1117]: 0.0023600656370133775
Loss at iteration [1118]: 0.002360027989604864
Loss at iteration [1119]: 0.002360027989604864
Loss at iteration [1120]: 0.0023599845285364497
Loss at iteration [1121]: 0.002359854071064437
Loss at iteration [1122]: 0.00235981290855501
Loss at iteration [1123]: 0.0023597564334282626
Loss at iteration [1124]: 0.0023596204258678117
Loss at iteration [1125]: 0.002359593484327937
Loss at iteration [1126]: 0.0023595337708030165
Loss at iteration [1127]: 0.0023594843394723593
Loss at iteration [1128]: 0.002359460846784595
Loss at iteration [1129]: 0.0023594374245864613
Loss at iteration [1130]: 0.0023593578765932805
Loss at iteration [1131]: 0.0023593578765932805
Loss at iteration [1132]: 0.002359344089919216
Loss at iteration [1133]: 0.0023592705972401216
Loss at iteration [1134]: 0.002359260194772812
Loss at iteration [1135]: 0.0023591661080240188
Loss at iteration [1136]: 0.0023590771542300565
Loss at iteration [1137]: 0.0023590460498349782
Loss at iteration [1138]: 0.002358665989867738
Loss at iteration [1139]: 0.002358624066829654
Loss at iteration [1140]: 0.002358290068957703
Loss at iteration [1141]: 0.002357815036348296
Loss at iteration [1142]: 0.002357815036348296
Loss at iteration [1143]: 0.0023577675900188686
Loss at iteration [1144]: 0.0023576754823104295
Loss at iteration [1145]: 0.002357491171895434
Loss at iteration [1146]: 0.0023574455926693976
Loss at iteration [1147]: 0.0023573953505808534
Loss at iteration [1148]: 0.002357295494622938
Loss at iteration [1149]: 0.002357265561134046
Loss at iteration [1150]: 0.002357154938742509
