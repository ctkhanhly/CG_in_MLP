Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.001
Beta type                             :HS
Total number of function evaluations  : 1438
Total number of iterations            : 416
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 2.5683581829071045
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 50.21347551917836%
Percentage of parameters < 1e-7       : 50.21347551917836%
Percentage of parameters < 1e-6       : 50.21347551917836%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.6323920134343911
Loss at iteration [2]: 0.6212491875910564
Loss at iteration [3]: 0.5540518476324952
Loss at iteration [4]: 0.44758997533203315
Loss at iteration [5]: 0.36746457790591464
Loss at iteration [6]: 0.34648923922414643
Loss at iteration [7]: 0.3370540619231746
Loss at iteration [8]: 0.3309730071764713
Loss at iteration [9]: 0.3309730071764713
Loss at iteration [10]: 0.32864356541069745
Loss at iteration [11]: 0.31575523716417536
Loss at iteration [12]: 0.30990705416585895
Loss at iteration [13]: 0.2849503981299028
Loss at iteration [14]: 0.2604097700203114
Loss at iteration [15]: 0.24569548149660403
Loss at iteration [16]: 0.23206376857788907
Loss at iteration [17]: 0.23206376857788907
Loss at iteration [18]: 0.23004154239902144
Loss at iteration [19]: 0.22628764171889024
Loss at iteration [20]: 0.2257257040550623
Loss at iteration [21]: 0.2225851866877489
Loss at iteration [22]: 0.22149433367015198
Loss at iteration [23]: 0.22049615603069625
Loss at iteration [24]: 0.22039119820060943
Loss at iteration [25]: 0.22039119820060943
Loss at iteration [26]: 0.21925174968524944
Loss at iteration [27]: 0.21819366308287882
Loss at iteration [28]: 0.21774863030820638
Loss at iteration [29]: 0.2176977873391257
Loss at iteration [30]: 0.21716008812062174
Loss at iteration [31]: 0.21690180917213225
Loss at iteration [32]: 0.21613273026364485
Loss at iteration [33]: 0.21613273026364485
Loss at iteration [34]: 0.2159118652122946
Loss at iteration [35]: 0.2155588916855936
Loss at iteration [36]: 0.21544563809386655
Loss at iteration [37]: 0.21542464714641066
Loss at iteration [38]: 0.21523181756854567
Loss at iteration [39]: 0.21522111802515825
Loss at iteration [40]: 0.21519530665302455
Loss at iteration [41]: 0.21519252457060212
Loss at iteration [42]: 0.21519252457060212
Loss at iteration [43]: 0.21518820465010755
Loss at iteration [44]: 0.21515698669888295
Loss at iteration [45]: 0.21513586729202366
Loss at iteration [46]: 0.21503059069341005
Loss at iteration [47]: 0.21496468229963944
Loss at iteration [48]: 0.21486750311746866
Loss at iteration [49]: 0.2148665273842771
Loss at iteration [50]: 0.21485989412020615
Loss at iteration [51]: 0.21485989412020615
Loss at iteration [52]: 0.21485795240410452
Loss at iteration [53]: 0.2148362253477743
Loss at iteration [54]: 0.21483294480456555
Loss at iteration [55]: 0.21481909542883565
Loss at iteration [56]: 0.21481866302848354
Loss at iteration [57]: 0.21481265166012908
Loss at iteration [58]: 0.2148116704488734
Loss at iteration [59]: 0.21480492552707145
Loss at iteration [60]: 0.21480492552707145
Loss at iteration [61]: 0.21480267518941304
Loss at iteration [62]: 0.21479739496608286
Loss at iteration [63]: 0.21478713731805144
Loss at iteration [64]: 0.21478672545581962
Loss at iteration [65]: 0.21478510430879375
Loss at iteration [66]: 0.21477962343361728
Loss at iteration [67]: 0.2147740594296208
Loss at iteration [68]: 0.2147740594296208
Loss at iteration [69]: 0.2147733174416839
Loss at iteration [70]: 0.21477029823618185
Loss at iteration [71]: 0.21476955629372566
Loss at iteration [72]: 0.21476580490448693
Loss at iteration [73]: 0.2147601772120471
Loss at iteration [74]: 0.21475915152153796
Loss at iteration [75]: 0.21475653541695885
Loss at iteration [76]: 0.21475638050072998
Loss at iteration [77]: 0.21475638050072998
Loss at iteration [78]: 0.21475493257893077
Loss at iteration [79]: 0.21475470573730263
Loss at iteration [80]: 0.21475164308851813
Loss at iteration [81]: 0.2147506540886058
Loss at iteration [82]: 0.21473673056020093
Loss at iteration [83]: 0.21472512251796522
Loss at iteration [84]: 0.21469481723778883
Loss at iteration [85]: 0.21469481723778883
Loss at iteration [86]: 0.2146934441443714
Loss at iteration [87]: 0.21468847228921392
Loss at iteration [88]: 0.21468473768989532
Loss at iteration [89]: 0.21468284992995812
Loss at iteration [90]: 0.21468259823444089
Loss at iteration [91]: 0.2146815947363643
Loss at iteration [92]: 0.21468143106737533
Loss at iteration [93]: 0.21468054418037272
Loss at iteration [94]: 0.21468054418037272
Loss at iteration [95]: 0.21468033704179806
Loss at iteration [96]: 0.2146796968062725
Loss at iteration [97]: 0.21467722666355923
Loss at iteration [98]: 0.21467674743440157
Loss at iteration [99]: 0.21467178447731913
Loss at iteration [100]: 0.21466648433377
Loss at iteration [101]: 0.21466648433377
Loss at iteration [102]: 0.21466290250562903
Loss at iteration [103]: 0.2146593363943762
Loss at iteration [104]: 0.21465793884479226
Loss at iteration [105]: 0.21465764821108943
Loss at iteration [106]: 0.21465719087890495
Loss at iteration [107]: 0.21465331234646828
Loss at iteration [108]: 0.21465248791608507
Loss at iteration [109]: 0.21465248791608507
Loss at iteration [110]: 0.21465186394792632
Loss at iteration [111]: 0.21465002316190707
Loss at iteration [112]: 0.21464991038777534
Loss at iteration [113]: 0.21464826204534274
Loss at iteration [114]: 0.21464821117422178
Loss at iteration [115]: 0.21464777738048774
Loss at iteration [116]: 0.21464749341278458
Loss at iteration [117]: 0.21464656528033405
Loss at iteration [118]: 0.21464656528033405
Loss at iteration [119]: 0.21464647154885533
Loss at iteration [120]: 0.21464613942836147
Loss at iteration [121]: 0.2146457118861957
Loss at iteration [122]: 0.21464550260125517
Loss at iteration [123]: 0.2146450909466565
Loss at iteration [124]: 0.21464504189286507
Loss at iteration [125]: 0.21464469603207909
Loss at iteration [126]: 0.21464469603207909
Loss at iteration [127]: 0.21464463683602708
Loss at iteration [128]: 0.21464443275100617
Loss at iteration [129]: 0.21464433255500812
Loss at iteration [130]: 0.21464412296000135
Loss at iteration [131]: 0.21464339144385733
Loss at iteration [132]: 0.2146425707495684
Loss at iteration [133]: 0.21464096625277324
Loss at iteration [134]: 0.21463134049774255
Loss at iteration [135]: 0.21463134049774255
Loss at iteration [136]: 0.2146303430828968
Loss at iteration [137]: 0.21462746189331824
Loss at iteration [138]: 0.2146273064494567
Loss at iteration [139]: 0.21462543431336764
Loss at iteration [140]: 0.21462520871923532
Loss at iteration [141]: 0.21462421984112656
Loss at iteration [142]: 0.21462392651971768
Loss at iteration [143]: 0.21462374677006027
Loss at iteration [144]: 0.21462360920751153
Loss at iteration [145]: 0.21462360920751153
Loss at iteration [146]: 0.2146235158693586
Loss at iteration [147]: 0.21462327670182
Loss at iteration [148]: 0.2146226566865732
Loss at iteration [149]: 0.214620878212145
Loss at iteration [150]: 0.21462073734182
Loss at iteration [151]: 0.2146166298917666
Loss at iteration [152]: 0.21461608040935012
Loss at iteration [153]: 0.21461608040935012
Loss at iteration [154]: 0.21461497906757507
Loss at iteration [155]: 0.21461407877494965
Loss at iteration [156]: 0.21461378967629588
Loss at iteration [157]: 0.21461365473004224
Loss at iteration [158]: 0.21461326070239495
Loss at iteration [159]: 0.2146130417959603
Loss at iteration [160]: 0.21461265862753265
Loss at iteration [161]: 0.21461260084180153
Loss at iteration [162]: 0.21461260084180153
Loss at iteration [163]: 0.21461256125020087
Loss at iteration [164]: 0.21461214851379873
Loss at iteration [165]: 0.21461205284624515
Loss at iteration [166]: 0.21461162589230723
Loss at iteration [167]: 0.21461123087509032
Loss at iteration [168]: 0.2146110968926392
Loss at iteration [169]: 0.21461098726669717
Loss at iteration [170]: 0.21461096107196742
Loss at iteration [171]: 0.21461096107196742
Loss at iteration [172]: 0.21461094102741146
Loss at iteration [173]: 0.21461040164921097
Loss at iteration [174]: 0.21461020040437787
Loss at iteration [175]: 0.21461001057505702
Loss at iteration [176]: 0.2146098940636856
Loss at iteration [177]: 0.21460954192357246
Loss at iteration [178]: 0.21460905132053149
Loss at iteration [179]: 0.21460905132053149
Loss at iteration [180]: 0.21460873718408952
Loss at iteration [181]: 0.21460795462438198
Loss at iteration [182]: 0.21460770756962458
Loss at iteration [183]: 0.2146076124065605
Loss at iteration [184]: 0.21460710456941928
Loss at iteration [185]: 0.21460595915459751
Loss at iteration [186]: 0.2146058699803376
Loss at iteration [187]: 0.2146058699803376
Loss at iteration [188]: 0.21460581706326984
Loss at iteration [189]: 0.21460555286825925
Loss at iteration [190]: 0.21460539997000122
Loss at iteration [191]: 0.21460497262322076
Loss at iteration [192]: 0.21460496906812931
Loss at iteration [193]: 0.21460485917176647
Loss at iteration [194]: 0.21460484710110134
Loss at iteration [195]: 0.21460471702757022
Loss at iteration [196]: 0.21460471702757022
Loss at iteration [197]: 0.21460465840075527
Loss at iteration [198]: 0.21460459232066373
Loss at iteration [199]: 0.21460410245434877
Loss at iteration [200]: 0.2146040607758205
Loss at iteration [201]: 0.21460365366158834
Loss at iteration [202]: 0.21460297094039865
Loss at iteration [203]: 0.21460191263880482
Loss at iteration [204]: 0.21460191263880482
Loss at iteration [205]: 0.21460171617762458
Loss at iteration [206]: 0.21459859023752817
Loss at iteration [207]: 0.21459850538799705
Loss at iteration [208]: 0.21459834080562445
Loss at iteration [209]: 0.21459833444566773
Loss at iteration [210]: 0.21459815988291744
Loss at iteration [211]: 0.2145981286650002
Loss at iteration [212]: 0.21459795835877504
Loss at iteration [213]: 0.21459795835877504
Loss at iteration [214]: 0.21459793309765024
Loss at iteration [215]: 0.21459777778589484
Loss at iteration [216]: 0.21459774905695447
Loss at iteration [217]: 0.21459742875963922
Loss at iteration [218]: 0.21459736066308885
Loss at iteration [219]: 0.21459622035962225
Loss at iteration [220]: 0.21459277377753042
Loss at iteration [221]: 0.21459277377753042
Loss at iteration [222]: 0.2145921098362689
Loss at iteration [223]: 0.21459181098019958
Loss at iteration [224]: 0.2145918037594258
Loss at iteration [225]: 0.2145917577224598
Loss at iteration [226]: 0.21459165001292638
Loss at iteration [227]: 0.21459158153587518
Loss at iteration [228]: 0.21459140445579003
Loss at iteration [229]: 0.21459133131828032
Loss at iteration [230]: 0.21459121282067634
Loss at iteration [231]: 0.21459121282067634
Loss at iteration [232]: 0.21459117990383383
Loss at iteration [233]: 0.2145908170579724
Loss at iteration [234]: 0.21459062976225457
Loss at iteration [235]: 0.21459061141278912
Loss at iteration [236]: 0.2145904698606982
Loss at iteration [237]: 0.21459039112488804
Loss at iteration [238]: 0.2145897645599288
Loss at iteration [239]: 0.2145897645599288
Loss at iteration [240]: 0.21458964515384463
Loss at iteration [241]: 0.21458931437383943
Loss at iteration [242]: 0.21458920044639035
Loss at iteration [243]: 0.21458898749543145
Loss at iteration [244]: 0.21458897054878445
Loss at iteration [245]: 0.2145886811079388
Loss at iteration [246]: 0.2145884879565312
Loss at iteration [247]: 0.2145884879565312
Loss at iteration [248]: 0.2145883686630384
Loss at iteration [249]: 0.21458803071428698
Loss at iteration [250]: 0.21458788883755867
Loss at iteration [251]: 0.21458781560324944
Loss at iteration [252]: 0.21458780665080007
Loss at iteration [253]: 0.2145877168825816
Loss at iteration [254]: 0.21458770181882092
Loss at iteration [255]: 0.21458764876741554
Loss at iteration [256]: 0.21458764876741554
Loss at iteration [257]: 0.21458764508594835
Loss at iteration [258]: 0.21458751921543637
Loss at iteration [259]: 0.21458005741461617
Loss at iteration [260]: 0.2145798429949925
Loss at iteration [261]: 0.2145784363640257
Loss at iteration [262]: 0.21457665845896529
Loss at iteration [263]: 0.21457626750800018
Loss at iteration [264]: 0.21457626750800018
Loss at iteration [265]: 0.21457620023649454
Loss at iteration [266]: 0.2145757468527558
Loss at iteration [267]: 0.21457545280948734
Loss at iteration [268]: 0.21457531458711843
Loss at iteration [269]: 0.21457529934886332
Loss at iteration [270]: 0.2145751104453136
Loss at iteration [271]: 0.21457509414487946
Loss at iteration [272]: 0.2145749851418366
Loss at iteration [273]: 0.2145749851418366
Loss at iteration [274]: 0.21457497255669022
Loss at iteration [275]: 0.2145749313346466
Loss at iteration [276]: 0.21457432874859153
Loss at iteration [277]: 0.21457374548504143
Loss at iteration [278]: 0.21457353248275599
Loss at iteration [279]: 0.2145735036635559
Loss at iteration [280]: 0.2145733706385542
Loss at iteration [281]: 0.2145733706385542
Loss at iteration [282]: 0.2145733119135307
Loss at iteration [283]: 0.21457316228406417
Loss at iteration [284]: 0.21457305633182455
Loss at iteration [285]: 0.21457272472635167
Loss at iteration [286]: 0.21457268012805503
Loss at iteration [287]: 0.21457180040153456
Loss at iteration [288]: 0.21457176968647362
Loss at iteration [289]: 0.21457176968647362
Loss at iteration [290]: 0.21457174166409038
Loss at iteration [291]: 0.21457139071067968
Loss at iteration [292]: 0.21457136679494396
Loss at iteration [293]: 0.21457128320144753
Loss at iteration [294]: 0.21457123505892156
Loss at iteration [295]: 0.21457116255628253
Loss at iteration [296]: 0.2145711230609603
Loss at iteration [297]: 0.2145710411301389
Loss at iteration [298]: 0.2145710411301389
Loss at iteration [299]: 0.21457100070577276
Loss at iteration [300]: 0.21457090354505656
Loss at iteration [301]: 0.2145707946350193
Loss at iteration [302]: 0.21457065347122972
Loss at iteration [303]: 0.2145706373646684
Loss at iteration [304]: 0.2145704670890264
Loss at iteration [305]: 0.21457045591755217
Loss at iteration [306]: 0.2145703956446023
Loss at iteration [307]: 0.2145703956446023
Loss at iteration [308]: 0.2145703912843959
Loss at iteration [309]: 0.21457036440910873
Loss at iteration [310]: 0.2145703587698522
Loss at iteration [311]: 0.21457032601653656
Loss at iteration [312]: 0.2145701782788671
Loss at iteration [313]: 0.2145698911387189
Loss at iteration [314]: 0.21456975862615776
Loss at iteration [315]: 0.21456919796714793
Loss at iteration [316]: 0.21456919796714793
Loss at iteration [317]: 0.21456894707361834
Loss at iteration [318]: 0.21456798045883893
Loss at iteration [319]: 0.21456763912671423
Loss at iteration [320]: 0.21456737031962758
Loss at iteration [321]: 0.2145673652110741
Loss at iteration [322]: 0.21456730895715834
Loss at iteration [323]: 0.21456728817759427
Loss at iteration [324]: 0.2145671046658786
Loss at iteration [325]: 0.2145671046658786
Loss at iteration [326]: 0.21456702349634665
Loss at iteration [327]: 0.21456696135770284
Loss at iteration [328]: 0.21456694377503943
Loss at iteration [329]: 0.21456693201029994
Loss at iteration [330]: 0.21456689916127403
Loss at iteration [331]: 0.21456688782762304
Loss at iteration [332]: 0.21456684060995762
Loss at iteration [333]: 0.21456683020819142
Loss at iteration [334]: 0.21456683020819142
Loss at iteration [335]: 0.21456682610847677
Loss at iteration [336]: 0.2145667931324117
Loss at iteration [337]: 0.21456674751786736
Loss at iteration [338]: 0.21456667736817836
Loss at iteration [339]: 0.2145665144162638
Loss at iteration [340]: 0.2145664003899302
Loss at iteration [341]: 0.21456629157493076
Loss at iteration [342]: 0.21456587722380965
Loss at iteration [343]: 0.21456587722380965
Loss at iteration [344]: 0.2145657797585787
Loss at iteration [345]: 0.21456507911574277
Loss at iteration [346]: 0.2145650563165633
Loss at iteration [347]: 0.21456492434824695
Loss at iteration [348]: 0.21456483595131584
Loss at iteration [349]: 0.2145646642193403
Loss at iteration [350]: 0.21456460976847413
Loss at iteration [351]: 0.21456459228209562
Loss at iteration [352]: 0.2145645804784914
Loss at iteration [353]: 0.2145645804784914
Loss at iteration [354]: 0.21456457506950194
Loss at iteration [355]: 0.2145645124244064
Loss at iteration [356]: 0.214564405444674
Loss at iteration [357]: 0.21456416627372357
Loss at iteration [358]: 0.21456413831934507
Loss at iteration [359]: 0.21456394820836322
Loss at iteration [360]: 0.21456392827478996
Loss at iteration [361]: 0.21456392827478996
Loss at iteration [362]: 0.21456390932850455
Loss at iteration [363]: 0.2145637880781781
Loss at iteration [364]: 0.21456377149624004
Loss at iteration [365]: 0.21456373518606353
Loss at iteration [366]: 0.21456365451445358
Loss at iteration [367]: 0.21456363183697133
Loss at iteration [368]: 0.2145635327333007
Loss at iteration [369]: 0.21456350373321348
Loss at iteration [370]: 0.21456350373321348
Loss at iteration [371]: 0.21456348804098868
Loss at iteration [372]: 0.21456339527185564
Loss at iteration [373]: 0.2145633916047315
Loss at iteration [374]: 0.21456333808601322
Loss at iteration [375]: 0.21456333362181368
Loss at iteration [376]: 0.2145632230837968
Loss at iteration [377]: 0.21456320948368604
Loss at iteration [378]: 0.21456320948368604
Loss at iteration [379]: 0.2145631964393822
Loss at iteration [380]: 0.21456313925641907
Loss at iteration [381]: 0.2145630795830801
Loss at iteration [382]: 0.2145630743609354
Loss at iteration [383]: 0.214563034532126
Loss at iteration [384]: 0.21456302044816444
Loss at iteration [385]: 0.21456298794891082
Loss at iteration [386]: 0.21456297298005728
Loss at iteration [387]: 0.21456297298005728
Loss at iteration [388]: 0.2145629632236383
Loss at iteration [389]: 0.21456292939232732
Loss at iteration [390]: 0.214562924714928
Loss at iteration [391]: 0.21456289395820258
Loss at iteration [392]: 0.21456288230234316
Loss at iteration [393]: 0.2145628424929952
Loss at iteration [394]: 0.21456283085547287
Loss at iteration [395]: 0.21456277674099447
Loss at iteration [396]: 0.21456275163197594
Loss at iteration [397]: 0.21456275163197594
Loss at iteration [398]: 0.21456274708205292
Loss at iteration [399]: 0.2145627214090907
Loss at iteration [400]: 0.21456271856251005
Loss at iteration [401]: 0.21456270126508223
Loss at iteration [402]: 0.2145626896081542
Loss at iteration [403]: 0.21456262680288124
Loss at iteration [404]: 0.21456257226853614
Loss at iteration [405]: 0.2145624038141937
Loss at iteration [406]: 0.2145624038141937
Loss at iteration [407]: 0.2145623492345376
Loss at iteration [408]: 0.2145621421334054
Loss at iteration [409]: 0.21456212735061522
Loss at iteration [410]: 0.21456205667895806
Loss at iteration [411]: 0.21456194716217986
Loss at iteration [412]: 0.21456186133409652
Loss at iteration [413]: 0.21456175558093704
Loss at iteration [414]: 0.2145616949065849
Loss at iteration [415]: 0.2145616949065849
Loss at iteration [416]: 0.21456166880570773
Loss at iteration [417]: 0.2145616088657735
Loss at iteration [418]: 0.21456153678289744
Loss at iteration [419]: 0.21456149310891104
Loss at iteration [420]: 0.21456148708357542
Loss at iteration [421]: 0.2145611528925009
Loss at iteration [422]: 0.21456112264704674
Loss at iteration [423]: 0.21456112264704674
Loss at iteration [424]: 0.21456109626633152
Loss at iteration [425]: 0.21456096692302784
Loss at iteration [426]: 0.21456088219718733
Loss at iteration [427]: 0.21456085992902388
Loss at iteration [428]: 0.21456083279396498
Loss at iteration [429]: 0.21456082807388238
Loss at iteration [430]: 0.214560794141121
Loss at iteration [431]: 0.2145607854100536
Loss at iteration [432]: 0.2145607854100536
Loss at iteration [433]: 0.21456077850135513
Loss at iteration [434]: 0.2145607419419568
Loss at iteration [435]: 0.214560733895754
Loss at iteration [436]: 0.21456068833316602
Loss at iteration [437]: 0.2145606816573161
Loss at iteration [438]: 0.21456064783287757
Loss at iteration [439]: 0.21456063775415257
Loss at iteration [440]: 0.21456062870007603
Loss at iteration [441]: 0.21456062870007603
Loss at iteration [442]: 0.21456062031767778
Loss at iteration [443]: 0.21456060669925436
Loss at iteration [444]: 0.21456057168589565
Loss at iteration [445]: 0.21456054685920914
Loss at iteration [446]: 0.21455945887732852
Loss at iteration [447]: 0.21455921643206136
Loss at iteration [448]: 0.21455869572359718
Loss at iteration [449]: 0.21455869572359718
Loss at iteration [450]: 0.21455858538250333
Loss at iteration [451]: 0.21455844199381718
Loss at iteration [452]: 0.21455834565920934
Loss at iteration [453]: 0.21455815569020548
Loss at iteration [454]: 0.21455814692806513
Loss at iteration [455]: 0.21455800585929366
Loss at iteration [456]: 0.21455800352237225
Loss at iteration [457]: 0.21455791207517813
Loss at iteration [458]: 0.21455791207517813
Loss at iteration [459]: 0.21455789896836067
Loss at iteration [460]: 0.2145578823375443
Loss at iteration [461]: 0.2145578492676604
Loss at iteration [462]: 0.214557796093759
Loss at iteration [463]: 0.21455778804086786
Loss at iteration [464]: 0.2145577537525655
Loss at iteration [465]: 0.21455775199800559
