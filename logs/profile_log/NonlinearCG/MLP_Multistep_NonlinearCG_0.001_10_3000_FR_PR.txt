Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : NonlinearCG
Learning rate                         : 0.001
Beta type                             :FR_PR
Total number of function evaluations  : 709
Total number of iterations            : 217
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 1.2722251415252686
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 50.265614701281834%
Percentage of parameters < 1e-7       : 50.265614701281834%
Percentage of parameters < 1e-6       : 50.26610658035828%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.6708908766400954
Loss at iteration [2]: 0.6611210579823792
Loss at iteration [3]: 0.6389411623247745
Loss at iteration [4]: 0.5869910073671518
Loss at iteration [5]: 0.4611657653004025
Loss at iteration [6]: 0.3788673452471159
Loss at iteration [7]: 0.34573890141084734
Loss at iteration [8]: 0.3409156557321103
Loss at iteration [9]: 0.3409156557321103
Loss at iteration [10]: 0.33760622112427513
Loss at iteration [11]: 0.2979525961001378
Loss at iteration [12]: 0.2943816851861674
Loss at iteration [13]: 0.2853300794166009
Loss at iteration [14]: 0.2793892352279114
Loss at iteration [15]: 0.25665601076197925
Loss at iteration [16]: 0.2463415261434607
Loss at iteration [17]: 0.23760297172162637
Loss at iteration [18]: 0.23760297172162637
Loss at iteration [19]: 0.2349817647562952
Loss at iteration [20]: 0.22473323539718032
Loss at iteration [21]: 0.22108015338850315
Loss at iteration [22]: 0.21900295276398907
Loss at iteration [23]: 0.2188772684326661
Loss at iteration [24]: 0.21806749447348409
Loss at iteration [25]: 0.21789800870612483
Loss at iteration [26]: 0.21746109514346057
Loss at iteration [27]: 0.21741619220426184
Loss at iteration [28]: 0.21741619220426184
Loss at iteration [29]: 0.21737613527452138
Loss at iteration [30]: 0.2167151012543145
Loss at iteration [31]: 0.21664342074061885
Loss at iteration [32]: 0.21623833538052176
Loss at iteration [33]: 0.2162003454631371
Loss at iteration [34]: 0.21602977399470982
Loss at iteration [35]: 0.21601376029582217
Loss at iteration [36]: 0.21566269527563886
Loss at iteration [37]: 0.21566269527563886
Loss at iteration [38]: 0.21562272564266755
Loss at iteration [39]: 0.2155164887177622
Loss at iteration [40]: 0.21549694345419712
Loss at iteration [41]: 0.215339058644645
Loss at iteration [42]: 0.21531318610184627
Loss at iteration [43]: 0.21519626590913143
Loss at iteration [44]: 0.2151891506230926
Loss at iteration [45]: 0.2151252855481114
Loss at iteration [46]: 0.2151252855481114
Loss at iteration [47]: 0.21511948084324686
Loss at iteration [48]: 0.21509201221749183
Loss at iteration [49]: 0.21508557378703305
Loss at iteration [50]: 0.2150563180323005
Loss at iteration [51]: 0.21504175677725265
Loss at iteration [52]: 0.2149924356252482
Loss at iteration [53]: 0.2149370360687115
Loss at iteration [54]: 0.2148808849568163
Loss at iteration [55]: 0.2148808849568163
Loss at iteration [56]: 0.21485981857417386
Loss at iteration [57]: 0.2148518043143652
Loss at iteration [58]: 0.21482537129612841
Loss at iteration [59]: 0.21480577838673814
Loss at iteration [60]: 0.21478536310959204
Loss at iteration [61]: 0.21478223797987522
Loss at iteration [62]: 0.21477584136207065
Loss at iteration [63]: 0.2147624009704713
Loss at iteration [64]: 0.2147624009704713
Loss at iteration [65]: 0.21476043844830622
Loss at iteration [66]: 0.2147535490857857
Loss at iteration [67]: 0.2147275303734427
Loss at iteration [68]: 0.21472502916080918
Loss at iteration [69]: 0.21472427431860885
Loss at iteration [70]: 0.21472062443172113
Loss at iteration [71]: 0.21471977094888772
Loss at iteration [72]: 0.21471977094888772
Loss at iteration [73]: 0.2147180438932289
Loss at iteration [74]: 0.21471735989009047
Loss at iteration [75]: 0.21471658320861423
Loss at iteration [76]: 0.21471607173645604
Loss at iteration [77]: 0.21471403274940645
Loss at iteration [78]: 0.21471079290825437
Loss at iteration [79]: 0.21470428091170352
Loss at iteration [80]: 0.21470214203312815
Loss at iteration [81]: 0.21470081809416325
Loss at iteration [82]: 0.21470081809416325
Loss at iteration [83]: 0.2147004824319553
Loss at iteration [84]: 0.21469848336918868
Loss at iteration [85]: 0.21469700294248348
Loss at iteration [86]: 0.21469078459655286
Loss at iteration [87]: 0.21468622918974495
Loss at iteration [88]: 0.2146791955588618
Loss at iteration [89]: 0.21467816250757676
Loss at iteration [90]: 0.21467365932635152
Loss at iteration [91]: 0.21467365932635152
Loss at iteration [92]: 0.21467344816235004
Loss at iteration [93]: 0.21467153370978018
Loss at iteration [94]: 0.21467104342582752
Loss at iteration [95]: 0.21466841152170893
Loss at iteration [96]: 0.21466817510560532
Loss at iteration [97]: 0.21466349800826845
Loss at iteration [98]: 0.21466268162303032
Loss at iteration [99]: 0.21466062129467117
Loss at iteration [100]: 0.21466062129467117
Loss at iteration [101]: 0.21466044314561042
Loss at iteration [102]: 0.21465960709541396
Loss at iteration [103]: 0.21465912202530857
Loss at iteration [104]: 0.21465824693202346
Loss at iteration [105]: 0.21465794283198658
Loss at iteration [106]: 0.21465600167538096
Loss at iteration [107]: 0.21465444591175153
Loss at iteration [108]: 0.21465141456494138
Loss at iteration [109]: 0.21465141456494138
Loss at iteration [110]: 0.2146510348783652
Loss at iteration [111]: 0.21465025138888688
Loss at iteration [112]: 0.2146479142781632
Loss at iteration [113]: 0.21464619137678045
Loss at iteration [114]: 0.21464586735306554
Loss at iteration [115]: 0.21464066505217305
Loss at iteration [116]: 0.21463923403700078
Loss at iteration [117]: 0.21463923403700078
Loss at iteration [118]: 0.21463836302447317
Loss at iteration [119]: 0.21463757497582397
Loss at iteration [120]: 0.21463642807189778
Loss at iteration [121]: 0.21463504998137586
Loss at iteration [122]: 0.21463461801850936
Loss at iteration [123]: 0.21463437479434463
Loss at iteration [124]: 0.2146343387067527
Loss at iteration [125]: 0.21463412486681976
Loss at iteration [126]: 0.21463412486681976
Loss at iteration [127]: 0.214634005611771
Loss at iteration [128]: 0.2146337252085866
Loss at iteration [129]: 0.21463339662372527
Loss at iteration [130]: 0.214631935495127
Loss at iteration [131]: 0.21463156825195198
Loss at iteration [132]: 0.21463068512725014
Loss at iteration [133]: 0.21463058685343195
Loss at iteration [134]: 0.21463024373562858
Loss at iteration [135]: 0.21463012806385717
Loss at iteration [136]: 0.21463012806385717
Loss at iteration [137]: 0.2146300344980593
Loss at iteration [138]: 0.21462972509363162
Loss at iteration [139]: 0.21462723624795627
Loss at iteration [140]: 0.21462695539580715
Loss at iteration [141]: 0.2146202245904499
Loss at iteration [142]: 0.21461564183553264
Loss at iteration [143]: 0.2146121941960541
Loss at iteration [144]: 0.21461192914232977
Loss at iteration [145]: 0.21461192914232977
Loss at iteration [146]: 0.2146117356673219
Loss at iteration [147]: 0.2146110621818559
Loss at iteration [148]: 0.21461096398476723
Loss at iteration [149]: 0.21461064840936403
Loss at iteration [150]: 0.21461047003116263
Loss at iteration [151]: 0.21461025201199577
Loss at iteration [152]: 0.214610125934263
Loss at iteration [153]: 0.21460990340606495
Loss at iteration [154]: 0.21460990340606495
Loss at iteration [155]: 0.214609736254653
Loss at iteration [156]: 0.21460945117682442
Loss at iteration [157]: 0.21460930215595247
Loss at iteration [158]: 0.21460917989272782
Loss at iteration [159]: 0.21460915873689912
Loss at iteration [160]: 0.21460897088579645
Loss at iteration [161]: 0.21460892994861885
Loss at iteration [162]: 0.2146075045756701
Loss at iteration [163]: 0.2146075045756701
Loss at iteration [164]: 0.21460724611240686
Loss at iteration [165]: 0.214606554585109
Loss at iteration [166]: 0.2146063289450677
Loss at iteration [167]: 0.2146060935542014
Loss at iteration [168]: 0.21460590494480938
Loss at iteration [169]: 0.2146058151374451
Loss at iteration [170]: 0.2146057651489006
Loss at iteration [171]: 0.2146055932118146
Loss at iteration [172]: 0.2146055932118146
Loss at iteration [173]: 0.21460544784277436
Loss at iteration [174]: 0.2146050142520283
Loss at iteration [175]: 0.2145948700109497
Loss at iteration [176]: 0.21459371200710184
Loss at iteration [177]: 0.21458242262801372
Loss at iteration [178]: 0.21458008631413547
Loss at iteration [179]: 0.21457549970349393
Loss at iteration [180]: 0.21457539415893262
Loss at iteration [181]: 0.21457539415893262
Loss at iteration [182]: 0.21457529883751922
Loss at iteration [183]: 0.21457484674597999
Loss at iteration [184]: 0.2145739467714287
Loss at iteration [185]: 0.21457338217333644
Loss at iteration [186]: 0.21457337182236902
Loss at iteration [187]: 0.2145730572088947
Loss at iteration [188]: 0.21457301384464755
Loss at iteration [189]: 0.21457298950530976
Loss at iteration [190]: 0.21457298950530976
Loss at iteration [191]: 0.21457297964420913
Loss at iteration [192]: 0.21457277312688472
Loss at iteration [193]: 0.21457251942309333
Loss at iteration [194]: 0.21457219146051082
Loss at iteration [195]: 0.2145717150529384
Loss at iteration [196]: 0.21457168658634226
Loss at iteration [197]: 0.21457150219822466
Loss at iteration [198]: 0.21457150219822466
Loss at iteration [199]: 0.2145714628166191
Loss at iteration [200]: 0.214571355171088
Loss at iteration [201]: 0.21457133883878082
Loss at iteration [202]: 0.2145711090677446
Loss at iteration [203]: 0.21457102342540094
Loss at iteration [204]: 0.21457082024935517
Loss at iteration [205]: 0.21457074677743107
Loss at iteration [206]: 0.21457073499224208
Loss at iteration [207]: 0.214570684381772
Loss at iteration [208]: 0.214570684381772
Loss at iteration [209]: 0.21457068131971765
Loss at iteration [210]: 0.21457065215255752
Loss at iteration [211]: 0.2145706426931456
Loss at iteration [212]: 0.2145706115239872
Loss at iteration [213]: 0.2145706078372307
Loss at iteration [214]: 0.2145705251214417
Loss at iteration [215]: 0.21457050437299227
Loss at iteration [216]: 0.2145704193103193
Loss at iteration [217]: 0.2145704138218674
Loss at iteration [218]: 0.2145704138218674
Loss at iteration [219]: 0.21457041000435853
Loss at iteration [220]: 0.2145703813875401
Loss at iteration [221]: 0.21457037638203497
Loss at iteration [222]: 0.21457034628568541
Loss at iteration [223]: 0.2145703016694462
Loss at iteration [224]: 0.2145699855041407
Loss at iteration [225]: 0.21456584442614904
Loss at iteration [226]: 0.21456584442614904
Loss at iteration [227]: 0.21456539669192273
Loss at iteration [228]: 0.21456500965968528
Loss at iteration [229]: 0.21456487545386999
Loss at iteration [230]: 0.21456473515719146
Loss at iteration [231]: 0.2145647007905199
Loss at iteration [232]: 0.2145646115546451
Loss at iteration [233]: 0.21456451611092175
Loss at iteration [234]: 0.21456451197618095
Loss at iteration [235]: 0.21456451197618095
Loss at iteration [236]: 0.21456450620400708
Loss at iteration [237]: 0.21456442738429196
Loss at iteration [238]: 0.2145644267554991
