Model name                            : MLP_Multistep
The number of input features          : 20
The number of output features         : 2
Optimizer name                        : Adam
Learning rate                         : 0.01
Max number of iterations              : 3000
Number of samples in training data    : 39
Number of samples in tests data       : 16
Total training time                   : 0.20265483856201172
Total number of parameters            : 205302
Percentage of parameters < 1e-9       : 56.92735579780032%
Percentage of parameters < 1e-7       : 56.92735579780032%
Percentage of parameters < 1e-6       : 56.92832997243086%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5738945429870739
Loss at iteration [2]: 1.0507637446412765
Loss at iteration [3]: 0.8241600488281134
Loss at iteration [4]: 0.4417457854666364
Loss at iteration [5]: 0.6348878293739545
***** Warning: Loss has increased *****
Loss at iteration [6]: 0.5356417040339516
Loss at iteration [7]: 0.42784163292897587
Loss at iteration [8]: 0.45441754370245563
***** Warning: Loss has increased *****
Loss at iteration [9]: 0.4268018998001957
Loss at iteration [10]: 0.4243223555288573
Loss at iteration [11]: 0.43412346713361843
***** Warning: Loss has increased *****
Loss at iteration [12]: 0.4127971672473534
Loss at iteration [13]: 0.3934755277408108
Loss at iteration [14]: 0.40892040177467903
***** Warning: Loss has increased *****
Loss at iteration [15]: 0.41542669599473736
***** Warning: Loss has increased *****
Loss at iteration [16]: 0.3981054103899249
Loss at iteration [17]: 0.3987635003472736
***** Warning: Loss has increased *****
Loss at iteration [18]: 0.40138350055657085
***** Warning: Loss has increased *****
Loss at iteration [19]: 0.39478431151998483
Loss at iteration [20]: 0.39341394817339576
Loss at iteration [21]: 0.3979599195965092
***** Warning: Loss has increased *****
Loss at iteration [22]: 0.3943551390863049
Loss at iteration [23]: 0.38920538789580794
Loss at iteration [24]: 0.39309685884753853
***** Warning: Loss has increased *****
Loss at iteration [25]: 0.39492659541948744
***** Warning: Loss has increased *****
Loss at iteration [26]: 0.39123816825629015
Loss at iteration [27]: 0.38958215781315697
Loss at iteration [28]: 0.39205854081494795
***** Warning: Loss has increased *****
Loss at iteration [29]: 0.39158615731724394
Loss at iteration [30]: 0.3898123517465629
Loss at iteration [31]: 0.39078008905578715
***** Warning: Loss has increased *****
Loss at iteration [32]: 0.3909006735726196
***** Warning: Loss has increased *****
Loss at iteration [33]: 0.38958539168519246
Loss at iteration [34]: 0.39003778962634555
***** Warning: Loss has increased *****
Loss at iteration [35]: 0.3906152282086544
***** Warning: Loss has increased *****
Loss at iteration [36]: 0.3895602351392422
Loss at iteration [37]: 0.3894612811463158
Loss at iteration [38]: 0.39008254121411756
***** Warning: Loss has increased *****
Loss at iteration [39]: 0.3896693963915123
Loss at iteration [40]: 0.3893541815229736
Loss at iteration [41]: 0.38986225076422804
***** Warning: Loss has increased *****
Loss at iteration [42]: 0.38940396342358763
Loss at iteration [43]: 0.3891422612936687
Loss at iteration [44]: 0.3895780310898663
***** Warning: Loss has increased *****
Loss at iteration [45]: 0.38950118284646223
Loss at iteration [46]: 0.38918125060618963
Loss at iteration [47]: 0.3893448032817067
***** Warning: Loss has increased *****
Loss at iteration [48]: 0.3893650665713626
***** Warning: Loss has increased *****
Loss at iteration [49]: 0.38916369894251507
Loss at iteration [50]: 0.3892759442376713
***** Warning: Loss has increased *****
Loss at iteration [51]: 0.38934134054840225
***** Warning: Loss has increased *****
Loss at iteration [52]: 0.3891601514634858
Loss at iteration [53]: 0.38918728783244744
***** Warning: Loss has increased *****
Loss at iteration [54]: 0.3892707224715567
***** Warning: Loss has increased *****
Loss at iteration [55]: 0.38914649817610125
Loss at iteration [56]: 0.38916091658713914
***** Warning: Loss has increased *****
Loss at iteration [57]: 0.38922529244358645
***** Warning: Loss has increased *****
Loss at iteration [58]: 0.38912011132965
Loss at iteration [59]: 0.3891803884440965
***** Warning: Loss has increased *****
Loss at iteration [60]: 0.389130201485225
Loss at iteration [61]: 0.38914147695656165
***** Warning: Loss has increased *****
Loss at iteration [62]: 0.3891598259296289
***** Warning: Loss has increased *****
Loss at iteration [63]: 0.3891040725790687
Loss at iteration [64]: 0.38913777712172115
***** Warning: Loss has increased *****
Loss at iteration [65]: 0.3891280655742636
Loss at iteration [66]: 0.3891057022766303
Loss at iteration [67]: 0.38913549705674827
***** Warning: Loss has increased *****
Loss at iteration [68]: 0.38911689982318404
Loss at iteration [69]: 0.3891010093379391
Loss at iteration [70]: 0.38912238020917794
***** Warning: Loss has increased *****
Loss at iteration [71]: 0.38910857716534347
Loss at iteration [72]: 0.3891075515268481
Loss at iteration [73]: 0.3891165119017057
***** Warning: Loss has increased *****
Loss at iteration [74]: 0.38910093997762035
Loss at iteration [75]: 0.38910370233827457
***** Warning: Loss has increased *****
Loss at iteration [76]: 0.38911249055144287
***** Warning: Loss has increased *****
Loss at iteration [77]: 0.38910038718787543
Loss at iteration [78]: 0.38910454738002553
***** Warning: Loss has increased *****
Loss at iteration [79]: 0.3891061965854671
***** Warning: Loss has increased *****
Loss at iteration [80]: 0.38909830686281505
Loss at iteration [81]: 0.3891035154562848
***** Warning: Loss has increased *****
Loss at iteration [82]: 0.3891034486673127
Loss at iteration [83]: 0.38909875224035073
Loss at iteration [84]: 0.3891030197060333
***** Warning: Loss has increased *****
Loss at iteration [85]: 0.38909947459640104
Loss at iteration [86]: 0.3890984346822711
Loss at iteration [87]: 0.38910245252609593
***** Warning: Loss has increased *****
Loss at iteration [88]: 0.38909865472195154
Loss at iteration [89]: 0.389098761471482
***** Warning: Loss has increased *****
Loss at iteration [90]: 0.3890999705340298
***** Warning: Loss has increased *****
Loss at iteration [91]: 0.38909778988445476
Loss at iteration [92]: 0.38909956397234946
***** Warning: Loss has increased *****
Loss at iteration [93]: 0.38909892005904656
Loss at iteration [94]: 0.3890973634602434
Loss at iteration [95]: 0.3890991221663167
***** Warning: Loss has increased *****
Loss at iteration [96]: 0.3890978303590491
Loss at iteration [97]: 0.38909790470467587
***** Warning: Loss has increased *****
Loss at iteration [98]: 0.3890986527826789
***** Warning: Loss has increased *****
Loss at iteration [99]: 0.3890972488373263
Loss at iteration [100]: 0.3890980104989964
***** Warning: Loss has increased *****
Loss at iteration [101]: 0.3890978202700402
Loss at iteration [102]: 0.38909744121131834
Loss at iteration [103]: 0.38909805439404405
***** Warning: Loss has increased *****
Loss at iteration [104]: 0.38909722061999386
Loss at iteration [105]: 0.38909748251787035
***** Warning: Loss has increased *****
Loss at iteration [106]: 0.3890977267200932
***** Warning: Loss has increased *****
Loss at iteration [107]: 0.38909723404728924
Loss at iteration [108]: 0.3890975674024563
***** Warning: Loss has increased *****
Loss at iteration [109]: 0.38909726311444165
Loss at iteration [110]: 0.38909729026315093
***** Warning: Loss has increased *****
Loss at iteration [111]: 0.3890974886333337
***** Warning: Loss has increased *****
Loss at iteration [112]: 0.38909714632185616
Loss at iteration [113]: 0.38909736310631476
***** Warning: Loss has increased *****
Loss at iteration [114]: 0.3890972284927136
Loss at iteration [115]: 0.3890971713442949
Loss at iteration [116]: 0.38909733624346504
***** Warning: Loss has increased *****
Loss at iteration [117]: 0.3890971317833131
Loss at iteration [118]: 0.3890972307742104
***** Warning: Loss has increased *****
Loss at iteration [119]: 0.38909716522755905
Loss at iteration [120]: 0.38909714991038585
Loss at iteration [121]: 0.3890972371152554
***** Warning: Loss has increased *****
Loss at iteration [122]: 0.38909709242267443
Loss at iteration [123]: 0.38909717123161225
***** Warning: Loss has increased *****
Loss at iteration [124]: 0.38909713938775103
Loss at iteration [125]: 0.3890971269957052
Loss at iteration [126]: 0.3890971596519442
***** Warning: Loss has increased *****
Loss at iteration [127]: 0.3890970868091155
Loss at iteration [128]: 0.3890971456939795
***** Warning: Loss has increased *****
Loss at iteration [129]: 0.3890971048059908
Loss at iteration [130]: 0.3890971132920348
***** Warning: Loss has increased *****
Loss at iteration [131]: 0.38909712414422654
***** Warning: Loss has increased *****
Loss at iteration [132]: 0.3890970820818737
Loss at iteration [133]: 0.38909712129434454
***** Warning: Loss has increased *****
Loss at iteration [134]: 0.38909709169810885
Loss at iteration [135]: 0.3890971054830488
***** Warning: Loss has increased *****
Loss at iteration [136]: 0.3890970962323246
Loss at iteration [137]: 0.38909708463856385
Loss at iteration [138]: 0.389097107231499
***** Warning: Loss has increased *****
Loss at iteration [139]: 0.38909708271956206
Loss at iteration [140]: 0.38909709571030143
***** Warning: Loss has increased *****
Loss at iteration [141]: 0.3890970854673202
Loss at iteration [142]: 0.3890970884755387
***** Warning: Loss has increased *****
Loss at iteration [143]: 0.3890970909346487
***** Warning: Loss has increased *****
Loss at iteration [144]: 0.38909708144247357
Loss at iteration [145]: 0.38909709043591445
***** Warning: Loss has increased *****
Loss at iteration [146]: 0.3890970793753637
Loss at iteration [147]: 0.38909708809830806
***** Warning: Loss has increased *****
Loss at iteration [148]: 0.3890970829145561
Loss at iteration [149]: 0.38909708171517815
Loss at iteration [150]: 0.3890970838379033
***** Warning: Loss has increased *****
Loss at iteration [151]: 0.3890970801766806
Loss at iteration [152]: 0.38909708475071414
***** Warning: Loss has increased *****
Loss at iteration [153]: 0.3890970785624886
Loss at iteration [154]: 0.3890970824545474
***** Warning: Loss has increased *****
Loss at iteration [155]: 0.38909708005407534
Loss at iteration [156]: 0.3890970808460986
***** Warning: Loss has increased *****
