Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : Adam
Learning rate                         : 0.001
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 6.591044902801514
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 49.719309709804854%
Percentage of parameters < 1e-7       : 49.719309709804854%
Percentage of parameters < 1e-6       : 49.719309709804854%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0072807453491277
Loss at iteration [2]: 0.778469281456991
Loss at iteration [3]: 0.5995972468831002
Loss at iteration [4]: 0.4513060921316482
Loss at iteration [5]: 0.3178808987327037
Loss at iteration [6]: 0.19970554320760234
Loss at iteration [7]: 0.10384476838308677
Loss at iteration [8]: 0.0389514402629253
Loss at iteration [9]: 0.013933162495313586
Loss at iteration [10]: 0.031513017911886036
***** Warning: Loss has increased *****
Loss at iteration [11]: 0.07105593404104636
***** Warning: Loss has increased *****
Loss at iteration [12]: 0.10058049977512847
***** Warning: Loss has increased *****
Loss at iteration [13]: 0.10321929187652926
***** Warning: Loss has increased *****
Loss at iteration [14]: 0.08227637566666347
Loss at iteration [15]: 0.051590184830076144
Loss at iteration [16]: 0.024723747315106594
Loss at iteration [17]: 0.010148229994990442
Loss at iteration [18]: 0.00886191377002359
Loss at iteration [19]: 0.016619640925159548
***** Warning: Loss has increased *****
Loss at iteration [20]: 0.02711565269050057
***** Warning: Loss has increased *****
Loss at iteration [21]: 0.034810266988350026
***** Warning: Loss has increased *****
Loss at iteration [22]: 0.036765073632868635
***** Warning: Loss has increased *****
Loss at iteration [23]: 0.032982712882601455
Loss at iteration [24]: 0.02530949214121781
Loss at iteration [25]: 0.016555530624258862
Loss at iteration [26]: 0.009199042321901899
Loss at iteration [27]: 0.005099628301018053
Loss at iteration [28]: 0.004909142594716313
Loss at iteration [29]: 0.007439920149422931
***** Warning: Loss has increased *****
Loss at iteration [30]: 0.010791747887307634
***** Warning: Loss has increased *****
Loss at iteration [31]: 0.013216941138318604
***** Warning: Loss has increased *****
Loss at iteration [32]: 0.013667132559313549
***** Warning: Loss has increased *****
Loss at iteration [33]: 0.012095989951608328
Loss at iteration [34]: 0.009246242388294934
Loss at iteration [35]: 0.006236385920998144
Loss at iteration [36]: 0.004086668154760279
Loss at iteration [37]: 0.0033489017897511517
Loss at iteration [38]: 0.0039491039614519075
***** Warning: Loss has increased *****
Loss at iteration [39]: 0.005244063665237219
***** Warning: Loss has increased *****
Loss at iteration [40]: 0.006426336057287503
***** Warning: Loss has increased *****
Loss at iteration [41]: 0.006911285230975271
***** Warning: Loss has increased *****
Loss at iteration [42]: 0.006517468141016742
Loss at iteration [43]: 0.005468781318462337
Loss at iteration [44]: 0.0042169697223267515
Loss at iteration [45]: 0.003240379556862462
Loss at iteration [46]: 0.002836515390214827
Loss at iteration [47]: 0.003005696828345865
***** Warning: Loss has increased *****
Loss at iteration [48]: 0.00350523370120948
***** Warning: Loss has increased *****
Loss at iteration [49]: 0.003998742645086315
***** Warning: Loss has increased *****
Loss at iteration [50]: 0.004218158371429213
***** Warning: Loss has increased *****
Loss at iteration [51]: 0.004072215956541049
Loss at iteration [52]: 0.0036530498437843456
Loss at iteration [53]: 0.0031587989136212095
Loss at iteration [54]: 0.0027886366282634583
Loss at iteration [55]: 0.002664196695844178
Loss at iteration [56]: 0.002783879860119905
***** Warning: Loss has increased *****
Loss at iteration [57]: 0.0030260594999513843
***** Warning: Loss has increased *****
Loss at iteration [58]: 0.003220506155864168
***** Warning: Loss has increased *****
Loss at iteration [59]: 0.0032530342959612496
***** Warning: Loss has increased *****
Loss at iteration [60]: 0.0031167259992337633
Loss at iteration [61]: 0.002893654451314903
Loss at iteration [62]: 0.002694860634907771
Loss at iteration [63]: 0.002599339723768529
Loss at iteration [64]: 0.002617729413309273
***** Warning: Loss has increased *****
Loss at iteration [65]: 0.002705430478828554
***** Warning: Loss has increased *****
Loss at iteration [66]: 0.0027944142389389735
***** Warning: Loss has increased *****
Loss at iteration [67]: 0.002829578053400495
***** Warning: Loss has increased *****
Loss at iteration [68]: 0.0027929519692812253
Loss at iteration [69]: 0.0027065464609746753
Loss at iteration [70]: 0.002614617923633172
Loss at iteration [71]: 0.002558291573968909
Loss at iteration [72]: 0.002554404594270918
Loss at iteration [73]: 0.002589691574090527
***** Warning: Loss has increased *****
Loss at iteration [74]: 0.002632672716826298
***** Warning: Loss has increased *****
Loss at iteration [75]: 0.002653278481052531
***** Warning: Loss has increased *****
Loss at iteration [76]: 0.002639258926973714
Loss at iteration [77]: 0.0025999124100821534
Loss at iteration [78]: 0.0025569595106223813
Loss at iteration [79]: 0.002530411416696435
Loss at iteration [80]: 0.002527921915803303
Loss at iteration [81]: 0.002542977074515475
***** Warning: Loss has increased *****
Loss at iteration [82]: 0.0025610878222173715
***** Warning: Loss has increased *****
Loss at iteration [83]: 0.0025689230211970522
***** Warning: Loss has increased *****
Loss at iteration [84]: 0.002561280600932193
Loss at iteration [85]: 0.0025426050428976697
Loss at iteration [86]: 0.002523043318621325
Loss at iteration [87]: 0.0025119289176282274
Loss at iteration [88]: 0.0025123311813842818
***** Warning: Loss has increased *****
Loss at iteration [89]: 0.002520242682748899
***** Warning: Loss has increased *****
Loss at iteration [90]: 0.0025279472022398477
***** Warning: Loss has increased *****
Loss at iteration [91]: 0.002529317301301334
***** Warning: Loss has increased *****
Loss at iteration [92]: 0.0025233142237708933
Loss at iteration [93]: 0.0025135970117399326
Loss at iteration [94]: 0.0025054118076842696
Loss at iteration [95]: 0.00250216847907392
Loss at iteration [96]: 0.0025038062727221833
***** Warning: Loss has increased *****
Loss at iteration [97]: 0.0025075042027366402
***** Warning: Loss has increased *****
Loss at iteration [98]: 0.0025097653675057612
***** Warning: Loss has increased *****
Loss at iteration [99]: 0.002508590748509623
Loss at iteration [100]: 0.002504479695956494
Loss at iteration [101]: 0.002499654257601822
Loss at iteration [102]: 0.0024965513286231192
Loss at iteration [103]: 0.002496151371418287
Loss at iteration [104]: 0.002497532316663559
***** Warning: Loss has increased *****
Loss at iteration [105]: 0.002498840927579205
***** Warning: Loss has increased *****
Loss at iteration [106]: 0.0024986623920750482
Loss at iteration [107]: 0.002496821118233649
Loss at iteration [108]: 0.002494289316788457
Loss at iteration [109]: 0.002492239239011039
Loss at iteration [110]: 0.0024913971122084585
Loss at iteration [111]: 0.002491603785545469
***** Warning: Loss has increased *****
Loss at iteration [112]: 0.0024921058860561626
***** Warning: Loss has increased *****
Loss at iteration [113]: 0.002492053136140851
Loss at iteration [114]: 0.0024911248317372047
Loss at iteration [115]: 0.002489684825365154
Loss at iteration [116]: 0.0024883790838601564
Loss at iteration [117]: 0.002487686362859067
Loss at iteration [118]: 0.002487566956067809
Loss at iteration [119]: 0.0024876273271424287
***** Warning: Loss has increased *****
Loss at iteration [120]: 0.002487444363388665
Loss at iteration [121]: 0.0024868495254677524
Loss at iteration [122]: 0.0024859935275299808
Loss at iteration [123]: 0.0024851652896633086
Loss at iteration [124]: 0.0024846037610614822
Loss at iteration [125]: 0.0024843390163240024
Loss at iteration [126]: 0.002484192710514595
Loss at iteration [127]: 0.002483937895181984
Loss at iteration [128]: 0.0024834632972581265
Loss at iteration [129]: 0.002482839884275305
Loss at iteration [130]: 0.002482244907279165
Loss at iteration [131]: 0.0024818033480786153
Loss at iteration [132]: 0.002481511388235754
Loss at iteration [133]: 0.0024812629518586754
Loss at iteration [134]: 0.002480944120253152
Loss at iteration [135]: 0.002480517281808357
Loss at iteration [136]: 0.0024800384980956645
Loss at iteration [137]: 0.0024795965295632792
Loss at iteration [138]: 0.002479235461509125
Loss at iteration [139]: 0.0024789396249336477
Loss at iteration [140]: 0.0024786516922701758
Loss at iteration [141]: 0.002478318968819059
Loss at iteration [142]: 0.002477938091664729
Loss at iteration [143]: 0.0024775418069064863
Loss at iteration [144]: 0.002477180251221913
Loss at iteration [145]: 0.002476859390300751
Loss at iteration [146]: 0.0024765583334205284
Loss at iteration [147]: 0.0024762517244180456
Loss at iteration [148]: 0.0024759277778244756
Loss at iteration [149]: 0.0024755784007906944
Loss at iteration [150]: 0.002475235410542479
Loss at iteration [151]: 0.002474917378580254
Loss at iteration [152]: 0.002474622326015507
Loss at iteration [153]: 0.002474333791823153
Loss at iteration [154]: 0.0024740391072732276
Loss at iteration [155]: 0.002473745522248914
Loss at iteration [156]: 0.0024734600955043586
Loss at iteration [157]: 0.0024731807393496186
Loss at iteration [158]: 0.0024729079249124234
Loss at iteration [159]: 0.0024726468287898195
Loss at iteration [160]: 0.002472382698884093
Loss at iteration [161]: 0.002472107882024935
Loss at iteration [162]: 0.0024718238266211246
Loss at iteration [163]: 0.0024715372725490977
Loss at iteration [164]: 0.0024712567209102193
Loss at iteration [165]: 0.0024709837099510246
Loss at iteration [166]: 0.0024707070064488254
Loss at iteration [167]: 0.0024704243389196325
Loss at iteration [168]: 0.0024701510978614857
Loss at iteration [169]: 0.0024698866681136205
Loss at iteration [170]: 0.00246961936311034
Loss at iteration [171]: 0.0024693504238204873
Loss at iteration [172]: 0.002469078471421787
Loss at iteration [173]: 0.002468803693243486
Loss at iteration [174]: 0.0024685286944373425
Loss at iteration [175]: 0.0024682598059226424
Loss at iteration [176]: 0.002467984202079099
Loss at iteration [177]: 0.0024677191711341448
Loss at iteration [178]: 0.0024674645946681085
Loss at iteration [179]: 0.0024672025840001294
Loss at iteration [180]: 0.00246693764954019
Loss at iteration [181]: 0.002466670816999043
Loss at iteration [182]: 0.0024664055397674686
Loss at iteration [183]: 0.0024661424961860204
Loss at iteration [184]: 0.0024658712683117653
Loss at iteration [185]: 0.0024655948132349197
Loss at iteration [186]: 0.002465326821870092
Loss at iteration [187]: 0.002465064107794662
Loss at iteration [188]: 0.0024647916649943175
Loss at iteration [189]: 0.0024645217186663295
Loss at iteration [190]: 0.0024642517765888394
Loss at iteration [191]: 0.0024639852078112913
Loss at iteration [192]: 0.0024637195380001493
Loss at iteration [193]: 0.0024634546030866907
Loss at iteration [194]: 0.0024631815971899495
Loss at iteration [195]: 0.0024629142151501183
Loss at iteration [196]: 0.0024626440227750344
Loss at iteration [197]: 0.00246237578406578
Loss at iteration [198]: 0.0024621067828367945
Loss at iteration [199]: 0.0024618375372451383
Loss at iteration [200]: 0.0024615736240294556
Loss at iteration [201]: 0.0024613225797715443
Loss at iteration [202]: 0.0024610697822528456
Loss at iteration [203]: 0.0024608114561296343
Loss at iteration [204]: 0.0024605666096936393
Loss at iteration [205]: 0.002460323105662891
Loss at iteration [206]: 0.0024600785559512695
Loss at iteration [207]: 0.0024598365530559643
Loss at iteration [208]: 0.002459590278030639
Loss at iteration [209]: 0.002459340216301055
Loss at iteration [210]: 0.0024590907097473617
Loss at iteration [211]: 0.0024588391579122484
Loss at iteration [212]: 0.002458594827021963
Loss at iteration [213]: 0.002458347152098938
Loss at iteration [214]: 0.0024581017390214606
Loss at iteration [215]: 0.0024578682809718155
Loss at iteration [216]: 0.0024576336166091456
Loss at iteration [217]: 0.002457403520187798
Loss at iteration [218]: 0.0024571715453064152
Loss at iteration [219]: 0.0024569372757384743
Loss at iteration [220]: 0.002456705700787221
Loss at iteration [221]: 0.002456475191699588
Loss at iteration [222]: 0.002456244161343687
Loss at iteration [223]: 0.002456015563825457
Loss at iteration [224]: 0.0024557940248779516
Loss at iteration [225]: 0.0024555660601113544
Loss at iteration [226]: 0.002455332271499966
Loss at iteration [227]: 0.0024551052169614414
Loss at iteration [228]: 0.0024548787126705287
Loss at iteration [229]: 0.002454643991907102
Loss at iteration [230]: 0.002454408085685754
Loss at iteration [231]: 0.0024541731757087926
Loss at iteration [232]: 0.0024539394020142782
Loss at iteration [233]: 0.002453701653637422
Loss at iteration [234]: 0.00245346448389623
Loss at iteration [235]: 0.002453228248037039
Loss at iteration [236]: 0.002452986595748311
Loss at iteration [237]: 0.0024527680242185834
Loss at iteration [238]: 0.0024525178593723987
Loss at iteration [239]: 0.0024522872040685442
Loss at iteration [240]: 0.0024520514866096664
Loss at iteration [241]: 0.002451814495049151
Loss at iteration [242]: 0.002451574611081613
Loss at iteration [243]: 0.002451338496096708
Loss at iteration [244]: 0.0024510985641012263
Loss at iteration [245]: 0.002450861285662847
Loss at iteration [246]: 0.002450629348362353
Loss at iteration [247]: 0.0024503879208291304
Loss at iteration [248]: 0.0024501584768149864
Loss at iteration [249]: 0.0024499241145117022
Loss at iteration [250]: 0.0024496871166632597
Loss at iteration [251]: 0.002449455310631068
Loss at iteration [252]: 0.0024492230446523213
Loss at iteration [253]: 0.0024489876904209107
Loss at iteration [254]: 0.0024487563823465995
Loss at iteration [255]: 0.0024485233273323927
Loss at iteration [256]: 0.002448290250215609
Loss at iteration [257]: 0.0024480471642206706
Loss at iteration [258]: 0.002447808235254309
Loss at iteration [259]: 0.0024475754681763367
Loss at iteration [260]: 0.002447341689369952
Loss at iteration [261]: 0.00244711981586659
Loss at iteration [262]: 0.0024468848472521
Loss at iteration [263]: 0.0024466422109249253
Loss at iteration [264]: 0.0024464072563804328
Loss at iteration [265]: 0.0024461701383302324
Loss at iteration [266]: 0.0024459448672556183
Loss at iteration [267]: 0.0024457152802140228
Loss at iteration [268]: 0.002445491290395197
Loss at iteration [269]: 0.0024452738168532773
Loss at iteration [270]: 0.002445057490576781
Loss at iteration [271]: 0.0024448409939048263
Loss at iteration [272]: 0.0024446209721265813
Loss at iteration [273]: 0.0024444017393400685
Loss at iteration [274]: 0.002444182739200348
Loss at iteration [275]: 0.0024439615937980343
Loss at iteration [276]: 0.002443739431390772
Loss at iteration [277]: 0.00244350743231017
Loss at iteration [278]: 0.0024432839187408127
Loss at iteration [279]: 0.002443057887557091
Loss at iteration [280]: 0.002442831447147954
Loss at iteration [281]: 0.0024426006348231364
Loss at iteration [282]: 0.002442378016291243
Loss at iteration [283]: 0.0024421501870138824
Loss at iteration [284]: 0.0024419292931922515
Loss at iteration [285]: 0.002441706570873516
Loss at iteration [286]: 0.0024414791378249484
Loss at iteration [287]: 0.0024412468727657314
Loss at iteration [288]: 0.0024410207502285003
Loss at iteration [289]: 0.002440795604765117
Loss at iteration [290]: 0.002440572903348598
Loss at iteration [291]: 0.002440350189086025
Loss at iteration [292]: 0.0024401258492431953
Loss at iteration [293]: 0.0024398926236013506
Loss at iteration [294]: 0.0024396545486873276
Loss at iteration [295]: 0.002439422504371693
Loss at iteration [296]: 0.0024391948669101227
Loss at iteration [297]: 0.002438970419757682
Loss at iteration [298]: 0.002438734799852114
Loss at iteration [299]: 0.002438499629246268
Loss at iteration [300]: 0.002438251830368597
Loss at iteration [301]: 0.002438002407477985
Loss at iteration [302]: 0.002437742803711668
Loss at iteration [303]: 0.002437492298248214
Loss at iteration [304]: 0.002437240944059695
Loss at iteration [305]: 0.002436989174407207
Loss at iteration [306]: 0.0024367305101394275
Loss at iteration [307]: 0.0024364702632258973
Loss at iteration [308]: 0.002436212819327955
Loss at iteration [309]: 0.0024359594440321714
Loss at iteration [310]: 0.0024357017402441374
Loss at iteration [311]: 0.0024354385980223238
Loss at iteration [312]: 0.0024351832695145383
Loss at iteration [313]: 0.0024349247701888586
Loss at iteration [314]: 0.0024346762524925306
Loss at iteration [315]: 0.0024344309991883905
Loss at iteration [316]: 0.0024341837339458875
Loss at iteration [317]: 0.002433957992327647
Loss at iteration [318]: 0.002433695938592615
Loss at iteration [319]: 0.0024334586622694826
Loss at iteration [320]: 0.002433224654514807
Loss at iteration [321]: 0.0024329807708661074
Loss at iteration [322]: 0.002432735529839022
Loss at iteration [323]: 0.002432489641866856
Loss at iteration [324]: 0.0024322378581123814
Loss at iteration [325]: 0.002431997734433718
Loss at iteration [326]: 0.0024317356789333574
Loss at iteration [327]: 0.0024314783335093733
Loss at iteration [328]: 0.002431220548667054
Loss at iteration [329]: 0.002430963424855988
Loss at iteration [330]: 0.002430711746621046
Loss at iteration [331]: 0.0024304535007274525
Loss at iteration [332]: 0.0024302110986432093
Loss at iteration [333]: 0.00242996769436932
Loss at iteration [334]: 0.0024297126444630854
Loss at iteration [335]: 0.00242945258490336
Loss at iteration [336]: 0.002429191943823573
Loss at iteration [337]: 0.002428938650280565
Loss at iteration [338]: 0.002428675232230481
Loss at iteration [339]: 0.0024284013582608864
Loss at iteration [340]: 0.002428112652509162
Loss at iteration [341]: 0.002427846228243036
Loss at iteration [342]: 0.002427574482064193
Loss at iteration [343]: 0.002427297242262621
Loss at iteration [344]: 0.002426999819352812
Loss at iteration [345]: 0.002426741253887593
Loss at iteration [346]: 0.002426478626644228
Loss at iteration [347]: 0.0024262229370873353
Loss at iteration [348]: 0.002425972609859966
Loss at iteration [349]: 0.002425713157289369
Loss at iteration [350]: 0.002425454221759235
Loss at iteration [351]: 0.0024252027316146083
Loss at iteration [352]: 0.002424952901806864
Loss at iteration [353]: 0.0024246932302142885
Loss at iteration [354]: 0.0024244343405165673
Loss at iteration [355]: 0.0024241793345910288
Loss at iteration [356]: 0.0024239293919463578
Loss at iteration [357]: 0.0024236922671323218
Loss at iteration [358]: 0.002423441090823972
Loss at iteration [359]: 0.0024231811446933115
Loss at iteration [360]: 0.002422910399738891
Loss at iteration [361]: 0.00242265781715944
Loss at iteration [362]: 0.0024224031167132606
Loss at iteration [363]: 0.002422157576736381
Loss at iteration [364]: 0.0024219024114296183
Loss at iteration [365]: 0.002421650479962513
Loss at iteration [366]: 0.0024213950376345606
Loss at iteration [367]: 0.0024211460045159947
Loss at iteration [368]: 0.002420892134641065
Loss at iteration [369]: 0.0024206369799417674
Loss at iteration [370]: 0.0024203814154874414
Loss at iteration [371]: 0.0024201207292338135
Loss at iteration [372]: 0.0024198693508150717
Loss at iteration [373]: 0.0024196121661918244
Loss at iteration [374]: 0.0024193488311084997
Loss at iteration [375]: 0.0024190928335592926
Loss at iteration [376]: 0.002418835648657799
Loss at iteration [377]: 0.002418572268520909
Loss at iteration [378]: 0.0024183229280599705
Loss at iteration [379]: 0.002418067354721229
Loss at iteration [380]: 0.0024178091521091243
Loss at iteration [381]: 0.002417556740828094
Loss at iteration [382]: 0.0024172990142957056
Loss at iteration [383]: 0.0024170401468593685
Loss at iteration [384]: 0.002416797623429367
Loss at iteration [385]: 0.002416546496668179
Loss at iteration [386]: 0.002416286773377078
Loss at iteration [387]: 0.0024160372205613884
Loss at iteration [388]: 0.0024157844222790414
Loss at iteration [389]: 0.0024155342475210635
Loss at iteration [390]: 0.002415279263850208
Loss at iteration [391]: 0.002415027306803972
Loss at iteration [392]: 0.0024147744875826663
Loss at iteration [393]: 0.002414518824751888
Loss at iteration [394]: 0.0024142691652090263
Loss at iteration [395]: 0.0024140148185067224
Loss at iteration [396]: 0.002413753773750309
Loss at iteration [397]: 0.0024134956526430645
Loss at iteration [398]: 0.0024132377953373597
Loss at iteration [399]: 0.0024129862477986074
Loss at iteration [400]: 0.0024127240090064724
Loss at iteration [401]: 0.002412464352394108
Loss at iteration [402]: 0.00241220651732018
Loss at iteration [403]: 0.002411959242062376
Loss at iteration [404]: 0.0024117013657467016
Loss at iteration [405]: 0.002411441520516473
Loss at iteration [406]: 0.002411187429849857
Loss at iteration [407]: 0.002410931344225837
Loss at iteration [408]: 0.0024106706681105983
Loss at iteration [409]: 0.002410411312901133
Loss at iteration [410]: 0.0024101478057345334
Loss at iteration [411]: 0.002409889619295521
Loss at iteration [412]: 0.002409627030350748
Loss at iteration [413]: 0.002409362909756815
Loss at iteration [414]: 0.0024091032580888672
Loss at iteration [415]: 0.002408842539340412
Loss at iteration [416]: 0.002408576032636305
Loss at iteration [417]: 0.002408311352473405
Loss at iteration [418]: 0.0024080492051052845
Loss at iteration [419]: 0.002407778210256005
Loss at iteration [420]: 0.0024075083549570486
Loss at iteration [421]: 0.0024072436128760775
Loss at iteration [422]: 0.0024069721168014247
Loss at iteration [423]: 0.002406695396546127
Loss at iteration [424]: 0.0024064194933882395
Loss at iteration [425]: 0.002406148085601247
Loss at iteration [426]: 0.002405871517932088
Loss at iteration [427]: 0.002405580091557642
Loss at iteration [428]: 0.002405278256230018
Loss at iteration [429]: 0.0024049779266568034
Loss at iteration [430]: 0.002404680932330243
Loss at iteration [431]: 0.002404376156757496
Loss at iteration [432]: 0.0024040726652056923
Loss at iteration [433]: 0.002403769936503478
Loss at iteration [434]: 0.002403464316191976
Loss at iteration [435]: 0.0024031614592167987
Loss at iteration [436]: 0.0024028538745234636
Loss at iteration [437]: 0.002402550122596758
Loss at iteration [438]: 0.0024022416394438055
Loss at iteration [439]: 0.0024019447451056005
Loss at iteration [440]: 0.0024016550147031497
Loss at iteration [441]: 0.0024013646114263974
Loss at iteration [442]: 0.0024010784498700974
Loss at iteration [443]: 0.002400800401970887
Loss at iteration [444]: 0.002400512599335289
Loss at iteration [445]: 0.0024002211937836923
Loss at iteration [446]: 0.0023999260018993528
Loss at iteration [447]: 0.002399640968037906
Loss at iteration [448]: 0.0023993835127780324
Loss at iteration [449]: 0.0023991252967039823
Loss at iteration [450]: 0.002398857600627184
Loss at iteration [451]: 0.002398586158715609
Loss at iteration [452]: 0.002398313141789412
Loss at iteration [453]: 0.0023980496147996276
Loss at iteration [454]: 0.002397781873974346
Loss at iteration [455]: 0.0023975062799926365
Loss at iteration [456]: 0.0023972370273333057
Loss at iteration [457]: 0.0023969723384442664
Loss at iteration [458]: 0.0023967014250799246
Loss at iteration [459]: 0.0023964307564580682
Loss at iteration [460]: 0.002396157581785837
Loss at iteration [461]: 0.0023958926471964125
Loss at iteration [462]: 0.0023956271189651984
Loss at iteration [463]: 0.0023953633480839507
Loss at iteration [464]: 0.002395101644813142
Loss at iteration [465]: 0.002394838621224086
Loss at iteration [466]: 0.0023945745884899365
Loss at iteration [467]: 0.0023943018317872863
Loss at iteration [468]: 0.0023940365346267465
Loss at iteration [469]: 0.002393771097556491
Loss at iteration [470]: 0.002393491487180233
Loss at iteration [471]: 0.0023932066341731074
Loss at iteration [472]: 0.002392920967487892
Loss at iteration [473]: 0.0023926234704250934
Loss at iteration [474]: 0.00239231248948353
Loss at iteration [475]: 0.0023919875874060625
Loss at iteration [476]: 0.002391671392368389
Loss at iteration [477]: 0.0023913585731998384
Loss at iteration [478]: 0.0023910521039895946
Loss at iteration [479]: 0.002390729042704538
Loss at iteration [480]: 0.0023904163202964914
Loss at iteration [481]: 0.002390112044309612
Loss at iteration [482]: 0.0023898540486078463
Loss at iteration [483]: 0.002389565144064142
Loss at iteration [484]: 0.0023892764350324953
Loss at iteration [485]: 0.0023890006626893453
Loss at iteration [486]: 0.002388722030518534
Loss at iteration [487]: 0.0023884469368190627
Loss at iteration [488]: 0.002388170999039742
Loss at iteration [489]: 0.002387887555137824
Loss at iteration [490]: 0.0023876089479631718
Loss at iteration [491]: 0.0023873240550803657
Loss at iteration [492]: 0.002387038327150224
Loss at iteration [493]: 0.002386747181239931
Loss at iteration [494]: 0.002386455505063301
Loss at iteration [495]: 0.0023861778746489125
Loss at iteration [496]: 0.0023858908364326496
Loss at iteration [497]: 0.002385603679850975
Loss at iteration [498]: 0.002385316652901278
Loss at iteration [499]: 0.0023850281206772637
Loss at iteration [500]: 0.0023847506383323945
Loss at iteration [501]: 0.0023844662703065935
Loss at iteration [502]: 0.0023841792919212425
Loss at iteration [503]: 0.0023838915205267996
Loss at iteration [504]: 0.0023836068134113608
Loss at iteration [505]: 0.0023833151890886857
Loss at iteration [506]: 0.0023830190096961945
Loss at iteration [507]: 0.002382725949774071
Loss at iteration [508]: 0.002382426527395355
Loss at iteration [509]: 0.0023821249507628914
Loss at iteration [510]: 0.002381834569444453
Loss at iteration [511]: 0.0023815477967180946
Loss at iteration [512]: 0.002381254095135513
Loss at iteration [513]: 0.0023809520482437866
Loss at iteration [514]: 0.002380660439217739
Loss at iteration [515]: 0.002380364175389861
Loss at iteration [516]: 0.002380065898795771
Loss at iteration [517]: 0.002379770848937792
Loss at iteration [518]: 0.002379466949385028
Loss at iteration [519]: 0.0023791698451344715
Loss at iteration [520]: 0.0023788728684457873
Loss at iteration [521]: 0.0023785738109098307
Loss at iteration [522]: 0.0023782779044157817
Loss at iteration [523]: 0.002377983121180095
Loss at iteration [524]: 0.0023776979649144125
Loss at iteration [525]: 0.0023774052861454736
Loss at iteration [526]: 0.002377111379390794
Loss at iteration [527]: 0.0023768226673452687
Loss at iteration [528]: 0.0023765351498679305
Loss at iteration [529]: 0.002376245798132061
Loss at iteration [530]: 0.0023759599099216146
Loss at iteration [531]: 0.0023756657752164825
Loss at iteration [532]: 0.0023753657129430717
Loss at iteration [533]: 0.002375075222258249
Loss at iteration [534]: 0.002374782246810459
Loss at iteration [535]: 0.0023744813583011183
Loss at iteration [536]: 0.0023741813568971216
Loss at iteration [537]: 0.0023738879190456762
Loss at iteration [538]: 0.002373591275677291
Loss at iteration [539]: 0.0023732884766942855
Loss at iteration [540]: 0.0023729990133946176
Loss at iteration [541]: 0.002372699544119612
Loss at iteration [542]: 0.0023723956277293436
Loss at iteration [543]: 0.0023721016062465522
Loss at iteration [544]: 0.0023717966988663726
Loss at iteration [545]: 0.0023714966486857655
Loss at iteration [546]: 0.002371196338349282
Loss at iteration [547]: 0.002370895939133782
Loss at iteration [548]: 0.0023705975111262337
Loss at iteration [549]: 0.0023702927239956494
Loss at iteration [550]: 0.0023699942913373052
Loss at iteration [551]: 0.0023696940690782447
Loss at iteration [552]: 0.0023693836774827067
Loss at iteration [553]: 0.0023690776298383605
Loss at iteration [554]: 0.0023687876804530904
Loss at iteration [555]: 0.0023684732871859463
Loss at iteration [556]: 0.00236816686946779
Loss at iteration [557]: 0.0023678696463207125
Loss at iteration [558]: 0.0023675596793725543
Loss at iteration [559]: 0.0023672347540615426
Loss at iteration [560]: 0.0023669285886537884
Loss at iteration [561]: 0.0023666245520586665
Loss at iteration [562]: 0.002366302127052977
Loss at iteration [563]: 0.002365990310207839
Loss at iteration [564]: 0.002365679301704527
Loss at iteration [565]: 0.0023653614822033518
Loss at iteration [566]: 0.0023650477568701806
Loss at iteration [567]: 0.002364744934060042
Loss at iteration [568]: 0.002364411602539501
Loss at iteration [569]: 0.0023640908922620565
Loss at iteration [570]: 0.002363783772922584
Loss at iteration [571]: 0.0023634603346308286
Loss at iteration [572]: 0.002363128845843605
Loss at iteration [573]: 0.0023628263295229117
Loss at iteration [574]: 0.002362509688375417
Loss at iteration [575]: 0.002362183664022213
Loss at iteration [576]: 0.0023618570680767
Loss at iteration [577]: 0.0023615385022583836
Loss at iteration [578]: 0.0023612141177998206
Loss at iteration [579]: 0.0023608918765512577
Loss at iteration [580]: 0.002360564195722201
Loss at iteration [581]: 0.002360238466807358
Loss at iteration [582]: 0.0023599193093589545
Loss at iteration [583]: 0.0023596075343747173
Loss at iteration [584]: 0.002359287241022855
Loss at iteration [585]: 0.0023589693961454604
Loss at iteration [586]: 0.0023586570072852584
Loss at iteration [587]: 0.00235834098367064
Loss at iteration [588]: 0.0023580021470794854
Loss at iteration [589]: 0.0023576982746561148
Loss at iteration [590]: 0.002357387849146277
Loss at iteration [591]: 0.002357052978201449
Loss at iteration [592]: 0.002356725148028803
Loss at iteration [593]: 0.0023564147079975
Loss at iteration [594]: 0.002356102640180492
Loss at iteration [595]: 0.002355774072131474
Loss at iteration [596]: 0.0023554465680783957
Loss at iteration [597]: 0.002355133068239586
Loss at iteration [598]: 0.002354811019247805
Loss at iteration [599]: 0.00235447920342454
Loss at iteration [600]: 0.002354152931088567
Loss at iteration [601]: 0.002353837522918213
Loss at iteration [602]: 0.002353509159865178
Loss at iteration [603]: 0.002353181240868095
Loss at iteration [604]: 0.0023528526837296986
Loss at iteration [605]: 0.0023525397639704796
Loss at iteration [606]: 0.0023522098494830907
Loss at iteration [607]: 0.002351878615661608
Loss at iteration [608]: 0.0023515531995564583
Loss at iteration [609]: 0.0023512333847438417
Loss at iteration [610]: 0.002350906580285148
Loss at iteration [611]: 0.0023505791650921763
Loss at iteration [612]: 0.0023502481352562763
Loss at iteration [613]: 0.0023499218602981635
Loss at iteration [614]: 0.002349578584629034
Loss at iteration [615]: 0.002349252887461955
Loss at iteration [616]: 0.0023489284743320417
Loss at iteration [617]: 0.002348597472740302
Loss at iteration [618]: 0.002348257249305185
Loss at iteration [619]: 0.002347919828833766
Loss at iteration [620]: 0.0023476072186596063
Loss at iteration [621]: 0.002347276372548951
Loss at iteration [622]: 0.0023469360648554647
Loss at iteration [623]: 0.002346604907468794
Loss at iteration [624]: 0.0023462706900026376
Loss at iteration [625]: 0.002345917784224962
Loss at iteration [626]: 0.002345580436531182
Loss at iteration [627]: 0.0023452586932269136
Loss at iteration [628]: 0.0023449190349918336
Loss at iteration [629]: 0.0023445827383361583
Loss at iteration [630]: 0.002344251915411839
Loss at iteration [631]: 0.002343925282914716
Loss at iteration [632]: 0.002343594733871422
Loss at iteration [633]: 0.0023432502310910507
Loss at iteration [634]: 0.0023429236085584747
Loss at iteration [635]: 0.002342597003934513
Loss at iteration [636]: 0.00234225776875321
Loss at iteration [637]: 0.002341918779673848
Loss at iteration [638]: 0.0023415860157752198
Loss at iteration [639]: 0.002341255716372081
Loss at iteration [640]: 0.0023409148990200294
Loss at iteration [641]: 0.002340582474631882
Loss at iteration [642]: 0.0023402499875662497
Loss at iteration [643]: 0.0023399017137004227
Loss at iteration [644]: 0.0023395776561580648
Loss at iteration [645]: 0.0023392421565080405
Loss at iteration [646]: 0.002338898921868587
Loss at iteration [647]: 0.0023385681171801697
Loss at iteration [648]: 0.0023382379214785475
Loss at iteration [649]: 0.0023378992024313803
Loss at iteration [650]: 0.0023375670693100556
Loss at iteration [651]: 0.00233723836104465
Loss at iteration [652]: 0.002336916515234745
Loss at iteration [653]: 0.002336574895342433
Loss at iteration [654]: 0.002336244072169709
Loss at iteration [655]: 0.0023359138801364553
Loss at iteration [656]: 0.0023355866610010245
Loss at iteration [657]: 0.0023352447174435705
Loss at iteration [658]: 0.0023348978962079306
Loss at iteration [659]: 0.002334558633239421
Loss at iteration [660]: 0.002334226377831829
Loss at iteration [661]: 0.0023338961924843824
Loss at iteration [662]: 0.0023335543006291056
Loss at iteration [663]: 0.0023332039168249444
Loss at iteration [664]: 0.002332866573351402
Loss at iteration [665]: 0.002332529229682079
Loss at iteration [666]: 0.0023321831831109906
Loss at iteration [667]: 0.002331841703263208
Loss at iteration [668]: 0.002331494970893415
Loss at iteration [669]: 0.0023311507455704094
Loss at iteration [670]: 0.0023308148937889878
Loss at iteration [671]: 0.0023304637667884505
Loss at iteration [672]: 0.002330121812313063
Loss at iteration [673]: 0.002329772193252616
Loss at iteration [674]: 0.002329436683604995
Loss at iteration [675]: 0.002329089916783398
Loss at iteration [676]: 0.0023287428551524124
Loss at iteration [677]: 0.0023284104090734027
Loss at iteration [678]: 0.0023280576977119938
Loss at iteration [679]: 0.0023277113125288757
Loss at iteration [680]: 0.0023273685226205144
Loss at iteration [681]: 0.0023270270059844644
Loss at iteration [682]: 0.0023266820264699136
Loss at iteration [683]: 0.002326324693670307
Loss at iteration [684]: 0.002325975285855705
Loss at iteration [685]: 0.0023256391408679063
Loss at iteration [686]: 0.0023252747105743468
Loss at iteration [687]: 0.002324935294026817
Loss at iteration [688]: 0.002324581523538014
Loss at iteration [689]: 0.002324247084447281
Loss at iteration [690]: 0.002323900028747282
Loss at iteration [691]: 0.0023235283504032776
Loss at iteration [692]: 0.002323172896181359
Loss at iteration [693]: 0.0023228312981738927
Loss at iteration [694]: 0.0023224859557163836
Loss at iteration [695]: 0.002322124878936447
Loss at iteration [696]: 0.002321784228084315
Loss at iteration [697]: 0.002321430746180281
Loss at iteration [698]: 0.0023210745866092614
Loss at iteration [699]: 0.0023207461391385373
Loss at iteration [700]: 0.0023203992467433297
Loss at iteration [701]: 0.002320049601185019
Loss at iteration [702]: 0.002319708047882547
Loss at iteration [703]: 0.0023193831495598007
Loss at iteration [704]: 0.002319025572082311
Loss at iteration [705]: 0.002318681652812878
Loss at iteration [706]: 0.00231835578440809
Loss at iteration [707]: 0.0023180128550633057
Loss at iteration [708]: 0.0023176523097654445
Loss at iteration [709]: 0.0023173036696929627
Loss at iteration [710]: 0.002316960993901112
Loss at iteration [711]: 0.0023165966158883344
Loss at iteration [712]: 0.00231621834202884
Loss at iteration [713]: 0.0023158616384199634
Loss at iteration [714]: 0.0023155281368302904
Loss at iteration [715]: 0.0023151635404157693
Loss at iteration [716]: 0.0023147792061532337
Loss at iteration [717]: 0.0023144126942336997
Loss at iteration [718]: 0.00231403318689209
Loss at iteration [719]: 0.00231366442185312
Loss at iteration [720]: 0.0023132884794008018
Loss at iteration [721]: 0.0023128927173309472
Loss at iteration [722]: 0.002312529995820016
Loss at iteration [723]: 0.00231213823159067
Loss at iteration [724]: 0.0023117524435724754
Loss at iteration [725]: 0.002311372040142387
Loss at iteration [726]: 0.0023109639595127788
Loss at iteration [727]: 0.0023105975707323253
Loss at iteration [728]: 0.002310218879717422
Loss at iteration [729]: 0.0023098311889932803
Loss at iteration [730]: 0.0023093962162016295
Loss at iteration [731]: 0.002308968347449112
Loss at iteration [732]: 0.0023085865885114675
Loss at iteration [733]: 0.0023082010036262882
Loss at iteration [734]: 0.002307825030714291
Loss at iteration [735]: 0.0023074612595346536
Loss at iteration [736]: 0.00230709993810796
Loss at iteration [737]: 0.002306716199518272
Loss at iteration [738]: 0.0023063509188718167
Loss at iteration [739]: 0.002306001123387767
Loss at iteration [740]: 0.0023056252768813147
Loss at iteration [741]: 0.002305277204985616
Loss at iteration [742]: 0.0023048903108492894
Loss at iteration [743]: 0.002304543430914818
Loss at iteration [744]: 0.002304167368005343
Loss at iteration [745]: 0.0023038413992160786
Loss at iteration [746]: 0.002303443003298955
Loss at iteration [747]: 0.0023031016874559586
Loss at iteration [748]: 0.0023027751917404637
Loss at iteration [749]: 0.002302414633109594
Loss at iteration [750]: 0.0023020275939338096
Loss at iteration [751]: 0.0023016335618914353
Loss at iteration [752]: 0.0023012716081913806
Loss at iteration [753]: 0.002300922960834418
Loss at iteration [754]: 0.0023005528260169944
Loss at iteration [755]: 0.002300190877686261
Loss at iteration [756]: 0.0022998135378781088
Loss at iteration [757]: 0.0022994333873768208
Loss at iteration [758]: 0.002299092510980553
Loss at iteration [759]: 0.0022986903318554883
Loss at iteration [760]: 0.002298345270995845
Loss at iteration [761]: 0.002297954845375263
Loss at iteration [762]: 0.0022976107859155304
Loss at iteration [763]: 0.002297257762850457
Loss at iteration [764]: 0.002296888580145077
Loss at iteration [765]: 0.002296515304402236
Loss at iteration [766]: 0.0022961481306799985
Loss at iteration [767]: 0.0022958233281058496
Loss at iteration [768]: 0.002295413068338329
Loss at iteration [769]: 0.0022950563365553476
Loss at iteration [770]: 0.0022947031591312632
Loss at iteration [771]: 0.0022943462152982775
Loss at iteration [772]: 0.002293977510641278
Loss at iteration [773]: 0.0022936245984957306
Loss at iteration [774]: 0.002293280636844606
Loss at iteration [775]: 0.0022929214192202968
Loss at iteration [776]: 0.0022925692293626924
Loss at iteration [777]: 0.0022921966632655564
Loss at iteration [778]: 0.0022918159592306407
Loss at iteration [779]: 0.002291529536510108
Loss at iteration [780]: 0.002291140174636315
Loss at iteration [781]: 0.0022907707399944283
Loss at iteration [782]: 0.0022904471664754052
Loss at iteration [783]: 0.00229011934252525
Loss at iteration [784]: 0.0022897654024765578
Loss at iteration [785]: 0.0022894132860564456
Loss at iteration [786]: 0.00228903673149411
Loss at iteration [787]: 0.002288660851125941
Loss at iteration [788]: 0.0022883421910539813
Loss at iteration [789]: 0.0022879778958556584
Loss at iteration [790]: 0.0022876259159802445
Loss at iteration [791]: 0.002287283790854147
Loss at iteration [792]: 0.0022869453038746033
Loss at iteration [793]: 0.0022865967293751177
Loss at iteration [794]: 0.0022862279113307895
Loss at iteration [795]: 0.0022858626129564864
Loss at iteration [796]: 0.002285586437449253
Loss at iteration [797]: 0.0022852254270452795
Loss at iteration [798]: 0.002284853279804312
Loss at iteration [799]: 0.0022845360454400287
Loss at iteration [800]: 0.002284195945891468
Loss at iteration [801]: 0.0022838410557650336
Loss at iteration [802]: 0.0022834862716381308
Loss at iteration [803]: 0.0022831197262401406
Loss at iteration [804]: 0.002282866290270304
Loss at iteration [805]: 0.0022825067184302043
Loss at iteration [806]: 0.002282084612756404
Loss at iteration [807]: 0.00228177518653476
Loss at iteration [808]: 0.0022814358718421007
Loss at iteration [809]: 0.0022811076265329857
Loss at iteration [810]: 0.0022807468866035967
Loss at iteration [811]: 0.002280463804841439
Loss at iteration [812]: 0.002280114922793742
Loss at iteration [813]: 0.00227978190320387
Loss at iteration [814]: 0.0022794576509716225
Loss at iteration [815]: 0.002279149985396634
Loss at iteration [816]: 0.0022787811185835637
Loss at iteration [817]: 0.002278440753416177
Loss at iteration [818]: 0.00227817381269306
Loss at iteration [819]: 0.0022778147879906822
Loss at iteration [820]: 0.00227744684727512
Loss at iteration [821]: 0.0022771195586518416
Loss at iteration [822]: 0.0022767850562727824
Loss at iteration [823]: 0.0022764435325356987
Loss at iteration [824]: 0.00227609994172465
Loss at iteration [825]: 0.002275751608304133
Loss at iteration [826]: 0.0022754159650300075
Loss at iteration [827]: 0.002275142332380183
Loss at iteration [828]: 0.0022747878136701876
Loss at iteration [829]: 0.0022744933801262957
Loss at iteration [830]: 0.0022741905941434525
Loss at iteration [831]: 0.0022738431726934713
Loss at iteration [832]: 0.0022734884354000235
Loss at iteration [833]: 0.002273134696468997
Loss at iteration [834]: 0.002272854083983039
Loss at iteration [835]: 0.0022725199117215613
Loss at iteration [836]: 0.0022721200711117
Loss at iteration [837]: 0.0022718073800368065
Loss at iteration [838]: 0.002271467921368291
Loss at iteration [839]: 0.002271177675708558
Loss at iteration [840]: 0.002270815702206549
Loss at iteration [841]: 0.0022705524114135607
Loss at iteration [842]: 0.0022702337010669033
Loss at iteration [843]: 0.002269904845842153
Loss at iteration [844]: 0.0022695546376563995
Loss at iteration [845]: 0.002269171645621966
Loss at iteration [846]: 0.0022688329103514467
Loss at iteration [847]: 0.002268545369849413
Loss at iteration [848]: 0.0022682173200032447
Loss at iteration [849]: 0.0022678528076580965
Loss at iteration [850]: 0.002267545357708875
Loss at iteration [851]: 0.0022671583534599805
Loss at iteration [852]: 0.0022668531996888944
Loss at iteration [853]: 0.002266543817687039
Loss at iteration [854]: 0.002266158212961257
Loss at iteration [855]: 0.0022657824795303495
Loss at iteration [856]: 0.002265392121085331
Loss at iteration [857]: 0.002265211383796692
Loss at iteration [858]: 0.0022648194581074135
Loss at iteration [859]: 0.002264444004816984
Loss at iteration [860]: 0.0022641433389972224
Loss at iteration [861]: 0.002263814153504074
Loss at iteration [862]: 0.0022634684633394
Loss at iteration [863]: 0.002263140027101405
Loss at iteration [864]: 0.00226275149404485
Loss at iteration [865]: 0.0022624524369274594
Loss at iteration [866]: 0.0022620182931144066
Loss at iteration [867]: 0.0022616216970926693
Loss at iteration [868]: 0.002261359525852038
Loss at iteration [869]: 0.0022610386371013664
Loss at iteration [870]: 0.0022606977324539446
Loss at iteration [871]: 0.0022603589566227056
Loss at iteration [872]: 0.002260164703876054
Loss at iteration [873]: 0.0022597269041965875
Loss at iteration [874]: 0.0022594236424320715
Loss at iteration [875]: 0.002259105488230521
Loss at iteration [876]: 0.0022587308028507954
Loss at iteration [877]: 0.0022584729074852715
Loss at iteration [878]: 0.002258190233281045
Loss at iteration [879]: 0.0022578854409579323
Loss at iteration [880]: 0.002257529788209346
Loss at iteration [881]: 0.0022572306885228583
Loss at iteration [882]: 0.002256904756862131
Loss at iteration [883]: 0.002256634533403218
Loss at iteration [884]: 0.002256339038438029
Loss at iteration [885]: 0.0022560432004293976
Loss at iteration [886]: 0.002255712044613195
Loss at iteration [887]: 0.002255489131696567
Loss at iteration [888]: 0.002255101687013164
Loss at iteration [889]: 0.002254888673647011
Loss at iteration [890]: 0.0022546342359146365
Loss at iteration [891]: 0.0022543411101718287
Loss at iteration [892]: 0.002253986348362789
Loss at iteration [893]: 0.0022536839223364896
Loss at iteration [894]: 0.002253358560864825
Loss at iteration [895]: 0.002253079496706804
Loss at iteration [896]: 0.002252854861905842
Loss at iteration [897]: 0.0022525504196010647
Loss at iteration [898]: 0.002252207933961389
Loss at iteration [899]: 0.002251915750731365
Loss at iteration [900]: 0.002251627788467733
Loss at iteration [901]: 0.002251352996807457
Loss at iteration [902]: 0.002251090303882743
Loss at iteration [903]: 0.002250751584143965
Loss at iteration [904]: 0.002250452182286317
Loss at iteration [905]: 0.002250225098605965
Loss at iteration [906]: 0.0022498778908106097
Loss at iteration [907]: 0.002249603388490215
Loss at iteration [908]: 0.0022493785635662802
Loss at iteration [909]: 0.002249074105026202
Loss at iteration [910]: 0.0022487510470425823
Loss at iteration [911]: 0.0022485689382130006
Loss at iteration [912]: 0.0022481906992238157
Loss at iteration [913]: 0.0022480038195158256
Loss at iteration [914]: 0.0022477224640284716
Loss at iteration [915]: 0.0022474235840043585
Loss at iteration [916]: 0.0022470253937194526
Loss at iteration [917]: 0.0022468718037233827
Loss at iteration [918]: 0.00224643767095988
Loss at iteration [919]: 0.002246050158704312
Loss at iteration [920]: 0.002245738936683904
Loss at iteration [921]: 0.0022455285544075465
Loss at iteration [922]: 0.0022452545571864228
Loss at iteration [923]: 0.002245132050435924
Loss at iteration [924]: 0.002244788097074203
Loss at iteration [925]: 0.002244427109322163
Loss at iteration [926]: 0.0022441997725482585
Loss at iteration [927]: 0.0022438375493551393
Loss at iteration [928]: 0.0022435506571377778
Loss at iteration [929]: 0.002243424667050299
Loss at iteration [930]: 0.0022430639847293783
Loss at iteration [931]: 0.0022426469605081453
Loss at iteration [932]: 0.0022423448694735455
Loss at iteration [933]: 0.002242080330438684
Loss at iteration [934]: 0.0022418392667671923
Loss at iteration [935]: 0.0022415371741645847
Loss at iteration [936]: 0.002241284519079573
Loss at iteration [937]: 0.002240960130655208
Loss at iteration [938]: 0.002240725847215705
Loss at iteration [939]: 0.002240454966487787
Loss at iteration [940]: 0.002240151444300328
Loss at iteration [941]: 0.0022398442454584127
Loss at iteration [942]: 0.0022396576214772823
Loss at iteration [943]: 0.002239456385800194
Loss at iteration [944]: 0.002238996148456544
Loss at iteration [945]: 0.0022388066098095382
Loss at iteration [946]: 0.002238613191006328
Loss at iteration [947]: 0.002238323247566223
Loss at iteration [948]: 0.002238064530240185
Loss at iteration [949]: 0.0022376911156319467
Loss at iteration [950]: 0.002237332441418135
Loss at iteration [951]: 0.0022372202867321487
Loss at iteration [952]: 0.0022370704211705774
Loss at iteration [953]: 0.0022366093133763944
Loss at iteration [954]: 0.0022363572915464415
Loss at iteration [955]: 0.0022361465159400516
Loss at iteration [956]: 0.002235731925869503
Loss at iteration [957]: 0.0022355837925615418
Loss at iteration [958]: 0.002235249296160538
Loss at iteration [959]: 0.0022350146708566988
Loss at iteration [960]: 0.0022347763897907283
Loss at iteration [961]: 0.0022343559805993556
Loss at iteration [962]: 0.0022340243277969235
Loss at iteration [963]: 0.0022338847809158487
Loss at iteration [964]: 0.0022337200627499296
Loss at iteration [965]: 0.0022333707089199953
Loss at iteration [966]: 0.0022328850936010622
Loss at iteration [967]: 0.002232695123927133
Loss at iteration [968]: 0.0022324735011647963
Loss at iteration [969]: 0.0022322393313439805
Loss at iteration [970]: 0.0022318362604824057
Loss at iteration [971]: 0.0022315987831605685
Loss at iteration [972]: 0.002231466875604573
Loss at iteration [973]: 0.0022309399759757337
Loss at iteration [974]: 0.0022306258364345544
Loss at iteration [975]: 0.0022304729345904916
Loss at iteration [976]: 0.0022302794969867265
Loss at iteration [977]: 0.002229858267217654
Loss at iteration [978]: 0.002229441919232335
Loss at iteration [979]: 0.0022291520092578686
Loss at iteration [980]: 0.0022290246983284676
Loss at iteration [981]: 0.00222864497801113
Loss at iteration [982]: 0.0022284970950478458
Loss at iteration [983]: 0.0022283307338813296
Loss at iteration [984]: 0.0022279122587250556
Loss at iteration [985]: 0.002227691455460241
Loss at iteration [986]: 0.002227510562602009
Loss at iteration [987]: 0.0022272130309234743
Loss at iteration [988]: 0.002226669742893152
Loss at iteration [989]: 0.00222669577285687
***** Warning: Loss has increased *****
Loss at iteration [990]: 0.0022265306669073935
Loss at iteration [991]: 0.002226100227871727
Loss at iteration [992]: 0.0022257478070628924
Loss at iteration [993]: 0.0022254633356511323
Loss at iteration [994]: 0.0022252801371328893
Loss at iteration [995]: 0.0022249259188441583
Loss at iteration [996]: 0.0022247040995338457
Loss at iteration [997]: 0.002224502951953816
Loss at iteration [998]: 0.0022242441675056155
Loss at iteration [999]: 0.0022238584216092076
Loss at iteration [1000]: 0.002223738375732026
Loss at iteration [1001]: 0.0022234672009970565
Loss at iteration [1002]: 0.0022231566221599317
Loss at iteration [1003]: 0.0022229018929039207
Loss at iteration [1004]: 0.0022225869428243623
Loss at iteration [1005]: 0.002222349140402056
Loss at iteration [1006]: 0.0022220943631456307
Loss at iteration [1007]: 0.0022219838634274175
Loss at iteration [1008]: 0.002221611883993183
Loss at iteration [1009]: 0.0022214294015995405
Loss at iteration [1010]: 0.002221214587406644
Loss at iteration [1011]: 0.002220814076637015
Loss at iteration [1012]: 0.0022206853075675175
Loss at iteration [1013]: 0.0022204592477477613
Loss at iteration [1014]: 0.002220200636450181
Loss at iteration [1015]: 0.0022197378471516087
Loss at iteration [1016]: 0.0022197469501361367
***** Warning: Loss has increased *****
Loss at iteration [1017]: 0.0022195727924574312
Loss at iteration [1018]: 0.0022192344899948312
Loss at iteration [1019]: 0.002218900391752264
Loss at iteration [1020]: 0.0022187970188971805
Loss at iteration [1021]: 0.0022186454554827327
Loss at iteration [1022]: 0.0022182550675169596
Loss at iteration [1023]: 0.0022178633205621055
Loss at iteration [1024]: 0.002217780712474795
Loss at iteration [1025]: 0.0022176639023404267
Loss at iteration [1026]: 0.002217226892698518
Loss at iteration [1027]: 0.002216908636585744
Loss at iteration [1028]: 0.002216816998176549
Loss at iteration [1029]: 0.0022165268179489884
Loss at iteration [1030]: 0.0022162373640474456
Loss at iteration [1031]: 0.0022160339345178685
Loss at iteration [1032]: 0.0022157016915912305
Loss at iteration [1033]: 0.002215544314567874
Loss at iteration [1034]: 0.0022153340748276252
Loss at iteration [1035]: 0.0022150430868495474
Loss at iteration [1036]: 0.0022147721191227415
Loss at iteration [1037]: 0.0022145588114344866
Loss at iteration [1038]: 0.0022141752214398526
Loss at iteration [1039]: 0.0022139462860790685
Loss at iteration [1040]: 0.0022137862470003574
Loss at iteration [1041]: 0.002213428475973066
Loss at iteration [1042]: 0.0022133736884750285
Loss at iteration [1043]: 0.0022130977200597907
Loss at iteration [1044]: 0.002212830649947375
Loss at iteration [1045]: 0.0022125536055155937
Loss at iteration [1046]: 0.0022122131537682535
Loss at iteration [1047]: 0.0022119765309733383
Loss at iteration [1048]: 0.0022117632127824673
Loss at iteration [1049]: 0.0022114546714639485
Loss at iteration [1050]: 0.002211264130070249
Loss at iteration [1051]: 0.002211130433092277
Loss at iteration [1052]: 0.002210748011838248
Loss at iteration [1053]: 0.0022106934409246775
Loss at iteration [1054]: 0.0022104673475939218
Loss at iteration [1055]: 0.002210019248037959
Loss at iteration [1056]: 0.002209855422982654
Loss at iteration [1057]: 0.0022095807708921184
Loss at iteration [1058]: 0.0022093860716942857
Loss at iteration [1059]: 0.0022092274846614197
Loss at iteration [1060]: 0.002208972347419287
Loss at iteration [1061]: 0.0022087156021765686
Loss at iteration [1062]: 0.0022085143093999813
Loss at iteration [1063]: 0.00220818837893534
Loss at iteration [1064]: 0.002207959070769514
Loss at iteration [1065]: 0.002207693779308842
Loss at iteration [1066]: 0.002207309406494253
Loss at iteration [1067]: 0.0022072523652087346
Loss at iteration [1068]: 0.0022071529171812597
Loss at iteration [1069]: 0.002206790242103934
Loss at iteration [1070]: 0.002206368259169748
Loss at iteration [1071]: 0.002206238646967017
Loss at iteration [1072]: 0.002205943708475092
Loss at iteration [1073]: 0.002205796073001484
Loss at iteration [1074]: 0.0022055059010530936
Loss at iteration [1075]: 0.002205340832376212
Loss at iteration [1076]: 0.0022049806854353973
Loss at iteration [1077]: 0.002204883691924539
Loss at iteration [1078]: 0.002204569515670977
Loss at iteration [1079]: 0.002204308674319967
Loss at iteration [1080]: 0.0022042881642551156
Loss at iteration [1081]: 0.002204098267700787
Loss at iteration [1082]: 0.0022040101994415715
Loss at iteration [1083]: 0.002203687624801439
Loss at iteration [1084]: 0.002203347858802783
Loss at iteration [1085]: 0.002203026656011006
Loss at iteration [1086]: 0.0022027378362203007
Loss at iteration [1087]: 0.002202645510540761
Loss at iteration [1088]: 0.002202558155341861
Loss at iteration [1089]: 0.002202143169371638
Loss at iteration [1090]: 0.002202024649754112
Loss at iteration [1091]: 0.0022018820282072105
Loss at iteration [1092]: 0.002201560146758466
Loss at iteration [1093]: 0.002201390736127559
Loss at iteration [1094]: 0.002201200130592053
Loss at iteration [1095]: 0.0022010488954198756
Loss at iteration [1096]: 0.0022006288279081293
Loss at iteration [1097]: 0.0022001823476373206
Loss at iteration [1098]: 0.002200167262031223
Loss at iteration [1099]: 0.002199896269620866
Loss at iteration [1100]: 0.0021995347524950025
Loss at iteration [1101]: 0.0021994497077762326
Loss at iteration [1102]: 0.002199149770728807
Loss at iteration [1103]: 0.0021988625627000213
Loss at iteration [1104]: 0.0021987311221135486
Loss at iteration [1105]: 0.002198443896656588
Loss at iteration [1106]: 0.002198113773444922
Loss at iteration [1107]: 0.002198202553133562
***** Warning: Loss has increased *****
Loss at iteration [1108]: 0.0021978984645626043
Loss at iteration [1109]: 0.0021974333767670632
Loss at iteration [1110]: 0.002197221194650975
Loss at iteration [1111]: 0.0021970250195408863
Loss at iteration [1112]: 0.0021968690237067308
Loss at iteration [1113]: 0.002196788082187909
Loss at iteration [1114]: 0.002196314953097363
Loss at iteration [1115]: 0.00219628823975195
Loss at iteration [1116]: 0.0021959833996976737
Loss at iteration [1117]: 0.002195747961368448
Loss at iteration [1118]: 0.0021954556964823356
Loss at iteration [1119]: 0.0021950986513790405
Loss at iteration [1120]: 0.0021948907520625473
Loss at iteration [1121]: 0.0021945620510128165
Loss at iteration [1122]: 0.0021944344518009568
Loss at iteration [1123]: 0.002194091637162378
Loss at iteration [1124]: 0.0021938830856318607
Loss at iteration [1125]: 0.0021938007019056227
Loss at iteration [1126]: 0.0021932472254495203
Loss at iteration [1127]: 0.0021933161368094148
***** Warning: Loss has increased *****
Loss at iteration [1128]: 0.0021927450873340053
Loss at iteration [1129]: 0.002192676290774725
Loss at iteration [1130]: 0.002192478907844953
Loss at iteration [1131]: 0.0021922791657550717
Loss at iteration [1132]: 0.0021918557020359614
Loss at iteration [1133]: 0.002191337016235702
Loss at iteration [1134]: 0.0021910867918158478
Loss at iteration [1135]: 0.002190645102989024
Loss at iteration [1136]: 0.0021901764764473363
Loss at iteration [1137]: 0.002190128887213481
Loss at iteration [1138]: 0.002189907990610409
Loss at iteration [1139]: 0.002189594743961934
Loss at iteration [1140]: 0.0021893525324723126
Loss at iteration [1141]: 0.002189173639150564
Loss at iteration [1142]: 0.0021887676691789404
Loss at iteration [1143]: 0.002188642064411985
Loss at iteration [1144]: 0.002188371573502893
Loss at iteration [1145]: 0.002188168575708363
Loss at iteration [1146]: 0.0021878671722694123
Loss at iteration [1147]: 0.002187577217067191
Loss at iteration [1148]: 0.002187278971233181
Loss at iteration [1149]: 0.0021871351974190162
Loss at iteration [1150]: 0.00218676334435693
Loss at iteration [1151]: 0.002186474234480398
Loss at iteration [1152]: 0.0021862375413856817
Loss at iteration [1153]: 0.0021859166020700917
Loss at iteration [1154]: 0.0021857049621935814
Loss at iteration [1155]: 0.0021854628763999235
Loss at iteration [1156]: 0.0021852339576528385
Loss at iteration [1157]: 0.002184918174902024
Loss at iteration [1158]: 0.0021844956307428913
Loss at iteration [1159]: 0.0021843765140644065
Loss at iteration [1160]: 0.0021839030150834836
Loss at iteration [1161]: 0.0021836504130489515
Loss at iteration [1162]: 0.0021835327417893597
Loss at iteration [1163]: 0.002183266414819319
Loss at iteration [1164]: 0.0021828551747745594
Loss at iteration [1165]: 0.0021828879189384033
***** Warning: Loss has increased *****
Loss at iteration [1166]: 0.0021824029380465913
Loss at iteration [1167]: 0.002182104609826081
Loss at iteration [1168]: 0.0021818662278803042
Loss at iteration [1169]: 0.002181780267398632
Loss at iteration [1170]: 0.002181387058759907
Loss at iteration [1171]: 0.002181291685435404
Loss at iteration [1172]: 0.002180775539489918
Loss at iteration [1173]: 0.0021805638119085403
Loss at iteration [1174]: 0.0021802568107641405
Loss at iteration [1175]: 0.0021800777544922294
Loss at iteration [1176]: 0.0021798910490293142
Loss at iteration [1177]: 0.002179486835792935
Loss at iteration [1178]: 0.0021793995220934287
Loss at iteration [1179]: 0.0021789711657854817
Loss at iteration [1180]: 0.0021787650789427994
Loss at iteration [1181]: 0.0021785064812014023
Loss at iteration [1182]: 0.0021781206686596823
Loss at iteration [1183]: 0.0021781397143298066
***** Warning: Loss has increased *****
Loss at iteration [1184]: 0.0021775730948540898
Loss at iteration [1185]: 0.0021774258768818633
Loss at iteration [1186]: 0.0021770892293146034
Loss at iteration [1187]: 0.0021768410333632793
Loss at iteration [1188]: 0.0021765561366537092
Loss at iteration [1189]: 0.002176317299933574
Loss at iteration [1190]: 0.0021760058572702465
Loss at iteration [1191]: 0.0021757892136740676
Loss at iteration [1192]: 0.002175527068460505
Loss at iteration [1193]: 0.0021754752880956646
Loss at iteration [1194]: 0.002175032798466051
Loss at iteration [1195]: 0.0021748613601330906
Loss at iteration [1196]: 0.0021745919041692157
Loss at iteration [1197]: 0.002174253957990228
Loss at iteration [1198]: 0.002174065783366869
Loss at iteration [1199]: 0.002173715283499599
Loss at iteration [1200]: 0.0021734020424073056
Loss at iteration [1201]: 0.0021731197755645863
Loss at iteration [1202]: 0.002173058594608957
Loss at iteration [1203]: 0.002172638946213032
Loss at iteration [1204]: 0.002172160732736355
Loss at iteration [1205]: 0.002172046828260385
Loss at iteration [1206]: 0.0021719001476592182
Loss at iteration [1207]: 0.002171496864520009
Loss at iteration [1208]: 0.0021710436359467073
Loss at iteration [1209]: 0.00217091478363493
Loss at iteration [1210]: 0.002170598141342767
Loss at iteration [1211]: 0.00217010142250721
Loss at iteration [1212]: 0.0021699912463582897
Loss at iteration [1213]: 0.0021699067664392416
Loss at iteration [1214]: 0.002169351668188301
Loss at iteration [1215]: 0.002169266087780186
Loss at iteration [1216]: 0.0021692375609807615
Loss at iteration [1217]: 0.0021688274594424566
Loss at iteration [1218]: 0.002168469148784713
Loss at iteration [1219]: 0.002168473849918832
***** Warning: Loss has increased *****
Loss at iteration [1220]: 0.0021677564829585233
Loss at iteration [1221]: 0.002167416180459793
Loss at iteration [1222]: 0.00216725884277578
Loss at iteration [1223]: 0.0021668816947013605
Loss at iteration [1224]: 0.0021666421045612237
Loss at iteration [1225]: 0.00216645805321915
Loss at iteration [1226]: 0.002166127157624507
Loss at iteration [1227]: 0.002165880791933053
Loss at iteration [1228]: 0.0021656625103411535
Loss at iteration [1229]: 0.002165293152207266
Loss at iteration [1230]: 0.002165016709702046
Loss at iteration [1231]: 0.0021648000291331114
Loss at iteration [1232]: 0.002164571393610334
Loss at iteration [1233]: 0.002164352907327039
Loss at iteration [1234]: 0.002164141815335563
Loss at iteration [1235]: 0.002164012782227011
Loss at iteration [1236]: 0.0021636652859115687
Loss at iteration [1237]: 0.0021635038380041906
Loss at iteration [1238]: 0.0021631826732002243
Loss at iteration [1239]: 0.002163123550209913
Loss at iteration [1240]: 0.002163046210739175
Loss at iteration [1241]: 0.0021625753990387408
Loss at iteration [1242]: 0.0021624475828936106
Loss at iteration [1243]: 0.002162465026513649
***** Warning: Loss has increased *****
Loss at iteration [1244]: 0.002161839787271651
Loss at iteration [1245]: 0.002161750771902262
Loss at iteration [1246]: 0.002161474114583301
Loss at iteration [1247]: 0.002161115043698193
Loss at iteration [1248]: 0.002161041209109275
Loss at iteration [1249]: 0.0021605895723731754
Loss at iteration [1250]: 0.002160699307477284
***** Warning: Loss has increased *****
Loss at iteration [1251]: 0.00216029986424649
Loss at iteration [1252]: 0.002159911142253877
Loss at iteration [1253]: 0.0021597930104846224
Loss at iteration [1254]: 0.002159461561753712
Loss at iteration [1255]: 0.0021592551193800294
Loss at iteration [1256]: 0.002159007430628132
Loss at iteration [1257]: 0.0021587399222276053
Loss at iteration [1258]: 0.002158529047020677
Loss at iteration [1259]: 0.002158267114267551
Loss at iteration [1260]: 0.002157993317244127
Loss at iteration [1261]: 0.002157800882225742
Loss at iteration [1262]: 0.0021576062411341407
Loss at iteration [1263]: 0.0021575418249667132
Loss at iteration [1264]: 0.0021571306058454005
Loss at iteration [1265]: 0.0021568328307971746
Loss at iteration [1266]: 0.002156628322677685
Loss at iteration [1267]: 0.002156352011359548
Loss at iteration [1268]: 0.002156128169520845
Loss at iteration [1269]: 0.0021557855107655574
Loss at iteration [1270]: 0.002155583458783883
Loss at iteration [1271]: 0.002155356184864512
Loss at iteration [1272]: 0.0021552333026422415
Loss at iteration [1273]: 0.002154888099978407
Loss at iteration [1274]: 0.0021546718373008198
Loss at iteration [1275]: 0.0021543703637285916
Loss at iteration [1276]: 0.0021543546681440815
Loss at iteration [1277]: 0.0021540092448364026
Loss at iteration [1278]: 0.0021539285034783856
Loss at iteration [1279]: 0.002153514669646218
Loss at iteration [1280]: 0.002153506246054623
Loss at iteration [1281]: 0.0021534056814905087
Loss at iteration [1282]: 0.002152933747337371
Loss at iteration [1283]: 0.0021532037222750495
***** Warning: Loss has increased *****
Loss at iteration [1284]: 0.002152856636219644
Loss at iteration [1285]: 0.002152226683421266
Loss at iteration [1286]: 0.0021523528515903917
***** Warning: Loss has increased *****
Loss at iteration [1287]: 0.0021519455117104316
Loss at iteration [1288]: 0.002151959699270072
***** Warning: Loss has increased *****
Loss at iteration [1289]: 0.0021515922560455003
Loss at iteration [1290]: 0.0021512266853890853
Loss at iteration [1291]: 0.0021511492284680624
Loss at iteration [1292]: 0.0021507538672640844
Loss at iteration [1293]: 0.0021508365194575713
***** Warning: Loss has increased *****
Loss at iteration [1294]: 0.002150255961477235
Loss at iteration [1295]: 0.002150303741495692
***** Warning: Loss has increased *****
Loss at iteration [1296]: 0.002150297055981706
Loss at iteration [1297]: 0.0021495789539001863
Loss at iteration [1298]: 0.0021493818023546668
Loss at iteration [1299]: 0.002149238283726727
Loss at iteration [1300]: 0.002148983358403135
Loss at iteration [1301]: 0.0021487344935656027
Loss at iteration [1302]: 0.002148265858980155
Loss at iteration [1303]: 0.002148316078759964
***** Warning: Loss has increased *****
Loss at iteration [1304]: 0.0021480357010113206
Loss at iteration [1305]: 0.002147526219545446
Loss at iteration [1306]: 0.0021474360789211196
Loss at iteration [1307]: 0.0021471771141454007
Loss at iteration [1308]: 0.0021469884183486694
Loss at iteration [1309]: 0.002146531929967834
Loss at iteration [1310]: 0.002146602075848544
***** Warning: Loss has increased *****
Loss at iteration [1311]: 0.002146208915337626
Loss at iteration [1312]: 0.0021457320024596827
Loss at iteration [1313]: 0.002145679139387417
Loss at iteration [1314]: 0.002145582800270214
Loss at iteration [1315]: 0.002145044784491109
Loss at iteration [1316]: 0.0021448681976457763
Loss at iteration [1317]: 0.002144707720690549
Loss at iteration [1318]: 0.0021444525646748943
Loss at iteration [1319]: 0.0021440223712828037
Loss at iteration [1320]: 0.002143778606054617
Loss at iteration [1321]: 0.002143486217594791
Loss at iteration [1322]: 0.002143300660071584
Loss at iteration [1323]: 0.0021429908541164377
Loss at iteration [1324]: 0.002142734501572812
Loss at iteration [1325]: 0.0021425363797116796
Loss at iteration [1326]: 0.0021422188530152235
Loss at iteration [1327]: 0.0021421178443828417
Loss at iteration [1328]: 0.0021417566507720355
Loss at iteration [1329]: 0.002141526294890384
Loss at iteration [1330]: 0.0021412400978948042
Loss at iteration [1331]: 0.0021410362870080166
Loss at iteration [1332]: 0.0021407564860230306
Loss at iteration [1333]: 0.002140506534957822
Loss at iteration [1334]: 0.0021401401516214186
Loss at iteration [1335]: 0.0021401653452813173
***** Warning: Loss has increased *****
Loss at iteration [1336]: 0.0021397918290908422
Loss at iteration [1337]: 0.0021394349395761035
Loss at iteration [1338]: 0.0021391389958662402
Loss at iteration [1339]: 0.002138959941447874
Loss at iteration [1340]: 0.002138705394332647
Loss at iteration [1341]: 0.002138601031972623
Loss at iteration [1342]: 0.0021383194510264025
Loss at iteration [1343]: 0.002138088730276912
Loss at iteration [1344]: 0.002137768964959785
Loss at iteration [1345]: 0.002137759914400633
Loss at iteration [1346]: 0.0021372525982522937
Loss at iteration [1347]: 0.0021370206592650955
Loss at iteration [1348]: 0.0021367587631490846
Loss at iteration [1349]: 0.002136591237166375
Loss at iteration [1350]: 0.0021361913019984653
Loss at iteration [1351]: 0.002135803198097561
Loss at iteration [1352]: 0.002135827002987556
***** Warning: Loss has increased *****
Loss at iteration [1353]: 0.0021354917080652724
Loss at iteration [1354]: 0.00213517822626388
Loss at iteration [1355]: 0.0021351261348128633
Loss at iteration [1356]: 0.0021346554667931853
Loss at iteration [1357]: 0.0021344061164791537
Loss at iteration [1358]: 0.002134254778638632
Loss at iteration [1359]: 0.0021337970241212213
Loss at iteration [1360]: 0.0021338218495385347
***** Warning: Loss has increased *****
Loss at iteration [1361]: 0.0021335603812603072
Loss at iteration [1362]: 0.0021333310866616557
Loss at iteration [1363]: 0.0021328580467168327
Loss at iteration [1364]: 0.002132774429359755
Loss at iteration [1365]: 0.0021324526406145206
Loss at iteration [1366]: 0.002132254957511195
Loss at iteration [1367]: 0.002131890023488337
Loss at iteration [1368]: 0.0021318174718324176
Loss at iteration [1369]: 0.0021315953539943204
Loss at iteration [1370]: 0.00213137196088494
Loss at iteration [1371]: 0.002131129100825961
Loss at iteration [1372]: 0.0021308788512470166
Loss at iteration [1373]: 0.002130570134044071
Loss at iteration [1374]: 0.0021304428684227263
Loss at iteration [1375]: 0.002130133799511662
Loss at iteration [1376]: 0.002130475927452864
***** Warning: Loss has increased *****
Loss at iteration [1377]: 0.0021302503708396387
Loss at iteration [1378]: 0.002129561803761993
Loss at iteration [1379]: 0.002129437068755391
Loss at iteration [1380]: 0.0021293183943678603
Loss at iteration [1381]: 0.0021287341398441607
Loss at iteration [1382]: 0.0021289875044192175
***** Warning: Loss has increased *****
Loss at iteration [1383]: 0.0021288777435622824
Loss at iteration [1384]: 0.0021275342265341677
Loss at iteration [1385]: 0.002127041403705038
Loss at iteration [1386]: 0.002126778845476526
Loss at iteration [1387]: 0.0021260470090753
Loss at iteration [1388]: 0.0021261883464726225
***** Warning: Loss has increased *****
Loss at iteration [1389]: 0.0021260412224402073
Loss at iteration [1390]: 0.002125047676242307
Loss at iteration [1391]: 0.002124598253920396
Loss at iteration [1392]: 0.002124945486039694
***** Warning: Loss has increased *****
Loss at iteration [1393]: 0.0021241830784724082
Loss at iteration [1394]: 0.0021239711140191822
Loss at iteration [1395]: 0.002123716643304786
Loss at iteration [1396]: 0.0021232871604110787
Loss at iteration [1397]: 0.00212287953728162
Loss at iteration [1398]: 0.0021227780267641383
Loss at iteration [1399]: 0.0021222046152592614
Loss at iteration [1400]: 0.0021215133164383176
Loss at iteration [1401]: 0.00212174506632079
***** Warning: Loss has increased *****
Loss at iteration [1402]: 0.002121161269801547
Loss at iteration [1403]: 0.0021209738214258986
Loss at iteration [1404]: 0.002120637515062143
Loss at iteration [1405]: 0.002119928308555501
Loss at iteration [1406]: 0.0021201254213098703
***** Warning: Loss has increased *****
Loss at iteration [1407]: 0.0021199009139257997
Loss at iteration [1408]: 0.0021191208252811272
Loss at iteration [1409]: 0.002119125278279702
***** Warning: Loss has increased *****
Loss at iteration [1410]: 0.0021192229063931585
***** Warning: Loss has increased *****
Loss at iteration [1411]: 0.0021188818995282956
Loss at iteration [1412]: 0.0021181601608462673
Loss at iteration [1413]: 0.002117816705130216
Loss at iteration [1414]: 0.0021178748332932726
***** Warning: Loss has increased *****
Loss at iteration [1415]: 0.0021183871643172575
***** Warning: Loss has increased *****
Loss at iteration [1416]: 0.002117377079979931
Loss at iteration [1417]: 0.0021167083401985045
Loss at iteration [1418]: 0.0021169429087362205
***** Warning: Loss has increased *****
Loss at iteration [1419]: 0.0021166130746589668
Loss at iteration [1420]: 0.00211611673669217
Loss at iteration [1421]: 0.002115456839116467
Loss at iteration [1422]: 0.0021157099262242656
***** Warning: Loss has increased *****
Loss at iteration [1423]: 0.002116101852551731
***** Warning: Loss has increased *****
Loss at iteration [1424]: 0.0021151063849844318
Loss at iteration [1425]: 0.002115655062878942
***** Warning: Loss has increased *****
Loss at iteration [1426]: 0.002115492159716102
Loss at iteration [1427]: 0.0021146718985852516
Loss at iteration [1428]: 0.0021143483463171237
Loss at iteration [1429]: 0.0021132639409703682
Loss at iteration [1430]: 0.002112723789634428
Loss at iteration [1431]: 0.00211373459137653
***** Warning: Loss has increased *****
Loss at iteration [1432]: 0.002113584346515849
Loss at iteration [1433]: 0.0021124981655235996
Loss at iteration [1434]: 0.0021121074636900224
Loss at iteration [1435]: 0.0021121194755069008
***** Warning: Loss has increased *****
Loss at iteration [1436]: 0.0021120288290589653
Loss at iteration [1437]: 0.0021117551412812717
Loss at iteration [1438]: 0.0021112390056966224
Loss at iteration [1439]: 0.002110550691165745
Loss at iteration [1440]: 0.00211048621614532
Loss at iteration [1441]: 0.002110378558071156
Loss at iteration [1442]: 0.0021097483057284376
Loss at iteration [1443]: 0.0021096805836388005
Loss at iteration [1444]: 0.002109477784489237
Loss at iteration [1445]: 0.002108846113706486
Loss at iteration [1446]: 0.0021085142875667895
Loss at iteration [1447]: 0.0021088198497273745
***** Warning: Loss has increased *****
Loss at iteration [1448]: 0.0021083681056181564
Loss at iteration [1449]: 0.00210850874241057
***** Warning: Loss has increased *****
Loss at iteration [1450]: 0.002108048892003563
Loss at iteration [1451]: 0.002107860186734809
Loss at iteration [1452]: 0.0021078928199489842
***** Warning: Loss has increased *****
Loss at iteration [1453]: 0.002107227822531324
Loss at iteration [1454]: 0.002106918246809694
Loss at iteration [1455]: 0.002106865830024318
Loss at iteration [1456]: 0.002106666880769381
Loss at iteration [1457]: 0.002107293476644487
***** Warning: Loss has increased *****
Loss at iteration [1458]: 0.002107586626708306
***** Warning: Loss has increased *****
Loss at iteration [1459]: 0.0021078662629581972
***** Warning: Loss has increased *****
Loss at iteration [1460]: 0.0021088897891099376
***** Warning: Loss has increased *****
Loss at iteration [1461]: 0.0021087264555512365
Loss at iteration [1462]: 0.0021074845139703036
Loss at iteration [1463]: 0.002105679369252032
Loss at iteration [1464]: 0.0021048126728366308
Loss at iteration [1465]: 0.0021039282022465838
Loss at iteration [1466]: 0.0021030860684475147
Loss at iteration [1467]: 0.002103826895096698
***** Warning: Loss has increased *****
Loss at iteration [1468]: 0.002103541034789197
Loss at iteration [1469]: 0.002103335798191651
Loss at iteration [1470]: 0.0021033619343333084
***** Warning: Loss has increased *****
Loss at iteration [1471]: 0.0021029136913302098
Loss at iteration [1472]: 0.0021030713163913196
***** Warning: Loss has increased *****
Loss at iteration [1473]: 0.0021025773139243973
Loss at iteration [1474]: 0.002102577125640276
