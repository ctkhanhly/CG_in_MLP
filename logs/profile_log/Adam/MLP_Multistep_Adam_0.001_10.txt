Model name                            : MLP_Multistep
The number of input features          : 10
The number of output features         : 2
Optimizer name                        : Adam
Learning rate                         : 0.001
Max number of iterations              : 3000
Number of samples in training data    : 71
Number of samples in tests data       : 30
Total training time                   : 0.829932451248169
Total number of parameters            : 203302
Percentage of parameters < 1e-9       : 50.1273966808%
Percentage of parameters < 1e-7       : 50.1273966808%
Percentage of parameters < 1e-6       : 50.12887231802934%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.592289771096433
Loss at iteration [2]: 1.4186540639509218
Loss at iteration [3]: 1.2797402452822133
Loss at iteration [4]: 1.1422394070693427
Loss at iteration [5]: 1.0055566530125017
Loss at iteration [6]: 0.8815480808460958
Loss at iteration [7]: 0.7878209369589075
Loss at iteration [8]: 0.7479607085310297
Loss at iteration [9]: 0.7600881718218491
***** Warning: Loss has increased *****
Loss at iteration [10]: 0.767299621634763
***** Warning: Loss has increased *****
Loss at iteration [11]: 0.7344328000672486
Loss at iteration [12]: 0.6832435271394885
Loss at iteration [13]: 0.6484475898209443
Loss at iteration [14]: 0.6396646984951014
Loss at iteration [15]: 0.6315399085072175
Loss at iteration [16]: 0.6038713122665004
Loss at iteration [17]: 0.5670313253094722
Loss at iteration [18]: 0.5403003060710719
Loss at iteration [19]: 0.5279937541269214
Loss at iteration [20]: 0.5159213664596871
Loss at iteration [21]: 0.49203413832174053
Loss at iteration [22]: 0.46142202007483873
Loss at iteration [23]: 0.4439984961168138
Loss at iteration [24]: 0.44049870169151606
Loss at iteration [25]: 0.427492592629424
Loss at iteration [26]: 0.40660289792111753
Loss at iteration [27]: 0.39915164156352856
Loss at iteration [28]: 0.39375056557735394
Loss at iteration [29]: 0.3738189614224096
Loss at iteration [30]: 0.35551860115052103
Loss at iteration [31]: 0.347341679316106
Loss at iteration [32]: 0.3324663639998101
Loss at iteration [33]: 0.3162874550034996
Loss at iteration [34]: 0.3090805088940581
Loss at iteration [35]: 0.298823017384873
Loss at iteration [36]: 0.2840785517708987
Loss at iteration [37]: 0.2759530726518239
Loss at iteration [38]: 0.26701283854792474
Loss at iteration [39]: 0.2560497486031285
Loss at iteration [40]: 0.252297825935059
Loss at iteration [41]: 0.24582044188807559
Loss at iteration [42]: 0.23878312652797398
Loss at iteration [43]: 0.23596103063059484
Loss at iteration [44]: 0.22913939023610053
Loss at iteration [45]: 0.2263943092031417
Loss at iteration [46]: 0.22370835381767867
Loss at iteration [47]: 0.22036951565919707
Loss at iteration [48]: 0.22012114253863693
Loss at iteration [49]: 0.21742992786381107
Loss at iteration [50]: 0.21756887423379143
***** Warning: Loss has increased *****
Loss at iteration [51]: 0.21616012689271166
Loss at iteration [52]: 0.2160671493450092
Loss at iteration [53]: 0.21633774316634022
***** Warning: Loss has increased *****
Loss at iteration [54]: 0.21620879121788275
Loss at iteration [55]: 0.2172587642834608
***** Warning: Loss has increased *****
Loss at iteration [56]: 0.21675524242194105
Loss at iteration [57]: 0.21746834482852753
***** Warning: Loss has increased *****
Loss at iteration [58]: 0.21677850762668058
Loss at iteration [59]: 0.21714483245711524
***** Warning: Loss has increased *****
Loss at iteration [60]: 0.2167955476453378
Loss at iteration [61]: 0.21675967425733084
Loss at iteration [62]: 0.21649172932400532
Loss at iteration [63]: 0.21600189716600107
Loss at iteration [64]: 0.21591668196194777
Loss at iteration [65]: 0.21549055895330244
Loss at iteration [66]: 0.21560085366582868
***** Warning: Loss has increased *****
Loss at iteration [67]: 0.2152507457956795
Loss at iteration [68]: 0.21531325204696897
***** Warning: Loss has increased *****
Loss at iteration [69]: 0.21501275569983044
Loss at iteration [70]: 0.2150538835546617
***** Warning: Loss has increased *****
Loss at iteration [71]: 0.2148977157008565
Loss at iteration [72]: 0.2149207841125939
***** Warning: Loss has increased *****
Loss at iteration [73]: 0.21486461636440668
Loss at iteration [74]: 0.214853767293499
Loss at iteration [75]: 0.2148515850026227
Loss at iteration [76]: 0.2148223079551316
Loss at iteration [77]: 0.2148756274675695
***** Warning: Loss has increased *****
Loss at iteration [78]: 0.21486173126951147
Loss at iteration [79]: 0.21491802943628893
***** Warning: Loss has increased *****
Loss at iteration [80]: 0.21486361823245112
Loss at iteration [81]: 0.21488457839558958
***** Warning: Loss has increased *****
Loss at iteration [82]: 0.2148166809027313
Loss at iteration [83]: 0.2148320310824402
***** Warning: Loss has increased *****
Loss at iteration [84]: 0.21476287359128454
Loss at iteration [85]: 0.2147590497628522
Loss at iteration [86]: 0.2146884797696698
Loss at iteration [87]: 0.21469137374312552
***** Warning: Loss has increased *****
Loss at iteration [88]: 0.21464860562176152
Loss at iteration [89]: 0.21466233367794793
***** Warning: Loss has increased *****
Loss at iteration [90]: 0.21463147113919648
Loss at iteration [91]: 0.21464547177851057
***** Warning: Loss has increased *****
Loss at iteration [92]: 0.21462748367224044
Loss at iteration [93]: 0.21464456218115366
***** Warning: Loss has increased *****
Loss at iteration [94]: 0.21463161685473808
Loss at iteration [95]: 0.2146418606263823
***** Warning: Loss has increased *****
Loss at iteration [96]: 0.21462994340718802
Loss at iteration [97]: 0.21463533286897868
***** Warning: Loss has increased *****
Loss at iteration [98]: 0.21462482502492258
Loss at iteration [99]: 0.2146246929292535
Loss at iteration [100]: 0.2146159227835547
Loss at iteration [101]: 0.21461295590339585
Loss at iteration [102]: 0.2146064818297746
Loss at iteration [103]: 0.21460114161454522
Loss at iteration [104]: 0.2145983515306893
Loss at iteration [105]: 0.2145938979666072
Loss at iteration [106]: 0.21459422802135963
***** Warning: Loss has increased *****
Loss at iteration [107]: 0.21458887994659456
Loss at iteration [108]: 0.21459021382454407
***** Warning: Loss has increased *****
Loss at iteration [109]: 0.21458554599507754
Loss at iteration [110]: 0.21458770461303656
***** Warning: Loss has increased *****
Loss at iteration [111]: 0.2145835756378078
Loss at iteration [112]: 0.21458515923718155
***** Warning: Loss has increased *****
Loss at iteration [113]: 0.21458267828874455
Loss at iteration [114]: 0.2145839007849559
***** Warning: Loss has increased *****
Loss at iteration [115]: 0.21458272075555576
Loss at iteration [116]: 0.21458251908058634
Loss at iteration [117]: 0.21458235729802985
Loss at iteration [118]: 0.21458117322676984
Loss at iteration [119]: 0.21458126721596535
***** Warning: Loss has increased *****
Loss at iteration [120]: 0.21457909294469255
Loss at iteration [121]: 0.2145790843582528
Loss at iteration [122]: 0.2145771705990822
Loss at iteration [123]: 0.21457706126945622
Loss at iteration [124]: 0.21457595052806053
Loss at iteration [125]: 0.2145756858860772
Loss at iteration [126]: 0.214575547212159
Loss at iteration [127]: 0.21457500984640004
Loss at iteration [128]: 0.21457527772078164
***** Warning: Loss has increased *****
Loss at iteration [129]: 0.21457466715492238
Loss at iteration [130]: 0.21457494543200178
***** Warning: Loss has increased *****
Loss at iteration [131]: 0.21457443575394813
Loss at iteration [132]: 0.21457428952242122
Loss at iteration [133]: 0.21457402017486116
Loss at iteration [134]: 0.214573524058791
Loss at iteration [135]: 0.21457335920864826
Loss at iteration [136]: 0.21457272027770463
Loss at iteration [137]: 0.21457254918739882
Loss at iteration [138]: 0.2145721086636412
Loss at iteration [139]: 0.21457177694120244
Loss at iteration [140]: 0.21457152984397115
Loss at iteration [141]: 0.2145711171685695
Loss at iteration [142]: 0.21457097548617018
Loss at iteration [143]: 0.21457060590159988
Loss at iteration [144]: 0.21457037775904472
Loss at iteration [145]: 0.21457013785832907
Loss at iteration [146]: 0.21456981800810015
Loss at iteration [147]: 0.2145696332666668
Loss at iteration [148]: 0.21456931166399684
Loss at iteration [149]: 0.2145690901943054
Loss at iteration [150]: 0.2145688427468037
Loss at iteration [151]: 0.21456854257269573
Loss at iteration [152]: 0.21456832129606562
Loss at iteration [153]: 0.2145680122541049
Loss at iteration [154]: 0.21456774574648424
Loss at iteration [155]: 0.21456747791639977
Loss at iteration [156]: 0.2145671804045175
Loss at iteration [157]: 0.21456694476216923
Loss at iteration [158]: 0.21456666953711775
Loss at iteration [159]: 0.21456641562471163
Loss at iteration [160]: 0.21456618488745852
Loss at iteration [161]: 0.21456592363586993
Loss at iteration [162]: 0.21456569345817372
Loss at iteration [163]: 0.2145654501065225
Loss at iteration [164]: 0.2145651996066403
Loss at iteration [165]: 0.21456496773875092
Loss at iteration [166]: 0.21456471227816015
Loss at iteration [167]: 0.2145644657864217
Loss at iteration [168]: 0.21456422556278623
Loss at iteration [169]: 0.21456397074652436
Loss at iteration [170]: 0.2145637301088223
Loss at iteration [171]: 0.21456348759081934
Loss at iteration [172]: 0.21456324464778434
Loss at iteration [173]: 0.21456300702706294
Loss at iteration [174]: 0.2145627684826357
Loss at iteration [175]: 0.21456252927247807
Loss at iteration [176]: 0.21456229535063462
Loss at iteration [177]: 0.21456205851512325
Loss at iteration [178]: 0.21456182545958644
Loss at iteration [179]: 0.21456159363319452
Loss at iteration [180]: 0.2145613564466161
Loss at iteration [181]: 0.21456112344047015
Loss at iteration [182]: 0.21456089244573517
Loss at iteration [183]: 0.21456065853682374
Loss at iteration [184]: 0.2145604276893372
Loss at iteration [185]: 0.21456019751735114
Loss at iteration [186]: 0.21455996653072815
Loss at iteration [187]: 0.21455973794096658
Loss at iteration [188]: 0.2145595092455869
Loss at iteration [189]: 0.21455928030636867
Loss at iteration [190]: 0.2145590542303934
Loss at iteration [191]: 0.2145588286918312
Loss at iteration [192]: 0.21455860276321573
Loss at iteration [193]: 0.21455837858937002
Loss at iteration [194]: 0.21455815508183598
Loss at iteration [195]: 0.2145579324661837
Loss at iteration [196]: 0.21455771081667982
Loss at iteration [197]: 0.21455748922640824
Loss at iteration [198]: 0.21455726743971087
Loss at iteration [199]: 0.2145570470639698
Loss at iteration [200]: 0.2145568281213374
Loss at iteration [201]: 0.2145566094046856
Loss at iteration [202]: 0.2145563917147482
Loss at iteration [203]: 0.21455617527528514
Loss at iteration [204]: 0.2145559592889434
Loss at iteration [205]: 0.21455574390638718
Loss at iteration [206]: 0.21455552959550217
Loss at iteration [207]: 0.21455531678438755
Loss at iteration [208]: 0.21455510603312136
Loss at iteration [209]: 0.2145548962561976
Loss at iteration [210]: 0.21455468718633272
Loss at iteration [211]: 0.21455447898664431
Loss at iteration [212]: 0.21455427188603224
Loss at iteration [213]: 0.21455406559396487
Loss at iteration [214]: 0.21455386007090302
Loss at iteration [215]: 0.21455365545254784
Loss at iteration [216]: 0.2145534518159232
Loss at iteration [217]: 0.214553249071485
Loss at iteration [218]: 0.21455304725191698
Loss at iteration [219]: 0.21455284654762052
Loss at iteration [220]: 0.2145526468480309
Loss at iteration [221]: 0.21455244805335477
Loss at iteration [222]: 0.21455225012910528
Loss at iteration [223]: 0.21455205314611703
Loss at iteration [224]: 0.2145518570723472
Loss at iteration [225]: 0.2145516621133239
Loss at iteration [226]: 0.21455146788412535
Loss at iteration [227]: 0.21455127453198702
Loss at iteration [228]: 0.21455108225826505
Loss at iteration [229]: 0.21455089085286222
Loss at iteration [230]: 0.21455070036018528
Loss at iteration [231]: 0.21455051080600918
Loss at iteration [232]: 0.21455032226680856
Loss at iteration [233]: 0.21455013461051814
Loss at iteration [234]: 0.21454994807401886
Loss at iteration [235]: 0.21454976246875648
Loss at iteration [236]: 0.21454957771035305
Loss at iteration [237]: 0.21454939385535388
Loss at iteration [238]: 0.21454921091711626
Loss at iteration [239]: 0.2145490289025564
Loss at iteration [240]: 0.21454884797680943
Loss at iteration [241]: 0.2145486679655958
Loss at iteration [242]: 0.21454848884997524
Loss at iteration [243]: 0.21454831081822018
Loss at iteration [244]: 0.21454813370382772
Loss at iteration [245]: 0.21454795750868044
Loss at iteration [246]: 0.2145477822395131
Loss at iteration [247]: 0.21454760816621027
Loss at iteration [248]: 0.21454743485052405
Loss at iteration [249]: 0.21454726243096034
Loss at iteration [250]: 0.21454709114863788
Loss at iteration [251]: 0.21454692079709234
Loss at iteration [252]: 0.21454675138276713
Loss at iteration [253]: 0.21454658290906167
Loss at iteration [254]: 0.21454641547689296
Loss at iteration [255]: 0.21454624895438879
Loss at iteration [256]: 0.21454608354084836
Loss at iteration [257]: 0.21454591908553486
Loss at iteration [258]: 0.2145457555677522
Loss at iteration [259]: 0.21454559299283754
Loss at iteration [260]: 0.21454543136519214
Loss at iteration [261]: 0.21454527069044751
Loss at iteration [262]: 0.21454511116481437
Loss at iteration [263]: 0.2145449525370547
Loss at iteration [264]: 0.21454479709607807
Loss at iteration [265]: 0.21454464369794024
Loss at iteration [266]: 0.2145444911929783
Loss at iteration [267]: 0.21454433961080108
Loss at iteration [268]: 0.2145441889667031
Loss at iteration [269]: 0.21454403937901345
Loss at iteration [270]: 0.21454389073820296
Loss at iteration [271]: 0.21454374305379786
Loss at iteration [272]: 0.2145435963604002
Loss at iteration [273]: 0.21454345035202227
Loss at iteration [274]: 0.2145433051760202
Loss at iteration [275]: 0.2145431608540457
Loss at iteration [276]: 0.214543017639353
Loss at iteration [277]: 0.2145428751345171
Loss at iteration [278]: 0.21454273355350295
Loss at iteration [279]: 0.21454259302810993
Loss at iteration [280]: 0.21454245341462538
Loss at iteration [281]: 0.21454231472515328
Loss at iteration [282]: 0.21454217697067654
Loss at iteration [283]: 0.21454204016225895
Loss at iteration [284]: 0.214541904496918
Loss at iteration [285]: 0.21454176969123745
Loss at iteration [286]: 0.214541635771918
Loss at iteration [287]: 0.21454150298372118
Loss at iteration [288]: 0.21454137113984112
Loss at iteration [289]: 0.21454124024863072
Loss at iteration [290]: 0.21454111032859707
Loss at iteration [291]: 0.21454098153254691
Loss at iteration [292]: 0.21454085359016045
Loss at iteration [293]: 0.21454072666491733
Loss at iteration [294]: 0.2145406007935705
Loss at iteration [295]: 0.2145404758761767
Loss at iteration [296]: 0.21454035191504836
Loss at iteration [297]: 0.21454022891671748
Loss at iteration [298]: 0.2145401068805847
Loss at iteration [299]: 0.21453998594798376
Loss at iteration [300]: 0.2145398659173563
Loss at iteration [301]: 0.21453974682040847
Loss at iteration [302]: 0.21453962880013208
Loss at iteration [303]: 0.21453951172021787
Loss at iteration [304]: 0.21453939559250051
Loss at iteration [305]: 0.21453928041173884
Loss at iteration [306]: 0.21453916663781808
Loss at iteration [307]: 0.21453905389287017
Loss at iteration [308]: 0.21453894182399516
Loss at iteration [309]: 0.21453883032699195
Loss at iteration [310]: 0.21453872030598778
Loss at iteration [311]: 0.21453861125633636
Loss at iteration [312]: 0.2145385030746477
Loss at iteration [313]: 0.21453839578022726
Loss at iteration [314]: 0.2145382893903041
Loss at iteration [315]: 0.21453818417031217
Loss at iteration [316]: 0.21453807971538802
Loss at iteration [317]: 0.21453797601354552
Loss at iteration [318]: 0.21453787338835217
Loss at iteration [319]: 0.2145377717363909
Loss at iteration [320]: 0.21453767096157186
Loss at iteration [321]: 0.21453757107161828
Loss at iteration [322]: 0.21453747205877144
Loss at iteration [323]: 0.2145373739357002
Loss at iteration [324]: 0.2145372770814004
Loss at iteration [325]: 0.2145371808682554
Loss at iteration [326]: 0.21453708519475145
Loss at iteration [327]: 0.21453699106125348
Loss at iteration [328]: 0.21453689881195093
Loss at iteration [329]: 0.2145368075827807
Loss at iteration [330]: 0.21453671750898007
Loss at iteration [331]: 0.2145366288772657
Loss at iteration [332]: 0.214536542133517
Loss at iteration [333]: 0.2145364577939135
Loss at iteration [334]: 0.2145363783672879
Loss at iteration [335]: 0.21453630849521593
Loss at iteration [336]: 0.2145362579234403
Loss at iteration [337]: 0.2145362465595824
Loss at iteration [338]: 0.2145363198300052
***** Warning: Loss has increased *****
Loss at iteration [339]: 0.21453657862182432
***** Warning: Loss has increased *****
Loss at iteration [340]: 0.21453725085743183
***** Warning: Loss has increased *****
Loss at iteration [341]: 0.2145388577109447
***** Warning: Loss has increased *****
Loss at iteration [342]: 0.21454260690157437
***** Warning: Loss has increased *****
Loss at iteration [343]: 0.21455133369272833
***** Warning: Loss has increased *****
Loss at iteration [344]: 0.21457156197608232
***** Warning: Loss has increased *****
Loss at iteration [345]: 0.21461902131623206
***** Warning: Loss has increased *****
Loss at iteration [346]: 0.2147279572222198
***** Warning: Loss has increased *****
Loss at iteration [347]: 0.21498366163249497
***** Warning: Loss has increased *****
Loss at iteration [348]: 0.21555105684916767
***** Warning: Loss has increased *****
Loss at iteration [349]: 0.21678000839852896
***** Warning: Loss has increased *****
Loss at iteration [350]: 0.2189648241595964
***** Warning: Loss has increased *****
Loss at iteration [351]: 0.22182095787016884
***** Warning: Loss has increased *****
Loss at iteration [352]: 0.2227371543367121
***** Warning: Loss has increased *****
Loss at iteration [353]: 0.21938069652996198
Loss at iteration [354]: 0.21496859835581944
Loss at iteration [355]: 0.21567349804533503
***** Warning: Loss has increased *****
Loss at iteration [356]: 0.21875842542583457
***** Warning: Loss has increased *****
Loss at iteration [357]: 0.21774019854454282
Loss at iteration [358]: 0.21479312817055998
Loss at iteration [359]: 0.2154923608618281
***** Warning: Loss has increased *****
Loss at iteration [360]: 0.21729956230552128
***** Warning: Loss has increased *****
Loss at iteration [361]: 0.2158897245321302
Loss at iteration [362]: 0.21454756486234694
Loss at iteration [363]: 0.21590167354428735
***** Warning: Loss has increased *****
Loss at iteration [364]: 0.21615107846871603
***** Warning: Loss has increased *****
Loss at iteration [365]: 0.21469977113126418
Loss at iteration [366]: 0.21500479604179176
***** Warning: Loss has increased *****
Loss at iteration [367]: 0.21582580364412216
***** Warning: Loss has increased *****
Loss at iteration [368]: 0.21495940812362674
Loss at iteration [369]: 0.21462539219113044
Loss at iteration [370]: 0.21537167234802862
***** Warning: Loss has increased *****
Loss at iteration [371]: 0.21507068368529722
Loss at iteration [372]: 0.2145363716262048
Loss at iteration [373]: 0.21500118543390626
***** Warning: Loss has increased *****
Loss at iteration [374]: 0.2150465573843473
***** Warning: Loss has increased *****
Loss at iteration [375]: 0.21456078547305363
Loss at iteration [376]: 0.21476061581656744
***** Warning: Loss has increased *****
Loss at iteration [377]: 0.2149576969302756
***** Warning: Loss has increased *****
Loss at iteration [378]: 0.21460899855816737
Loss at iteration [379]: 0.21462214372929497
***** Warning: Loss has increased *****
Loss at iteration [380]: 0.2148483964794121
***** Warning: Loss has increased *****
Loss at iteration [381]: 0.2146482041022553
Loss at iteration [382]: 0.21455524078783628
Loss at iteration [383]: 0.21474381632477016
***** Warning: Loss has increased *****
Loss at iteration [384]: 0.21466775611880526
Loss at iteration [385]: 0.21453506014531712
Loss at iteration [386]: 0.2146564559492183
***** Warning: Loss has increased *****
Loss at iteration [387]: 0.21466517180275066
***** Warning: Loss has increased *****
Loss at iteration [388]: 0.21454033449777904
Loss at iteration [389]: 0.2145927790284121
***** Warning: Loss has increased *****
Loss at iteration [390]: 0.21464526078583648
***** Warning: Loss has increased *****
Loss at iteration [391]: 0.21455532272225394
Loss at iteration [392]: 0.2145539270622949
Loss at iteration [393]: 0.21461575029695168
***** Warning: Loss has increased *****
Loss at iteration [394]: 0.21456913535561625
Loss at iteration [395]: 0.2145364708113438
Loss at iteration [396]: 0.21458437755185414
***** Warning: Loss has increased *****
Loss at iteration [397]: 0.21457547413859895
Loss at iteration [398]: 0.21453433942692426
Loss at iteration [399]: 0.21455785787238868
***** Warning: Loss has increased *****
Loss at iteration [400]: 0.21457260297353953
***** Warning: Loss has increased *****
Loss at iteration [401]: 0.21454034901480606
Loss at iteration [402]: 0.2145406774077828
***** Warning: Loss has increased *****
Loss at iteration [403]: 0.21456253006605477
***** Warning: Loss has increased *****
Loss at iteration [404]: 0.2145473360504245
Loss at iteration [405]: 0.2145337053863536
Loss at iteration [406]: 0.21454967127591726
***** Warning: Loss has increased *****
Loss at iteration [407]: 0.21455027972374022
***** Warning: Loss has increased *****
Loss at iteration [408]: 0.21453445448562214
Loss at iteration [409]: 0.21453900034337978
***** Warning: Loss has increased *****
Loss at iteration [410]: 0.21454774695049
***** Warning: Loss has increased *****
Loss at iteration [411]: 0.2145382960069928
Loss at iteration [412]: 0.21453360553611012
Loss at iteration [413]: 0.21454175812090576
***** Warning: Loss has increased *****
Loss at iteration [414]: 0.21454081918720355
Loss at iteration [415]: 0.21453332097057
Loss at iteration [416]: 0.21453589098349468
***** Warning: Loss has increased *****
Loss at iteration [417]: 0.21453997563329694
***** Warning: Loss has increased *****
Loss at iteration [418]: 0.21453543788483745
Loss at iteration [419]: 0.2145329070094038
Loss at iteration [420]: 0.21453672166169138
***** Warning: Loss has increased *****
Loss at iteration [421]: 0.21453679483501467
***** Warning: Loss has increased *****
Loss at iteration [422]: 0.21453303526670298
Loss at iteration [423]: 0.21453359276351097
***** Warning: Loss has increased *****
Loss at iteration [424]: 0.21453595813566434
***** Warning: Loss has increased *****
Loss at iteration [425]: 0.21453433150732656
Loss at iteration [426]: 0.2145324276252097
Loss at iteration [427]: 0.2145338606527825
***** Warning: Loss has increased *****
Loss at iteration [428]: 0.2145346703866418
***** Warning: Loss has increased *****
Loss at iteration [429]: 0.2145329697639731
Loss at iteration [430]: 0.21453240488776373
Loss at iteration [431]: 0.2145336319038739
***** Warning: Loss has increased *****
Loss at iteration [432]: 0.21453356859377148
Loss at iteration [433]: 0.21453234234625787
Loss at iteration [434]: 0.2145324049114837
***** Warning: Loss has increased *****
Loss at iteration [435]: 0.21453318368400037
***** Warning: Loss has increased *****
Loss at iteration [436]: 0.21453281033598148
Loss at iteration [437]: 0.2145320582258532
Loss at iteration [438]: 0.2145323050878743
***** Warning: Loss has increased *****
Loss at iteration [439]: 0.21453272958320607
***** Warning: Loss has increased *****
Loss at iteration [440]: 0.21453233389912466
Loss at iteration [441]: 0.21453189751090718
Loss at iteration [442]: 0.2145321353884162
***** Warning: Loss has increased *****
Loss at iteration [443]: 0.21453235075116475
***** Warning: Loss has increased *****
Loss at iteration [444]: 0.21453202938110136
Loss at iteration [445]: 0.2145317686419017
Loss at iteration [446]: 0.21453194284915672
***** Warning: Loss has increased *****
Loss at iteration [447]: 0.2145320534584639
***** Warning: Loss has increased *****
Loss at iteration [448]: 0.21453181952514516
Loss at iteration [449]: 0.21453164795820895
Loss at iteration [450]: 0.21453175738014255
***** Warning: Loss has increased *****
Loss at iteration [451]: 0.21453182044776792
***** Warning: Loss has increased *****
Loss at iteration [452]: 0.21453165892464277
Loss at iteration [453]: 0.21453153135411662
Loss at iteration [454]: 0.21453158871848077
***** Warning: Loss has increased *****
Loss at iteration [455]: 0.2145316305637561
***** Warning: Loss has increased *****
Loss at iteration [456]: 0.2145315244341744
Loss at iteration [457]: 0.2145314214285511
Loss at iteration [458]: 0.21453143982058814
***** Warning: Loss has increased *****
Loss at iteration [459]: 0.2145314695008997
***** Warning: Loss has increased *****
Loss at iteration [460]: 0.2145314041964018
Loss at iteration [461]: 0.21453131917279578
Loss at iteration [462]: 0.21453130927593478
Loss at iteration [463]: 0.21453132780220086
***** Warning: Loss has increased *****
Loss at iteration [464]: 0.2145312915954982
Loss at iteration [465]: 0.21453122393642063
Loss at iteration [466]: 0.21453119542999383
Loss at iteration [467]: 0.21453120103613693
***** Warning: Loss has increased *****
Loss at iteration [468]: 0.21453118332288376
Loss at iteration [469]: 0.21453113366253668
Loss at iteration [470]: 0.2145310958970549
Loss at iteration [471]: 0.2145310876570534
Loss at iteration [472]: 0.2145310782900086
Loss at iteration [473]: 0.21453104558248584
Loss at iteration [474]: 0.21453100763651614
Loss at iteration [475]: 0.21453098746886787
Loss at iteration [476]: 0.2145309777048063
Loss at iteration [477]: 0.21453095761722688
Loss at iteration [478]: 0.2145309261543129
Loss at iteration [479]: 0.21453089944901144
Loss at iteration [480]: 0.21453088411768448
Loss at iteration [481]: 0.2145308698419012
Loss at iteration [482]: 0.2145308472376491
Loss at iteration [483]: 0.21453082085030736
Loss at iteration [484]: 0.21453079998342997
Loss at iteration [485]: 0.2145307849556418
Loss at iteration [486]: 0.21453076847980504
Loss at iteration [487]: 0.21453074706439776
Loss at iteration [488]: 0.21453072483092572
Loss at iteration [489]: 0.21453070651263756
Loss at iteration [490]: 0.2145306911516134
Loss at iteration [491]: 0.21453067450366978
Loss at iteration [492]: 0.21453065513022992
Loss at iteration [493]: 0.21453063569115494
Loss at iteration [494]: 0.21453061863689454
Loss at iteration [495]: 0.2145306033144883
Loss at iteration [496]: 0.21453058740928305
Loss at iteration [497]: 0.2145305700210055
Loss at iteration [498]: 0.2145305524807137
Loss at iteration [499]: 0.2145305363170822
Loss at iteration [500]: 0.21453052140700224
Loss at iteration [501]: 0.21453050650151795
Loss at iteration [502]: 0.21453049083575806
Loss at iteration [503]: 0.21453047490613592
Loss at iteration [504]: 0.21453045965183476
Loss at iteration [505]: 0.21453044530956422
Loss at iteration [506]: 0.2145304313530775
Loss at iteration [507]: 0.21453041710635862
Loss at iteration [508]: 0.21453040256512088
Loss at iteration [509]: 0.2145303882661924
Loss at iteration [510]: 0.2145303745650751
Loss at iteration [511]: 0.2145303613312233
Loss at iteration [512]: 0.2145303482332878
Loss at iteration [513]: 0.21453033504774105
Loss at iteration [514]: 0.21453032187517787
Loss at iteration [515]: 0.21453030901029774
Loss at iteration [516]: 0.21453029652905
Loss at iteration [517]: 0.21453028432592275
Loss at iteration [518]: 0.21453027222728427
Loss at iteration [519]: 0.214530260193822
Loss at iteration [520]: 0.21453024822546823
Loss at iteration [521]: 0.21453023643223965
Loss at iteration [522]: 0.21453022492852278
Loss at iteration [523]: 0.21453021366135835
Loss at iteration [524]: 0.21453020253151292
Loss at iteration [525]: 0.21453019148732963
Loss at iteration [526]: 0.2145301805423854
Loss at iteration [527]: 0.21453016974909395
Loss at iteration [528]: 0.21453015914846357
Loss at iteration [529]: 0.21453014874200496
Loss at iteration [530]: 0.21453013883565764
Loss at iteration [531]: 0.21453012907733685
Loss at iteration [532]: 0.2145301194098123
Loss at iteration [533]: 0.21453010986847598
Loss at iteration [534]: 0.21453010041850984
Loss at iteration [535]: 0.21453009109958812
Loss at iteration [536]: 0.21453008203983204
Loss at iteration [537]: 0.2145300730910903
Loss at iteration [538]: 0.21453006423019358
Loss at iteration [539]: 0.214530055550787
Loss at iteration [540]: 0.21453004697412517
Loss at iteration [541]: 0.2145300385034626
Loss at iteration [542]: 0.2145300301506456
Loss at iteration [543]: 0.21453002192771006
Loss at iteration [544]: 0.21453001383729883
Loss at iteration [545]: 0.21453000587359078
Loss at iteration [546]: 0.21452999802686176
Loss at iteration [547]: 0.2145299902893616
Loss at iteration [548]: 0.2145299826631701
Loss at iteration [549]: 0.21452997513725552
Loss at iteration [550]: 0.21452996771922006
Loss at iteration [551]: 0.21452996040812208
Loss at iteration [552]: 0.21452995321282484
Loss at iteration [553]: 0.2145299461177018
Loss at iteration [554]: 0.21452993920780541
Loss at iteration [555]: 0.21452993245148033
Loss at iteration [556]: 0.21452992579537267
Loss at iteration [557]: 0.21452991924048093
Loss at iteration [558]: 0.21452991277749806
Loss at iteration [559]: 0.21452990640776562
Loss at iteration [560]: 0.21452990014113915
Loss at iteration [561]: 0.21452989394026684
Loss at iteration [562]: 0.21452988784960159
Loss at iteration [563]: 0.21452988184445712
Loss at iteration [564]: 0.21452987592487727
Loss at iteration [565]: 0.21452987009243715
Loss at iteration [566]: 0.21452986435997892
Loss at iteration [567]: 0.21452985869710234
Loss at iteration [568]: 0.21452985311094167
Loss at iteration [569]: 0.21452984761662122
Loss at iteration [570]: 0.21452984221046445
Loss at iteration [571]: 0.2145298368715761
Loss at iteration [572]: 0.2145298316178285
Loss at iteration [573]: 0.21452982643720697
Loss at iteration [574]: 0.21452982132947696
Loss at iteration [575]: 0.21452981629478682
Loss at iteration [576]: 0.21452981133992172
Loss at iteration [577]: 0.21452980647749445
Loss at iteration [578]: 0.2145298016647667
Loss at iteration [579]: 0.21452979692486948
Loss at iteration [580]: 0.21452979226857244
Loss at iteration [581]: 0.21452978768486294
Loss at iteration [582]: 0.21452978317956825
Loss at iteration [583]: 0.21452977875369159
Loss at iteration [584]: 0.21452977440860505
Loss at iteration [585]: 0.21452977015166938
Loss at iteration [586]: 0.21452976602082952
Loss at iteration [587]: 0.2145297619831692
Loss at iteration [588]: 0.214529758147162
Loss at iteration [589]: 0.21452975452813594
Loss at iteration [590]: 0.2145297512612901
Loss at iteration [591]: 0.21452974846957004
Loss at iteration [592]: 0.21452974648357445
Loss at iteration [593]: 0.21452974578704678
