Model name                            : MLP_Multistep
The number of input features          : 3
The number of output features         : 2
Optimizer name                        : Adam
Learning rate                         : 0.001
Max number of iterations              : 3000
Number of samples in training data    : 171
Number of samples in tests data       : 73
Total training time                   : 7.202215671539307
Total number of parameters            : 201902
Percentage of parameters < 1e-9       : 51.221879921942325%
Percentage of parameters < 1e-7       : 51.221879921942325%
Percentage of parameters < 1e-6       : 51.222375211736384%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5498663969119402
Loss at iteration [2]: 1.471818397627941
Loss at iteration [3]: 1.4063211773242281
Loss at iteration [4]: 1.3467717707469093
Loss at iteration [5]: 1.293832459307908
Loss at iteration [6]: 1.2589973633004932
Loss at iteration [7]: 1.2505132005578594
Loss at iteration [8]: 1.2667298623124752
***** Warning: Loss has increased *****
Loss at iteration [9]: 1.272222859876932
***** Warning: Loss has increased *****
Loss at iteration [10]: 1.2589770736368338
Loss at iteration [11]: 1.2374074057814108
Loss at iteration [12]: 1.2182093621779733
Loss at iteration [13]: 1.206205583412051
Loss at iteration [14]: 1.199900053929627
Loss at iteration [15]: 1.1961364506308596
Loss at iteration [16]: 1.191043849714526
Loss at iteration [17]: 1.1831790930603499
Loss at iteration [18]: 1.1718726751984316
Loss at iteration [19]: 1.1582761649959346
Loss at iteration [20]: 1.145487768994344
Loss at iteration [21]: 1.134976861709356
Loss at iteration [22]: 1.1274335369143729
Loss at iteration [23]: 1.1197526928652553
Loss at iteration [24]: 1.11035039066709
Loss at iteration [25]: 1.1012945630029016
Loss at iteration [26]: 1.0927397972322916
Loss at iteration [27]: 1.08598035820956
Loss at iteration [28]: 1.0775259711081382
Loss at iteration [29]: 1.0638056331254833
Loss at iteration [30]: 1.0544577267775521
Loss at iteration [31]: 1.0510811489391954
Loss at iteration [32]: 1.043634129729552
Loss at iteration [33]: 1.0300730372675166
Loss at iteration [34]: 1.0205003883118977
Loss at iteration [35]: 1.0153922760908867
Loss at iteration [36]: 1.0087407116490623
Loss at iteration [37]: 0.9972320533885947
Loss at iteration [38]: 0.9865781665589676
Loss at iteration [39]: 0.9795471881821705
Loss at iteration [40]: 0.9738640929324204
Loss at iteration [41]: 0.9687870684170264
Loss at iteration [42]: 0.9618100216594927
Loss at iteration [43]: 0.9510301736561787
Loss at iteration [44]: 0.939150225060761
Loss at iteration [45]: 0.9310676299992467
Loss at iteration [46]: 0.9266657012316236
Loss at iteration [47]: 0.9251090409480673
Loss at iteration [48]: 0.9219674024062158
Loss at iteration [49]: 0.9123186243164753
Loss at iteration [50]: 0.8971639169068804
Loss at iteration [51]: 0.8888112927061068
Loss at iteration [52]: 0.8879704598890967
Loss at iteration [53]: 0.8859715875017391
Loss at iteration [54]: 0.8771092701884962
Loss at iteration [55]: 0.8640249009603694
Loss at iteration [56]: 0.8568081692392341
Loss at iteration [57]: 0.8556410599114134
Loss at iteration [58]: 0.8541807373273433
Loss at iteration [59]: 0.8467945261075293
Loss at iteration [60]: 0.8349145661842422
Loss at iteration [61]: 0.8269875885701565
Loss at iteration [62]: 0.8255407110972897
Loss at iteration [63]: 0.8245148708328883
Loss at iteration [64]: 0.8211925592218244
Loss at iteration [65]: 0.8136376649187251
Loss at iteration [66]: 0.8045973633003434
Loss at iteration [67]: 0.798409100928825
Loss at iteration [68]: 0.7962942667369602
Loss at iteration [69]: 0.7958928192200614
Loss at iteration [70]: 0.7959004147691484
***** Warning: Loss has increased *****
Loss at iteration [71]: 0.7952930035563611
Loss at iteration [72]: 0.7905059570735053
Loss at iteration [73]: 0.7818596470815564
Loss at iteration [74]: 0.7743021177338937
Loss at iteration [75]: 0.7713296412353235
Loss at iteration [76]: 0.7716983794231033
***** Warning: Loss has increased *****
Loss at iteration [77]: 0.772316105332694
***** Warning: Loss has increased *****
Loss at iteration [78]: 0.7713726622436445
Loss at iteration [79]: 0.7656751574242927
Loss at iteration [80]: 0.7583670248262327
Loss at iteration [81]: 0.7527357000796735
Loss at iteration [82]: 0.7500639668668668
Loss at iteration [83]: 0.7488533295333087
Loss at iteration [84]: 0.7481352477179237
Loss at iteration [85]: 0.7476471710573495
Loss at iteration [86]: 0.74773813006228
***** Warning: Loss has increased *****
Loss at iteration [87]: 0.7472574548956301
Loss at iteration [88]: 0.743696643887076
Loss at iteration [89]: 0.7375687890014713
Loss at iteration [90]: 0.7309984118660625
Loss at iteration [91]: 0.727314744928143
Loss at iteration [92]: 0.7258713737200645
Loss at iteration [93]: 0.7251162841716601
Loss at iteration [94]: 0.7248091309030695
Loss at iteration [95]: 0.7249981096040107
***** Warning: Loss has increased *****
Loss at iteration [96]: 0.7242822306587172
Loss at iteration [97]: 0.7225277710239494
Loss at iteration [98]: 0.718948548110709
Loss at iteration [99]: 0.7135391742435282
Loss at iteration [100]: 0.7078351298525019
Loss at iteration [101]: 0.7038527135054653
Loss at iteration [102]: 0.7015360514116207
Loss at iteration [103]: 0.7007836910647439
Loss at iteration [104]: 0.7005665076679876
Loss at iteration [105]: 0.7007803339408998
***** Warning: Loss has increased *****
Loss at iteration [106]: 0.7026465214559061
***** Warning: Loss has increased *****
Loss at iteration [107]: 0.7049479534578591
***** Warning: Loss has increased *****
Loss at iteration [108]: 0.7071579156295728
***** Warning: Loss has increased *****
Loss at iteration [109]: 0.7037368183485299
Loss at iteration [110]: 0.6931267923429503
Loss at iteration [111]: 0.6828820183048849
Loss at iteration [112]: 0.6790995741755151
Loss at iteration [113]: 0.6821512174671569
***** Warning: Loss has increased *****
Loss at iteration [114]: 0.6860746342680474
***** Warning: Loss has increased *****
Loss at iteration [115]: 0.6860150688798331
Loss at iteration [116]: 0.6808963442917018
Loss at iteration [117]: 0.6723228495396807
Loss at iteration [118]: 0.665565933831675
Loss at iteration [119]: 0.6644797272739684
Loss at iteration [120]: 0.6671960890935428
***** Warning: Loss has increased *****
Loss at iteration [121]: 0.6681459274139672
***** Warning: Loss has increased *****
Loss at iteration [122]: 0.6659642892773601
Loss at iteration [123]: 0.6608744045782946
Loss at iteration [124]: 0.6548550461673568
Loss at iteration [125]: 0.6502766729716313
Loss at iteration [126]: 0.6479633127346613
Loss at iteration [127]: 0.6479378567856483
Loss at iteration [128]: 0.6493708077613891
***** Warning: Loss has increased *****
Loss at iteration [129]: 0.6508807014697247
***** Warning: Loss has increased *****
Loss at iteration [130]: 0.6517853728132905
***** Warning: Loss has increased *****
Loss at iteration [131]: 0.6495938353401693
Loss at iteration [132]: 0.6431273380416926
Loss at iteration [133]: 0.634873057629771
Loss at iteration [134]: 0.6283659492891175
Loss at iteration [135]: 0.624316385620621
Loss at iteration [136]: 0.6240211271499909
Loss at iteration [137]: 0.6240388256150368
***** Warning: Loss has increased *****
Loss at iteration [138]: 0.6249623102819466
***** Warning: Loss has increased *****
Loss at iteration [139]: 0.6292559380831975
***** Warning: Loss has increased *****
Loss at iteration [140]: 0.637536111293703
***** Warning: Loss has increased *****
Loss at iteration [141]: 0.6487377706085152
***** Warning: Loss has increased *****
Loss at iteration [142]: 0.6494211217776302
***** Warning: Loss has increased *****
Loss at iteration [143]: 0.62967046479319
Loss at iteration [144]: 0.6063557635962514
Loss at iteration [145]: 0.601233953788453
Loss at iteration [146]: 0.6113630498514322
***** Warning: Loss has increased *****
Loss at iteration [147]: 0.616108780425009
***** Warning: Loss has increased *****
Loss at iteration [148]: 0.6083456737183491
Loss at iteration [149]: 0.5943089807322274
Loss at iteration [150]: 0.5863544454769186
Loss at iteration [151]: 0.5892084939205079
***** Warning: Loss has increased *****
Loss at iteration [152]: 0.5935547663305928
***** Warning: Loss has increased *****
Loss at iteration [153]: 0.5907834199467503
Loss at iteration [154]: 0.5816870796358272
Loss at iteration [155]: 0.5724239869555153
Loss at iteration [156]: 0.5695257652957925
Loss at iteration [157]: 0.5706723703285052
***** Warning: Loss has increased *****
Loss at iteration [158]: 0.5715777259548942
***** Warning: Loss has increased *****
Loss at iteration [159]: 0.5697610965190865
Loss at iteration [160]: 0.5647206798411135
Loss at iteration [161]: 0.5595829405450461
Loss at iteration [162]: 0.5539490049037094
Loss at iteration [163]: 0.5502622182397866
Loss at iteration [164]: 0.5461742728561935
Loss at iteration [165]: 0.5421178469867283
Loss at iteration [166]: 0.538400708052965
Loss at iteration [167]: 0.5357460274448113
Loss at iteration [168]: 0.5328552505815367
Loss at iteration [169]: 0.5289294150305924
Loss at iteration [170]: 0.5264721478401843
Loss at iteration [171]: 0.5254356619197659
Loss at iteration [172]: 0.5301008998444321
***** Warning: Loss has increased *****
Loss at iteration [173]: 0.5560739101313622
***** Warning: Loss has increased *****
Loss at iteration [174]: 0.6403706969149224
***** Warning: Loss has increased *****
Loss at iteration [175]: 0.7812877816216182
***** Warning: Loss has increased *****
Loss at iteration [176]: 0.7484467848077964
Loss at iteration [177]: 0.5149956555590998
Loss at iteration [178]: 0.6504159746501016
***** Warning: Loss has increased *****
Loss at iteration [179]: 0.6321650762468687
Loss at iteration [180]: 0.5176347801347869
Loss at iteration [181]: 0.6447489233984717
***** Warning: Loss has increased *****
Loss at iteration [182]: 0.5127825975047946
Loss at iteration [183]: 0.5804314048846339
***** Warning: Loss has increased *****
Loss at iteration [184]: 0.539471778952471
Loss at iteration [185]: 0.5253997473213664
Loss at iteration [186]: 0.5542414974447671
***** Warning: Loss has increased *****
Loss at iteration [187]: 0.4955168703338035
Loss at iteration [188]: 0.5499504668667615
***** Warning: Loss has increased *****
Loss at iteration [189]: 0.485493437664359
Loss at iteration [190]: 0.5318341357969031
***** Warning: Loss has increased *****
Loss at iteration [191]: 0.48431040856769464
Loss at iteration [192]: 0.510985853885255
***** Warning: Loss has increased *****
Loss at iteration [193]: 0.4868645197722319
Loss at iteration [194]: 0.48995848949980175
***** Warning: Loss has increased *****
Loss at iteration [195]: 0.4887023356414669
Loss at iteration [196]: 0.47271227406444477
Loss at iteration [197]: 0.48737542775930365
***** Warning: Loss has increased *****
Loss at iteration [198]: 0.4628915487842037
Loss at iteration [199]: 0.4790882336566097
***** Warning: Loss has increased *****
Loss at iteration [200]: 0.45989324491107963
Loss at iteration [201]: 0.4634487998896261
***** Warning: Loss has increased *****
Loss at iteration [202]: 0.4612664457488103
Loss at iteration [203]: 0.4516646102173974
Loss at iteration [204]: 0.4593774053584569
***** Warning: Loss has increased *****
Loss at iteration [205]: 0.444555593074946
Loss at iteration [206]: 0.44966777134095354
***** Warning: Loss has increased *****
Loss at iteration [207]: 0.44205159313946
Loss at iteration [208]: 0.4369137222012734
Loss at iteration [209]: 0.4401270787834601
***** Warning: Loss has increased *****
Loss at iteration [210]: 0.43005835265482756
Loss at iteration [211]: 0.43032188336203403
***** Warning: Loss has increased *****
Loss at iteration [212]: 0.42882431168531754
Loss at iteration [213]: 0.42114901378175906
Loss at iteration [214]: 0.42268628091525
***** Warning: Loss has increased *****
Loss at iteration [215]: 0.4192433230913301
Loss at iteration [216]: 0.4128017239908579
Loss at iteration [217]: 0.41310666166610804
***** Warning: Loss has increased *****
Loss at iteration [218]: 0.4101733394312449
Loss at iteration [219]: 0.40477755286836237
Loss at iteration [220]: 0.4041136332856055
Loss at iteration [221]: 0.40156807656693144
Loss at iteration [222]: 0.39630090964919223
Loss at iteration [223]: 0.39412899417290503
Loss at iteration [224]: 0.39195034250033195
Loss at iteration [225]: 0.38859268576378736
Loss at iteration [226]: 0.38449396109070133
Loss at iteration [227]: 0.381945587825178
Loss at iteration [228]: 0.3798628092237258
Loss at iteration [229]: 0.3774247199837932
Loss at iteration [230]: 0.37313371240632
Loss at iteration [231]: 0.36972091812359376
Loss at iteration [232]: 0.3671395038337354
Loss at iteration [233]: 0.36420796597263466
Loss at iteration [234]: 0.3624120947958007
Loss at iteration [235]: 0.36042139547979346
Loss at iteration [236]: 0.35646077971664564
Loss at iteration [237]: 0.35314854950235197
Loss at iteration [238]: 0.3501130202165136
Loss at iteration [239]: 0.3464682049954081
Loss at iteration [240]: 0.3432453394814569
Loss at iteration [241]: 0.33969258627853927
Loss at iteration [242]: 0.33735120947806013
Loss at iteration [243]: 0.3353796467322778
Loss at iteration [244]: 0.33236399364519703
Loss at iteration [245]: 0.33019557977275155
Loss at iteration [246]: 0.33414483939768025
***** Warning: Loss has increased *****
Loss at iteration [247]: 0.3474616033105954
***** Warning: Loss has increased *****
Loss at iteration [248]: 0.37854235462603136
***** Warning: Loss has increased *****
Loss at iteration [249]: 0.45859460343308006
***** Warning: Loss has increased *****
Loss at iteration [250]: 0.5607816490993518
***** Warning: Loss has increased *****
Loss at iteration [251]: 0.5680365996599502
***** Warning: Loss has increased *****
Loss at iteration [252]: 0.3739154037405061
Loss at iteration [253]: 0.34075475072847045
Loss at iteration [254]: 0.4677534740648298
***** Warning: Loss has increased *****
Loss at iteration [255]: 0.40553920698565915
Loss at iteration [256]: 0.3104955885404839
Loss at iteration [257]: 0.3944217217063987
***** Warning: Loss has increased *****
Loss at iteration [258]: 0.3815058387652019
Loss at iteration [259]: 0.3057792456553918
Loss at iteration [260]: 0.3468109207151256
***** Warning: Loss has increased *****
Loss at iteration [261]: 0.3576660263751185
***** Warning: Loss has increased *****
Loss at iteration [262]: 0.30147976076424143
Loss at iteration [263]: 0.3251004325944089
***** Warning: Loss has increased *****
Loss at iteration [264]: 0.33523480754802054
***** Warning: Loss has increased *****
Loss at iteration [265]: 0.2959234944232172
Loss at iteration [266]: 0.30914914893374584
***** Warning: Loss has increased *****
Loss at iteration [267]: 0.3200212431262892
***** Warning: Loss has increased *****
Loss at iteration [268]: 0.2925451309848928
Loss at iteration [269]: 0.29336825928966614
***** Warning: Loss has increased *****
Loss at iteration [270]: 0.3071207168181708
***** Warning: Loss has increased *****
Loss at iteration [271]: 0.285534508742926
Loss at iteration [272]: 0.28381166234218325
Loss at iteration [273]: 0.294219985319458
***** Warning: Loss has increased *****
Loss at iteration [274]: 0.2832664135322301
Loss at iteration [275]: 0.27485931612109676
Loss at iteration [276]: 0.2837214700679071
***** Warning: Loss has increased *****
Loss at iteration [277]: 0.27942532178739204
Loss at iteration [278]: 0.2678667229147931
Loss at iteration [279]: 0.2713166525271612
***** Warning: Loss has increased *****
Loss at iteration [280]: 0.2743697479708425
***** Warning: Loss has increased *****
Loss at iteration [281]: 0.2674146723752944
Loss at iteration [282]: 0.26048904490018926
Loss at iteration [283]: 0.26325184590746864
***** Warning: Loss has increased *****
Loss at iteration [284]: 0.26412259955942996
***** Warning: Loss has increased *****
Loss at iteration [285]: 0.25792666647371754
Loss at iteration [286]: 0.2536935300463502
Loss at iteration [287]: 0.2538706598235771
***** Warning: Loss has increased *****
Loss at iteration [288]: 0.2550648401605726
***** Warning: Loss has increased *****
Loss at iteration [289]: 0.25221851634873316
Loss at iteration [290]: 0.24774795647619613
Loss at iteration [291]: 0.24584643570678502
Loss at iteration [292]: 0.24498177063851123
Loss at iteration [293]: 0.24447803660501946
Loss at iteration [294]: 0.2432573846036977
Loss at iteration [295]: 0.24105007317892296
Loss at iteration [296]: 0.23820486644033032
Loss at iteration [297]: 0.23536269115710812
Loss at iteration [298]: 0.23324294647976698
Loss at iteration [299]: 0.2321704396268371
Loss at iteration [300]: 0.23138316383535426
Loss at iteration [301]: 0.23066816837555448
Loss at iteration [302]: 0.23047114681900052
Loss at iteration [303]: 0.23091577250443576
***** Warning: Loss has increased *****
Loss at iteration [304]: 0.23362274569894656
***** Warning: Loss has increased *****
Loss at iteration [305]: 0.2388251556680139
***** Warning: Loss has increased *****
Loss at iteration [306]: 0.24823966350266854
***** Warning: Loss has increased *****
Loss at iteration [307]: 0.26185969627758116
***** Warning: Loss has increased *****
Loss at iteration [308]: 0.2849662727734498
***** Warning: Loss has increased *****
Loss at iteration [309]: 0.3069549361648453
***** Warning: Loss has increased *****
Loss at iteration [310]: 0.32787717880753753
***** Warning: Loss has increased *****
Loss at iteration [311]: 0.31274992684931835
Loss at iteration [312]: 0.2683538028347083
Loss at iteration [313]: 0.22430994768912205
Loss at iteration [314]: 0.22020754188260405
Loss at iteration [315]: 0.24827378958045304
***** Warning: Loss has increased *****
Loss at iteration [316]: 0.2703440489042834
***** Warning: Loss has increased *****
Loss at iteration [317]: 0.2662474233290968
Loss at iteration [318]: 0.2341865249537452
Loss at iteration [319]: 0.21132259687757726
Loss at iteration [320]: 0.21450550377939803
***** Warning: Loss has increased *****
Loss at iteration [321]: 0.2326547045592045
***** Warning: Loss has increased *****
Loss at iteration [322]: 0.24442361159779913
***** Warning: Loss has increased *****
Loss at iteration [323]: 0.23325158049097913
Loss at iteration [324]: 0.213992992547035
Loss at iteration [325]: 0.2046389154252998
Loss at iteration [326]: 0.21095804068548818
***** Warning: Loss has increased *****
Loss at iteration [327]: 0.2230206808522644
***** Warning: Loss has increased *****
Loss at iteration [328]: 0.2263478997388178
***** Warning: Loss has increased *****
Loss at iteration [329]: 0.2194810614772412
Loss at iteration [330]: 0.20673917384298962
Loss at iteration [331]: 0.1997888840730606
Loss at iteration [332]: 0.20118349651569828
***** Warning: Loss has increased *****
Loss at iteration [333]: 0.2075518319897277
***** Warning: Loss has increased *****
Loss at iteration [334]: 0.2142478984576735
***** Warning: Loss has increased *****
Loss at iteration [335]: 0.21547815326316846
***** Warning: Loss has increased *****
Loss at iteration [336]: 0.21100010869569952
Loss at iteration [337]: 0.2026802457947089
Loss at iteration [338]: 0.19567711739512708
Loss at iteration [339]: 0.19266495110896067
Loss at iteration [340]: 0.19367594064593416
***** Warning: Loss has increased *****
Loss at iteration [341]: 0.19713163633916927
***** Warning: Loss has increased *****
Loss at iteration [342]: 0.20279406804029268
***** Warning: Loss has increased *****
Loss at iteration [343]: 0.20968189293575157
***** Warning: Loss has increased *****
Loss at iteration [344]: 0.2141484730348864
***** Warning: Loss has increased *****
Loss at iteration [345]: 0.21690433906662973
***** Warning: Loss has increased *****
Loss at iteration [346]: 0.21565416336162418
Loss at iteration [347]: 0.21576393168062163
***** Warning: Loss has increased *****
Loss at iteration [348]: 0.21107611589576408
Loss at iteration [349]: 0.2035680527931798
Loss at iteration [350]: 0.19481720264326458
Loss at iteration [351]: 0.18834985222507175
Loss at iteration [352]: 0.18426623464230468
Loss at iteration [353]: 0.1829105776225568
Loss at iteration [354]: 0.18290581374787718
Loss at iteration [355]: 0.18438148685294217
***** Warning: Loss has increased *****
Loss at iteration [356]: 0.18751043641816048
***** Warning: Loss has increased *****
Loss at iteration [357]: 0.19303089033372686
***** Warning: Loss has increased *****
Loss at iteration [358]: 0.2036807473591538
***** Warning: Loss has increased *****
Loss at iteration [359]: 0.2186423474872917
***** Warning: Loss has increased *****
Loss at iteration [360]: 0.2462088694941283
***** Warning: Loss has increased *****
Loss at iteration [361]: 0.2733455119934456
***** Warning: Loss has increased *****
Loss at iteration [362]: 0.2938153600539713
***** Warning: Loss has increased *****
Loss at iteration [363]: 0.27432174159846523
Loss at iteration [364]: 0.22793093630835742
Loss at iteration [365]: 0.18302629493710007
Loss at iteration [366]: 0.18311980001782738
***** Warning: Loss has increased *****
Loss at iteration [367]: 0.21362520049607497
***** Warning: Loss has increased *****
Loss at iteration [368]: 0.22839689743367755
***** Warning: Loss has increased *****
Loss at iteration [369]: 0.21291918509601454
Loss at iteration [370]: 0.18205060980950083
Loss at iteration [371]: 0.17459553302544736
Loss at iteration [372]: 0.19111426479134375
***** Warning: Loss has increased *****
Loss at iteration [373]: 0.2049261485231253
***** Warning: Loss has increased *****
Loss at iteration [374]: 0.2025822813793201
Loss at iteration [375]: 0.18339804992302788
Loss at iteration [376]: 0.17092179043316563
Loss at iteration [377]: 0.17269280900764078
***** Warning: Loss has increased *****
Loss at iteration [378]: 0.18388041399033164
***** Warning: Loss has increased *****
Loss at iteration [379]: 0.19402491019760018
***** Warning: Loss has increased *****
Loss at iteration [380]: 0.18990056268367045
Loss at iteration [381]: 0.17892440207115198
Loss at iteration [382]: 0.16871498611256314
Loss at iteration [383]: 0.16763041294576533
Loss at iteration [384]: 0.1738547052836113
***** Warning: Loss has increased *****
Loss at iteration [385]: 0.1796892606568694
***** Warning: Loss has increased *****
Loss at iteration [386]: 0.1810517698456225
***** Warning: Loss has increased *****
Loss at iteration [387]: 0.17622680793954656
Loss at iteration [388]: 0.16862920582719965
Loss at iteration [389]: 0.16370960966193215
Loss at iteration [390]: 0.1622672956713117
Loss at iteration [391]: 0.16400456036087308
***** Warning: Loss has increased *****
Loss at iteration [392]: 0.16701533635540447
***** Warning: Loss has increased *****
Loss at iteration [393]: 0.17057654843983658
***** Warning: Loss has increased *****
Loss at iteration [394]: 0.17283917830887718
***** Warning: Loss has increased *****
Loss at iteration [395]: 0.1740872113306086
***** Warning: Loss has increased *****
Loss at iteration [396]: 0.1729389285962963
Loss at iteration [397]: 0.17094509317911674
Loss at iteration [398]: 0.1668474244618559
Loss at iteration [399]: 0.1624051155800313
Loss at iteration [400]: 0.15873811570073904
Loss at iteration [401]: 0.15627154367807206
Loss at iteration [402]: 0.1548460560351244
Loss at iteration [403]: 0.15445536249984235
Loss at iteration [404]: 0.1549658189366802
***** Warning: Loss has increased *****
Loss at iteration [405]: 0.15621257493922425
***** Warning: Loss has increased *****
Loss at iteration [406]: 0.15960544393570322
***** Warning: Loss has increased *****
Loss at iteration [407]: 0.1658224061842626
***** Warning: Loss has increased *****
Loss at iteration [408]: 0.1796616555895453
***** Warning: Loss has increased *****
Loss at iteration [409]: 0.20443779417462474
***** Warning: Loss has increased *****
Loss at iteration [410]: 0.25320925743540756
***** Warning: Loss has increased *****
Loss at iteration [411]: 0.3120192574360807
***** Warning: Loss has increased *****
Loss at iteration [412]: 0.3758583753347141
***** Warning: Loss has increased *****
Loss at iteration [413]: 0.3317337423911345
Loss at iteration [414]: 0.21986086139147354
Loss at iteration [415]: 0.15488249008592858
Loss at iteration [416]: 0.20948902152759571
***** Warning: Loss has increased *****
Loss at iteration [417]: 0.2602005403221264
***** Warning: Loss has increased *****
Loss at iteration [418]: 0.20453035777646497
Loss at iteration [419]: 0.15237662857701026
Loss at iteration [420]: 0.18229830104771522
***** Warning: Loss has increased *****
Loss at iteration [421]: 0.21267281971462637
***** Warning: Loss has increased *****
Loss at iteration [422]: 0.1828584204586701
Loss at iteration [423]: 0.1498911443744316
Loss at iteration [424]: 0.16873158393257257
***** Warning: Loss has increased *****
Loss at iteration [425]: 0.1913316011964643
***** Warning: Loss has increased *****
Loss at iteration [426]: 0.17021507006444386
Loss at iteration [427]: 0.1486154823545772
Loss at iteration [428]: 0.15960644729795273
***** Warning: Loss has increased *****
Loss at iteration [429]: 0.1734412542663333
***** Warning: Loss has increased *****
Loss at iteration [430]: 0.16164440281353784
Loss at iteration [431]: 0.14607914615470044
Loss at iteration [432]: 0.1523831905961071
***** Warning: Loss has increased *****
Loss at iteration [433]: 0.16347466298600105
***** Warning: Loss has increased *****
Loss at iteration [434]: 0.15629361756852608
Loss at iteration [435]: 0.14497097967943018
Loss at iteration [436]: 0.1453317312781285
***** Warning: Loss has increased *****
Loss at iteration [437]: 0.15243242251667988
***** Warning: Loss has increased *****
Loss at iteration [438]: 0.15264812778779552
***** Warning: Loss has increased *****
Loss at iteration [439]: 0.14490336480165442
Loss at iteration [440]: 0.14150293341641038
Loss at iteration [441]: 0.14475507095750112
***** Warning: Loss has increased *****
Loss at iteration [442]: 0.14797225179928022
***** Warning: Loss has increased *****
Loss at iteration [443]: 0.14606771693603626
Loss at iteration [444]: 0.14150071621429625
Loss at iteration [445]: 0.13940373183704208
Loss at iteration [446]: 0.14034322341571157
***** Warning: Loss has increased *****
Loss at iteration [447]: 0.14183846613301027
***** Warning: Loss has increased *****
Loss at iteration [448]: 0.14269992671923962
***** Warning: Loss has increased *****
Loss at iteration [449]: 0.14085198937955276
Loss at iteration [450]: 0.13811062083308195
Loss at iteration [451]: 0.13645020056211044
Loss at iteration [452]: 0.13634804504233602
Loss at iteration [453]: 0.1372386520633843
***** Warning: Loss has increased *****
Loss at iteration [454]: 0.1378289604596511
***** Warning: Loss has increased *****
Loss at iteration [455]: 0.137891417509433
***** Warning: Loss has increased *****
Loss at iteration [456]: 0.1370265465304158
Loss at iteration [457]: 0.13556599447466855
Loss at iteration [458]: 0.1340485371273807
Loss at iteration [459]: 0.13307672182945507
Loss at iteration [460]: 0.1324549833213518
Loss at iteration [461]: 0.13193447744408987
Loss at iteration [462]: 0.13174812995611415
Loss at iteration [463]: 0.13177712292578375
***** Warning: Loss has increased *****
Loss at iteration [464]: 0.1317781301588457
***** Warning: Loss has increased *****
Loss at iteration [465]: 0.13258272540838248
***** Warning: Loss has increased *****
Loss at iteration [466]: 0.1342563309009449
***** Warning: Loss has increased *****
Loss at iteration [467]: 0.1365295213223442
***** Warning: Loss has increased *****
Loss at iteration [468]: 0.14024624100530966
***** Warning: Loss has increased *****
Loss at iteration [469]: 0.14542303862587788
***** Warning: Loss has increased *****
Loss at iteration [470]: 0.15498653028861586
***** Warning: Loss has increased *****
Loss at iteration [471]: 0.16727792504135755
***** Warning: Loss has increased *****
Loss at iteration [472]: 0.18659786494884403
***** Warning: Loss has increased *****
Loss at iteration [473]: 0.2010486197888399
***** Warning: Loss has increased *****
Loss at iteration [474]: 0.21433557402974784
***** Warning: Loss has increased *****
Loss at iteration [475]: 0.20548836663591694
Loss at iteration [476]: 0.1820548110416251
Loss at iteration [477]: 0.14911714202068113
Loss at iteration [478]: 0.12846068336842695
Loss at iteration [479]: 0.12984182415645434
***** Warning: Loss has increased *****
Loss at iteration [480]: 0.14736631023403005
***** Warning: Loss has increased *****
Loss at iteration [481]: 0.16666805396899373
***** Warning: Loss has increased *****
Loss at iteration [482]: 0.17183732167597823
***** Warning: Loss has increased *****
Loss at iteration [483]: 0.1615651850753542
Loss at iteration [484]: 0.14050778175389617
Loss at iteration [485]: 0.12576237561560671
Loss at iteration [486]: 0.12549921652618906
Loss at iteration [487]: 0.13594745716946918
***** Warning: Loss has increased *****
Loss at iteration [488]: 0.14804316816098526
***** Warning: Loss has increased *****
Loss at iteration [489]: 0.151854358137152
***** Warning: Loss has increased *****
Loss at iteration [490]: 0.14766549761615239
Loss at iteration [491]: 0.1357667865506631
Loss at iteration [492]: 0.1256026647014739
Loss at iteration [493]: 0.12164187012715853
Loss at iteration [494]: 0.12418437546748083
***** Warning: Loss has increased *****
Loss at iteration [495]: 0.129698586103737
***** Warning: Loss has increased *****
Loss at iteration [496]: 0.13418732372642544
***** Warning: Loss has increased *****
Loss at iteration [497]: 0.13495779423137175
***** Warning: Loss has increased *****
Loss at iteration [498]: 0.13090623630121972
Loss at iteration [499]: 0.12558091043493358
Loss at iteration [500]: 0.12138134968965855
Loss at iteration [501]: 0.11882718706349557
Loss at iteration [502]: 0.11867052156580418
Loss at iteration [503]: 0.12029497464642698
***** Warning: Loss has increased *****
Loss at iteration [504]: 0.12277840547933941
***** Warning: Loss has increased *****
Loss at iteration [505]: 0.12503740282054532
***** Warning: Loss has increased *****
Loss at iteration [506]: 0.12714024918018424
***** Warning: Loss has increased *****
Loss at iteration [507]: 0.12786805963475836
***** Warning: Loss has increased *****
Loss at iteration [508]: 0.12860698720037428
***** Warning: Loss has increased *****
Loss at iteration [509]: 0.12908719258044016
***** Warning: Loss has increased *****
Loss at iteration [510]: 0.12944013171990357
***** Warning: Loss has increased *****
Loss at iteration [511]: 0.1290412965614759
Loss at iteration [512]: 0.129455449555259
***** Warning: Loss has increased *****
Loss at iteration [513]: 0.12822949887678475
Loss at iteration [514]: 0.12647844232931235
Loss at iteration [515]: 0.12350961823957292
Loss at iteration [516]: 0.12047298930430894
Loss at iteration [517]: 0.1176638516282747
Loss at iteration [518]: 0.11558071003303595
Loss at iteration [519]: 0.11394892403054016
Loss at iteration [520]: 0.11285009312081906
Loss at iteration [521]: 0.11253819862508854
Loss at iteration [522]: 0.11223064891146468
Loss at iteration [523]: 0.11175766115946874
Loss at iteration [524]: 0.1113755160109014
Loss at iteration [525]: 0.11120805313356608
Loss at iteration [526]: 0.1110512116900345
Loss at iteration [527]: 0.11090037590305475
Loss at iteration [528]: 0.11131299765727372
***** Warning: Loss has increased *****
Loss at iteration [529]: 0.11294786726352934
***** Warning: Loss has increased *****
Loss at iteration [530]: 0.1175303035426579
***** Warning: Loss has increased *****
Loss at iteration [531]: 0.13002486298721022
***** Warning: Loss has increased *****
Loss at iteration [532]: 0.1591784172971744
***** Warning: Loss has increased *****
Loss at iteration [533]: 0.23483009281858858
***** Warning: Loss has increased *****
Loss at iteration [534]: 0.3618860929709429
***** Warning: Loss has increased *****
Loss at iteration [535]: 0.5738263154294714
***** Warning: Loss has increased *****
Loss at iteration [536]: 0.532686514143163
Loss at iteration [537]: 0.28242138443670173
Loss at iteration [538]: 0.11920362216424293
Loss at iteration [539]: 0.2965788145000611
***** Warning: Loss has increased *****
Loss at iteration [540]: 0.3148190514631057
***** Warning: Loss has increased *****
Loss at iteration [541]: 0.12580548629913832
Loss at iteration [542]: 0.22981985628279747
***** Warning: Loss has increased *****
Loss at iteration [543]: 0.2618206794257538
***** Warning: Loss has increased *****
Loss at iteration [544]: 0.1213925309897923
Loss at iteration [545]: 0.2205378506620139
***** Warning: Loss has increased *****
Loss at iteration [546]: 0.20771255197772312
Loss at iteration [547]: 0.12149824786425152
Loss at iteration [548]: 0.21495188548984318
***** Warning: Loss has increased *****
Loss at iteration [549]: 0.16133809777019348
Loss at iteration [550]: 0.13183906083910074
Loss at iteration [551]: 0.19459345660702973
***** Warning: Loss has increased *****
Loss at iteration [552]: 0.12899250600936987
Loss at iteration [553]: 0.1422199529603756
***** Warning: Loss has increased *****
Loss at iteration [554]: 0.16592460919532692
***** Warning: Loss has increased *****
Loss at iteration [555]: 0.11502660311466541
Loss at iteration [556]: 0.14624061684619005
***** Warning: Loss has increased *****
Loss at iteration [557]: 0.1404837588385051
Loss at iteration [558]: 0.11383593900614347
Loss at iteration [559]: 0.1418780820695953
***** Warning: Loss has increased *****
Loss at iteration [560]: 0.12273200244169499
Loss at iteration [561]: 0.11715287010288504
Loss at iteration [562]: 0.13417331994044998
***** Warning: Loss has increased *****
Loss at iteration [563]: 0.11357538514071171
Loss at iteration [564]: 0.11942359285309068
***** Warning: Loss has increased *****
Loss at iteration [565]: 0.12523853188963127
***** Warning: Loss has increased *****
Loss at iteration [566]: 0.10995114897873402
Loss at iteration [567]: 0.1191693202555529
***** Warning: Loss has increased *****
Loss at iteration [568]: 0.11908600710871044
Loss at iteration [569]: 0.10884335394997434
Loss at iteration [570]: 0.1172859883595712
***** Warning: Loss has increased *****
Loss at iteration [571]: 0.11440757701757269
Loss at iteration [572]: 0.10801595262812923
Loss at iteration [573]: 0.11516605680001654
***** Warning: Loss has increased *****
Loss at iteration [574]: 0.11181672415868074
Loss at iteration [575]: 0.1069006880115028
Loss at iteration [576]: 0.11271383453431529
***** Warning: Loss has increased *****
Loss at iteration [577]: 0.10953681565283899
Loss at iteration [578]: 0.10610137290926848
Loss at iteration [579]: 0.11031313452622375
***** Warning: Loss has increased *****
Loss at iteration [580]: 0.10788063046577713
Loss at iteration [581]: 0.10515293270057133
Loss at iteration [582]: 0.10811644917110728
***** Warning: Loss has increased *****
Loss at iteration [583]: 0.10682541713593101
Loss at iteration [584]: 0.10405678849374632
Loss at iteration [585]: 0.10621730020502046
***** Warning: Loss has increased *****
Loss at iteration [586]: 0.10606839594648379
Loss at iteration [587]: 0.10335079948510104
Loss at iteration [588]: 0.10440364752781986
***** Warning: Loss has increased *****
Loss at iteration [589]: 0.10476244316749452
***** Warning: Loss has increased *****
Loss at iteration [590]: 0.10253663269932102
Loss at iteration [591]: 0.10264806297066528
***** Warning: Loss has increased *****
Loss at iteration [592]: 0.10332623501967847
***** Warning: Loss has increased *****
Loss at iteration [593]: 0.10182889306875756
Loss at iteration [594]: 0.10114226477124472
Loss at iteration [595]: 0.10199095672353765
***** Warning: Loss has increased *****
Loss at iteration [596]: 0.10146768442799418
Loss at iteration [597]: 0.10029966785397135
Loss at iteration [598]: 0.10045817583562872
***** Warning: Loss has increased *****
Loss at iteration [599]: 0.10069997072710633
***** Warning: Loss has increased *****
Loss at iteration [600]: 0.09988605553091587
Loss at iteration [601]: 0.09933853997562528
Loss at iteration [602]: 0.09949360177660484
***** Warning: Loss has increased *****
Loss at iteration [603]: 0.09923719551513481
Loss at iteration [604]: 0.0985269259969481
Loss at iteration [605]: 0.09827287619699407
Loss at iteration [606]: 0.09845226853934
***** Warning: Loss has increased *****
Loss at iteration [607]: 0.09825511751847825
Loss at iteration [608]: 0.09760815428271072
Loss at iteration [609]: 0.09719483541292741
Loss at iteration [610]: 0.09721484635777267
***** Warning: Loss has increased *****
Loss at iteration [611]: 0.09699113795553692
Loss at iteration [612]: 0.09643289764382847
Loss at iteration [613]: 0.09612259568628186
Loss at iteration [614]: 0.09599398813886253
Loss at iteration [615]: 0.09574038772265026
Loss at iteration [616]: 0.09538926701347418
Loss at iteration [617]: 0.09501840668431656
Loss at iteration [618]: 0.09485254500390079
Loss at iteration [619]: 0.09456139071222579
Loss at iteration [620]: 0.09436824780735541
Loss at iteration [621]: 0.09411066105607951
Loss at iteration [622]: 0.09375658789731983
Loss at iteration [623]: 0.09351804543536732
Loss at iteration [624]: 0.09332151627922779
Loss at iteration [625]: 0.0931028843227267
Loss at iteration [626]: 0.0928153812681447
Loss at iteration [627]: 0.09266061003026975
Loss at iteration [628]: 0.0924222912394793
Loss at iteration [629]: 0.09208712489523896
Loss at iteration [630]: 0.09183235345388714
Loss at iteration [631]: 0.09175734795235871
Loss at iteration [632]: 0.091462194000881
Loss at iteration [633]: 0.09124763000115765
Loss at iteration [634]: 0.09104457236986435
Loss at iteration [635]: 0.09079337976430894
Loss at iteration [636]: 0.09049215070905076
Loss at iteration [637]: 0.09022062634834724
Loss at iteration [638]: 0.0899918044992027
Loss at iteration [639]: 0.08986157349538737
Loss at iteration [640]: 0.08964281155091816
Loss at iteration [641]: 0.08940454268253578
Loss at iteration [642]: 0.08925345620253373
Loss at iteration [643]: 0.08897332775538315
Loss at iteration [644]: 0.0887392624103117
Loss at iteration [645]: 0.088450801417721
Loss at iteration [646]: 0.08818090295097303
Loss at iteration [647]: 0.08791175918772572
Loss at iteration [648]: 0.08770326126544978
Loss at iteration [649]: 0.08748510130262353
Loss at iteration [650]: 0.08740111886607088
Loss at iteration [651]: 0.08716701816571827
Loss at iteration [652]: 0.08717563646806711
***** Warning: Loss has increased *****
Loss at iteration [653]: 0.08719802321706634
***** Warning: Loss has increased *****
Loss at iteration [654]: 0.08741418246204637
***** Warning: Loss has increased *****
Loss at iteration [655]: 0.0876247906529592
***** Warning: Loss has increased *****
Loss at iteration [656]: 0.08802844278876874
***** Warning: Loss has increased *****
Loss at iteration [657]: 0.08856537892044429
***** Warning: Loss has increased *****
Loss at iteration [658]: 0.08935861786317768
***** Warning: Loss has increased *****
Loss at iteration [659]: 0.09074357202023924
***** Warning: Loss has increased *****
Loss at iteration [660]: 0.09210755323155123
***** Warning: Loss has increased *****
Loss at iteration [661]: 0.09350563963626486
***** Warning: Loss has increased *****
Loss at iteration [662]: 0.09472645973598057
***** Warning: Loss has increased *****
Loss at iteration [663]: 0.09646191943302536
***** Warning: Loss has increased *****
Loss at iteration [664]: 0.09723868370106793
***** Warning: Loss has increased *****
Loss at iteration [665]: 0.09869289800534024
***** Warning: Loss has increased *****
Loss at iteration [666]: 0.09973199526218045
***** Warning: Loss has increased *****
Loss at iteration [667]: 0.10257116217884345
***** Warning: Loss has increased *****
Loss at iteration [668]: 0.1050996887918025
***** Warning: Loss has increased *****
Loss at iteration [669]: 0.10825794854289324
***** Warning: Loss has increased *****
Loss at iteration [670]: 0.10956344559811099
***** Warning: Loss has increased *****
Loss at iteration [671]: 0.11193577735046244
***** Warning: Loss has increased *****
Loss at iteration [672]: 0.11059377058434014
Loss at iteration [673]: 0.10731694568342663
Loss at iteration [674]: 0.10078619777334384
Loss at iteration [675]: 0.09434150419367726
Loss at iteration [676]: 0.088716235655706
Loss at iteration [677]: 0.085148206667582
Loss at iteration [678]: 0.08317881327066652
Loss at iteration [679]: 0.08235636043451267
Loss at iteration [680]: 0.08233109894499138
Loss at iteration [681]: 0.08314003746389759
***** Warning: Loss has increased *****
Loss at iteration [682]: 0.08457858498160781
***** Warning: Loss has increased *****
Loss at iteration [683]: 0.08692673783586674
***** Warning: Loss has increased *****
Loss at iteration [684]: 0.09121436064413971
***** Warning: Loss has increased *****
Loss at iteration [685]: 0.09809184064304106
***** Warning: Loss has increased *****
Loss at iteration [686]: 0.11088070964206447
***** Warning: Loss has increased *****
Loss at iteration [687]: 0.12552811473383788
***** Warning: Loss has increased *****
Loss at iteration [688]: 0.14458088525356919
***** Warning: Loss has increased *****
Loss at iteration [689]: 0.15810624658085579
***** Warning: Loss has increased *****
Loss at iteration [690]: 0.17461787698495418
***** Warning: Loss has increased *****
Loss at iteration [691]: 0.16754602720307493
Loss at iteration [692]: 0.14694052144193892
Loss at iteration [693]: 0.10972067374330557
Loss at iteration [694]: 0.08447638684139742
Loss at iteration [695]: 0.08575229672052945
***** Warning: Loss has increased *****
Loss at iteration [696]: 0.1054488693399299
***** Warning: Loss has increased *****
Loss at iteration [697]: 0.12394108603208392
***** Warning: Loss has increased *****
Loss at iteration [698]: 0.12214823677762826
Loss at iteration [699]: 0.10480873846897747
Loss at iteration [700]: 0.0862729383541672
Loss at iteration [701]: 0.08082786736602168
Loss at iteration [702]: 0.08867829169296687
***** Warning: Loss has increased *****
Loss at iteration [703]: 0.09969508489493388
***** Warning: Loss has increased *****
Loss at iteration [704]: 0.103938194450024
***** Warning: Loss has increased *****
Loss at iteration [705]: 0.09698196413237359
Loss at iteration [706]: 0.08746013888716037
Loss at iteration [707]: 0.08053381894597339
Loss at iteration [708]: 0.07925618126463504
Loss at iteration [709]: 0.08257389046707143
***** Warning: Loss has increased *****
Loss at iteration [710]: 0.08730832347444518
***** Warning: Loss has increased *****
Loss at iteration [711]: 0.09110336880870759
***** Warning: Loss has increased *****
Loss at iteration [712]: 0.09100567800371347
Loss at iteration [713]: 0.08844369672718379
Loss at iteration [714]: 0.08391158488447792
Loss at iteration [715]: 0.07998528680339441
Loss at iteration [716]: 0.07787462935797462
Loss at iteration [717]: 0.07790757919273321
***** Warning: Loss has increased *****
Loss at iteration [718]: 0.07943889611172322
***** Warning: Loss has increased *****
Loss at iteration [719]: 0.08164781548910044
***** Warning: Loss has increased *****
Loss at iteration [720]: 0.08360829016754873
***** Warning: Loss has increased *****
Loss at iteration [721]: 0.08446439094494179
***** Warning: Loss has increased *****
Loss at iteration [722]: 0.08466106500349917
***** Warning: Loss has increased *****
Loss at iteration [723]: 0.08390126043725231
Loss at iteration [724]: 0.08285531057159506
Loss at iteration [725]: 0.08126462471506876
Loss at iteration [726]: 0.0798361513100765
Loss at iteration [727]: 0.07879128484252691
Loss at iteration [728]: 0.07779176401743888
Loss at iteration [729]: 0.07720321446380018
Loss at iteration [730]: 0.07666106403861889
Loss at iteration [731]: 0.07600084544694327
Loss at iteration [732]: 0.07566773880363717
Loss at iteration [733]: 0.07543983701991634
Loss at iteration [734]: 0.07521641398969539
Loss at iteration [735]: 0.07514091892197775
Loss at iteration [736]: 0.07486009827969121
Loss at iteration [737]: 0.07482860940657839
Loss at iteration [738]: 0.07487325134065793
***** Warning: Loss has increased *****
Loss at iteration [739]: 0.07549217468104899
***** Warning: Loss has increased *****
Loss at iteration [740]: 0.07669544885546971
***** Warning: Loss has increased *****
Loss at iteration [741]: 0.07921003751448248
***** Warning: Loss has increased *****
Loss at iteration [742]: 0.08397380820728993
***** Warning: Loss has increased *****
Loss at iteration [743]: 0.09253046387201386
***** Warning: Loss has increased *****
Loss at iteration [744]: 0.10943212180839615
***** Warning: Loss has increased *****
Loss at iteration [745]: 0.13552761946186076
***** Warning: Loss has increased *****
Loss at iteration [746]: 0.18566329359983527
***** Warning: Loss has increased *****
Loss at iteration [747]: 0.23924463966890025
***** Warning: Loss has increased *****
Loss at iteration [748]: 0.3111857263989875
***** Warning: Loss has increased *****
Loss at iteration [749]: 0.28611616568039916
Loss at iteration [750]: 0.19083947529254194
Loss at iteration [751]: 0.08794329689835727
Loss at iteration [752]: 0.10474651133229743
***** Warning: Loss has increased *****
Loss at iteration [753]: 0.17753126577858577
***** Warning: Loss has increased *****
Loss at iteration [754]: 0.16065654976930077
Loss at iteration [755]: 0.0901269516023311
Loss at iteration [756]: 0.0895175494996953
Loss at iteration [757]: 0.13821110741844664
***** Warning: Loss has increased *****
Loss at iteration [758]: 0.13227946732061016
Loss at iteration [759]: 0.08460632457440373
Loss at iteration [760]: 0.08750306125399356
***** Warning: Loss has increased *****
Loss at iteration [761]: 0.12008308523909414
***** Warning: Loss has increased *****
Loss at iteration [762]: 0.1072747201116454
Loss at iteration [763]: 0.07838378879574674
Loss at iteration [764]: 0.0860604897441625
***** Warning: Loss has increased *****
Loss at iteration [765]: 0.10422104154217088
***** Warning: Loss has increased *****
Loss at iteration [766]: 0.09260958536442926
Loss at iteration [767]: 0.07521315511845689
Loss at iteration [768]: 0.08366812590977163
***** Warning: Loss has increased *****
Loss at iteration [769]: 0.09455403608876885
***** Warning: Loss has increased *****
Loss at iteration [770]: 0.08351938376875946
Loss at iteration [771]: 0.07378045898617473
Loss at iteration [772]: 0.08101940567703723
***** Warning: Loss has increased *****
Loss at iteration [773]: 0.08720078649435542
***** Warning: Loss has increased *****
Loss at iteration [774]: 0.08105875128173007
Loss at iteration [775]: 0.07291831618104075
Loss at iteration [776]: 0.07589506758322079
***** Warning: Loss has increased *****
Loss at iteration [777]: 0.08209451919130707
***** Warning: Loss has increased *****
Loss at iteration [778]: 0.07932257122418793
Loss at iteration [779]: 0.07288417329122496
Loss at iteration [780]: 0.07268443889537006
Loss at iteration [781]: 0.0770967688527837
***** Warning: Loss has increased *****
Loss at iteration [782]: 0.07782050625001911
***** Warning: Loss has increased *****
Loss at iteration [783]: 0.07363557461800235
Loss at iteration [784]: 0.07100151878310561
Loss at iteration [785]: 0.07253976003486282
***** Warning: Loss has increased *****
Loss at iteration [786]: 0.07427600204922952
***** Warning: Loss has increased *****
Loss at iteration [787]: 0.07372442869538406
Loss at iteration [788]: 0.07140507449181716
Loss at iteration [789]: 0.07046939531368701
Loss at iteration [790]: 0.07146740259813215
***** Warning: Loss has increased *****
Loss at iteration [791]: 0.07254591216884693
***** Warning: Loss has increased *****
Loss at iteration [792]: 0.07223155542305754
Loss at iteration [793]: 0.07064646670263666
Loss at iteration [794]: 0.0698159012245156
Loss at iteration [795]: 0.07002129125646894
***** Warning: Loss has increased *****
Loss at iteration [796]: 0.07078563666899518
***** Warning: Loss has increased *****
Loss at iteration [797]: 0.07095767493465253
***** Warning: Loss has increased *****
Loss at iteration [798]: 0.07022963389982022
Loss at iteration [799]: 0.069218988270844
Loss at iteration [800]: 0.06875748164152803
Loss at iteration [801]: 0.06902313209938032
***** Warning: Loss has increased *****
Loss at iteration [802]: 0.0693975779376196
***** Warning: Loss has increased *****
Loss at iteration [803]: 0.0695217724427303
***** Warning: Loss has increased *****
Loss at iteration [804]: 0.06908726945652129
Loss at iteration [805]: 0.06852040896675934
Loss at iteration [806]: 0.0681351122035039
Loss at iteration [807]: 0.06794911739575046
Loss at iteration [808]: 0.06812516975739587
***** Warning: Loss has increased *****
Loss at iteration [809]: 0.06831201939482957
***** Warning: Loss has increased *****
Loss at iteration [810]: 0.0682877243455656
Loss at iteration [811]: 0.06814591805063537
Loss at iteration [812]: 0.06779799301704396
Loss at iteration [813]: 0.06753209200408254
Loss at iteration [814]: 0.06716674575981148
Loss at iteration [815]: 0.06696270400778603
Loss at iteration [816]: 0.06701276044465429
***** Warning: Loss has increased *****
Loss at iteration [817]: 0.06699044571053861
Loss at iteration [818]: 0.06690676048415134
Loss at iteration [819]: 0.06681501636272791
Loss at iteration [820]: 0.0667749219682155
Loss at iteration [821]: 0.0666621458980123
Loss at iteration [822]: 0.06646882752386012
Loss at iteration [823]: 0.06630296222663824
Loss at iteration [824]: 0.06627968943072803
Loss at iteration [825]: 0.0660804070426288
Loss at iteration [826]: 0.065931954873931
Loss at iteration [827]: 0.06578423706013066
Loss at iteration [828]: 0.06570836746464183
Loss at iteration [829]: 0.06568481207217479
Loss at iteration [830]: 0.06566898926135917
Loss at iteration [831]: 0.06575154689535365
***** Warning: Loss has increased *****
Loss at iteration [832]: 0.06602882220326421
***** Warning: Loss has increased *****
Loss at iteration [833]: 0.06623319831912004
***** Warning: Loss has increased *****
Loss at iteration [834]: 0.06682187152038581
***** Warning: Loss has increased *****
Loss at iteration [835]: 0.06776205461432543
***** Warning: Loss has increased *****
Loss at iteration [836]: 0.06975207395612612
***** Warning: Loss has increased *****
Loss at iteration [837]: 0.07307825245113997
***** Warning: Loss has increased *****
Loss at iteration [838]: 0.07807554712851654
***** Warning: Loss has increased *****
Loss at iteration [839]: 0.08487173349755132
***** Warning: Loss has increased *****
Loss at iteration [840]: 0.09628661107647249
***** Warning: Loss has increased *****
Loss at iteration [841]: 0.11096567871933352
***** Warning: Loss has increased *****
Loss at iteration [842]: 0.1339062285130418
***** Warning: Loss has increased *****
Loss at iteration [843]: 0.1528583874789504
***** Warning: Loss has increased *****
Loss at iteration [844]: 0.17360070135533132
***** Warning: Loss has increased *****
Loss at iteration [845]: 0.16549248522249987
Loss at iteration [846]: 0.14135167284423375
Loss at iteration [847]: 0.09762230730043707
Loss at iteration [848]: 0.06907655486182457
Loss at iteration [849]: 0.06889142248353115
Loss at iteration [850]: 0.08799960360963019
***** Warning: Loss has increased *****
Loss at iteration [851]: 0.10150093206244505
***** Warning: Loss has increased *****
Loss at iteration [852]: 0.09291957158474114
Loss at iteration [853]: 0.07408058433199083
Loss at iteration [854]: 0.06471006480835827
Loss at iteration [855]: 0.07134264748871409
***** Warning: Loss has increased *****
Loss at iteration [856]: 0.0825994890446334
***** Warning: Loss has increased *****
Loss at iteration [857]: 0.08406463988138323
***** Warning: Loss has increased *****
Loss at iteration [858]: 0.0756583157047103
Loss at iteration [859]: 0.06604129033762024
Loss at iteration [860]: 0.06451597366571693
Loss at iteration [861]: 0.0701813899430735
***** Warning: Loss has increased *****
Loss at iteration [862]: 0.07571256891175723
***** Warning: Loss has increased *****
Loss at iteration [863]: 0.07629830213030817
***** Warning: Loss has increased *****
Loss at iteration [864]: 0.07071864112167987
Loss at iteration [865]: 0.06492080800690893
Loss at iteration [866]: 0.06317272937107692
Loss at iteration [867]: 0.06578024226928139
***** Warning: Loss has increased *****
Loss at iteration [868]: 0.06942879852627884
***** Warning: Loss has increased *****
Loss at iteration [869]: 0.07096986180401968
***** Warning: Loss has increased *****
Loss at iteration [870]: 0.06970605685255953
Loss at iteration [871]: 0.06616472433837446
Loss at iteration [872]: 0.0630377007218749
Loss at iteration [873]: 0.062371512212829106
Loss at iteration [874]: 0.06371601979643594
***** Warning: Loss has increased *****
Loss at iteration [875]: 0.06563260627716072
***** Warning: Loss has increased *****
Loss at iteration [876]: 0.06642763932507657
***** Warning: Loss has increased *****
Loss at iteration [877]: 0.06607388463462728
Loss at iteration [878]: 0.06449592624515789
Loss at iteration [879]: 0.06275829696717397
Loss at iteration [880]: 0.0616974202362581
Loss at iteration [881]: 0.061331307158381775
Loss at iteration [882]: 0.06160507984512601
***** Warning: Loss has increased *****
Loss at iteration [883]: 0.062331701846753795
***** Warning: Loss has increased *****
Loss at iteration [884]: 0.06318326543006618
***** Warning: Loss has increased *****
Loss at iteration [885]: 0.06381241235200943
***** Warning: Loss has increased *****
Loss at iteration [886]: 0.06439729159998626
***** Warning: Loss has increased *****
Loss at iteration [887]: 0.06479980546432126
***** Warning: Loss has increased *****
Loss at iteration [888]: 0.06507715810338818
***** Warning: Loss has increased *****
Loss at iteration [889]: 0.0650947700302859
***** Warning: Loss has increased *****
Loss at iteration [890]: 0.06489371738363825
Loss at iteration [891]: 0.06412907468661522
Loss at iteration [892]: 0.06357455092367024
Loss at iteration [893]: 0.06276968958883668
Loss at iteration [894]: 0.06208213965206151
Loss at iteration [895]: 0.06124397064592358
Loss at iteration [896]: 0.060693213495522556
Loss at iteration [897]: 0.06032790926015566
Loss at iteration [898]: 0.059969380300728445
Loss at iteration [899]: 0.05966236965414647
Loss at iteration [900]: 0.05955959466848096
Loss at iteration [901]: 0.05950432285725909
Loss at iteration [902]: 0.059335202453827646
Loss at iteration [903]: 0.05916279626867964
Loss at iteration [904]: 0.05908559718319549
Loss at iteration [905]: 0.0590477738134704
Loss at iteration [906]: 0.05901838846750821
Loss at iteration [907]: 0.05899489545716459
Loss at iteration [908]: 0.058943692633176274
Loss at iteration [909]: 0.05905779607903586
***** Warning: Loss has increased *****
Loss at iteration [910]: 0.059511276550931705
***** Warning: Loss has increased *****
Loss at iteration [911]: 0.060678462478831026
***** Warning: Loss has increased *****
Loss at iteration [912]: 0.06327991894556255
***** Warning: Loss has increased *****
Loss at iteration [913]: 0.0688292807036495
***** Warning: Loss has increased *****
Loss at iteration [914]: 0.07952040005196886
***** Warning: Loss has increased *****
Loss at iteration [915]: 0.10224484220126497
***** Warning: Loss has increased *****
Loss at iteration [916]: 0.14035579561597092
***** Warning: Loss has increased *****
Loss at iteration [917]: 0.2130310912986464
***** Warning: Loss has increased *****
Loss at iteration [918]: 0.2884099962168714
***** Warning: Loss has increased *****
Loss at iteration [919]: 0.37666377593921224
***** Warning: Loss has increased *****
Loss at iteration [920]: 0.28759922035361657
Loss at iteration [921]: 0.13848417827856532
Loss at iteration [922]: 0.06700352645098114
Loss at iteration [923]: 0.156667463435918
***** Warning: Loss has increased *****
Loss at iteration [924]: 0.19074581186811437
***** Warning: Loss has increased *****
Loss at iteration [925]: 0.08566981513213932
Loss at iteration [926]: 0.09028576684088321
***** Warning: Loss has increased *****
Loss at iteration [927]: 0.15562727584095243
***** Warning: Loss has increased *****
Loss at iteration [928]: 0.09858030875013171
Loss at iteration [929]: 0.07061327385462728
Loss at iteration [930]: 0.11934248793917562
***** Warning: Loss has increased *****
Loss at iteration [931]: 0.09895782937175657
Loss at iteration [932]: 0.06540723791764326
Loss at iteration [933]: 0.09538265841079381
***** Warning: Loss has increased *****
Loss at iteration [934]: 0.09352381627226508
Loss at iteration [935]: 0.06507033599169562
Loss at iteration [936]: 0.08041742936995938
***** Warning: Loss has increased *****
Loss at iteration [937]: 0.08728854134001017
***** Warning: Loss has increased *****
Loss at iteration [938]: 0.0664376621346504
Loss at iteration [939]: 0.07002404308794456
***** Warning: Loss has increased *****
Loss at iteration [940]: 0.08091507899876675
***** Warning: Loss has increased *****
Loss at iteration [941]: 0.06825356444506851
Loss at iteration [942]: 0.06389455366216948
Loss at iteration [943]: 0.07470516415536321
***** Warning: Loss has increased *****
Loss at iteration [944]: 0.07004598702761572
Loss at iteration [945]: 0.061135784684746126
Loss at iteration [946]: 0.06794485375531689
***** Warning: Loss has increased *****
Loss at iteration [947]: 0.07038333662523225
***** Warning: Loss has increased *****
Loss at iteration [948]: 0.062102331681027655
Loss at iteration [949]: 0.061825270790928186
Loss at iteration [950]: 0.06730908130232607
***** Warning: Loss has increased *****
Loss at iteration [951]: 0.06423739419319291
Loss at iteration [952]: 0.05963645049148654
Loss at iteration [953]: 0.062186957966711365
***** Warning: Loss has increased *****
Loss at iteration [954]: 0.06395363178466767
***** Warning: Loss has increased *****
Loss at iteration [955]: 0.06055621997711786
Loss at iteration [956]: 0.05889454977333302
Loss at iteration [957]: 0.061116644758575964
***** Warning: Loss has increased *****
Loss at iteration [958]: 0.061869985024513005
***** Warning: Loss has increased *****
Loss at iteration [959]: 0.05935382795148538
Loss at iteration [960]: 0.05846401783112459
Loss at iteration [961]: 0.059939239488526266
***** Warning: Loss has increased *****
Loss at iteration [962]: 0.06025889236093282
***** Warning: Loss has increased *****
Loss at iteration [963]: 0.058995031509485554
Loss at iteration [964]: 0.05815440461235044
Loss at iteration [965]: 0.05888661658310561
***** Warning: Loss has increased *****
Loss at iteration [966]: 0.05932469267645591
***** Warning: Loss has increased *****
Loss at iteration [967]: 0.05866499998228969
Loss at iteration [968]: 0.057795557830836924
Loss at iteration [969]: 0.05805099155652448
***** Warning: Loss has increased *****
Loss at iteration [970]: 0.058422672579850544
***** Warning: Loss has increased *****
Loss at iteration [971]: 0.058177956649983105
Loss at iteration [972]: 0.057318679452546074
Loss at iteration [973]: 0.057049646280818864
Loss at iteration [974]: 0.05733347737614339
***** Warning: Loss has increased *****
Loss at iteration [975]: 0.05746513288995009
***** Warning: Loss has increased *****
Loss at iteration [976]: 0.05719156799218363
Loss at iteration [977]: 0.05673964943336837
Loss at iteration [978]: 0.056499845105832974
Loss at iteration [979]: 0.05662303020426298
***** Warning: Loss has increased *****
Loss at iteration [980]: 0.05653748572103587
Loss at iteration [981]: 0.056298617948374534
Loss at iteration [982]: 0.056077515010799155
Loss at iteration [983]: 0.05601476803759737
Loss at iteration [984]: 0.05600163816169502
Loss at iteration [985]: 0.05605451254530233
***** Warning: Loss has increased *****
Loss at iteration [986]: 0.05603824375773929
Loss at iteration [987]: 0.055767623132979026
Loss at iteration [988]: 0.05545808171222995
Loss at iteration [989]: 0.0553018463818298
Loss at iteration [990]: 0.05540853528140939
***** Warning: Loss has increased *****
Loss at iteration [991]: 0.05559476776654861
***** Warning: Loss has increased *****
Loss at iteration [992]: 0.05557457899313912
Loss at iteration [993]: 0.05519019845195681
Loss at iteration [994]: 0.05499796418702425
Loss at iteration [995]: 0.05491997273628742
Loss at iteration [996]: 0.054684516419354776
Loss at iteration [997]: 0.05468928806840452
***** Warning: Loss has increased *****
Loss at iteration [998]: 0.05503102555977828
***** Warning: Loss has increased *****
Loss at iteration [999]: 0.0549865955152774
Loss at iteration [1000]: 0.05460844016671152
Loss at iteration [1001]: 0.054606297486460835
Loss at iteration [1002]: 0.05452523972166246
Loss at iteration [1003]: 0.05451674092057641
Loss at iteration [1004]: 0.05432903447022526
Loss at iteration [1005]: 0.05409731689384883
Loss at iteration [1006]: 0.054217952780674164
***** Warning: Loss has increased *****
Loss at iteration [1007]: 0.05413155569380803
Loss at iteration [1008]: 0.05392021223803592
Loss at iteration [1009]: 0.0537957621533928
Loss at iteration [1010]: 0.053841398534850086
***** Warning: Loss has increased *****
Loss at iteration [1011]: 0.05372627767713011
Loss at iteration [1012]: 0.05353009057107742
Loss at iteration [1013]: 0.053339677694580576
Loss at iteration [1014]: 0.05356290606409343
***** Warning: Loss has increased *****
Loss at iteration [1015]: 0.05344044993230528
Loss at iteration [1016]: 0.05326754439105197
Loss at iteration [1017]: 0.05336332162730624
***** Warning: Loss has increased *****
Loss at iteration [1018]: 0.05330777375234143
Loss at iteration [1019]: 0.05310067249212578
Loss at iteration [1020]: 0.0528980218865703
Loss at iteration [1021]: 0.05289412853379795
Loss at iteration [1022]: 0.052932859816845944
***** Warning: Loss has increased *****
Loss at iteration [1023]: 0.05282964374279536
Loss at iteration [1024]: 0.052627780811743394
Loss at iteration [1025]: 0.052703521331028534
***** Warning: Loss has increased *****
Loss at iteration [1026]: 0.052773657064151415
***** Warning: Loss has increased *****
Loss at iteration [1027]: 0.052793630567381684
***** Warning: Loss has increased *****
Loss at iteration [1028]: 0.05281567758769529
***** Warning: Loss has increased *****
Loss at iteration [1029]: 0.0533039035064129
***** Warning: Loss has increased *****
Loss at iteration [1030]: 0.05375279883935715
***** Warning: Loss has increased *****
Loss at iteration [1031]: 0.05463747286630684
***** Warning: Loss has increased *****
Loss at iteration [1032]: 0.05565254232540344
***** Warning: Loss has increased *****
Loss at iteration [1033]: 0.05708224059497408
***** Warning: Loss has increased *****
Loss at iteration [1034]: 0.058862226507456976
***** Warning: Loss has increased *****
Loss at iteration [1035]: 0.06197898134402742
***** Warning: Loss has increased *****
Loss at iteration [1036]: 0.06449610545230615
***** Warning: Loss has increased *****
Loss at iteration [1037]: 0.06829459476838579
***** Warning: Loss has increased *****
Loss at iteration [1038]: 0.07134864639070625
***** Warning: Loss has increased *****
Loss at iteration [1039]: 0.07547172488808408
***** Warning: Loss has increased *****
Loss at iteration [1040]: 0.07695284941019423
***** Warning: Loss has increased *****
Loss at iteration [1041]: 0.07746831125271732
***** Warning: Loss has increased *****
Loss at iteration [1042]: 0.07349677042223711
Loss at iteration [1043]: 0.06945571772291807
Loss at iteration [1044]: 0.06299603282677228
Loss at iteration [1045]: 0.05693471041855598
Loss at iteration [1046]: 0.05283925461189043
Loss at iteration [1047]: 0.05146430814006435
Loss at iteration [1048]: 0.05190627522350548
***** Warning: Loss has increased *****
Loss at iteration [1049]: 0.05397527717489059
***** Warning: Loss has increased *****
Loss at iteration [1050]: 0.05676683206766273
***** Warning: Loss has increased *****
Loss at iteration [1051]: 0.058971555688031384
***** Warning: Loss has increased *****
Loss at iteration [1052]: 0.06035705446932198
***** Warning: Loss has increased *****
Loss at iteration [1053]: 0.06016233601448344
Loss at iteration [1054]: 0.059634422855757596
Loss at iteration [1055]: 0.05740320489886388
Loss at iteration [1056]: 0.0550748490598428
Loss at iteration [1057]: 0.05272483471029086
Loss at iteration [1058]: 0.05116206277168227
Loss at iteration [1059]: 0.050404971149571536
Loss at iteration [1060]: 0.050454436234642124
***** Warning: Loss has increased *****
Loss at iteration [1061]: 0.0509497358397285
***** Warning: Loss has increased *****
Loss at iteration [1062]: 0.051952145438526956
***** Warning: Loss has increased *****
Loss at iteration [1063]: 0.05341351513186485
***** Warning: Loss has increased *****
Loss at iteration [1064]: 0.05529260361753764
***** Warning: Loss has increased *****
Loss at iteration [1065]: 0.057174354391658105
***** Warning: Loss has increased *****
Loss at iteration [1066]: 0.058492893042373005
***** Warning: Loss has increased *****
Loss at iteration [1067]: 0.05934447511440643
***** Warning: Loss has increased *****
Loss at iteration [1068]: 0.05981119361137353
***** Warning: Loss has increased *****
Loss at iteration [1069]: 0.06099435654107623
***** Warning: Loss has increased *****
Loss at iteration [1070]: 0.06202462451502881
***** Warning: Loss has increased *****
Loss at iteration [1071]: 0.06388974821248805
***** Warning: Loss has increased *****
Loss at iteration [1072]: 0.06479357737564231
***** Warning: Loss has increased *****
Loss at iteration [1073]: 0.06559438079516282
***** Warning: Loss has increased *****
Loss at iteration [1074]: 0.06508616180969974
Loss at iteration [1075]: 0.06458662773583791
Loss at iteration [1076]: 0.06262953652357632
Loss at iteration [1077]: 0.06072563805456783
Loss at iteration [1078]: 0.05806926655307849
Loss at iteration [1079]: 0.05526318669391512
Loss at iteration [1080]: 0.05248900282053016
Loss at iteration [1081]: 0.05058071419955534
Loss at iteration [1082]: 0.04960327523690639
Loss at iteration [1083]: 0.04889066119599008
Loss at iteration [1084]: 0.04858084580065765
Loss at iteration [1085]: 0.049003178571334856
***** Warning: Loss has increased *****
Loss at iteration [1086]: 0.04950545694060477
***** Warning: Loss has increased *****
Loss at iteration [1087]: 0.050085899503222564
***** Warning: Loss has increased *****
Loss at iteration [1088]: 0.051354673482001276
***** Warning: Loss has increased *****
Loss at iteration [1089]: 0.05323503877234203
***** Warning: Loss has increased *****
Loss at iteration [1090]: 0.056531293600739684
***** Warning: Loss has increased *****
Loss at iteration [1091]: 0.06121048982541327
***** Warning: Loss has increased *****
Loss at iteration [1092]: 0.0693663511121606
***** Warning: Loss has increased *****
Loss at iteration [1093]: 0.08027229389989242
***** Warning: Loss has increased *****
Loss at iteration [1094]: 0.09754871980318104
***** Warning: Loss has increased *****
Loss at iteration [1095]: 0.1170748061610552
***** Warning: Loss has increased *****
Loss at iteration [1096]: 0.14497258189633275
***** Warning: Loss has increased *****
Loss at iteration [1097]: 0.15662332376867052
***** Warning: Loss has increased *****
Loss at iteration [1098]: 0.15539089934736727
Loss at iteration [1099]: 0.11287671976178823
Loss at iteration [1100]: 0.06767665701931548
Loss at iteration [1101]: 0.050054919589178355
Loss at iteration [1102]: 0.06913947582258863
***** Warning: Loss has increased *****
Loss at iteration [1103]: 0.09427988686760128
***** Warning: Loss has increased *****
Loss at iteration [1104]: 0.09000370573405593
Loss at iteration [1105]: 0.06713977886254674
Loss at iteration [1106]: 0.05025962931434925
Loss at iteration [1107]: 0.05727511210082902
***** Warning: Loss has increased *****
Loss at iteration [1108]: 0.07334253541308185
***** Warning: Loss has increased *****
Loss at iteration [1109]: 0.0733787100643148
***** Warning: Loss has increased *****
Loss at iteration [1110]: 0.06023387556075324
Loss at iteration [1111]: 0.049244070343299744
Loss at iteration [1112]: 0.05292564285871856
***** Warning: Loss has increased *****
Loss at iteration [1113]: 0.061891345208874515
***** Warning: Loss has increased *****
Loss at iteration [1114]: 0.06199581743776338
***** Warning: Loss has increased *****
Loss at iteration [1115]: 0.05535973623203715
Loss at iteration [1116]: 0.049076067074888034
Loss at iteration [1117]: 0.04936942144044742
***** Warning: Loss has increased *****
Loss at iteration [1118]: 0.054178546094507535
***** Warning: Loss has increased *****
Loss at iteration [1119]: 0.056641835458664226
***** Warning: Loss has increased *****
Loss at iteration [1120]: 0.05397516215904921
Loss at iteration [1121]: 0.049529975411021936
Loss at iteration [1122]: 0.04750804852300734
Loss at iteration [1123]: 0.04951566005401336
***** Warning: Loss has increased *****
Loss at iteration [1124]: 0.052645986793717826
***** Warning: Loss has increased *****
Loss at iteration [1125]: 0.054000717168529826
***** Warning: Loss has increased *****
Loss at iteration [1126]: 0.05222649525311481
Loss at iteration [1127]: 0.04906870550149298
Loss at iteration [1128]: 0.04716623248629506
Loss at iteration [1129]: 0.047095259720804084
Loss at iteration [1130]: 0.04816768295678276
***** Warning: Loss has increased *****
Loss at iteration [1131]: 0.04908948107516755
***** Warning: Loss has increased *****
Loss at iteration [1132]: 0.049283456702271515
***** Warning: Loss has increased *****
Loss at iteration [1133]: 0.04890879336560268
Loss at iteration [1134]: 0.047981242089045396
Loss at iteration [1135]: 0.046791190714122265
Loss at iteration [1136]: 0.04593684561114475
Loss at iteration [1137]: 0.046375028805020264
***** Warning: Loss has increased *****
Loss at iteration [1138]: 0.04701461071620767
***** Warning: Loss has increased *****
Loss at iteration [1139]: 0.04745718315633556
***** Warning: Loss has increased *****
Loss at iteration [1140]: 0.047624931373448746
***** Warning: Loss has increased *****
Loss at iteration [1141]: 0.04794871959457691
***** Warning: Loss has increased *****
Loss at iteration [1142]: 0.04803057309658517
***** Warning: Loss has increased *****
Loss at iteration [1143]: 0.04765143922670676
Loss at iteration [1144]: 0.04688795509493897
Loss at iteration [1145]: 0.046266546451238885
Loss at iteration [1146]: 0.045715549014106335
Loss at iteration [1147]: 0.04519018323825277
Loss at iteration [1148]: 0.04518777892777854
Loss at iteration [1149]: 0.045132295802859844
Loss at iteration [1150]: 0.04520234275427785
***** Warning: Loss has increased *****
Loss at iteration [1151]: 0.045386821695128705
***** Warning: Loss has increased *****
Loss at iteration [1152]: 0.04573706037571548
***** Warning: Loss has increased *****
Loss at iteration [1153]: 0.04603835789318563
***** Warning: Loss has increased *****
Loss at iteration [1154]: 0.04628560640631702
***** Warning: Loss has increased *****
Loss at iteration [1155]: 0.04696444004393402
***** Warning: Loss has increased *****
Loss at iteration [1156]: 0.04751237451373483
***** Warning: Loss has increased *****
Loss at iteration [1157]: 0.048030026326028284
***** Warning: Loss has increased *****
Loss at iteration [1158]: 0.048735420481485214
***** Warning: Loss has increased *****
Loss at iteration [1159]: 0.04921488576033432
***** Warning: Loss has increased *****
Loss at iteration [1160]: 0.04997607930047569
***** Warning: Loss has increased *****
Loss at iteration [1161]: 0.05047356732998006
***** Warning: Loss has increased *****
Loss at iteration [1162]: 0.051575598713462345
***** Warning: Loss has increased *****
Loss at iteration [1163]: 0.05227659789277632
***** Warning: Loss has increased *****
Loss at iteration [1164]: 0.054151253278204046
***** Warning: Loss has increased *****
Loss at iteration [1165]: 0.05527347128949188
***** Warning: Loss has increased *****
Loss at iteration [1166]: 0.056565027500235315
***** Warning: Loss has increased *****
Loss at iteration [1167]: 0.05681494472237268
***** Warning: Loss has increased *****
Loss at iteration [1168]: 0.05746807322661203
***** Warning: Loss has increased *****
Loss at iteration [1169]: 0.05654909557353711
Loss at iteration [1170]: 0.05502330348868167
Loss at iteration [1171]: 0.05237949796007708
Loss at iteration [1172]: 0.050481868394090475
Loss at iteration [1173]: 0.04830893603770529
Loss at iteration [1174]: 0.046300200227209126
Loss at iteration [1175]: 0.044993585836390584
Loss at iteration [1176]: 0.04404566015064633
Loss at iteration [1177]: 0.04348020360668873
Loss at iteration [1178]: 0.04354095163814287
***** Warning: Loss has increased *****
Loss at iteration [1179]: 0.04381941293134617
***** Warning: Loss has increased *****
Loss at iteration [1180]: 0.0444222919997241
***** Warning: Loss has increased *****
Loss at iteration [1181]: 0.045199265985366735
***** Warning: Loss has increased *****
Loss at iteration [1182]: 0.04586660338243865
***** Warning: Loss has increased *****
Loss at iteration [1183]: 0.046838549604564815
***** Warning: Loss has increased *****
Loss at iteration [1184]: 0.048260172958959346
***** Warning: Loss has increased *****
Loss at iteration [1185]: 0.05048483774305536
***** Warning: Loss has increased *****
Loss at iteration [1186]: 0.053668657617568505
***** Warning: Loss has increased *****
Loss at iteration [1187]: 0.058550897416006546
***** Warning: Loss has increased *****
Loss at iteration [1188]: 0.06492729107770516
***** Warning: Loss has increased *****
Loss at iteration [1189]: 0.07589659808035014
***** Warning: Loss has increased *****
Loss at iteration [1190]: 0.08625937479957324
***** Warning: Loss has increased *****
Loss at iteration [1191]: 0.1009846505139874
***** Warning: Loss has increased *****
Loss at iteration [1192]: 0.10617413269733134
***** Warning: Loss has increased *****
Loss at iteration [1193]: 0.10909320737515206
***** Warning: Loss has increased *****
Loss at iteration [1194]: 0.09352674206408636
Loss at iteration [1195]: 0.07215873912438353
Loss at iteration [1196]: 0.050554583300997116
Loss at iteration [1197]: 0.04387218239011706
Loss at iteration [1198]: 0.052395589300606685
***** Warning: Loss has increased *****
Loss at iteration [1199]: 0.064515424208276
***** Warning: Loss has increased *****
Loss at iteration [1200]: 0.07079812597760977
***** Warning: Loss has increased *****
Loss at iteration [1201]: 0.06366474268148369
Loss at iteration [1202]: 0.05228809046483051
Loss at iteration [1203]: 0.04392134788354346
Loss at iteration [1204]: 0.04437747488281842
***** Warning: Loss has increased *****
Loss at iteration [1205]: 0.05112137360845724
***** Warning: Loss has increased *****
Loss at iteration [1206]: 0.05683512846917602
***** Warning: Loss has increased *****
Loss at iteration [1207]: 0.058088185879324954
***** Warning: Loss has increased *****
Loss at iteration [1208]: 0.05285838327964245
Loss at iteration [1209]: 0.04655035362678836
Loss at iteration [1210]: 0.042776992324188884
Loss at iteration [1211]: 0.043106627020396485
***** Warning: Loss has increased *****
Loss at iteration [1212]: 0.045979168330583214
***** Warning: Loss has increased *****
Loss at iteration [1213]: 0.0490313843507906
***** Warning: Loss has increased *****
Loss at iteration [1214]: 0.050638783136472985
***** Warning: Loss has increased *****
Loss at iteration [1215]: 0.04994019524631264
Loss at iteration [1216]: 0.047605876661627744
Loss at iteration [1217]: 0.044363262973447434
Loss at iteration [1218]: 0.04200701935466715
Loss at iteration [1219]: 0.041726932673605846
Loss at iteration [1220]: 0.04314206372346636
***** Warning: Loss has increased *****
Loss at iteration [1221]: 0.04524635239541709
***** Warning: Loss has increased *****
Loss at iteration [1222]: 0.04682962991274856
***** Warning: Loss has increased *****
Loss at iteration [1223]: 0.04825896839350705
***** Warning: Loss has increased *****
Loss at iteration [1224]: 0.048962806006589484
***** Warning: Loss has increased *****
Loss at iteration [1225]: 0.049535495798697746
***** Warning: Loss has increased *****
Loss at iteration [1226]: 0.04864573721189656
Loss at iteration [1227]: 0.0479548387722223
Loss at iteration [1228]: 0.046580925373162795
Loss at iteration [1229]: 0.046181572714442634
Loss at iteration [1230]: 0.04526249274560156
Loss at iteration [1231]: 0.04400644763415806
Loss at iteration [1232]: 0.04267173104290993
Loss at iteration [1233]: 0.04138725262724525
Loss at iteration [1234]: 0.04056546869626253
Loss at iteration [1235]: 0.040138507808609745
Loss at iteration [1236]: 0.04027383677880597
***** Warning: Loss has increased *****
Loss at iteration [1237]: 0.040418132941140036
***** Warning: Loss has increased *****
Loss at iteration [1238]: 0.040565733796636055
***** Warning: Loss has increased *****
Loss at iteration [1239]: 0.04054210697051168
Loss at iteration [1240]: 0.04087642544527732
***** Warning: Loss has increased *****
Loss at iteration [1241]: 0.041530649629061875
***** Warning: Loss has increased *****
Loss at iteration [1242]: 0.042250114427011536
***** Warning: Loss has increased *****
Loss at iteration [1243]: 0.04320380199551964
***** Warning: Loss has increased *****
Loss at iteration [1244]: 0.04486039832587801
***** Warning: Loss has increased *****
Loss at iteration [1245]: 0.04720801443704629
***** Warning: Loss has increased *****
Loss at iteration [1246]: 0.05184889985549694
***** Warning: Loss has increased *****
Loss at iteration [1247]: 0.05832816127075852
***** Warning: Loss has increased *****
Loss at iteration [1248]: 0.06874436318642314
***** Warning: Loss has increased *****
Loss at iteration [1249]: 0.07934204342647432
***** Warning: Loss has increased *****
Loss at iteration [1250]: 0.09833394790838876
***** Warning: Loss has increased *****
Loss at iteration [1251]: 0.11131843310303459
***** Warning: Loss has increased *****
Loss at iteration [1252]: 0.1283399854117911
***** Warning: Loss has increased *****
Loss at iteration [1253]: 0.12149391302379396
Loss at iteration [1254]: 0.10138186116205657
Loss at iteration [1255]: 0.0633711249080184
Loss at iteration [1256]: 0.041587480138109825
Loss at iteration [1257]: 0.04815442180158834
***** Warning: Loss has increased *****
Loss at iteration [1258]: 0.06907263590534518
***** Warning: Loss has increased *****
Loss at iteration [1259]: 0.08454522583635266
***** Warning: Loss has increased *****
Loss at iteration [1260]: 0.07518033141451322
Loss at iteration [1261]: 0.054923880281043704
Loss at iteration [1262]: 0.04077298725979905
Loss at iteration [1263]: 0.04523171541584083
***** Warning: Loss has increased *****
Loss at iteration [1264]: 0.05811753483520397
***** Warning: Loss has increased *****
Loss at iteration [1265]: 0.061523608663766376
***** Warning: Loss has increased *****
Loss at iteration [1266]: 0.05402086281103635
Loss at iteration [1267]: 0.04316698066039632
Loss at iteration [1268]: 0.039783870666544915
Loss at iteration [1269]: 0.04466848096546941
***** Warning: Loss has increased *****
Loss at iteration [1270]: 0.051241783233253055
***** Warning: Loss has increased *****
Loss at iteration [1271]: 0.05326978939761509
***** Warning: Loss has increased *****
Loss at iteration [1272]: 0.04846601029126435
Loss at iteration [1273]: 0.041986921563516535
Loss at iteration [1274]: 0.038987481205757804
Loss at iteration [1275]: 0.040823156040988594
***** Warning: Loss has increased *****
Loss at iteration [1276]: 0.04513951927746688
***** Warning: Loss has increased *****
Loss at iteration [1277]: 0.04769242634032858
***** Warning: Loss has increased *****
Loss at iteration [1278]: 0.048018563034767266
***** Warning: Loss has increased *****
Loss at iteration [1279]: 0.04549845415561583
Loss at iteration [1280]: 0.041901014894280914
Loss at iteration [1281]: 0.03915566816937423
Loss at iteration [1282]: 0.03834628091994681
Loss at iteration [1283]: 0.03954030009742733
***** Warning: Loss has increased *****
Loss at iteration [1284]: 0.04168919117144026
***** Warning: Loss has increased *****
Loss at iteration [1285]: 0.043983211810557504
***** Warning: Loss has increased *****
Loss at iteration [1286]: 0.04567521367463792
***** Warning: Loss has increased *****
Loss at iteration [1287]: 0.04717334453305002
***** Warning: Loss has increased *****
Loss at iteration [1288]: 0.046357656789519426
Loss at iteration [1289]: 0.044972035301599905
Loss at iteration [1290]: 0.04224199770036818
Loss at iteration [1291]: 0.04031458143560935
Loss at iteration [1292]: 0.03838829351508036
Loss at iteration [1293]: 0.03735549530379916
Loss at iteration [1294]: 0.03722668910747875
Loss at iteration [1295]: 0.03773870636086388
***** Warning: Loss has increased *****
Loss at iteration [1296]: 0.03884049940022265
***** Warning: Loss has increased *****
Loss at iteration [1297]: 0.04013037140083737
***** Warning: Loss has increased *****
Loss at iteration [1298]: 0.04142032607512545
***** Warning: Loss has increased *****
Loss at iteration [1299]: 0.04222574564517996
***** Warning: Loss has increased *****
Loss at iteration [1300]: 0.042568869631949176
***** Warning: Loss has increased *****
Loss at iteration [1301]: 0.042178754797717634
Loss at iteration [1302]: 0.04209206421895821
Loss at iteration [1303]: 0.04180552468229102
Loss at iteration [1304]: 0.0418206011692553
***** Warning: Loss has increased *****
Loss at iteration [1305]: 0.04170676585158624
Loss at iteration [1306]: 0.04180618757043798
***** Warning: Loss has increased *****
Loss at iteration [1307]: 0.041802722922045905
Loss at iteration [1308]: 0.04207691392041683
***** Warning: Loss has increased *****
Loss at iteration [1309]: 0.042113201762518426
***** Warning: Loss has increased *****
Loss at iteration [1310]: 0.04246436969600513
***** Warning: Loss has increased *****
Loss at iteration [1311]: 0.04251938136291561
***** Warning: Loss has increased *****
Loss at iteration [1312]: 0.04248531863754456
Loss at iteration [1313]: 0.041497119262203466
Loss at iteration [1314]: 0.04055688441668484
Loss at iteration [1315]: 0.03974541479981088
Loss at iteration [1316]: 0.03922996621988843
Loss at iteration [1317]: 0.038548080244577745
Loss at iteration [1318]: 0.03813012282433109
Loss at iteration [1319]: 0.03749803615739312
Loss at iteration [1320]: 0.03716782986744609
Loss at iteration [1321]: 0.03692494946181912
Loss at iteration [1322]: 0.036600660687194
Loss at iteration [1323]: 0.03650457150512957
Loss at iteration [1324]: 0.03627332899375479
Loss at iteration [1325]: 0.0360410478334464
Loss at iteration [1326]: 0.03610042745746263
***** Warning: Loss has increased *****
Loss at iteration [1327]: 0.036846465500934435
***** Warning: Loss has increased *****
Loss at iteration [1328]: 0.037817286147254804
***** Warning: Loss has increased *****
Loss at iteration [1329]: 0.03948612105067796
***** Warning: Loss has increased *****
Loss at iteration [1330]: 0.042643745875134594
***** Warning: Loss has increased *****
Loss at iteration [1331]: 0.04762549816731265
***** Warning: Loss has increased *****
Loss at iteration [1332]: 0.05870005257782971
***** Warning: Loss has increased *****
Loss at iteration [1333]: 0.07557270557458858
***** Warning: Loss has increased *****
Loss at iteration [1334]: 0.10893532857746233
***** Warning: Loss has increased *****
Loss at iteration [1335]: 0.14449070281036688
***** Warning: Loss has increased *****
Loss at iteration [1336]: 0.20184021871368688
***** Warning: Loss has increased *****
Loss at iteration [1337]: 0.19976119899407388
Loss at iteration [1338]: 0.16503777523182403
Loss at iteration [1339]: 0.07442854832977117
Loss at iteration [1340]: 0.03853587284000142
Loss at iteration [1341]: 0.07994987136224856
***** Warning: Loss has increased *****
Loss at iteration [1342]: 0.11340279723631032
***** Warning: Loss has increased *****
Loss at iteration [1343]: 0.09286225431091796
Loss at iteration [1344]: 0.0440395341891246
Loss at iteration [1345]: 0.0484155470803362
***** Warning: Loss has increased *****
Loss at iteration [1346]: 0.08353080541804363
***** Warning: Loss has increased *****
Loss at iteration [1347]: 0.07510137417602511
Loss at iteration [1348]: 0.043922641666667546
Loss at iteration [1349]: 0.04153944746769034
Loss at iteration [1350]: 0.06307948357275553
***** Warning: Loss has increased *****
Loss at iteration [1351]: 0.06468268420704958
***** Warning: Loss has increased *****
Loss at iteration [1352]: 0.04292577504131804
Loss at iteration [1353]: 0.03814188567761781
Loss at iteration [1354]: 0.05287454133792233
***** Warning: Loss has increased *****
Loss at iteration [1355]: 0.05525604492008962
***** Warning: Loss has increased *****
Loss at iteration [1356]: 0.04289218803191343
Loss at iteration [1357]: 0.03584572139906646
Loss at iteration [1358]: 0.04343400806619486
***** Warning: Loss has increased *****
Loss at iteration [1359]: 0.049377269632957355
***** Warning: Loss has increased *****
Loss at iteration [1360]: 0.04204593167444494
Loss at iteration [1361]: 0.03513532802908571
Loss at iteration [1362]: 0.03747617192156388
***** Warning: Loss has increased *****
Loss at iteration [1363]: 0.04317937698951836
***** Warning: Loss has increased *****
Loss at iteration [1364]: 0.043301741328792935
***** Warning: Loss has increased *****
Loss at iteration [1365]: 0.03744036211092769
Loss at iteration [1366]: 0.03423053622916709
Loss at iteration [1367]: 0.03656128401106546
***** Warning: Loss has increased *****
Loss at iteration [1368]: 0.039844041119880845
***** Warning: Loss has increased *****
Loss at iteration [1369]: 0.03930325992467838
Loss at iteration [1370]: 0.035906178456038954
Loss at iteration [1371]: 0.033760785064526996
Loss at iteration [1372]: 0.03432224243027701
***** Warning: Loss has increased *****
Loss at iteration [1373]: 0.0362158771944821
***** Warning: Loss has increased *****
Loss at iteration [1374]: 0.03723527917730141
***** Warning: Loss has increased *****
Loss at iteration [1375]: 0.036184243709408405
Loss at iteration [1376]: 0.034505346863512594
Loss at iteration [1377]: 0.033376292063958986
Loss at iteration [1378]: 0.033318077694506995
Loss at iteration [1379]: 0.03421410325336064
***** Warning: Loss has increased *****
Loss at iteration [1380]: 0.03509654265059588
***** Warning: Loss has increased *****
Loss at iteration [1381]: 0.03505077637066136
Loss at iteration [1382]: 0.03431162053882648
Loss at iteration [1383]: 0.033302814824480935
Loss at iteration [1384]: 0.03274990753719744
Loss at iteration [1385]: 0.03305075533712943
***** Warning: Loss has increased *****
Loss at iteration [1386]: 0.03364203247759726
***** Warning: Loss has increased *****
Loss at iteration [1387]: 0.034016633765502145
***** Warning: Loss has increased *****
Loss at iteration [1388]: 0.03416756006619007
***** Warning: Loss has increased *****
Loss at iteration [1389]: 0.03379061325940899
Loss at iteration [1390]: 0.03315606854740281
Loss at iteration [1391]: 0.032520067931514775
Loss at iteration [1392]: 0.03232978885424918
Loss at iteration [1393]: 0.032464390831226354
***** Warning: Loss has increased *****
Loss at iteration [1394]: 0.03263779864632511
***** Warning: Loss has increased *****
Loss at iteration [1395]: 0.03301246129445093
***** Warning: Loss has increased *****
Loss at iteration [1396]: 0.03316810474672265
***** Warning: Loss has increased *****
Loss at iteration [1397]: 0.033321058869713475
***** Warning: Loss has increased *****
Loss at iteration [1398]: 0.033057907886558
Loss at iteration [1399]: 0.03288412964072913
Loss at iteration [1400]: 0.03254146388359861
Loss at iteration [1401]: 0.032259068389799424
Loss at iteration [1402]: 0.03187504664897974
Loss at iteration [1403]: 0.03187194974770245
Loss at iteration [1404]: 0.03174479481657365
Loss at iteration [1405]: 0.031709945311929624
Loss at iteration [1406]: 0.0319097507239603
***** Warning: Loss has increased *****
Loss at iteration [1407]: 0.03217554920160492
***** Warning: Loss has increased *****
Loss at iteration [1408]: 0.032275459071521705
***** Warning: Loss has increased *****
Loss at iteration [1409]: 0.03246973798729301
***** Warning: Loss has increased *****
Loss at iteration [1410]: 0.032799718009116546
***** Warning: Loss has increased *****
Loss at iteration [1411]: 0.03293845368913239
***** Warning: Loss has increased *****
Loss at iteration [1412]: 0.03312885866378391
***** Warning: Loss has increased *****
Loss at iteration [1413]: 0.033079183383516474
Loss at iteration [1414]: 0.033251081825807556
***** Warning: Loss has increased *****
Loss at iteration [1415]: 0.033256632851938424
***** Warning: Loss has increased *****
Loss at iteration [1416]: 0.03306230662669565
Loss at iteration [1417]: 0.03290185457780284
Loss at iteration [1418]: 0.03260156270009938
Loss at iteration [1419]: 0.03208215329903022
Loss at iteration [1420]: 0.03178190768785302
Loss at iteration [1421]: 0.03150945182027995
Loss at iteration [1422]: 0.031449173273665906
Loss at iteration [1423]: 0.031183534177292128
Loss at iteration [1424]: 0.030916734645036855
Loss at iteration [1425]: 0.03087822842466541
Loss at iteration [1426]: 0.030743966857514425
Loss at iteration [1427]: 0.030749650625474394
***** Warning: Loss has increased *****
Loss at iteration [1428]: 0.03073671940023523
Loss at iteration [1429]: 0.030757799692195543
***** Warning: Loss has increased *****
Loss at iteration [1430]: 0.031196656368656314
***** Warning: Loss has increased *****
Loss at iteration [1431]: 0.031717123473250584
***** Warning: Loss has increased *****
Loss at iteration [1432]: 0.032559147666921105
***** Warning: Loss has increased *****
Loss at iteration [1433]: 0.03371029391885501
***** Warning: Loss has increased *****
Loss at iteration [1434]: 0.036110654379821396
***** Warning: Loss has increased *****
Loss at iteration [1435]: 0.039465410980054266
***** Warning: Loss has increased *****
Loss at iteration [1436]: 0.04488239948542777
***** Warning: Loss has increased *****
Loss at iteration [1437]: 0.052571972208325025
***** Warning: Loss has increased *****
Loss at iteration [1438]: 0.06791123534374033
***** Warning: Loss has increased *****
Loss at iteration [1439]: 0.08459468250066911
***** Warning: Loss has increased *****
Loss at iteration [1440]: 0.11210943653216758
***** Warning: Loss has increased *****
Loss at iteration [1441]: 0.1217434438456749
***** Warning: Loss has increased *****
Loss at iteration [1442]: 0.12914111644885118
***** Warning: Loss has increased *****
Loss at iteration [1443]: 0.09618614472610483
Loss at iteration [1444]: 0.05706304281500963
Loss at iteration [1445]: 0.032398043192831905
Loss at iteration [1446]: 0.042671469907320354
***** Warning: Loss has increased *****
Loss at iteration [1447]: 0.06584894466201452
***** Warning: Loss has increased *****
Loss at iteration [1448]: 0.06627753438372709
***** Warning: Loss has increased *****
Loss at iteration [1449]: 0.04829341307800631
Loss at iteration [1450]: 0.03238811299025049
Loss at iteration [1451]: 0.03721914270384781
***** Warning: Loss has increased *****
Loss at iteration [1452]: 0.05090679836848493
***** Warning: Loss has increased *****
Loss at iteration [1453]: 0.050438722311926025
Loss at iteration [1454]: 0.03885071353721094
Loss at iteration [1455]: 0.031118132914850914
Loss at iteration [1456]: 0.0358638494981418
***** Warning: Loss has increased *****
Loss at iteration [1457]: 0.04367045642047442
***** Warning: Loss has increased *****
Loss at iteration [1458]: 0.04227096913003833
Loss at iteration [1459]: 0.0350909262501349
Loss at iteration [1460]: 0.03057629711320403
Loss at iteration [1461]: 0.03289782344261173
***** Warning: Loss has increased *****
Loss at iteration [1462]: 0.03721539521598818
***** Warning: Loss has increased *****
Loss at iteration [1463]: 0.03704363111160886
Loss at iteration [1464]: 0.03344013183909241
Loss at iteration [1465]: 0.03024099031546545
Loss at iteration [1466]: 0.030767767673031833
***** Warning: Loss has increased *****
Loss at iteration [1467]: 0.03354713251372887
***** Warning: Loss has increased *****
Loss at iteration [1468]: 0.034796500704503616
***** Warning: Loss has increased *****
Loss at iteration [1469]: 0.03374068428891865
Loss at iteration [1470]: 0.03117378767238201
Loss at iteration [1471]: 0.02966019312045197
Loss at iteration [1472]: 0.029619237055942257
Loss at iteration [1473]: 0.030619818692700824
***** Warning: Loss has increased *****
Loss at iteration [1474]: 0.0316134729057235
***** Warning: Loss has increased *****
Loss at iteration [1475]: 0.031786056350633904
***** Warning: Loss has increased *****
Loss at iteration [1476]: 0.031206280751962807
Loss at iteration [1477]: 0.030026104764104784
Loss at iteration [1478]: 0.029354530368578966
Loss at iteration [1479]: 0.029041908650177816
Loss at iteration [1480]: 0.029179287430438922
***** Warning: Loss has increased *****
Loss at iteration [1481]: 0.02969085844669816
***** Warning: Loss has increased *****
Loss at iteration [1482]: 0.030024500868430472
***** Warning: Loss has increased *****
Loss at iteration [1483]: 0.030127663125628933
***** Warning: Loss has increased *****
Loss at iteration [1484]: 0.029699435485055235
Loss at iteration [1485]: 0.02941443061791795
Loss at iteration [1486]: 0.02911025717326869
Loss at iteration [1487]: 0.028730837460191994
Loss at iteration [1488]: 0.02863307559792986
Loss at iteration [1489]: 0.02866258434591747
***** Warning: Loss has increased *****
Loss at iteration [1490]: 0.02865272185092254
Loss at iteration [1491]: 0.028597169297427654
Loss at iteration [1492]: 0.028616083642949625
***** Warning: Loss has increased *****
Loss at iteration [1493]: 0.02872432950902475
***** Warning: Loss has increased *****
Loss at iteration [1494]: 0.02872910074770677
***** Warning: Loss has increased *****
Loss at iteration [1495]: 0.028843575751624728
***** Warning: Loss has increased *****
Loss at iteration [1496]: 0.028937779932765084
***** Warning: Loss has increased *****
Loss at iteration [1497]: 0.028972568035283537
***** Warning: Loss has increased *****
Loss at iteration [1498]: 0.02909196951348052
***** Warning: Loss has increased *****
Loss at iteration [1499]: 0.029435404964695575
***** Warning: Loss has increased *****
Loss at iteration [1500]: 0.02975016396257055
***** Warning: Loss has increased *****
Loss at iteration [1501]: 0.029964341386402953
***** Warning: Loss has increased *****
Loss at iteration [1502]: 0.030503475967220174
***** Warning: Loss has increased *****
Loss at iteration [1503]: 0.031184499582868355
***** Warning: Loss has increased *****
Loss at iteration [1504]: 0.03183968455397864
***** Warning: Loss has increased *****
Loss at iteration [1505]: 0.03233211597996351
***** Warning: Loss has increased *****
Loss at iteration [1506]: 0.03329714800591021
***** Warning: Loss has increased *****
Loss at iteration [1507]: 0.0339929126267946
***** Warning: Loss has increased *****
Loss at iteration [1508]: 0.03458685430818764
***** Warning: Loss has increased *****
Loss at iteration [1509]: 0.03458876594350476
***** Warning: Loss has increased *****
Loss at iteration [1510]: 0.03543049838008384
***** Warning: Loss has increased *****
Loss at iteration [1511]: 0.03603619887538148
***** Warning: Loss has increased *****
Loss at iteration [1512]: 0.036983410674074645
***** Warning: Loss has increased *****
Loss at iteration [1513]: 0.03711577683674134
***** Warning: Loss has increased *****
Loss at iteration [1514]: 0.03709812809698051
Loss at iteration [1515]: 0.03629477980724757
Loss at iteration [1516]: 0.03530419582693773
Loss at iteration [1517]: 0.03375523549321999
Loss at iteration [1518]: 0.032439446849466584
Loss at iteration [1519]: 0.031105371788442766
Loss at iteration [1520]: 0.0299182419124289
Loss at iteration [1521]: 0.028948400688351916
Loss at iteration [1522]: 0.02824572701845989
Loss at iteration [1523]: 0.02790719634918148
Loss at iteration [1524]: 0.027741696988901998
Loss at iteration [1525]: 0.027581833768615185
Loss at iteration [1526]: 0.02741855792791428
Loss at iteration [1527]: 0.02758591818797157
***** Warning: Loss has increased *****
Loss at iteration [1528]: 0.027890302419937712
***** Warning: Loss has increased *****
Loss at iteration [1529]: 0.028330619011629553
***** Warning: Loss has increased *****
Loss at iteration [1530]: 0.028878469286772387
***** Warning: Loss has increased *****
Loss at iteration [1531]: 0.029746929750501833
***** Warning: Loss has increased *****
Loss at iteration [1532]: 0.030616232732545254
***** Warning: Loss has increased *****
Loss at iteration [1533]: 0.03204110522237069
***** Warning: Loss has increased *****
Loss at iteration [1534]: 0.03367918998521942
***** Warning: Loss has increased *****
Loss at iteration [1535]: 0.03757923489981381
***** Warning: Loss has increased *****
Loss at iteration [1536]: 0.04296050967742415
***** Warning: Loss has increased *****
Loss at iteration [1537]: 0.05265674452232342
***** Warning: Loss has increased *****
Loss at iteration [1538]: 0.06331679452120213
***** Warning: Loss has increased *****
Loss at iteration [1539]: 0.08028983596584269
***** Warning: Loss has increased *****
Loss at iteration [1540]: 0.09175937992141792
***** Warning: Loss has increased *****
Loss at iteration [1541]: 0.10585507383813468
***** Warning: Loss has increased *****
Loss at iteration [1542]: 0.09751701000978062
Loss at iteration [1543]: 0.08246404278415778
Loss at iteration [1544]: 0.05053305993434352
Loss at iteration [1545]: 0.029983683615053134
Loss at iteration [1546]: 0.03309636056746368
***** Warning: Loss has increased *****
Loss at iteration [1547]: 0.04914769760989088
***** Warning: Loss has increased *****
Loss at iteration [1548]: 0.05945722384147088
***** Warning: Loss has increased *****
Loss at iteration [1549]: 0.049537449110071984
Loss at iteration [1550]: 0.03469018519695453
Loss at iteration [1551]: 0.028356205772118122
Loss at iteration [1552]: 0.035248140999420634
***** Warning: Loss has increased *****
Loss at iteration [1553]: 0.04348083025941738
***** Warning: Loss has increased *****
Loss at iteration [1554]: 0.04152911643636313
Loss at iteration [1555]: 0.03377422969266099
Loss at iteration [1556]: 0.02816560548623903
Loss at iteration [1557]: 0.03000005974475197
***** Warning: Loss has increased *****
Loss at iteration [1558]: 0.03532487355332001
***** Warning: Loss has increased *****
Loss at iteration [1559]: 0.03658535686614619
***** Warning: Loss has increased *****
Loss at iteration [1560]: 0.033369096299145
Loss at iteration [1561]: 0.028749149271141496
Loss at iteration [1562]: 0.027901770490340223
Loss at iteration [1563]: 0.030469872562257506
***** Warning: Loss has increased *****
Loss at iteration [1564]: 0.03275918044539933
***** Warning: Loss has increased *****
Loss at iteration [1565]: 0.032630834549397754
Loss at iteration [1566]: 0.0300307399451413
Loss at iteration [1567]: 0.0278722777461715
Loss at iteration [1568]: 0.027413337095111026
Loss at iteration [1569]: 0.02856859832581773
***** Warning: Loss has increased *****
Loss at iteration [1570]: 0.030148824863072354
***** Warning: Loss has increased *****
Loss at iteration [1571]: 0.030695224650833085
***** Warning: Loss has increased *****
Loss at iteration [1572]: 0.029714233395218132
Loss at iteration [1573]: 0.028016147779798124
Loss at iteration [1574]: 0.026980148985893945
Loss at iteration [1575]: 0.02685128259785601
Loss at iteration [1576]: 0.027417693445074456
***** Warning: Loss has increased *****
Loss at iteration [1577]: 0.02849963914270126
***** Warning: Loss has increased *****
Loss at iteration [1578]: 0.02936936469043401
***** Warning: Loss has increased *****
Loss at iteration [1579]: 0.029713296284719738
***** Warning: Loss has increased *****
Loss at iteration [1580]: 0.029319374947137895
Loss at iteration [1581]: 0.028631850815716
Loss at iteration [1582]: 0.02764833248406534
Loss at iteration [1583]: 0.026958815076166646
Loss at iteration [1584]: 0.026548934630935953
Loss at iteration [1585]: 0.026490898966242012
Loss at iteration [1586]: 0.026659395720197068
***** Warning: Loss has increased *****
Loss at iteration [1587]: 0.026987967045483098
***** Warning: Loss has increased *****
Loss at iteration [1588]: 0.027243915904457655
***** Warning: Loss has increased *****
Loss at iteration [1589]: 0.027842156561337265
***** Warning: Loss has increased *****
Loss at iteration [1590]: 0.029085123487658605
***** Warning: Loss has increased *****
Loss at iteration [1591]: 0.030557969210569757
***** Warning: Loss has increased *****
Loss at iteration [1592]: 0.03261421833754191
***** Warning: Loss has increased *****
Loss at iteration [1593]: 0.03429978437572647
***** Warning: Loss has increased *****
Loss at iteration [1594]: 0.03551595149580732
***** Warning: Loss has increased *****
Loss at iteration [1595]: 0.034451266809987134
Loss at iteration [1596]: 0.032595846712302305
Loss at iteration [1597]: 0.03011583446519697
Loss at iteration [1598]: 0.02856035901910362
Loss at iteration [1599]: 0.02715330680242274
Loss at iteration [1600]: 0.026282060064316454
Loss at iteration [1601]: 0.02619636711086151
Loss at iteration [1602]: 0.0265200652524605
***** Warning: Loss has increased *****
Loss at iteration [1603]: 0.02719955781265694
***** Warning: Loss has increased *****
Loss at iteration [1604]: 0.027859567671698164
***** Warning: Loss has increased *****
Loss at iteration [1605]: 0.02844241350557999
***** Warning: Loss has increased *****
Loss at iteration [1606]: 0.02895463952899956
***** Warning: Loss has increased *****
Loss at iteration [1607]: 0.029911719140429217
***** Warning: Loss has increased *****
Loss at iteration [1608]: 0.030696547453854348
***** Warning: Loss has increased *****
Loss at iteration [1609]: 0.03179377564597498
***** Warning: Loss has increased *****
Loss at iteration [1610]: 0.03290143439992733
***** Warning: Loss has increased *****
Loss at iteration [1611]: 0.03427025579025542
***** Warning: Loss has increased *****
Loss at iteration [1612]: 0.034488072053905625
***** Warning: Loss has increased *****
Loss at iteration [1613]: 0.03491416499054779
***** Warning: Loss has increased *****
Loss at iteration [1614]: 0.034216401419783736
Loss at iteration [1615]: 0.03391543878516837
Loss at iteration [1616]: 0.03272745117818376
Loss at iteration [1617]: 0.031886831821825404
Loss at iteration [1618]: 0.03044567789113774
Loss at iteration [1619]: 0.029111190758144046
Loss at iteration [1620]: 0.027911560724919213
Loss at iteration [1621]: 0.026928903542261518
Loss at iteration [1622]: 0.026344716843544627
Loss at iteration [1623]: 0.025997601584566737
Loss at iteration [1624]: 0.025687223954833922
Loss at iteration [1625]: 0.025493501572259058
Loss at iteration [1626]: 0.025660216977524592
***** Warning: Loss has increased *****
Loss at iteration [1627]: 0.02591737519137493
***** Warning: Loss has increased *****
Loss at iteration [1628]: 0.026264385215723426
***** Warning: Loss has increased *****
Loss at iteration [1629]: 0.02678811404203841
***** Warning: Loss has increased *****
Loss at iteration [1630]: 0.027824868259019864
***** Warning: Loss has increased *****
Loss at iteration [1631]: 0.02911968358770509
***** Warning: Loss has increased *****
Loss at iteration [1632]: 0.031210518824753744
***** Warning: Loss has increased *****
Loss at iteration [1633]: 0.034022877180947556
***** Warning: Loss has increased *****
Loss at iteration [1634]: 0.03926285763607996
***** Warning: Loss has increased *****
Loss at iteration [1635]: 0.04601262946429087
***** Warning: Loss has increased *****
Loss at iteration [1636]: 0.058686083392228305
***** Warning: Loss has increased *****
Loss at iteration [1637]: 0.07209525507739452
***** Warning: Loss has increased *****
Loss at iteration [1638]: 0.09426323304220273
***** Warning: Loss has increased *****
Loss at iteration [1639]: 0.10320235857131152
***** Warning: Loss has increased *****
Loss at iteration [1640]: 0.11599838751022613
***** Warning: Loss has increased *****
Loss at iteration [1641]: 0.09766983784071771
Loss at iteration [1642]: 0.07126326859233775
Loss at iteration [1643]: 0.037132128340479915
Loss at iteration [1644]: 0.028086272936788677
Loss at iteration [1645]: 0.04584139298213224
***** Warning: Loss has increased *****
Loss at iteration [1646]: 0.061246642668594156
***** Warning: Loss has increased *****
Loss at iteration [1647]: 0.05815258750425079
Loss at iteration [1648]: 0.038196157216435
Loss at iteration [1649]: 0.027205727073277473
Loss at iteration [1650]: 0.034388162072093366
***** Warning: Loss has increased *****
Loss at iteration [1651]: 0.04501351161200911
***** Warning: Loss has increased *****
Loss at iteration [1652]: 0.04483618547562313
Loss at iteration [1653]: 0.033951833448790956
Loss at iteration [1654]: 0.026811703697822643
Loss at iteration [1655]: 0.0296112508288849
***** Warning: Loss has increased *****
Loss at iteration [1656]: 0.03648353277621555
***** Warning: Loss has increased *****
Loss at iteration [1657]: 0.038787134796403694
***** Warning: Loss has increased *****
Loss at iteration [1658]: 0.03288547506659574
Loss at iteration [1659]: 0.027434471647485234
Loss at iteration [1660]: 0.02647356318465705
Loss at iteration [1661]: 0.029784093560756075
***** Warning: Loss has increased *****
Loss at iteration [1662]: 0.03350119400199882
***** Warning: Loss has increased *****
Loss at iteration [1663]: 0.0337612903717634
***** Warning: Loss has increased *****
Loss at iteration [1664]: 0.03082919443539676
Loss at iteration [1665]: 0.02706318881642862
Loss at iteration [1666]: 0.025783415759191554
Loss at iteration [1667]: 0.02724151916584481
***** Warning: Loss has increased *****
Loss at iteration [1668]: 0.02950254441286457
***** Warning: Loss has increased *****
Loss at iteration [1669]: 0.03070221165479511
***** Warning: Loss has increased *****
Loss at iteration [1670]: 0.029573317663765232
Loss at iteration [1671]: 0.02760998269583396
Loss at iteration [1672]: 0.025805135760402966
Loss at iteration [1673]: 0.02525888450789976
Loss at iteration [1674]: 0.02597378184358746
***** Warning: Loss has increased *****
Loss at iteration [1675]: 0.02727588512100262
***** Warning: Loss has increased *****
Loss at iteration [1676]: 0.02848099017879962
***** Warning: Loss has increased *****
Loss at iteration [1677]: 0.029002609738122413
***** Warning: Loss has increased *****
Loss at iteration [1678]: 0.02888963127931214
Loss at iteration [1679]: 0.027870688943975395
Loss at iteration [1680]: 0.026636966843638327
Loss at iteration [1681]: 0.025467588510554712
Loss at iteration [1682]: 0.02507753218193116
Loss at iteration [1683]: 0.025222648825420777
***** Warning: Loss has increased *****
Loss at iteration [1684]: 0.025699413958734874
***** Warning: Loss has increased *****
Loss at iteration [1685]: 0.026307206606159577
***** Warning: Loss has increased *****
Loss at iteration [1686]: 0.026831115798658822
***** Warning: Loss has increased *****
Loss at iteration [1687]: 0.02748043342537176
***** Warning: Loss has increased *****
Loss at iteration [1688]: 0.02762360891211905
***** Warning: Loss has increased *****
Loss at iteration [1689]: 0.027463628218319283
Loss at iteration [1690]: 0.026876400726287858
Loss at iteration [1691]: 0.026388702159807886
Loss at iteration [1692]: 0.02593596470576606
Loss at iteration [1693]: 0.025553863809501948
Loss at iteration [1694]: 0.025196703937141088
Loss at iteration [1695]: 0.02495829168412611
Loss at iteration [1696]: 0.024752121430865536
Loss at iteration [1697]: 0.02475757636829941
***** Warning: Loss has increased *****
Loss at iteration [1698]: 0.024809340982084847
***** Warning: Loss has increased *****
Loss at iteration [1699]: 0.024836105820775483
***** Warning: Loss has increased *****
Loss at iteration [1700]: 0.024897924494308395
***** Warning: Loss has increased *****
Loss at iteration [1701]: 0.02493236818983191
***** Warning: Loss has increased *****
Loss at iteration [1702]: 0.02517506860278528
***** Warning: Loss has increased *****
Loss at iteration [1703]: 0.025448907368754638
***** Warning: Loss has increased *****
Loss at iteration [1704]: 0.025840656415777173
***** Warning: Loss has increased *****
Loss at iteration [1705]: 0.02640188460635647
***** Warning: Loss has increased *****
Loss at iteration [1706]: 0.027146267783850434
***** Warning: Loss has increased *****
Loss at iteration [1707]: 0.02780096519267972
***** Warning: Loss has increased *****
Loss at iteration [1708]: 0.028526303997632434
***** Warning: Loss has increased *****
Loss at iteration [1709]: 0.029121685993099597
***** Warning: Loss has increased *****
Loss at iteration [1710]: 0.03019631156668649
***** Warning: Loss has increased *****
Loss at iteration [1711]: 0.030918542261121652
***** Warning: Loss has increased *****
Loss at iteration [1712]: 0.032229006895637284
***** Warning: Loss has increased *****
Loss at iteration [1713]: 0.03303264626859601
***** Warning: Loss has increased *****
Loss at iteration [1714]: 0.03412033824283549
***** Warning: Loss has increased *****
Loss at iteration [1715]: 0.03390947025819483
Loss at iteration [1716]: 0.03367676363297331
Loss at iteration [1717]: 0.03226834760974791
Loss at iteration [1718]: 0.0307387997496621
Loss at iteration [1719]: 0.028523524966142935
Loss at iteration [1720]: 0.02680093839974224
Loss at iteration [1721]: 0.025336024154280087
Loss at iteration [1722]: 0.02456572009697217
Loss at iteration [1723]: 0.024380591928447163
Loss at iteration [1724]: 0.024696606908313885
***** Warning: Loss has increased *****
Loss at iteration [1725]: 0.0253535991388586
***** Warning: Loss has increased *****
Loss at iteration [1726]: 0.026254498886589366
***** Warning: Loss has increased *****
Loss at iteration [1727]: 0.027474700074204293
***** Warning: Loss has increased *****
Loss at iteration [1728]: 0.0287672769124407
***** Warning: Loss has increased *****
Loss at iteration [1729]: 0.03054807593206437
***** Warning: Loss has increased *****
Loss at iteration [1730]: 0.03212102266629618
***** Warning: Loss has increased *****
Loss at iteration [1731]: 0.034294811209608096
***** Warning: Loss has increased *****
Loss at iteration [1732]: 0.03589595743779213
***** Warning: Loss has increased *****
Loss at iteration [1733]: 0.03816216044339503
***** Warning: Loss has increased *****
Loss at iteration [1734]: 0.039188388312597314
***** Warning: Loss has increased *****
Loss at iteration [1735]: 0.04128209936224908
***** Warning: Loss has increased *****
Loss at iteration [1736]: 0.04178471802144015
***** Warning: Loss has increased *****
Loss at iteration [1737]: 0.042244600612041036
***** Warning: Loss has increased *****
Loss at iteration [1738]: 0.03980450460033086
Loss at iteration [1739]: 0.03710537580175779
Loss at iteration [1740]: 0.03237985887192617
Loss at iteration [1741]: 0.028522960072782034
Loss at iteration [1742]: 0.025683084232531805
Loss at iteration [1743]: 0.02436903824581734
Loss at iteration [1744]: 0.024515938350258757
***** Warning: Loss has increased *****
Loss at iteration [1745]: 0.025744935728821867
***** Warning: Loss has increased *****
Loss at iteration [1746]: 0.027448903034670735
***** Warning: Loss has increased *****
Loss at iteration [1747]: 0.0289861577568987
***** Warning: Loss has increased *****
Loss at iteration [1748]: 0.03036397228994128
***** Warning: Loss has increased *****
Loss at iteration [1749]: 0.03096134857944657
***** Warning: Loss has increased *****
Loss at iteration [1750]: 0.031358377562980745
***** Warning: Loss has increased *****
Loss at iteration [1751]: 0.031134561042390013
Loss at iteration [1752]: 0.030696451860944125
Loss at iteration [1753]: 0.029530318244664257
Loss at iteration [1754]: 0.028264845509409744
Loss at iteration [1755]: 0.026878706191512867
Loss at iteration [1756]: 0.02576559994994222
Loss at iteration [1757]: 0.024847268874941035
Loss at iteration [1758]: 0.0242937828856009
Loss at iteration [1759]: 0.02404561175290098
Loss at iteration [1760]: 0.02402825325432214
Loss at iteration [1761]: 0.024306431419482518
***** Warning: Loss has increased *****
Loss at iteration [1762]: 0.02470601985915662
***** Warning: Loss has increased *****
Loss at iteration [1763]: 0.025245717015100314
***** Warning: Loss has increased *****
Loss at iteration [1764]: 0.025931599123347082
***** Warning: Loss has increased *****
Loss at iteration [1765]: 0.02721873300984505
***** Warning: Loss has increased *****
Loss at iteration [1766]: 0.028905697869186227
***** Warning: Loss has increased *****
Loss at iteration [1767]: 0.031742663553371485
***** Warning: Loss has increased *****
Loss at iteration [1768]: 0.03520563861417595
***** Warning: Loss has increased *****
Loss at iteration [1769]: 0.04091882941520266
***** Warning: Loss has increased *****
Loss at iteration [1770]: 0.047173139354135696
***** Warning: Loss has increased *****
Loss at iteration [1771]: 0.05841215152506877
***** Warning: Loss has increased *****
Loss at iteration [1772]: 0.07014658175168569
***** Warning: Loss has increased *****
Loss at iteration [1773]: 0.09326147023605626
***** Warning: Loss has increased *****
Loss at iteration [1774]: 0.10329492841181617
***** Warning: Loss has increased *****
Loss at iteration [1775]: 0.12040670633314092
***** Warning: Loss has increased *****
Loss at iteration [1776]: 0.09965893824961151
Loss at iteration [1777]: 0.06807813316604187
Loss at iteration [1778]: 0.03247825560836971
Loss at iteration [1779]: 0.02901711907653641
Loss at iteration [1780]: 0.05090370573529774
***** Warning: Loss has increased *****
Loss at iteration [1781]: 0.0624599362191602
***** Warning: Loss has increased *****
Loss at iteration [1782]: 0.05205561667278611
Loss at iteration [1783]: 0.030933945137251458
Loss at iteration [1784]: 0.027540357214049918
Loss at iteration [1785]: 0.04090700242317773
***** Warning: Loss has increased *****
Loss at iteration [1786]: 0.047775936855083787
***** Warning: Loss has increased *****
Loss at iteration [1787]: 0.03993425552609399
Loss at iteration [1788]: 0.02736992556352933
Loss at iteration [1789]: 0.027369634404098626
Loss at iteration [1790]: 0.03620516647153294
***** Warning: Loss has increased *****
Loss at iteration [1791]: 0.038683678548817546
***** Warning: Loss has increased *****
Loss at iteration [1792]: 0.0327589798930605
Loss at iteration [1793]: 0.025747757957129237
Loss at iteration [1794]: 0.025973602856655148
***** Warning: Loss has increased *****
Loss at iteration [1795]: 0.030996208091683696
***** Warning: Loss has increased *****
Loss at iteration [1796]: 0.03311830952002267
***** Warning: Loss has increased *****
Loss at iteration [1797]: 0.030711772603247565
Loss at iteration [1798]: 0.026234094411862318
Loss at iteration [1799]: 0.024352852447306145
Loss at iteration [1800]: 0.025877299761982782
***** Warning: Loss has increased *****
Loss at iteration [1801]: 0.028488530537804924
***** Warning: Loss has increased *****
Loss at iteration [1802]: 0.029743704218341598
***** Warning: Loss has increased *****
Loss at iteration [1803]: 0.02822237357889763
Loss at iteration [1804]: 0.025782512918624226
Loss at iteration [1805]: 0.024153506407771797
Loss at iteration [1806]: 0.024219567658013256
***** Warning: Loss has increased *****
Loss at iteration [1807]: 0.025365780870400256
***** Warning: Loss has increased *****
Loss at iteration [1808]: 0.026597932061361594
***** Warning: Loss has increased *****
Loss at iteration [1809]: 0.027359451550083517
***** Warning: Loss has increased *****
Loss at iteration [1810]: 0.027002211445080873
Loss at iteration [1811]: 0.026171466280215303
Loss at iteration [1812]: 0.024911918023811555
Loss at iteration [1813]: 0.02403994106660797
Loss at iteration [1814]: 0.02376422933859645
Loss at iteration [1815]: 0.02400636532280812
***** Warning: Loss has increased *****
Loss at iteration [1816]: 0.02454370594773856
***** Warning: Loss has increased *****
Loss at iteration [1817]: 0.024976356429424727
***** Warning: Loss has increased *****
Loss at iteration [1818]: 0.02528810083767925
***** Warning: Loss has increased *****
Loss at iteration [1819]: 0.025333884576483084
***** Warning: Loss has increased *****
Loss at iteration [1820]: 0.02517174832968826
Loss at iteration [1821]: 0.024776201578318214
Loss at iteration [1822]: 0.024332730720890906
Loss at iteration [1823]: 0.023950748507486436
Loss at iteration [1824]: 0.023672114546351847
Loss at iteration [1825]: 0.023519385233618552
Loss at iteration [1826]: 0.02354541576534444
***** Warning: Loss has increased *****
Loss at iteration [1827]: 0.02365866244233246
***** Warning: Loss has increased *****
Loss at iteration [1828]: 0.023788692654607823
***** Warning: Loss has increased *****
Loss at iteration [1829]: 0.023988493122603392
***** Warning: Loss has increased *****
Loss at iteration [1830]: 0.024160515790742416
***** Warning: Loss has increased *****
Loss at iteration [1831]: 0.024409411353882048
***** Warning: Loss has increased *****
Loss at iteration [1832]: 0.024598825915798542
***** Warning: Loss has increased *****
Loss at iteration [1833]: 0.024848362932723727
***** Warning: Loss has increased *****
Loss at iteration [1834]: 0.02502156678606003
***** Warning: Loss has increased *****
Loss at iteration [1835]: 0.02533216800371892
***** Warning: Loss has increased *****
Loss at iteration [1836]: 0.025489328526346902
***** Warning: Loss has increased *****
Loss at iteration [1837]: 0.025841077477844608
***** Warning: Loss has increased *****
Loss at iteration [1838]: 0.02609972885320641
***** Warning: Loss has increased *****
Loss at iteration [1839]: 0.026515133664478437
***** Warning: Loss has increased *****
Loss at iteration [1840]: 0.026683023726120355
***** Warning: Loss has increased *****
Loss at iteration [1841]: 0.02733376282902247
***** Warning: Loss has increased *****
Loss at iteration [1842]: 0.027862239678612514
***** Warning: Loss has increased *****
Loss at iteration [1843]: 0.028651183182553927
***** Warning: Loss has increased *****
Loss at iteration [1844]: 0.02926017302717721
***** Warning: Loss has increased *****
Loss at iteration [1845]: 0.030085066664160984
***** Warning: Loss has increased *****
Loss at iteration [1846]: 0.03010368924970234
***** Warning: Loss has increased *****
Loss at iteration [1847]: 0.030118104878196574
***** Warning: Loss has increased *****
Loss at iteration [1848]: 0.029446728463262597
Loss at iteration [1849]: 0.02931750467807167
Loss at iteration [1850]: 0.02873397102462855
Loss at iteration [1851]: 0.028113208902702254
Loss at iteration [1852]: 0.026943774081540046
Loss at iteration [1853]: 0.02588407045288125
Loss at iteration [1854]: 0.024727093919357346
Loss at iteration [1855]: 0.02385409500203819
Loss at iteration [1856]: 0.023303343741873806
Loss at iteration [1857]: 0.02324757938754083
Loss at iteration [1858]: 0.02344773236925395
***** Warning: Loss has increased *****
Loss at iteration [1859]: 0.023855003879231892
***** Warning: Loss has increased *****
Loss at iteration [1860]: 0.024633800960027798
***** Warning: Loss has increased *****
Loss at iteration [1861]: 0.025413659788860583
***** Warning: Loss has increased *****
Loss at iteration [1862]: 0.02628871761769407
***** Warning: Loss has increased *****
Loss at iteration [1863]: 0.027056665780684264
***** Warning: Loss has increased *****
Loss at iteration [1864]: 0.028265092323684734
***** Warning: Loss has increased *****
Loss at iteration [1865]: 0.029224803079934632
***** Warning: Loss has increased *****
Loss at iteration [1866]: 0.030753603651902557
***** Warning: Loss has increased *****
Loss at iteration [1867]: 0.03182402378231895
***** Warning: Loss has increased *****
Loss at iteration [1868]: 0.033930753672862454
***** Warning: Loss has increased *****
Loss at iteration [1869]: 0.0353052005151886
***** Warning: Loss has increased *****
Loss at iteration [1870]: 0.03757156338205674
***** Warning: Loss has increased *****
Loss at iteration [1871]: 0.03820584722895533
***** Warning: Loss has increased *****
Loss at iteration [1872]: 0.04005855786093596
***** Warning: Loss has increased *****
Loss at iteration [1873]: 0.03935704060163502
Loss at iteration [1874]: 0.03893382337971661
Loss at iteration [1875]: 0.035791271954395625
Loss at iteration [1876]: 0.03275120424140643
Loss at iteration [1877]: 0.028427918815244087
Loss at iteration [1878]: 0.02498393423625167
Loss at iteration [1879]: 0.02332889618606738
Loss at iteration [1880]: 0.023710678433412596
***** Warning: Loss has increased *****
Loss at iteration [1881]: 0.02548682969400565
***** Warning: Loss has increased *****
Loss at iteration [1882]: 0.027456132566030588
***** Warning: Loss has increased *****
Loss at iteration [1883]: 0.029341046469077554
***** Warning: Loss has increased *****
Loss at iteration [1884]: 0.029949284509106344
***** Warning: Loss has increased *****
Loss at iteration [1885]: 0.03011477214428278
***** Warning: Loss has increased *****
Loss at iteration [1886]: 0.0288226749042349
Loss at iteration [1887]: 0.027241048236325477
Loss at iteration [1888]: 0.025345443716920026
Loss at iteration [1889]: 0.023856187243857824
Loss at iteration [1890]: 0.02308030284470701
Loss at iteration [1891]: 0.023098972186726817
***** Warning: Loss has increased *****
Loss at iteration [1892]: 0.02361832648783587
***** Warning: Loss has increased *****
Loss at iteration [1893]: 0.024483582801371542
***** Warning: Loss has increased *****
Loss at iteration [1894]: 0.02552616675863044
***** Warning: Loss has increased *****
Loss at iteration [1895]: 0.026396042083881823
***** Warning: Loss has increased *****
Loss at iteration [1896]: 0.027307572959780493
***** Warning: Loss has increased *****
Loss at iteration [1897]: 0.02781739800219049
***** Warning: Loss has increased *****
Loss at iteration [1898]: 0.02836671380234015
***** Warning: Loss has increased *****
Loss at iteration [1899]: 0.028484098280235542
***** Warning: Loss has increased *****
Loss at iteration [1900]: 0.02871362404377019
***** Warning: Loss has increased *****
Loss at iteration [1901]: 0.028503809278347808
Loss at iteration [1902]: 0.028534019898207405
***** Warning: Loss has increased *****
Loss at iteration [1903]: 0.02769608700848454
Loss at iteration [1904]: 0.027084663707050562
Loss at iteration [1905]: 0.026070280611611533
Loss at iteration [1906]: 0.025194941057283027
Loss at iteration [1907]: 0.02435399708114595
Loss at iteration [1908]: 0.0237973325663401
Loss at iteration [1909]: 0.023268235835595175
Loss at iteration [1910]: 0.022943412420217284
Loss at iteration [1911]: 0.02278886679541353
Loss at iteration [1912]: 0.022727551518630207
Loss at iteration [1913]: 0.022789283993358507
***** Warning: Loss has increased *****
Loss at iteration [1914]: 0.022886405961831825
***** Warning: Loss has increased *****
Loss at iteration [1915]: 0.0230300461991063
***** Warning: Loss has increased *****
Loss at iteration [1916]: 0.02325180837058398
***** Warning: Loss has increased *****
Loss at iteration [1917]: 0.02358595580682963
***** Warning: Loss has increased *****
Loss at iteration [1918]: 0.02409832071629674
***** Warning: Loss has increased *****
Loss at iteration [1919]: 0.02500627450781392
***** Warning: Loss has increased *****
Loss at iteration [1920]: 0.02621453014691012
***** Warning: Loss has increased *****
Loss at iteration [1921]: 0.028463372817227512
***** Warning: Loss has increased *****
Loss at iteration [1922]: 0.031531673194220365
***** Warning: Loss has increased *****
Loss at iteration [1923]: 0.03817046880577504
***** Warning: Loss has increased *****
Loss at iteration [1924]: 0.04730765718848819
***** Warning: Loss has increased *****
Loss at iteration [1925]: 0.0673153614990907
***** Warning: Loss has increased *****
Loss at iteration [1926]: 0.0895251996165951
***** Warning: Loss has increased *****
Loss at iteration [1927]: 0.1364261419135321
***** Warning: Loss has increased *****
Loss at iteration [1928]: 0.1545871424002647
***** Warning: Loss has increased *****
Loss at iteration [1929]: 0.18152154300131793
***** Warning: Loss has increased *****
Loss at iteration [1930]: 0.11065697196388184
Loss at iteration [1931]: 0.04142395574438055
Loss at iteration [1932]: 0.03281341828819554
Loss at iteration [1933]: 0.07814495788626424
***** Warning: Loss has increased *****
Loss at iteration [1934]: 0.11368303069376424
***** Warning: Loss has increased *****
Loss at iteration [1935]: 0.07188898155661942
Loss at iteration [1936]: 0.029443712180283892
Loss at iteration [1937]: 0.04549728016246334
***** Warning: Loss has increased *****
Loss at iteration [1938]: 0.07251081662326823
***** Warning: Loss has increased *****
Loss at iteration [1939]: 0.05605410891133329
Loss at iteration [1940]: 0.027825542846829084
Loss at iteration [1941]: 0.04023515951798462
***** Warning: Loss has increased *****
Loss at iteration [1942]: 0.05887778271085283
***** Warning: Loss has increased *****
Loss at iteration [1943]: 0.043183357746904724
Loss at iteration [1944]: 0.0262397954550686
Loss at iteration [1945]: 0.035809438013727116
***** Warning: Loss has increased *****
Loss at iteration [1946]: 0.04560766549369588
***** Warning: Loss has increased *****
Loss at iteration [1947]: 0.03628096376867834
Loss at iteration [1948]: 0.025370756432544782
Loss at iteration [1949]: 0.03160998321875572
***** Warning: Loss has increased *****
Loss at iteration [1950]: 0.039511689625096526
***** Warning: Loss has increased *****
Loss at iteration [1951]: 0.03284066201320799
Loss at iteration [1952]: 0.024804162314717684
Loss at iteration [1953]: 0.02727478022643791
***** Warning: Loss has increased *****
Loss at iteration [1954]: 0.033425525505412036
***** Warning: Loss has increased *****
Loss at iteration [1955]: 0.03238321824612967
Loss at iteration [1956]: 0.02588698926749866
Loss at iteration [1957]: 0.02434494592995495
Loss at iteration [1958]: 0.028073650459931276
***** Warning: Loss has increased *****
Loss at iteration [1959]: 0.029673667678902185
***** Warning: Loss has increased *****
Loss at iteration [1960]: 0.02698761697321866
Loss at iteration [1961]: 0.023844085716458917
Loss at iteration [1962]: 0.02454363338383175
***** Warning: Loss has increased *****
Loss at iteration [1963]: 0.02682204592820103
***** Warning: Loss has increased *****
Loss at iteration [1964]: 0.02690024079756803
***** Warning: Loss has increased *****
Loss at iteration [1965]: 0.024979595471132366
Loss at iteration [1966]: 0.0233636743836426
Loss at iteration [1967]: 0.02410245409774894
***** Warning: Loss has increased *****
Loss at iteration [1968]: 0.025714030815694983
***** Warning: Loss has increased *****
Loss at iteration [1969]: 0.025918607359611476
***** Warning: Loss has increased *****
Loss at iteration [1970]: 0.024800991274058673
Loss at iteration [1971]: 0.023369922911179654
Loss at iteration [1972]: 0.023189483937395165
Loss at iteration [1973]: 0.024009025273911157
***** Warning: Loss has increased *****
Loss at iteration [1974]: 0.024582870821472962
***** Warning: Loss has increased *****
Loss at iteration [1975]: 0.024416659938489394
Loss at iteration [1976]: 0.023557708612725266
Loss at iteration [1977]: 0.022996584729117357
Loss at iteration [1978]: 0.02322925547786538
***** Warning: Loss has increased *****
Loss at iteration [1979]: 0.023705168292898433
***** Warning: Loss has increased *****
Loss at iteration [1980]: 0.0238953069986292
***** Warning: Loss has increased *****
Loss at iteration [1981]: 0.023585756460658656
Loss at iteration [1982]: 0.023099446463130592
Loss at iteration [1983]: 0.02287239116492942
Loss at iteration [1984]: 0.02296810640558966
***** Warning: Loss has increased *****
Loss at iteration [1985]: 0.023248715980846376
***** Warning: Loss has increased *****
Loss at iteration [1986]: 0.023384035256011047
***** Warning: Loss has increased *****
Loss at iteration [1987]: 0.023246703636796477
Loss at iteration [1988]: 0.02299796835970709
Loss at iteration [1989]: 0.022834043343946866
Loss at iteration [1990]: 0.022784689390470562
Loss at iteration [1991]: 0.022911441065624795
***** Warning: Loss has increased *****
Loss at iteration [1992]: 0.022996521380125055
***** Warning: Loss has increased *****
Loss at iteration [1993]: 0.022966139421992247
Loss at iteration [1994]: 0.022881195298448717
Loss at iteration [1995]: 0.022814133887316426
Loss at iteration [1996]: 0.022748466610873597
Loss at iteration [1997]: 0.022715808541141136
Loss at iteration [1998]: 0.022706338929132826
Loss at iteration [1999]: 0.022814954579950603
***** Warning: Loss has increased *****
Loss at iteration [2000]: 0.022872773075225592
***** Warning: Loss has increased *****
Loss at iteration [2001]: 0.022831875207519985
Loss at iteration [2002]: 0.022721210237846814
Loss at iteration [2003]: 0.022668375459141774
Loss at iteration [2004]: 0.02265310275653567
Loss at iteration [2005]: 0.022629822587182668
Loss at iteration [2006]: 0.0226008987130455
Loss at iteration [2007]: 0.022637861133671645
***** Warning: Loss has increased *****
Loss at iteration [2008]: 0.022683376457306687
***** Warning: Loss has increased *****
Loss at iteration [2009]: 0.02265454195341317
Loss at iteration [2010]: 0.022586515922435425
Loss at iteration [2011]: 0.02256303615486508
Loss at iteration [2012]: 0.02256281651637875
Loss at iteration [2013]: 0.022530588606748586
Loss at iteration [2014]: 0.02248523450170722
Loss at iteration [2015]: 0.022520583436874485
***** Warning: Loss has increased *****
Loss at iteration [2016]: 0.022538505496621765
***** Warning: Loss has increased *****
Loss at iteration [2017]: 0.02252545697494478
Loss at iteration [2018]: 0.022495808759841657
Loss at iteration [2019]: 0.02250402316061286
***** Warning: Loss has increased *****
Loss at iteration [2020]: 0.022532864568367627
***** Warning: Loss has increased *****
Loss at iteration [2021]: 0.02260071832807936
***** Warning: Loss has increased *****
Loss at iteration [2022]: 0.02272467441088932
***** Warning: Loss has increased *****
Loss at iteration [2023]: 0.022844699182469223
***** Warning: Loss has increased *****
Loss at iteration [2024]: 0.022903896562381943
***** Warning: Loss has increased *****
Loss at iteration [2025]: 0.02298649578152583
***** Warning: Loss has increased *****
Loss at iteration [2026]: 0.023005057851139453
***** Warning: Loss has increased *****
Loss at iteration [2027]: 0.02296779944058428
Loss at iteration [2028]: 0.022817441986747
Loss at iteration [2029]: 0.022717179653206396
Loss at iteration [2030]: 0.022595914018460835
Loss at iteration [2031]: 0.022488631449401015
Loss at iteration [2032]: 0.022390468705999752
Loss at iteration [2033]: 0.022352296550420748
Loss at iteration [2034]: 0.022328131850273738
Loss at iteration [2035]: 0.02229259099426668
Loss at iteration [2036]: 0.022305573729450155
***** Warning: Loss has increased *****
Loss at iteration [2037]: 0.02233348138930867
***** Warning: Loss has increased *****
Loss at iteration [2038]: 0.022351419593891236
***** Warning: Loss has increased *****
Loss at iteration [2039]: 0.022364441629566172
***** Warning: Loss has increased *****
Loss at iteration [2040]: 0.022433945529627658
***** Warning: Loss has increased *****
Loss at iteration [2041]: 0.02251788720265573
***** Warning: Loss has increased *****
Loss at iteration [2042]: 0.02266237157008115
***** Warning: Loss has increased *****
Loss at iteration [2043]: 0.022879503462347395
***** Warning: Loss has increased *****
Loss at iteration [2044]: 0.02325900606211004
***** Warning: Loss has increased *****
Loss at iteration [2045]: 0.023655834523733755
***** Warning: Loss has increased *****
Loss at iteration [2046]: 0.02430705559543932
***** Warning: Loss has increased *****
Loss at iteration [2047]: 0.02511176062163186
***** Warning: Loss has increased *****
Loss at iteration [2048]: 0.026498973471683653
***** Warning: Loss has increased *****
Loss at iteration [2049]: 0.02789710770226567
***** Warning: Loss has increased *****
Loss at iteration [2050]: 0.03035187118770557
***** Warning: Loss has increased *****
Loss at iteration [2051]: 0.032086113901384376
***** Warning: Loss has increased *****
Loss at iteration [2052]: 0.034452840914564235
***** Warning: Loss has increased *****
Loss at iteration [2053]: 0.03505144134209265
***** Warning: Loss has increased *****
Loss at iteration [2054]: 0.035713475292642315
***** Warning: Loss has increased *****
Loss at iteration [2055]: 0.033566982599512414
Loss at iteration [2056]: 0.031015463905026658
Loss at iteration [2057]: 0.026911599831196512
Loss at iteration [2058]: 0.023792842773905552
Loss at iteration [2059]: 0.02236631453529375
Loss at iteration [2060]: 0.022797174167165618
***** Warning: Loss has increased *****
Loss at iteration [2061]: 0.024350699087808576
***** Warning: Loss has increased *****
Loss at iteration [2062]: 0.02591516478924629
***** Warning: Loss has increased *****
Loss at iteration [2063]: 0.026820516877246528
***** Warning: Loss has increased *****
Loss at iteration [2064]: 0.026535536008107263
Loss at iteration [2065]: 0.025640393801866977
Loss at iteration [2066]: 0.024105925362977777
Loss at iteration [2067]: 0.02283858852846807
Loss at iteration [2068]: 0.022267678374442386
Loss at iteration [2069]: 0.022469467959630098
***** Warning: Loss has increased *****
Loss at iteration [2070]: 0.023151053767119872
***** Warning: Loss has increased *****
Loss at iteration [2071]: 0.02393653003644937
***** Warning: Loss has increased *****
Loss at iteration [2072]: 0.02456251803530442
***** Warning: Loss has increased *****
Loss at iteration [2073]: 0.024778748844580055
***** Warning: Loss has increased *****
Loss at iteration [2074]: 0.024668611179783605
Loss at iteration [2075]: 0.024191872306132803
Loss at iteration [2076]: 0.023531693844212814
Loss at iteration [2077]: 0.022811841570253986
Loss at iteration [2078]: 0.022326268876436087
Loss at iteration [2079]: 0.02208878641264229
Loss at iteration [2080]: 0.022147848510627306
***** Warning: Loss has increased *****
Loss at iteration [2081]: 0.02238369920807934
***** Warning: Loss has increased *****
Loss at iteration [2082]: 0.022676571591600872
***** Warning: Loss has increased *****
Loss at iteration [2083]: 0.023023143724185298
***** Warning: Loss has increased *****
Loss at iteration [2084]: 0.023368968903289194
***** Warning: Loss has increased *****
Loss at iteration [2085]: 0.02375760656346949
***** Warning: Loss has increased *****
Loss at iteration [2086]: 0.02404616570108419
***** Warning: Loss has increased *****
Loss at iteration [2087]: 0.024454330858845064
***** Warning: Loss has increased *****
Loss at iteration [2088]: 0.02475310277918297
***** Warning: Loss has increased *****
Loss at iteration [2089]: 0.02514352012419118
***** Warning: Loss has increased *****
Loss at iteration [2090]: 0.025411460310691858
***** Warning: Loss has increased *****
Loss at iteration [2091]: 0.0258342169774662
***** Warning: Loss has increased *****
Loss at iteration [2092]: 0.026010103830821238
***** Warning: Loss has increased *****
Loss at iteration [2093]: 0.02634244478186651
***** Warning: Loss has increased *****
Loss at iteration [2094]: 0.02624017389939137
Loss at iteration [2095]: 0.026656859769004403
***** Warning: Loss has increased *****
Loss at iteration [2096]: 0.0267606104811391
***** Warning: Loss has increased *****
Loss at iteration [2097]: 0.027182158789137777
***** Warning: Loss has increased *****
Loss at iteration [2098]: 0.027188022458424846
***** Warning: Loss has increased *****
Loss at iteration [2099]: 0.027564084810887707
***** Warning: Loss has increased *****
Loss at iteration [2100]: 0.02735422663374389
Loss at iteration [2101]: 0.027302416924839056
Loss at iteration [2102]: 0.02680369529751209
Loss at iteration [2103]: 0.026227673690952052
Loss at iteration [2104]: 0.025248235759743384
Loss at iteration [2105]: 0.02429011452395876
Loss at iteration [2106]: 0.023290215255161718
Loss at iteration [2107]: 0.022555385704614333
Loss at iteration [2108]: 0.022114776179125866
Loss at iteration [2109]: 0.02195241491069327
Loss at iteration [2110]: 0.02198550915371311
***** Warning: Loss has increased *****
Loss at iteration [2111]: 0.022188899107143528
***** Warning: Loss has increased *****
Loss at iteration [2112]: 0.022532227232318326
***** Warning: Loss has increased *****
Loss at iteration [2113]: 0.022870154634862916
***** Warning: Loss has increased *****
Loss at iteration [2114]: 0.023347774359752656
***** Warning: Loss has increased *****
Loss at iteration [2115]: 0.02390281735415907
***** Warning: Loss has increased *****
Loss at iteration [2116]: 0.024565393536729395
***** Warning: Loss has increased *****
Loss at iteration [2117]: 0.025217429875754663
***** Warning: Loss has increased *****
Loss at iteration [2118]: 0.026129903617320833
***** Warning: Loss has increased *****
Loss at iteration [2119]: 0.02694244514643894
***** Warning: Loss has increased *****
Loss at iteration [2120]: 0.02830380345508076
***** Warning: Loss has increased *****
Loss at iteration [2121]: 0.02956176969254806
***** Warning: Loss has increased *****
Loss at iteration [2122]: 0.03220059125047478
***** Warning: Loss has increased *****
Loss at iteration [2123]: 0.03461697147591569
***** Warning: Loss has increased *****
Loss at iteration [2124]: 0.039442084694665924
***** Warning: Loss has increased *****
Loss at iteration [2125]: 0.0442947871227355
***** Warning: Loss has increased *****
Loss at iteration [2126]: 0.053953722368669715
***** Warning: Loss has increased *****
Loss at iteration [2127]: 0.06019316207583718
***** Warning: Loss has increased *****
Loss at iteration [2128]: 0.07263820056806777
***** Warning: Loss has increased *****
Loss at iteration [2129]: 0.07212930102545619
Loss at iteration [2130]: 0.07057943723939644
Loss at iteration [2131]: 0.05075644019325783
Loss at iteration [2132]: 0.031773673503072385
Loss at iteration [2133]: 0.02283827236055934
Loss at iteration [2134]: 0.02854655523652057
***** Warning: Loss has increased *****
Loss at iteration [2135]: 0.04022862841660374
***** Warning: Loss has increased *****
Loss at iteration [2136]: 0.044543568926095754
***** Warning: Loss has increased *****
Loss at iteration [2137]: 0.03941677976038534
Loss at iteration [2138]: 0.02832493276706409
Loss at iteration [2139]: 0.022766573761642907
Loss at iteration [2140]: 0.02628491164646356
***** Warning: Loss has increased *****
Loss at iteration [2141]: 0.032567841611540665
***** Warning: Loss has increased *****
Loss at iteration [2142]: 0.03471036376053997
***** Warning: Loss has increased *****
Loss at iteration [2143]: 0.02979146508483685
Loss at iteration [2144]: 0.02416009613966112
Loss at iteration [2145]: 0.022619758358636097
Loss at iteration [2146]: 0.02548394661059726
***** Warning: Loss has increased *****
Loss at iteration [2147]: 0.029032295673767874
***** Warning: Loss has increased *****
Loss at iteration [2148]: 0.029327035105630537
***** Warning: Loss has increased *****
Loss at iteration [2149]: 0.027111917973202476
Loss at iteration [2150]: 0.023880001858617522
Loss at iteration [2151]: 0.02226117116400828
Loss at iteration [2152]: 0.02295906004848569
***** Warning: Loss has increased *****
Loss at iteration [2153]: 0.02480528150686439
***** Warning: Loss has increased *****
Loss at iteration [2154]: 0.026085271841928312
***** Warning: Loss has increased *****
Loss at iteration [2155]: 0.025581832913952607
Loss at iteration [2156]: 0.024191723013160006
Loss at iteration [2157]: 0.022664148665962836
Loss at iteration [2158]: 0.022064702176625404
Loss at iteration [2159]: 0.022518984010631404
***** Warning: Loss has increased *****
Loss at iteration [2160]: 0.023451853238026996
***** Warning: Loss has increased *****
Loss at iteration [2161]: 0.024220521474037617
***** Warning: Loss has increased *****
Loss at iteration [2162]: 0.02424931979403046
***** Warning: Loss has increased *****
Loss at iteration [2163]: 0.023984597902014133
Loss at iteration [2164]: 0.02332457676904425
Loss at iteration [2165]: 0.022625578768463928
Loss at iteration [2166]: 0.02208488378145845
Loss at iteration [2167]: 0.021906053753033575
Loss at iteration [2168]: 0.02204378710272598
***** Warning: Loss has increased *****
Loss at iteration [2169]: 0.022343369695441824
***** Warning: Loss has increased *****
Loss at iteration [2170]: 0.02261297167899755
***** Warning: Loss has increased *****
Loss at iteration [2171]: 0.02272865610449932
***** Warning: Loss has increased *****
Loss at iteration [2172]: 0.022697793928407414
Loss at iteration [2173]: 0.022498644860009867
Loss at iteration [2174]: 0.022268394986142873
Loss at iteration [2175]: 0.02209571098207823
Loss at iteration [2176]: 0.02196294749689917
Loss at iteration [2177]: 0.02184675503461002
Loss at iteration [2178]: 0.02176789677306056
Loss at iteration [2179]: 0.02171997976921893
Loss at iteration [2180]: 0.021733337818402965
***** Warning: Loss has increased *****
Loss at iteration [2181]: 0.02176270309085426
***** Warning: Loss has increased *****
Loss at iteration [2182]: 0.02182282623324919
***** Warning: Loss has increased *****
Loss at iteration [2183]: 0.021895029328783965
***** Warning: Loss has increased *****
Loss at iteration [2184]: 0.02201813797017304
***** Warning: Loss has increased *****
Loss at iteration [2185]: 0.02214137387668051
***** Warning: Loss has increased *****
Loss at iteration [2186]: 0.022258456049345836
***** Warning: Loss has increased *****
Loss at iteration [2187]: 0.022448962627393824
***** Warning: Loss has increased *****
Loss at iteration [2188]: 0.022669546801756518
***** Warning: Loss has increased *****
Loss at iteration [2189]: 0.02298330402883154
***** Warning: Loss has increased *****
Loss at iteration [2190]: 0.023278521469934633
***** Warning: Loss has increased *****
Loss at iteration [2191]: 0.02376645821783986
***** Warning: Loss has increased *****
Loss at iteration [2192]: 0.024253871056636237
***** Warning: Loss has increased *****
Loss at iteration [2193]: 0.024896329362099432
***** Warning: Loss has increased *****
Loss at iteration [2194]: 0.025362858345827348
***** Warning: Loss has increased *****
Loss at iteration [2195]: 0.026097253162542217
***** Warning: Loss has increased *****
Loss at iteration [2196]: 0.026669714708965703
***** Warning: Loss has increased *****
Loss at iteration [2197]: 0.02751002343024126
***** Warning: Loss has increased *****
Loss at iteration [2198]: 0.027849215641357745
***** Warning: Loss has increased *****
Loss at iteration [2199]: 0.02830082344538532
***** Warning: Loss has increased *****
Loss at iteration [2200]: 0.028030890556894988
Loss at iteration [2201]: 0.027650675531680748
Loss at iteration [2202]: 0.026648124384740783
Loss at iteration [2203]: 0.025657631954668234
Loss at iteration [2204]: 0.024496509258277503
Loss at iteration [2205]: 0.02335777113308928
Loss at iteration [2206]: 0.022445737431412024
Loss at iteration [2207]: 0.021864167803933875
Loss at iteration [2208]: 0.02163963944456122
Loss at iteration [2209]: 0.02173336631352011
***** Warning: Loss has increased *****
Loss at iteration [2210]: 0.022092046968883925
***** Warning: Loss has increased *****
Loss at iteration [2211]: 0.022535911841403793
***** Warning: Loss has increased *****
Loss at iteration [2212]: 0.023058353580787932
***** Warning: Loss has increased *****
Loss at iteration [2213]: 0.023703816803062768
***** Warning: Loss has increased *****
Loss at iteration [2214]: 0.024428625535260998
***** Warning: Loss has increased *****
Loss at iteration [2215]: 0.024967875504162882
***** Warning: Loss has increased *****
Loss at iteration [2216]: 0.025484610815249813
***** Warning: Loss has increased *****
Loss at iteration [2217]: 0.025619321724365037
***** Warning: Loss has increased *****
Loss at iteration [2218]: 0.025770698742974354
***** Warning: Loss has increased *****
Loss at iteration [2219]: 0.025504747483387113
Loss at iteration [2220]: 0.025335247753736512
Loss at iteration [2221]: 0.025001644175747156
Loss at iteration [2222]: 0.024636174019837982
Loss at iteration [2223]: 0.02412462732964195
Loss at iteration [2224]: 0.023648490249717685
Loss at iteration [2225]: 0.02314939544210816
Loss at iteration [2226]: 0.02270280867871
Loss at iteration [2227]: 0.022296483466466264
Loss at iteration [2228]: 0.022008934974516198
Loss at iteration [2229]: 0.021767540879044957
Loss at iteration [2230]: 0.021589546570354414
Loss at iteration [2231]: 0.021510095673755322
Loss at iteration [2232]: 0.021498745674682768
Loss at iteration [2233]: 0.021516519871275227
***** Warning: Loss has increased *****
Loss at iteration [2234]: 0.02157469420143646
***** Warning: Loss has increased *****
Loss at iteration [2235]: 0.021718044987171198
***** Warning: Loss has increased *****
Loss at iteration [2236]: 0.021878762820062375
***** Warning: Loss has increased *****
Loss at iteration [2237]: 0.02212697183215872
***** Warning: Loss has increased *****
Loss at iteration [2238]: 0.022519269090047384
***** Warning: Loss has increased *****
Loss at iteration [2239]: 0.02317395290547287
***** Warning: Loss has increased *****
Loss at iteration [2240]: 0.02405805402460943
***** Warning: Loss has increased *****
Loss at iteration [2241]: 0.02570081165374123
***** Warning: Loss has increased *****
Loss at iteration [2242]: 0.0280579604593375
***** Warning: Loss has increased *****
Loss at iteration [2243]: 0.03250206078803558
***** Warning: Loss has increased *****
Loss at iteration [2244]: 0.038381669058951594
***** Warning: Loss has increased *****
Loss at iteration [2245]: 0.05154405569151103
***** Warning: Loss has increased *****
Loss at iteration [2246]: 0.06847414051392864
***** Warning: Loss has increased *****
Loss at iteration [2247]: 0.1084808999446649
***** Warning: Loss has increased *****
Loss at iteration [2248]: 0.14472026910607905
***** Warning: Loss has increased *****
Loss at iteration [2249]: 0.23337327873599267
***** Warning: Loss has increased *****
Loss at iteration [2250]: 0.21675294294680392
Loss at iteration [2251]: 0.17602774336219298
Loss at iteration [2252]: 0.04851280971999067
Loss at iteration [2253]: 0.04829196745850877
Loss at iteration [2254]: 0.1289160772911314
***** Warning: Loss has increased *****
Loss at iteration [2255]: 0.09160655408532031
Loss at iteration [2256]: 0.029941672666396035
Loss at iteration [2257]: 0.06023599972367282
***** Warning: Loss has increased *****
Loss at iteration [2258]: 0.08562403460073577
***** Warning: Loss has increased *****
Loss at iteration [2259]: 0.046025233778113094
Loss at iteration [2260]: 0.03264398205540815
Loss at iteration [2261]: 0.06667399295483954
***** Warning: Loss has increased *****
Loss at iteration [2262]: 0.05983439947789777
Loss at iteration [2263]: 0.027676653587391065
Loss at iteration [2264]: 0.04518654532926465
***** Warning: Loss has increased *****
Loss at iteration [2265]: 0.05958544808572041
***** Warning: Loss has increased *****
Loss at iteration [2266]: 0.032939101411334056
Loss at iteration [2267]: 0.03049303807377063
Loss at iteration [2268]: 0.04866810844990988
***** Warning: Loss has increased *****
Loss at iteration [2269]: 0.036792049572944414
Loss at iteration [2270]: 0.025121783195537217
Loss at iteration [2271]: 0.03630431689549134
***** Warning: Loss has increased *****
Loss at iteration [2272]: 0.03731739041959382
***** Warning: Loss has increased *****
Loss at iteration [2273]: 0.02607669361713948
Loss at iteration [2274]: 0.02722734396565416
***** Warning: Loss has increased *****
Loss at iteration [2275]: 0.03434842725768658
***** Warning: Loss has increased *****
Loss at iteration [2276]: 0.02985486131708409
Loss at iteration [2277]: 0.023641496212979365
Loss at iteration [2278]: 0.028046827499616327
***** Warning: Loss has increased *****
Loss at iteration [2279]: 0.031628962905536205
***** Warning: Loss has increased *****
Loss at iteration [2280]: 0.02626459218894033
Loss at iteration [2281]: 0.022984377204483954
Loss at iteration [2282]: 0.026415529946077265
***** Warning: Loss has increased *****
Loss at iteration [2283]: 0.028371432846634716
***** Warning: Loss has increased *****
Loss at iteration [2284]: 0.025591285246754275
Loss at iteration [2285]: 0.022600814858097756
Loss at iteration [2286]: 0.02413613624835663
***** Warning: Loss has increased *****
Loss at iteration [2287]: 0.026599576621659127
***** Warning: Loss has increased *****
Loss at iteration [2288]: 0.025374154568675766
Loss at iteration [2289]: 0.022812156059175824
Loss at iteration [2290]: 0.022531023832666575
Loss at iteration [2291]: 0.024166309374883867
***** Warning: Loss has increased *****
Loss at iteration [2292]: 0.024975145930707106
***** Warning: Loss has increased *****
Loss at iteration [2293]: 0.023584915439468043
Loss at iteration [2294]: 0.022186452833432263
Loss at iteration [2295]: 0.022307734860850966
***** Warning: Loss has increased *****
Loss at iteration [2296]: 0.023296715486628233
***** Warning: Loss has increased *****
Loss at iteration [2297]: 0.023577877458736744
***** Warning: Loss has increased *****
Loss at iteration [2298]: 0.022773553217715896
Loss at iteration [2299]: 0.021964212583651617
Loss at iteration [2300]: 0.022009604207866306
***** Warning: Loss has increased *****
Loss at iteration [2301]: 0.02259545714161483
***** Warning: Loss has increased *****
Loss at iteration [2302]: 0.022991349424016195
***** Warning: Loss has increased *****
Loss at iteration [2303]: 0.022636174889174604
Loss at iteration [2304]: 0.022033128647361202
Loss at iteration [2305]: 0.021757600865986427
Loss at iteration [2306]: 0.02201660025032359
***** Warning: Loss has increased *****
Loss at iteration [2307]: 0.022377182311237323
***** Warning: Loss has increased *****
Loss at iteration [2308]: 0.022397239690492683
***** Warning: Loss has increased *****
Loss at iteration [2309]: 0.02212240285319938
Loss at iteration [2310]: 0.021806179184648035
Loss at iteration [2311]: 0.021711866772323028
Loss at iteration [2312]: 0.02188122126474227
***** Warning: Loss has increased *****
Loss at iteration [2313]: 0.022064004839650105
***** Warning: Loss has increased *****
Loss at iteration [2314]: 0.02203702795292829
Loss at iteration [2315]: 0.021843054621400085
Loss at iteration [2316]: 0.021663345854080255
Loss at iteration [2317]: 0.02166207122654276
Loss at iteration [2318]: 0.021775658133090046
***** Warning: Loss has increased *****
Loss at iteration [2319]: 0.021856810537773497
***** Warning: Loss has increased *****
Loss at iteration [2320]: 0.021853506149300088
Loss at iteration [2321]: 0.021741198156162454
Loss at iteration [2322]: 0.021609087394177134
Loss at iteration [2323]: 0.02156568981269591
Loss at iteration [2324]: 0.02160634502406283
***** Warning: Loss has increased *****
Loss at iteration [2325]: 0.02166095576316367
***** Warning: Loss has increased *****
Loss at iteration [2326]: 0.0216684211905904
***** Warning: Loss has increased *****
Loss at iteration [2327]: 0.021646364911578377
Loss at iteration [2328]: 0.021594838095594017
Loss at iteration [2329]: 0.0215073345493679
Loss at iteration [2330]: 0.0214529309463386
Loss at iteration [2331]: 0.02145924352362816
***** Warning: Loss has increased *****
Loss at iteration [2332]: 0.021474613989192325
***** Warning: Loss has increased *****
Loss at iteration [2333]: 0.021482440530371315
***** Warning: Loss has increased *****
Loss at iteration [2334]: 0.021501431878324775
***** Warning: Loss has increased *****
Loss at iteration [2335]: 0.02149150466090209
Loss at iteration [2336]: 0.021458549024993768
Loss at iteration [2337]: 0.021453788458424942
Loss at iteration [2338]: 0.021441317100456474
Loss at iteration [2339]: 0.02141683659305419
Loss at iteration [2340]: 0.02139658239953627
Loss at iteration [2341]: 0.02140969292803323
***** Warning: Loss has increased *****
Loss at iteration [2342]: 0.021431542092793562
***** Warning: Loss has increased *****
Loss at iteration [2343]: 0.021419328286699235
Loss at iteration [2344]: 0.02142991431076664
***** Warning: Loss has increased *****
Loss at iteration [2345]: 0.021434131474516793
***** Warning: Loss has increased *****
Loss at iteration [2346]: 0.02140765732806899
Loss at iteration [2347]: 0.02137294584488383
Loss at iteration [2348]: 0.02135112331289844
Loss at iteration [2349]: 0.02133761485358232
Loss at iteration [2350]: 0.021323465092344163
Loss at iteration [2351]: 0.021315960857657665
Loss at iteration [2352]: 0.021323948118922152
***** Warning: Loss has increased *****
Loss at iteration [2353]: 0.021320895832353404
Loss at iteration [2354]: 0.021334267862507778
***** Warning: Loss has increased *****
Loss at iteration [2355]: 0.021328688893760474
Loss at iteration [2356]: 0.021315326778815025
Loss at iteration [2357]: 0.02130903118484596
Loss at iteration [2358]: 0.021337033686390647
***** Warning: Loss has increased *****
Loss at iteration [2359]: 0.021338601192161213
***** Warning: Loss has increased *****
Loss at iteration [2360]: 0.021311215225713016
Loss at iteration [2361]: 0.021302715904253002
Loss at iteration [2362]: 0.021313814341186565
***** Warning: Loss has increased *****
Loss at iteration [2363]: 0.021309800298137886
Loss at iteration [2364]: 0.02128343646429034
Loss at iteration [2365]: 0.021280060895022393
Loss at iteration [2366]: 0.021272428488498536
Loss at iteration [2367]: 0.02124534966338686
Loss at iteration [2368]: 0.021256827781481885
***** Warning: Loss has increased *****
Loss at iteration [2369]: 0.021261983928073977
***** Warning: Loss has increased *****
Loss at iteration [2370]: 0.021255217207169323
Loss at iteration [2371]: 0.021245136335551938
Loss at iteration [2372]: 0.021224710629927847
Loss at iteration [2373]: 0.02122690253113428
***** Warning: Loss has increased *****
Loss at iteration [2374]: 0.02123027976111409
***** Warning: Loss has increased *****
Loss at iteration [2375]: 0.0212321672941831
***** Warning: Loss has increased *****
Loss at iteration [2376]: 0.021231832786064223
Loss at iteration [2377]: 0.02122731988813543
Loss at iteration [2378]: 0.021249620540251247
***** Warning: Loss has increased *****
Loss at iteration [2379]: 0.021255136136199036
***** Warning: Loss has increased *****
Loss at iteration [2380]: 0.02126418199288797
***** Warning: Loss has increased *****
Loss at iteration [2381]: 0.02127239434679529
***** Warning: Loss has increased *****
Loss at iteration [2382]: 0.02132390088475211
***** Warning: Loss has increased *****
Loss at iteration [2383]: 0.021344391866626646
***** Warning: Loss has increased *****
Loss at iteration [2384]: 0.021351193430182984
***** Warning: Loss has increased *****
Loss at iteration [2385]: 0.021354208441560708
***** Warning: Loss has increased *****
Loss at iteration [2386]: 0.021359605609690937
***** Warning: Loss has increased *****
Loss at iteration [2387]: 0.021354814224351648
Loss at iteration [2388]: 0.02135848970125921
***** Warning: Loss has increased *****
Loss at iteration [2389]: 0.02137098737353531
***** Warning: Loss has increased *****
Loss at iteration [2390]: 0.021394660897734605
***** Warning: Loss has increased *****
Loss at iteration [2391]: 0.02139164120094891
Loss at iteration [2392]: 0.021400729468598848
***** Warning: Loss has increased *****
Loss at iteration [2393]: 0.02139616023940711
Loss at iteration [2394]: 0.021410891771673473
***** Warning: Loss has increased *****
Loss at iteration [2395]: 0.021429298806247096
***** Warning: Loss has increased *****
Loss at iteration [2396]: 0.021468870075808055
***** Warning: Loss has increased *****
Loss at iteration [2397]: 0.0214793543962643
***** Warning: Loss has increased *****
Loss at iteration [2398]: 0.021583786841362605
***** Warning: Loss has increased *****
Loss at iteration [2399]: 0.021667714464217214
***** Warning: Loss has increased *****
Loss at iteration [2400]: 0.021761800169391526
***** Warning: Loss has increased *****
Loss at iteration [2401]: 0.021859470658399667
***** Warning: Loss has increased *****
Loss at iteration [2402]: 0.022023580729115783
***** Warning: Loss has increased *****
Loss at iteration [2403]: 0.022133908420557815
***** Warning: Loss has increased *****
Loss at iteration [2404]: 0.02234861450050034
***** Warning: Loss has increased *****
Loss at iteration [2405]: 0.022574329179489525
***** Warning: Loss has increased *****
Loss at iteration [2406]: 0.022846104747240727
***** Warning: Loss has increased *****
Loss at iteration [2407]: 0.023073082598337305
***** Warning: Loss has increased *****
Loss at iteration [2408]: 0.023502121692251333
***** Warning: Loss has increased *****
Loss at iteration [2409]: 0.02393476842163827
***** Warning: Loss has increased *****
Loss at iteration [2410]: 0.024553961703069685
***** Warning: Loss has increased *****
Loss at iteration [2411]: 0.024999063184793405
***** Warning: Loss has increased *****
Loss at iteration [2412]: 0.02550508632228057
***** Warning: Loss has increased *****
Loss at iteration [2413]: 0.025560149169336722
***** Warning: Loss has increased *****
Loss at iteration [2414]: 0.025723650422738484
***** Warning: Loss has increased *****
Loss at iteration [2415]: 0.02536591117411063
Loss at iteration [2416]: 0.025067548516320134
Loss at iteration [2417]: 0.024418813948100814
Loss at iteration [2418]: 0.023775980579192207
Loss at iteration [2419]: 0.022992126430411926
Loss at iteration [2420]: 0.022225607651771694
Loss at iteration [2421]: 0.0215292196900691
Loss at iteration [2422]: 0.021168535346223608
Loss at iteration [2423]: 0.021083104041641734
Loss at iteration [2424]: 0.02118574046456328
***** Warning: Loss has increased *****
Loss at iteration [2425]: 0.021459859719697938
***** Warning: Loss has increased *****
Loss at iteration [2426]: 0.021843315319915447
***** Warning: Loss has increased *****
Loss at iteration [2427]: 0.022399686476097
***** Warning: Loss has increased *****
Loss at iteration [2428]: 0.023046560778065443
***** Warning: Loss has increased *****
Loss at iteration [2429]: 0.023793309684011457
***** Warning: Loss has increased *****
Loss at iteration [2430]: 0.024345886310366226
***** Warning: Loss has increased *****
Loss at iteration [2431]: 0.024949176851308943
***** Warning: Loss has increased *****
Loss at iteration [2432]: 0.02510736684676626
***** Warning: Loss has increased *****
Loss at iteration [2433]: 0.025166054375177258
***** Warning: Loss has increased *****
Loss at iteration [2434]: 0.024745873711152555
Loss at iteration [2435]: 0.024329718373479952
Loss at iteration [2436]: 0.023616362605249068
Loss at iteration [2437]: 0.022947756344346683
Loss at iteration [2438]: 0.022243226180639744
Loss at iteration [2439]: 0.021759515471214564
Loss at iteration [2440]: 0.021365697668060752
Loss at iteration [2441]: 0.0211267693594012
Loss at iteration [2442]: 0.021013999701262637
Loss at iteration [2443]: 0.021023892534763187
***** Warning: Loss has increased *****
Loss at iteration [2444]: 0.02114234820761587
***** Warning: Loss has increased *****
Loss at iteration [2445]: 0.021323680863831337
***** Warning: Loss has increased *****
Loss at iteration [2446]: 0.02163323919737071
***** Warning: Loss has increased *****
Loss at iteration [2447]: 0.021998262340638418
***** Warning: Loss has increased *****
Loss at iteration [2448]: 0.02249057002383039
***** Warning: Loss has increased *****
Loss at iteration [2449]: 0.023073122911992934
***** Warning: Loss has increased *****
Loss at iteration [2450]: 0.023930271713435435
***** Warning: Loss has increased *****
Loss at iteration [2451]: 0.024710949366979654
***** Warning: Loss has increased *****
Loss at iteration [2452]: 0.025887458535460733
***** Warning: Loss has increased *****
Loss at iteration [2453]: 0.027049571039136678
***** Warning: Loss has increased *****
Loss at iteration [2454]: 0.02931310293081211
***** Warning: Loss has increased *****
Loss at iteration [2455]: 0.031412232528928746
***** Warning: Loss has increased *****
Loss at iteration [2456]: 0.03534480728707841
***** Warning: Loss has increased *****
Loss at iteration [2457]: 0.03836650911415561
***** Warning: Loss has increased *****
Loss at iteration [2458]: 0.04392761891277804
***** Warning: Loss has increased *****
Loss at iteration [2459]: 0.045678037779333144
***** Warning: Loss has increased *****
Loss at iteration [2460]: 0.04929106034865438
***** Warning: Loss has increased *****
Loss at iteration [2461]: 0.0447347324774509
Loss at iteration [2462]: 0.03930651959436948
Loss at iteration [2463]: 0.029544842211003095
Loss at iteration [2464]: 0.022720384473489477
Loss at iteration [2465]: 0.021739068880053804
Loss at iteration [2466]: 0.025726889959495354
***** Warning: Loss has increased *****
Loss at iteration [2467]: 0.03084821663405815
***** Warning: Loss has increased *****
Loss at iteration [2468]: 0.03255614291236257
***** Warning: Loss has increased *****
Loss at iteration [2469]: 0.03121554273520581
Loss at iteration [2470]: 0.02620318551428229
Loss at iteration [2471]: 0.022183913755296016
Loss at iteration [2472]: 0.021387322688726604
Loss at iteration [2473]: 0.02351559780063462
***** Warning: Loss has increased *****
Loss at iteration [2474]: 0.026234649232017335
***** Warning: Loss has increased *****
Loss at iteration [2475]: 0.026954533168065644
***** Warning: Loss has increased *****
Loss at iteration [2476]: 0.02611293888523934
Loss at iteration [2477]: 0.023783643002435066
Loss at iteration [2478]: 0.02182473986995656
Loss at iteration [2479]: 0.021056659108028366
Loss at iteration [2480]: 0.021558088200398437
***** Warning: Loss has increased *****
Loss at iteration [2481]: 0.022834645004042996
***** Warning: Loss has increased *****
Loss at iteration [2482]: 0.02398622392224607
***** Warning: Loss has increased *****
Loss at iteration [2483]: 0.02447320847086826
***** Warning: Loss has increased *****
Loss at iteration [2484]: 0.02406013421528315
Loss at iteration [2485]: 0.023015961635304942
Loss at iteration [2486]: 0.0217910150568073
Loss at iteration [2487]: 0.021069341160858325
Loss at iteration [2488]: 0.021034714267715488
Loss at iteration [2489]: 0.021552424002692114
***** Warning: Loss has increased *****
Loss at iteration [2490]: 0.02227881755792053
***** Warning: Loss has increased *****
Loss at iteration [2491]: 0.022812184809647577
***** Warning: Loss has increased *****
Loss at iteration [2492]: 0.023057978566407622
***** Warning: Loss has increased *****
Loss at iteration [2493]: 0.022932898519519445
Loss at iteration [2494]: 0.022512140595359713
Loss at iteration [2495]: 0.021886379624826653
Loss at iteration [2496]: 0.02132771353947167
Loss at iteration [2497]: 0.02095605729839225
Loss at iteration [2498]: 0.02087191651407051
Loss at iteration [2499]: 0.020990941814127147
***** Warning: Loss has increased *****
Loss at iteration [2500]: 0.021229495703696335
***** Warning: Loss has increased *****
Loss at iteration [2501]: 0.021497765739949805
***** Warning: Loss has increased *****
Loss at iteration [2502]: 0.021682814081859514
***** Warning: Loss has increased *****
Loss at iteration [2503]: 0.021829813334026938
***** Warning: Loss has increased *****
Loss at iteration [2504]: 0.021783044722400773
Loss at iteration [2505]: 0.021674545172848395
Loss at iteration [2506]: 0.021518812153064712
Loss at iteration [2507]: 0.021316739586665474
Loss at iteration [2508]: 0.021099467102313956
Loss at iteration [2509]: 0.02098986972633266
Loss at iteration [2510]: 0.020887154561790418
Loss at iteration [2511]: 0.020816796756315623
Loss at iteration [2512]: 0.020811916414208573
Loss at iteration [2513]: 0.020874455812426848
***** Warning: Loss has increased *****
Loss at iteration [2514]: 0.0209543722938912
***** Warning: Loss has increased *****
Loss at iteration [2515]: 0.02101814477913151
***** Warning: Loss has increased *****
Loss at iteration [2516]: 0.021099885102293155
***** Warning: Loss has increased *****
Loss at iteration [2517]: 0.02118628252985932
***** Warning: Loss has increased *****
Loss at iteration [2518]: 0.021273170583210092
***** Warning: Loss has increased *****
Loss at iteration [2519]: 0.021368092779799876
***** Warning: Loss has increased *****
Loss at iteration [2520]: 0.021524740747732626
***** Warning: Loss has increased *****
Loss at iteration [2521]: 0.021676735216494342
***** Warning: Loss has increased *****
Loss at iteration [2522]: 0.02190919930156685
***** Warning: Loss has increased *****
Loss at iteration [2523]: 0.02213807762797065
***** Warning: Loss has increased *****
Loss at iteration [2524]: 0.0225201446953288
***** Warning: Loss has increased *****
Loss at iteration [2525]: 0.022766376990912607
***** Warning: Loss has increased *****
Loss at iteration [2526]: 0.02302412156226161
***** Warning: Loss has increased *****
Loss at iteration [2527]: 0.023174794814402978
***** Warning: Loss has increased *****
Loss at iteration [2528]: 0.023374238453372317
***** Warning: Loss has increased *****
Loss at iteration [2529]: 0.02341761105948941
***** Warning: Loss has increased *****
Loss at iteration [2530]: 0.02356351500012667
***** Warning: Loss has increased *****
Loss at iteration [2531]: 0.023551446649157263
Loss at iteration [2532]: 0.023607021625814937
***** Warning: Loss has increased *****
Loss at iteration [2533]: 0.023490022018698455
Loss at iteration [2534]: 0.023498553237673175
***** Warning: Loss has increased *****
Loss at iteration [2535]: 0.02343927492996873
Loss at iteration [2536]: 0.02336093741272401
Loss at iteration [2537]: 0.023056753414631294
Loss at iteration [2538]: 0.022771199604336444
Loss at iteration [2539]: 0.02232034894984885
Loss at iteration [2540]: 0.021912958209248802
Loss at iteration [2541]: 0.021506615903870464
Loss at iteration [2542]: 0.021204691899181723
Loss at iteration [2543]: 0.020956082861210312
Loss at iteration [2544]: 0.02078332551742351
Loss at iteration [2545]: 0.020710938535064286
Loss at iteration [2546]: 0.020729711884337712
***** Warning: Loss has increased *****
Loss at iteration [2547]: 0.020794833845690742
***** Warning: Loss has increased *****
Loss at iteration [2548]: 0.020905378834845032
***** Warning: Loss has increased *****
Loss at iteration [2549]: 0.0210818994865394
***** Warning: Loss has increased *****
Loss at iteration [2550]: 0.0213284551156159
***** Warning: Loss has increased *****
Loss at iteration [2551]: 0.021697574319110124
***** Warning: Loss has increased *****
Loss at iteration [2552]: 0.022251512026600105
***** Warning: Loss has increased *****
Loss at iteration [2553]: 0.02314897328923567
***** Warning: Loss has increased *****
Loss at iteration [2554]: 0.02437954710852151
***** Warning: Loss has increased *****
Loss at iteration [2555]: 0.0265030873825842
***** Warning: Loss has increased *****
Loss at iteration [2556]: 0.029155477164157152
***** Warning: Loss has increased *****
Loss at iteration [2557]: 0.034496252497642176
***** Warning: Loss has increased *****
Loss at iteration [2558]: 0.0408943180573968
***** Warning: Loss has increased *****
Loss at iteration [2559]: 0.05428288002070649
***** Warning: Loss has increased *****
Loss at iteration [2560]: 0.06556459340282168
***** Warning: Loss has increased *****
Loss at iteration [2561]: 0.09334720640049349
***** Warning: Loss has increased *****
Loss at iteration [2562]: 0.10827421896706355
***** Warning: Loss has increased *****
Loss at iteration [2563]: 0.14547861083204153
***** Warning: Loss has increased *****
Loss at iteration [2564]: 0.10858255957166729
Loss at iteration [2565]: 0.06319462339760516
Loss at iteration [2566]: 0.024832369307189607
Loss at iteration [2567]: 0.04464675649054968
***** Warning: Loss has increased *****
Loss at iteration [2568]: 0.07919648551185052
***** Warning: Loss has increased *****
Loss at iteration [2569]: 0.06217939262841678
Loss at iteration [2570]: 0.029635503449606127
Loss at iteration [2571]: 0.029797627333284806
***** Warning: Loss has increased *****
Loss at iteration [2572]: 0.05219967182342608
***** Warning: Loss has increased *****
Loss at iteration [2573]: 0.05167573550647951
Loss at iteration [2574]: 0.027726067727123282
Loss at iteration [2575]: 0.02794387323641227
***** Warning: Loss has increased *****
Loss at iteration [2576]: 0.044603259396517964
***** Warning: Loss has increased *****
Loss at iteration [2577]: 0.04010213173519531
Loss at iteration [2578]: 0.025616273160281765
Loss at iteration [2579]: 0.02526186743252625
Loss at iteration [2580]: 0.035088027421949115
***** Warning: Loss has increased *****
Loss at iteration [2581]: 0.03486848894240027
Loss at iteration [2582]: 0.02465306242920614
Loss at iteration [2583]: 0.023673275438083768
Loss at iteration [2584]: 0.030808349959654167
***** Warning: Loss has increased *****
Loss at iteration [2585]: 0.030826071118520086
***** Warning: Loss has increased *****
Loss at iteration [2586]: 0.024835193645796748
Loss at iteration [2587]: 0.021930018265284273
Loss at iteration [2588]: 0.025553083385171586
***** Warning: Loss has increased *****
Loss at iteration [2589]: 0.028801441098032694
***** Warning: Loss has increased *****
Loss at iteration [2590]: 0.025868730195088535
Loss at iteration [2591]: 0.02205436316506689
Loss at iteration [2592]: 0.02209840897742117
***** Warning: Loss has increased *****
Loss at iteration [2593]: 0.02484178319773928
***** Warning: Loss has increased *****
Loss at iteration [2594]: 0.026048881672244055
***** Warning: Loss has increased *****
Loss at iteration [2595]: 0.023978522888378314
Loss at iteration [2596]: 0.021603221635426433
Loss at iteration [2597]: 0.02142866705472034
Loss at iteration [2598]: 0.02297102144553849
***** Warning: Loss has increased *****
Loss at iteration [2599]: 0.02426858633398243
***** Warning: Loss has increased *****
Loss at iteration [2600]: 0.02364947780938389
Loss at iteration [2601]: 0.02217724729482405
Loss at iteration [2602]: 0.02110218207645725
Loss at iteration [2603]: 0.021272124796178352
***** Warning: Loss has increased *****
Loss at iteration [2604]: 0.022134740601057107
***** Warning: Loss has increased *****
Loss at iteration [2605]: 0.02255832436609862
***** Warning: Loss has increased *****
Loss at iteration [2606]: 0.022234719792704822
Loss at iteration [2607]: 0.02143620443464408
Loss at iteration [2608]: 0.020931983633932436
Loss at iteration [2609]: 0.02104100485611591
***** Warning: Loss has increased *****
Loss at iteration [2610]: 0.02154212030287452
***** Warning: Loss has increased *****
Loss at iteration [2611]: 0.021900090984572584
***** Warning: Loss has increased *****
Loss at iteration [2612]: 0.021737723555226695
Loss at iteration [2613]: 0.02134097102123406
Loss at iteration [2614]: 0.02095728754475121
Loss at iteration [2615]: 0.020833769847907097
Loss at iteration [2616]: 0.020969505122664957
***** Warning: Loss has increased *****
Loss at iteration [2617]: 0.021160233705130874
***** Warning: Loss has increased *****
Loss at iteration [2618]: 0.02125092787633109
***** Warning: Loss has increased *****
Loss at iteration [2619]: 0.021187199121273915
Loss at iteration [2620]: 0.02103060390203694
Loss at iteration [2621]: 0.0208454667692949
Loss at iteration [2622]: 0.020762850514111583
Loss at iteration [2623]: 0.020819065340428065
***** Warning: Loss has increased *****
Loss at iteration [2624]: 0.0209013790827073
***** Warning: Loss has increased *****
Loss at iteration [2625]: 0.02100489023458396
***** Warning: Loss has increased *****
Loss at iteration [2626]: 0.021043143096808343
***** Warning: Loss has increased *****
Loss at iteration [2627]: 0.02099629997131389
Loss at iteration [2628]: 0.020874772039914433
Loss at iteration [2629]: 0.02075855289919376
Loss at iteration [2630]: 0.020743781028747188
Loss at iteration [2631]: 0.02074016013373544
Loss at iteration [2632]: 0.020743016608483864
***** Warning: Loss has increased *****
Loss at iteration [2633]: 0.02079376063732181
***** Warning: Loss has increased *****
Loss at iteration [2634]: 0.020844590142333645
***** Warning: Loss has increased *****
Loss at iteration [2635]: 0.020850549081611006
***** Warning: Loss has increased *****
Loss at iteration [2636]: 0.020817873535110307
Loss at iteration [2637]: 0.020758111138596477
Loss at iteration [2638]: 0.020711630022643993
Loss at iteration [2639]: 0.020666462043336824
Loss at iteration [2640]: 0.02065322830771825
Loss at iteration [2641]: 0.0206434998511118
Loss at iteration [2642]: 0.02064173847387181
Loss at iteration [2643]: 0.02064456739892042
***** Warning: Loss has increased *****
Loss at iteration [2644]: 0.020677153935554714
***** Warning: Loss has increased *****
Loss at iteration [2645]: 0.02071423530470052
***** Warning: Loss has increased *****
Loss at iteration [2646]: 0.020734415386868366
***** Warning: Loss has increased *****
Loss at iteration [2647]: 0.02074278561114496
***** Warning: Loss has increased *****
Loss at iteration [2648]: 0.020715056930387866
Loss at iteration [2649]: 0.02067522118995252
Loss at iteration [2650]: 0.02066386870448368
Loss at iteration [2651]: 0.020629503747511234
Loss at iteration [2652]: 0.020597264057407267
Loss at iteration [2653]: 0.020590310051477885
Loss at iteration [2654]: 0.020588503284610093
Loss at iteration [2655]: 0.02057893877760611
Loss at iteration [2656]: 0.02061847195211793
***** Warning: Loss has increased *****
Loss at iteration [2657]: 0.020630991995057787
***** Warning: Loss has increased *****
Loss at iteration [2658]: 0.02061852375547396
Loss at iteration [2659]: 0.020633358523055798
***** Warning: Loss has increased *****
Loss at iteration [2660]: 0.020670527698472857
***** Warning: Loss has increased *****
Loss at iteration [2661]: 0.020695786057165287
***** Warning: Loss has increased *****
Loss at iteration [2662]: 0.020721611466504072
***** Warning: Loss has increased *****
Loss at iteration [2663]: 0.02076927015161174
***** Warning: Loss has increased *****
Loss at iteration [2664]: 0.020873033280676682
***** Warning: Loss has increased *****
Loss at iteration [2665]: 0.0209588803964419
***** Warning: Loss has increased *****
Loss at iteration [2666]: 0.021029899184071162
***** Warning: Loss has increased *****
Loss at iteration [2667]: 0.021057575699239387
***** Warning: Loss has increased *****
Loss at iteration [2668]: 0.02109824045966709
***** Warning: Loss has increased *****
Loss at iteration [2669]: 0.02109998413971944
***** Warning: Loss has increased *****
Loss at iteration [2670]: 0.02111614428444846
***** Warning: Loss has increased *****
Loss at iteration [2671]: 0.021113703879155975
Loss at iteration [2672]: 0.021134762456992504
***** Warning: Loss has increased *****
Loss at iteration [2673]: 0.021224377806049577
***** Warning: Loss has increased *****
Loss at iteration [2674]: 0.0212899263997561
***** Warning: Loss has increased *****
Loss at iteration [2675]: 0.021279466491712935
Loss at iteration [2676]: 0.021257414025608216
Loss at iteration [2677]: 0.02121903542429889
Loss at iteration [2678]: 0.02124022549163385
***** Warning: Loss has increased *****
Loss at iteration [2679]: 0.021296023421316312
***** Warning: Loss has increased *****
Loss at iteration [2680]: 0.02137318188087966
***** Warning: Loss has increased *****
Loss at iteration [2681]: 0.021394101424224876
***** Warning: Loss has increased *****
Loss at iteration [2682]: 0.021429256942834204
***** Warning: Loss has increased *****
Loss at iteration [2683]: 0.021439112186736883
***** Warning: Loss has increased *****
Loss at iteration [2684]: 0.021476482216053444
***** Warning: Loss has increased *****
Loss at iteration [2685]: 0.02147832658039571
***** Warning: Loss has increased *****
Loss at iteration [2686]: 0.02152426762388592
***** Warning: Loss has increased *****
Loss at iteration [2687]: 0.021628036787438947
***** Warning: Loss has increased *****
Loss at iteration [2688]: 0.02173747376985888
***** Warning: Loss has increased *****
Loss at iteration [2689]: 0.021757795648632013
***** Warning: Loss has increased *****
Loss at iteration [2690]: 0.021768638471477184
***** Warning: Loss has increased *****
Loss at iteration [2691]: 0.021746204508164165
Loss at iteration [2692]: 0.021806018162597737
***** Warning: Loss has increased *****
Loss at iteration [2693]: 0.021771612184814184
Loss at iteration [2694]: 0.021799091187402805
***** Warning: Loss has increased *****
Loss at iteration [2695]: 0.02176181442032133
Loss at iteration [2696]: 0.021768626640335322
***** Warning: Loss has increased *****
Loss at iteration [2697]: 0.02167969048297979
Loss at iteration [2698]: 0.021548927426930365
Loss at iteration [2699]: 0.021434145350817623
Loss at iteration [2700]: 0.021333896642732927
Loss at iteration [2701]: 0.021168764737972648
Loss at iteration [2702]: 0.020999898595966557
Loss at iteration [2703]: 0.020859180622568436
Loss at iteration [2704]: 0.020752945132236484
Loss at iteration [2705]: 0.02065587762930154
Loss at iteration [2706]: 0.02057563163880036
Loss at iteration [2707]: 0.020509824806383338
Loss at iteration [2708]: 0.02046842432358386
Loss at iteration [2709]: 0.0204413544816974
Loss at iteration [2710]: 0.0203999435693442
Loss at iteration [2711]: 0.020379225375324543
Loss at iteration [2712]: 0.02035893844654107
Loss at iteration [2713]: 0.02039097586044258
***** Warning: Loss has increased *****
Loss at iteration [2714]: 0.02039881383440783
***** Warning: Loss has increased *****
Loss at iteration [2715]: 0.020396578172911673
Loss at iteration [2716]: 0.020407792315972107
***** Warning: Loss has increased *****
Loss at iteration [2717]: 0.020449751648406365
***** Warning: Loss has increased *****
Loss at iteration [2718]: 0.020544238462171988
***** Warning: Loss has increased *****
Loss at iteration [2719]: 0.020685119293953457
***** Warning: Loss has increased *****
Loss at iteration [2720]: 0.020925754737280422
***** Warning: Loss has increased *****
Loss at iteration [2721]: 0.021395165026774604
***** Warning: Loss has increased *****
Loss at iteration [2722]: 0.022176572063136488
***** Warning: Loss has increased *****
Loss at iteration [2723]: 0.02376269071590299
***** Warning: Loss has increased *****
Loss at iteration [2724]: 0.026437454230771883
***** Warning: Loss has increased *****
Loss at iteration [2725]: 0.031738875242834946
***** Warning: Loss has increased *****
Loss at iteration [2726]: 0.03956805119060944
***** Warning: Loss has increased *****
Loss at iteration [2727]: 0.05793544706245579
***** Warning: Loss has increased *****
Loss at iteration [2728]: 0.0782911535840844
***** Warning: Loss has increased *****
Loss at iteration [2729]: 0.1264155913285442
***** Warning: Loss has increased *****
Loss at iteration [2730]: 0.14500107136057233
***** Warning: Loss has increased *****
Loss at iteration [2731]: 0.19422215908293186
***** Warning: Loss has increased *****
Loss at iteration [2732]: 0.128109839682374
Loss at iteration [2733]: 0.053598029917966715
Loss at iteration [2734]: 0.02940660737652141
Loss at iteration [2735]: 0.07800188682276887
***** Warning: Loss has increased *****
Loss at iteration [2736]: 0.09366250254480402
***** Warning: Loss has increased *****
Loss at iteration [2737]: 0.039336821214439745
Loss at iteration [2738]: 0.03407643647825234
Loss at iteration [2739]: 0.0714831495327832
***** Warning: Loss has increased *****
Loss at iteration [2740]: 0.05440482611167981
Loss at iteration [2741]: 0.02573296692329873
Loss at iteration [2742]: 0.040361587508826804
***** Warning: Loss has increased *****
Loss at iteration [2743]: 0.053485406253854964
***** Warning: Loss has increased *****
Loss at iteration [2744]: 0.036144799017577625
Loss at iteration [2745]: 0.02477452282214274
Loss at iteration [2746]: 0.039661378056430545
***** Warning: Loss has increased *****
Loss at iteration [2747]: 0.0445494304371303
***** Warning: Loss has increased *****
Loss at iteration [2748]: 0.027396996891866823
Loss at iteration [2749]: 0.02533822472863776
Loss at iteration [2750]: 0.03686191398580401
***** Warning: Loss has increased *****
Loss at iteration [2751]: 0.03358952084402035
Loss at iteration [2752]: 0.023586347469680857
Loss at iteration [2753]: 0.025257624548232434
***** Warning: Loss has increased *****
Loss at iteration [2754]: 0.03207418241580062
***** Warning: Loss has increased *****
Loss at iteration [2755]: 0.02967073397054733
Loss at iteration [2756]: 0.022646065234977544
Loss at iteration [2757]: 0.024167260693724383
***** Warning: Loss has increased *****
Loss at iteration [2758]: 0.029179550374290483
***** Warning: Loss has increased *****
Loss at iteration [2759]: 0.02722353053753261
Loss at iteration [2760]: 0.022468456566068194
Loss at iteration [2761]: 0.022096325688360997
Loss at iteration [2762]: 0.02541780703354185
***** Warning: Loss has increased *****
Loss at iteration [2763]: 0.026478662426755624
***** Warning: Loss has increased *****
Loss at iteration [2764]: 0.023427565809764545
Loss at iteration [2765]: 0.021248969000182284
Loss at iteration [2766]: 0.02235244412072282
***** Warning: Loss has increased *****
Loss at iteration [2767]: 0.024169320299507885
***** Warning: Loss has increased *****
Loss at iteration [2768]: 0.02382086459105265
Loss at iteration [2769]: 0.02182303960751913
Loss at iteration [2770]: 0.021084887012559984
Loss at iteration [2771]: 0.02210194248072016
***** Warning: Loss has increased *****
Loss at iteration [2772]: 0.023020245198285074
***** Warning: Loss has increased *****
Loss at iteration [2773]: 0.02266127799164519
Loss at iteration [2774]: 0.021413242294970297
Loss at iteration [2775]: 0.020972685401608116
Loss at iteration [2776]: 0.02153127460340311
***** Warning: Loss has increased *****
Loss at iteration [2777]: 0.02208493360911464
***** Warning: Loss has increased *****
Loss at iteration [2778]: 0.021853608396930253
Loss at iteration [2779]: 0.021137162175664286
Loss at iteration [2780]: 0.02084460386112592
Loss at iteration [2781]: 0.021143840128061163
***** Warning: Loss has increased *****
Loss at iteration [2782]: 0.021507117324312125
***** Warning: Loss has increased *****
Loss at iteration [2783]: 0.02153201205758933
***** Warning: Loss has increased *****
Loss at iteration [2784]: 0.02113257863697504
Loss at iteration [2785]: 0.020811003463514575
Loss at iteration [2786]: 0.02078549175417266
Loss at iteration [2787]: 0.020976017424606638
***** Warning: Loss has increased *****
Loss at iteration [2788]: 0.02111122873563439
***** Warning: Loss has increased *****
Loss at iteration [2789]: 0.021077747150247576
Loss at iteration [2790]: 0.020864630761787305
Loss at iteration [2791]: 0.020689554519004092
Loss at iteration [2792]: 0.0207022266620252
***** Warning: Loss has increased *****
Loss at iteration [2793]: 0.020818139474273738
***** Warning: Loss has increased *****
Loss at iteration [2794]: 0.020891342879706662
***** Warning: Loss has increased *****
Loss at iteration [2795]: 0.02086859408790004
Loss at iteration [2796]: 0.020735965326830898
Loss at iteration [2797]: 0.02061282979923474
Loss at iteration [2798]: 0.020593094385969533
Loss at iteration [2799]: 0.020654450870939537
***** Warning: Loss has increased *****
Loss at iteration [2800]: 0.020728500725409264
***** Warning: Loss has increased *****
Loss at iteration [2801]: 0.020722797416307313
Loss at iteration [2802]: 0.02069699544985908
Loss at iteration [2803]: 0.020626238167130558
Loss at iteration [2804]: 0.020557563954235764
Loss at iteration [2805]: 0.020546176522885186
Loss at iteration [2806]: 0.020573418024821388
***** Warning: Loss has increased *****
Loss at iteration [2807]: 0.02059654047169808
***** Warning: Loss has increased *****
Loss at iteration [2808]: 0.020596463473259163
Loss at iteration [2809]: 0.020574495445878055
Loss at iteration [2810]: 0.020522064393880626
Loss at iteration [2811]: 0.02051293420970127
Loss at iteration [2812]: 0.02049224355959231
Loss at iteration [2813]: 0.02047856581572308
Loss at iteration [2814]: 0.020508027651762385
***** Warning: Loss has increased *****
Loss at iteration [2815]: 0.020508504372049072
***** Warning: Loss has increased *****
Loss at iteration [2816]: 0.020473537133433525
Loss at iteration [2817]: 0.020462053056130712
Loss at iteration [2818]: 0.020447403226869828
Loss at iteration [2819]: 0.02041046685981108
Loss at iteration [2820]: 0.020417947687702536
***** Warning: Loss has increased *****
Loss at iteration [2821]: 0.020412406617825136
Loss at iteration [2822]: 0.020408002946836337
Loss at iteration [2823]: 0.020400832416088973
Loss at iteration [2824]: 0.020381057050407714
Loss at iteration [2825]: 0.020385741554720317
***** Warning: Loss has increased *****
Loss at iteration [2826]: 0.020388320677283276
***** Warning: Loss has increased *****
Loss at iteration [2827]: 0.020367403649088515
Loss at iteration [2828]: 0.02037916023155431
***** Warning: Loss has increased *****
Loss at iteration [2829]: 0.020384918759904638
***** Warning: Loss has increased *****
Loss at iteration [2830]: 0.02037425511253756
Loss at iteration [2831]: 0.0203638795267489
Loss at iteration [2832]: 0.02034548562272409
Loss at iteration [2833]: 0.02034660321034741
***** Warning: Loss has increased *****
Loss at iteration [2834]: 0.020344187185224974
Loss at iteration [2835]: 0.02031740224606966
Loss at iteration [2836]: 0.020324922472119783
***** Warning: Loss has increased *****
Loss at iteration [2837]: 0.02031232168225142
Loss at iteration [2838]: 0.020292928913841345
Loss at iteration [2839]: 0.02028979690325134
Loss at iteration [2840]: 0.020266568873783298
Loss at iteration [2841]: 0.020293588032091164
***** Warning: Loss has increased *****
Loss at iteration [2842]: 0.020303512943409676
***** Warning: Loss has increased *****
Loss at iteration [2843]: 0.020276869448257677
Loss at iteration [2844]: 0.020265952952465933
Loss at iteration [2845]: 0.02025769393164411
Loss at iteration [2846]: 0.020246091769216663
Loss at iteration [2847]: 0.020228977386894272
Loss at iteration [2848]: 0.020209531199067356
Loss at iteration [2849]: 0.020246936194097013
***** Warning: Loss has increased *****
Loss at iteration [2850]: 0.020253080246761705
***** Warning: Loss has increased *****
Loss at iteration [2851]: 0.02025320108387951
***** Warning: Loss has increased *****
Loss at iteration [2852]: 0.020254612800786238
***** Warning: Loss has increased *****
Loss at iteration [2853]: 0.02027686092692681
***** Warning: Loss has increased *****
Loss at iteration [2854]: 0.020320596913697378
***** Warning: Loss has increased *****
Loss at iteration [2855]: 0.020359164074541723
***** Warning: Loss has increased *****
Loss at iteration [2856]: 0.020434595581746984
***** Warning: Loss has increased *****
Loss at iteration [2857]: 0.02050555667721445
***** Warning: Loss has increased *****
Loss at iteration [2858]: 0.02058868311962498
***** Warning: Loss has increased *****
Loss at iteration [2859]: 0.02070114862143783
***** Warning: Loss has increased *****
Loss at iteration [2860]: 0.020798335877778393
***** Warning: Loss has increased *****
Loss at iteration [2861]: 0.020934051684267957
***** Warning: Loss has increased *****
Loss at iteration [2862]: 0.021108381706095247
***** Warning: Loss has increased *****
Loss at iteration [2863]: 0.021219079520444974
***** Warning: Loss has increased *****
Loss at iteration [2864]: 0.021447776015020112
***** Warning: Loss has increased *****
Loss at iteration [2865]: 0.021730834792366815
***** Warning: Loss has increased *****
Loss at iteration [2866]: 0.022161828050482796
***** Warning: Loss has increased *****
Loss at iteration [2867]: 0.022560271539207477
***** Warning: Loss has increased *****
Loss at iteration [2868]: 0.02323994467386606
***** Warning: Loss has increased *****
Loss at iteration [2869]: 0.023890909468391886
***** Warning: Loss has increased *****
Loss at iteration [2870]: 0.024822288896281776
***** Warning: Loss has increased *****
Loss at iteration [2871]: 0.02520273810939938
***** Warning: Loss has increased *****
Loss at iteration [2872]: 0.025739228979058393
***** Warning: Loss has increased *****
Loss at iteration [2873]: 0.02550174469181928
Loss at iteration [2874]: 0.025387791383306536
Loss at iteration [2875]: 0.024348633475519736
Loss at iteration [2876]: 0.023224736269075133
Loss at iteration [2877]: 0.021864126251106574
Loss at iteration [2878]: 0.02082064225566268
Loss at iteration [2879]: 0.020291109257841916
Loss at iteration [2880]: 0.020166613304603266
Loss at iteration [2881]: 0.020386300241463262
***** Warning: Loss has increased *****
Loss at iteration [2882]: 0.02079414588333699
***** Warning: Loss has increased *****
Loss at iteration [2883]: 0.021362360722895393
***** Warning: Loss has increased *****
Loss at iteration [2884]: 0.02187933365859836
***** Warning: Loss has increased *****
Loss at iteration [2885]: 0.022369400182383432
***** Warning: Loss has increased *****
Loss at iteration [2886]: 0.022597216357961008
***** Warning: Loss has increased *****
Loss at iteration [2887]: 0.02272660922224
***** Warning: Loss has increased *****
Loss at iteration [2888]: 0.02239404533926758
Loss at iteration [2889]: 0.021944646649536513
Loss at iteration [2890]: 0.02126981011228548
Loss at iteration [2891]: 0.020702965313265063
Loss at iteration [2892]: 0.020281439300697413
Loss at iteration [2893]: 0.020086373373877548
Loss at iteration [2894]: 0.020054207757400463
Loss at iteration [2895]: 0.020128260986729457
***** Warning: Loss has increased *****
Loss at iteration [2896]: 0.020291852992484188
***** Warning: Loss has increased *****
Loss at iteration [2897]: 0.020483421690661214
***** Warning: Loss has increased *****
Loss at iteration [2898]: 0.020682003778779167
***** Warning: Loss has increased *****
Loss at iteration [2899]: 0.020858333116313837
***** Warning: Loss has increased *****
Loss at iteration [2900]: 0.020961562284615577
***** Warning: Loss has increased *****
Loss at iteration [2901]: 0.020999314317912222
***** Warning: Loss has increased *****
Loss at iteration [2902]: 0.021019720388804047
***** Warning: Loss has increased *****
Loss at iteration [2903]: 0.020947758673206255
Loss at iteration [2904]: 0.020978694110039255
***** Warning: Loss has increased *****
Loss at iteration [2905]: 0.020949336302532425
Loss at iteration [2906]: 0.020957405218987146
***** Warning: Loss has increased *****
Loss at iteration [2907]: 0.020918737985327705
Loss at iteration [2908]: 0.02087661725574087
Loss at iteration [2909]: 0.02079824497353174
Loss at iteration [2910]: 0.02074062369377435
Loss at iteration [2911]: 0.020616853771598522
Loss at iteration [2912]: 0.020556926048781347
Loss at iteration [2913]: 0.02045289381547057
Loss at iteration [2914]: 0.020360531228814586
Loss at iteration [2915]: 0.020287914892024805
Loss at iteration [2916]: 0.020241277066716898
Loss at iteration [2917]: 0.020250730330777608
***** Warning: Loss has increased *****
Loss at iteration [2918]: 0.020294980142235644
***** Warning: Loss has increased *****
Loss at iteration [2919]: 0.020326045008912046
***** Warning: Loss has increased *****
Loss at iteration [2920]: 0.020409691027906882
***** Warning: Loss has increased *****
Loss at iteration [2921]: 0.02050903089028995
***** Warning: Loss has increased *****
Loss at iteration [2922]: 0.020623688460574843
***** Warning: Loss has increased *****
Loss at iteration [2923]: 0.020777170648085135
***** Warning: Loss has increased *****
Loss at iteration [2924]: 0.02106525294815706
***** Warning: Loss has increased *****
Loss at iteration [2925]: 0.021619030147250664
***** Warning: Loss has increased *****
Loss at iteration [2926]: 0.022468083386541028
***** Warning: Loss has increased *****
Loss at iteration [2927]: 0.023604314945029885
***** Warning: Loss has increased *****
Loss at iteration [2928]: 0.025560801396496236
***** Warning: Loss has increased *****
Loss at iteration [2929]: 0.02782785994343323
***** Warning: Loss has increased *****
Loss at iteration [2930]: 0.032254903584252086
***** Warning: Loss has increased *****
Loss at iteration [2931]: 0.03691170215683344
***** Warning: Loss has increased *****
Loss at iteration [2932]: 0.046310183244439224
***** Warning: Loss has increased *****
Loss at iteration [2933]: 0.05312581279584539
***** Warning: Loss has increased *****
Loss at iteration [2934]: 0.06767214824875643
***** Warning: Loss has increased *****
Loss at iteration [2935]: 0.06902663335819727
***** Warning: Loss has increased *****
Loss at iteration [2936]: 0.07518332279247077
***** Warning: Loss has increased *****
Loss at iteration [2937]: 0.05545643373713051
Loss at iteration [2938]: 0.03444020461415675
Loss at iteration [2939]: 0.021563537496359333
Loss at iteration [2940]: 0.0288954447010907
***** Warning: Loss has increased *****
Loss at iteration [2941]: 0.04262253874269379
***** Warning: Loss has increased *****
Loss at iteration [2942]: 0.04089671612650193
Loss at iteration [2943]: 0.029946103645379616
Loss at iteration [2944]: 0.021380219708980474
Loss at iteration [2945]: 0.025258684837380876
***** Warning: Loss has increased *****
Loss at iteration [2946]: 0.03339671276895716
***** Warning: Loss has increased *****
Loss at iteration [2947]: 0.032467388246687066
Loss at iteration [2948]: 0.02614484409443999
Loss at iteration [2949]: 0.021039347008163425
Loss at iteration [2950]: 0.022699801143745785
***** Warning: Loss has increased *****
Loss at iteration [2951]: 0.027404901870866177
***** Warning: Loss has increased *****
Loss at iteration [2952]: 0.02840913164715978
***** Warning: Loss has increased *****
Loss at iteration [2953]: 0.025189808225142087
Loss at iteration [2954]: 0.021125359528656806
Loss at iteration [2955]: 0.021000758477270648
Loss at iteration [2956]: 0.023614487149945452
***** Warning: Loss has increased *****
Loss at iteration [2957]: 0.02544017123962346
***** Warning: Loss has increased *****
Loss at iteration [2958]: 0.025150204172169458
Loss at iteration [2959]: 0.02260064996806221
Loss at iteration [2960]: 0.020586888421537674
Loss at iteration [2961]: 0.02049160386187363
Loss at iteration [2962]: 0.021789347526845405
***** Warning: Loss has increased *****
Loss at iteration [2963]: 0.023146398182759365
***** Warning: Loss has increased *****
Loss at iteration [2964]: 0.023004088477614284
Loss at iteration [2965]: 0.02203823410635806
Loss at iteration [2966]: 0.020817366346104704
Loss at iteration [2967]: 0.02011135105705543
Loss at iteration [2968]: 0.020251680248328345
***** Warning: Loss has increased *****
Loss at iteration [2969]: 0.02089883237444535
***** Warning: Loss has increased *****
Loss at iteration [2970]: 0.0214322447075223
***** Warning: Loss has increased *****
Loss at iteration [2971]: 0.021460252318840033
***** Warning: Loss has increased *****
Loss at iteration [2972]: 0.021085336367150147
Loss at iteration [2973]: 0.020508832545125068
Loss at iteration [2974]: 0.02010319245786783
Loss at iteration [2975]: 0.019976635393326378
Loss at iteration [2976]: 0.020139228486160437
***** Warning: Loss has increased *****
Loss at iteration [2977]: 0.020484070250704797
***** Warning: Loss has increased *****
Loss at iteration [2978]: 0.02068736207456856
***** Warning: Loss has increased *****
Loss at iteration [2979]: 0.02076290258382843
***** Warning: Loss has increased *****
Loss at iteration [2980]: 0.020629387592277375
Loss at iteration [2981]: 0.02036461880479337
Loss at iteration [2982]: 0.020098302579303614
Loss at iteration [2983]: 0.019929556859878416
Loss at iteration [2984]: 0.019866084202629485
Loss at iteration [2985]: 0.019940846652729265
***** Warning: Loss has increased *****
Loss at iteration [2986]: 0.020096651057112537
***** Warning: Loss has increased *****
Loss at iteration [2987]: 0.02022891825784855
***** Warning: Loss has increased *****
Loss at iteration [2988]: 0.020293544437453754
***** Warning: Loss has increased *****
Loss at iteration [2989]: 0.020267790771864358
Loss at iteration [2990]: 0.02019427715070445
Loss at iteration [2991]: 0.020069949757900652
Loss at iteration [2992]: 0.01993565245333744
Loss at iteration [2993]: 0.019833887504766296
Loss at iteration [2994]: 0.019776962332934862
Loss at iteration [2995]: 0.019774157478190432
Loss at iteration [2996]: 0.01979230649104912
***** Warning: Loss has increased *****
Loss at iteration [2997]: 0.019854543784561838
***** Warning: Loss has increased *****
Loss at iteration [2998]: 0.01991509168157579
***** Warning: Loss has increased *****
Loss at iteration [2999]: 0.01997626190139686
***** Warning: Loss has increased *****
Loss at iteration [3000]: 0.0200127705330591
***** Warning: Loss has increased *****
