Model name                            : MLP
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : Adam
Learning rate                         : 0.01
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 13.81578516960144
Total number of parameters            : 101001
Percentage of parameters < 1e-9       : 61.27761111276126%
Percentage of parameters < 1e-7       : 61.27761111276126%
Percentage of parameters < 1e-6       : 61.27761111276126%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0314787512012518
Loss at iteration [2]: 0.02691553602874991
Loss at iteration [3]: 2.1432060284131196
***** Warning: Loss has increased *****
Loss at iteration [4]: 0.29676830950508054
Loss at iteration [5]: 0.6983513869862731
***** Warning: Loss has increased *****
Loss at iteration [6]: 0.5955817316983386
Loss at iteration [7]: 0.42866777312434073
Loss at iteration [8]: 0.30030773411105094
Loss at iteration [9]: 0.18009339529031693
Loss at iteration [10]: 0.07227687104806108
Loss at iteration [11]: 0.02064973606103187
Loss at iteration [12]: 0.034997005516784356
***** Warning: Loss has increased *****
Loss at iteration [13]: 0.06871944339903104
***** Warning: Loss has increased *****
Loss at iteration [14]: 0.0690460369460503
***** Warning: Loss has increased *****
Loss at iteration [15]: 0.040886689389146434
Loss at iteration [16]: 0.014207627291227374
Loss at iteration [17]: 0.01020147317666639
Loss at iteration [18]: 0.026173601436892305
***** Warning: Loss has increased *****
Loss at iteration [19]: 0.03904993634725518
***** Warning: Loss has increased *****
Loss at iteration [20]: 0.03487784397706373
Loss at iteration [21]: 0.01977080375465504
Loss at iteration [22]: 0.0084610600106649
Loss at iteration [23]: 0.006339840944251068
Loss at iteration [24]: 0.010253826538621802
***** Warning: Loss has increased *****
Loss at iteration [25]: 0.014395128807001644
***** Warning: Loss has increased *****
Loss at iteration [26]: 0.014975395656246088
***** Warning: Loss has increased *****
Loss at iteration [27]: 0.011956996112734382
Loss at iteration [28]: 0.007337846193713478
Loss at iteration [29]: 0.004224891747666417
Loss at iteration [30]: 0.004648417358065633
***** Warning: Loss has increased *****
Loss at iteration [31]: 0.0078028109828211425
***** Warning: Loss has increased *****
Loss at iteration [32]: 0.01017948798294083
***** Warning: Loss has increased *****
Loss at iteration [33]: 0.008945284228869501
Loss at iteration [34]: 0.005592435820234437
Loss at iteration [35]: 0.0033091214458975614
Loss at iteration [36]: 0.003463802952034794
***** Warning: Loss has increased *****
Loss at iteration [37]: 0.004874980977263526
***** Warning: Loss has increased *****
Loss at iteration [38]: 0.005764223274484723
***** Warning: Loss has increased *****
Loss at iteration [39]: 0.00554514358501752
Loss at iteration [40]: 0.004643732766446614
Loss at iteration [41]: 0.0035691822562133233
Loss at iteration [42]: 0.002916568285927549
Loss at iteration [43]: 0.0032394960713037144
***** Warning: Loss has increased *****
Loss at iteration [44]: 0.004180279030168832
***** Warning: Loss has increased *****
Loss at iteration [45]: 0.004597613587573985
***** Warning: Loss has increased *****
Loss at iteration [46]: 0.003951771190742285
Loss at iteration [47]: 0.002969778190933283
Loss at iteration [48]: 0.0026538635521311336
Loss at iteration [49]: 0.0030475948292561883
***** Warning: Loss has increased *****
Loss at iteration [50]: 0.003448131679647254
***** Warning: Loss has increased *****
Loss at iteration [51]: 0.0034579752557293134
***** Warning: Loss has increased *****
Loss at iteration [52]: 0.0032069687252821487
Loss at iteration [53]: 0.002893263444921438
Loss at iteration [54]: 0.0026721687796204805
Loss at iteration [55]: 0.002716139598792589
***** Warning: Loss has increased *****
Loss at iteration [56]: 0.0029883976010896146
***** Warning: Loss has increased *****
Loss at iteration [57]: 0.003139803096961532
***** Warning: Loss has increased *****
Loss at iteration [58]: 0.002940288660771044
Loss at iteration [59]: 0.0026316358459660644
Loss at iteration [60]: 0.0025662126002295553
Loss at iteration [61]: 0.0027197086041656268
***** Warning: Loss has increased *****
Loss at iteration [62]: 0.0028276380839856656
***** Warning: Loss has increased *****
Loss at iteration [63]: 0.0027882866536768384
Loss at iteration [64]: 0.002686886359406921
Loss at iteration [65]: 0.002594580495630182
Loss at iteration [66]: 0.002555701851523102
Loss at iteration [67]: 0.0026089008887410195
***** Warning: Loss has increased *****
Loss at iteration [68]: 0.002689295362649325
***** Warning: Loss has increased *****
Loss at iteration [69]: 0.002671730894898486
Loss at iteration [70]: 0.0025660297104941336
Loss at iteration [71]: 0.0025092095489920663
Loss at iteration [72]: 0.0025492536653534026
***** Warning: Loss has increased *****
Loss at iteration [73]: 0.002597218551117501
***** Warning: Loss has increased *****
Loss at iteration [74]: 0.002590419396314427
Loss at iteration [75]: 0.0025547514693259376
Loss at iteration [76]: 0.002523620561729378
Loss at iteration [77]: 0.0025101971266898522
Loss at iteration [78]: 0.0025253575601214802
***** Warning: Loss has increased *****
Loss at iteration [79]: 0.0025500648363240208
***** Warning: Loss has increased *****
Loss at iteration [80]: 0.0025416245671960433
Loss at iteration [81]: 0.0025039055766447453
Loss at iteration [82]: 0.0024871798016170067
Loss at iteration [83]: 0.0025025753594739974
***** Warning: Loss has increased *****
Loss at iteration [84]: 0.0025151000819895837
***** Warning: Loss has increased *****
Loss at iteration [85]: 0.0025067050885880337
Loss at iteration [86]: 0.0024920469506667527
Loss at iteration [87]: 0.002482011805784631
Loss at iteration [88]: 0.002479511379397856
Loss at iteration [89]: 0.002485965463929433
***** Warning: Loss has increased *****
Loss at iteration [90]: 0.0024895815968270013
***** Warning: Loss has increased *****
Loss at iteration [91]: 0.0024788452587996474
Loss at iteration [92]: 0.0024655530035900207
Loss at iteration [93]: 0.0024650413471425862
Loss at iteration [94]: 0.002470056274117143
***** Warning: Loss has increased *****
Loss at iteration [95]: 0.002468007894081197
Loss at iteration [96]: 0.002460961146489817
Loss at iteration [97]: 0.002455055960557615
Loss at iteration [98]: 0.002451942964766985
Loss at iteration [99]: 0.002452327532257047
***** Warning: Loss has increased *****
Loss at iteration [100]: 0.002453064855375378
***** Warning: Loss has increased *****
Loss at iteration [101]: 0.0024489383209860636
Loss at iteration [102]: 0.0024424091143191956
Loss at iteration [103]: 0.0024402082108800397
Loss at iteration [104]: 0.0024408300553013323
***** Warning: Loss has increased *****
Loss at iteration [105]: 0.002439049436138305
Loss at iteration [106]: 0.0024351146868421933
Loss at iteration [107]: 0.002431524640911249
Loss at iteration [108]: 0.0024287940158937865
Loss at iteration [109]: 0.0024272840451262097
Loss at iteration [110]: 0.0024258426503573362
Loss at iteration [111]: 0.002422473670899906
Loss at iteration [112]: 0.002418440553732359
Loss at iteration [113]: 0.0024161984733395425
Loss at iteration [114]: 0.002414567461940627
Loss at iteration [115]: 0.00241188774249979
Loss at iteration [116]: 0.002409963560959403
Loss at iteration [117]: 0.0024072849944944633
Loss at iteration [118]: 0.002402908373238117
Loss at iteration [119]: 0.0023995965682173507
Loss at iteration [120]: 0.0023975029067689045
Loss at iteration [121]: 0.002394624800202637
Loss at iteration [122]: 0.002390831660712733
Loss at iteration [123]: 0.0023867985801678806
Loss at iteration [124]: 0.002382599134539388
Loss at iteration [125]: 0.002381055169463492
Loss at iteration [126]: 0.002379022999962344
Loss at iteration [127]: 0.0023754800428503996
Loss at iteration [128]: 0.0023775684127186197
***** Warning: Loss has increased *****
Loss at iteration [129]: 0.0023700417363460113
Loss at iteration [130]: 0.0023712736172826227
***** Warning: Loss has increased *****
Loss at iteration [131]: 0.002369475254076517
Loss at iteration [132]: 0.0023661865031716052
Loss at iteration [133]: 0.0023651736051522525
Loss at iteration [134]: 0.002364702189573233
Loss at iteration [135]: 0.0023609438435919067
Loss at iteration [136]: 0.002358652440354056
Loss at iteration [137]: 0.0023579985872371574
Loss at iteration [138]: 0.0023561464264118858
Loss at iteration [139]: 0.0023548055402137346
Loss at iteration [140]: 0.0023534738044899963
Loss at iteration [141]: 0.002350761736443333
Loss at iteration [142]: 0.002349920089801012
Loss at iteration [143]: 0.002347289142727386
Loss at iteration [144]: 0.002346518718578515
Loss at iteration [145]: 0.002344077043465482
Loss at iteration [146]: 0.0023413319094100923
Loss at iteration [147]: 0.0023388140130980073
Loss at iteration [148]: 0.0023355466085151965
Loss at iteration [149]: 0.0023347940506180565
Loss at iteration [150]: 0.002332835206839768
Loss at iteration [151]: 0.0023319082941411742
Loss at iteration [152]: 0.0023300340011569816
Loss at iteration [153]: 0.0023290111847670382
Loss at iteration [154]: 0.0023276970234922044
Loss at iteration [155]: 0.0023267198142882207
Loss at iteration [156]: 0.002325615833094132
Loss at iteration [157]: 0.0023240766763972516
Loss at iteration [158]: 0.0023231730822372075
Loss at iteration [159]: 0.002321196824976948
Loss at iteration [160]: 0.002320087948245999
Loss at iteration [161]: 0.0023185584145224494
Loss at iteration [162]: 0.002317303365095419
Loss at iteration [163]: 0.002315631586479296
Loss at iteration [164]: 0.002314464497035963
Loss at iteration [165]: 0.0023129087157394957
Loss at iteration [166]: 0.002312105404096115
Loss at iteration [167]: 0.0023108022649737307
Loss at iteration [168]: 0.0023102518567590363
Loss at iteration [169]: 0.0023085776577735674
Loss at iteration [170]: 0.0023071356061372763
Loss at iteration [171]: 0.0023059243864575313
Loss at iteration [172]: 0.0023049523116001015
Loss at iteration [173]: 0.0023038797715463763
Loss at iteration [174]: 0.0023027995933300838
Loss at iteration [175]: 0.002301825394655479
Loss at iteration [176]: 0.002300611910566178
Loss at iteration [177]: 0.0022994898743664187
Loss at iteration [178]: 0.0022981857579296118
Loss at iteration [179]: 0.002297618606256923
Loss at iteration [180]: 0.002296792310282997
Loss at iteration [181]: 0.0022955318782319424
Loss at iteration [182]: 0.0022944410521037
Loss at iteration [183]: 0.0022935999394107934
Loss at iteration [184]: 0.0022929734779166483
Loss at iteration [185]: 0.0022918922137146556
Loss at iteration [186]: 0.002290954262444388
Loss at iteration [187]: 0.0022900236672456736
Loss at iteration [188]: 0.0022889917073123387
Loss at iteration [189]: 0.002288109926871138
Loss at iteration [190]: 0.002287383835590513
Loss at iteration [191]: 0.0022863314190389433
Loss at iteration [192]: 0.0022855585355597272
Loss at iteration [193]: 0.0022847167518375957
Loss at iteration [194]: 0.002283944821423459
Loss at iteration [195]: 0.0022831024634727445
Loss at iteration [196]: 0.0022824067198766924
Loss at iteration [197]: 0.0022817304454720517
Loss at iteration [198]: 0.00228093831666273
Loss at iteration [199]: 0.00228010753243505
Loss at iteration [200]: 0.0022791823907822397
Loss at iteration [201]: 0.0022786438087582586
Loss at iteration [202]: 0.00227789529263189
Loss at iteration [203]: 0.0022768351147616477
Loss at iteration [204]: 0.002276192138477163
Loss at iteration [205]: 0.0022755536731191538
Loss at iteration [206]: 0.002275002790810456
Loss at iteration [207]: 0.002274200305359632
Loss at iteration [208]: 0.0022732954988076365
Loss at iteration [209]: 0.0022727132722595213
Loss at iteration [210]: 0.0022722113987354974
Loss at iteration [211]: 0.002271374008120779
Loss at iteration [212]: 0.002270738190492532
Loss at iteration [213]: 0.002270161574329322
Loss at iteration [214]: 0.0022696013679991416
Loss at iteration [215]: 0.00226903349832661
Loss at iteration [216]: 0.002268240459103573
Loss at iteration [217]: 0.0022675735416992854
Loss at iteration [218]: 0.0022668819520846634
Loss at iteration [219]: 0.0022661821166646925
Loss at iteration [220]: 0.002265461485351368
Loss at iteration [221]: 0.0022648834278818242
Loss at iteration [222]: 0.002264401074722937
Loss at iteration [223]: 0.0022638764559391017
Loss at iteration [224]: 0.002263134690155712
Loss at iteration [225]: 0.0022625687882930606
Loss at iteration [226]: 0.0022619556438073273
Loss at iteration [227]: 0.0022615425783096437
Loss at iteration [228]: 0.002261019747347426
Loss at iteration [229]: 0.0022603747278624247
Loss at iteration [230]: 0.0022597990283857367
Loss at iteration [231]: 0.002259217562092792
Loss at iteration [232]: 0.002258758741650252
Loss at iteration [233]: 0.0022580318183671928
Loss at iteration [234]: 0.0022577804525992467
Loss at iteration [235]: 0.00225743028306333
Loss at iteration [236]: 0.0022567069836853574
Loss at iteration [237]: 0.0022562097537501867
Loss at iteration [238]: 0.002255349538767963
Loss at iteration [239]: 0.002255351273606754
***** Warning: Loss has increased *****
Loss at iteration [240]: 0.002254288704142669
Loss at iteration [241]: 0.002253461817304336
Loss at iteration [242]: 0.0022532766532141206
Loss at iteration [243]: 0.002252570960814069
Loss at iteration [244]: 0.0022519181427517487
Loss at iteration [245]: 0.002250922088766173
Loss at iteration [246]: 0.00225043818181113
Loss at iteration [247]: 0.0022500572481523543
Loss at iteration [248]: 0.0022494513363779116
Loss at iteration [249]: 0.0022484380513617497
Loss at iteration [250]: 0.002247775915861799
Loss at iteration [251]: 0.0022475545482493514
Loss at iteration [252]: 0.0022474232645373344
Loss at iteration [253]: 0.0022492729806112307
***** Warning: Loss has increased *****
Loss at iteration [254]: 0.0022483920592081376
Loss at iteration [255]: 0.00224482477927717
Loss at iteration [256]: 0.002250172369849102
***** Warning: Loss has increased *****
Loss at iteration [257]: 0.002253110427563226
***** Warning: Loss has increased *****
Loss at iteration [258]: 0.002244517431256304
Loss at iteration [259]: 0.002257033236777541
***** Warning: Loss has increased *****
Loss at iteration [260]: 0.0022427749646513396
Loss at iteration [261]: 0.002248514788311481
***** Warning: Loss has increased *****
Loss at iteration [262]: 0.002240502413928029
Loss at iteration [263]: 0.0022473705828192775
***** Warning: Loss has increased *****
Loss at iteration [264]: 0.002239662792623863
Loss at iteration [265]: 0.0022442669590871027
***** Warning: Loss has increased *****
Loss at iteration [266]: 0.002238274404821863
Loss at iteration [267]: 0.002242817749999
***** Warning: Loss has increased *****
Loss at iteration [268]: 0.0022373377902394223
Loss at iteration [269]: 0.0022406981251166503
***** Warning: Loss has increased *****
Loss at iteration [270]: 0.002236611457653949
Loss at iteration [271]: 0.0022412287196991636
***** Warning: Loss has increased *****
Loss at iteration [272]: 0.0022353764488586064
Loss at iteration [273]: 0.0022402224889092296
***** Warning: Loss has increased *****
Loss at iteration [274]: 0.002234359634408729
Loss at iteration [275]: 0.0022375528181422227
***** Warning: Loss has increased *****
Loss at iteration [276]: 0.002233081759088193
Loss at iteration [277]: 0.002236165879469819
***** Warning: Loss has increased *****
Loss at iteration [278]: 0.002231795589211027
Loss at iteration [279]: 0.0022341772748784706
***** Warning: Loss has increased *****
Loss at iteration [280]: 0.0022312292966899564
Loss at iteration [281]: 0.0022332357102115515
***** Warning: Loss has increased *****
Loss at iteration [282]: 0.0022294384495264883
Loss at iteration [283]: 0.0022310555457531298
***** Warning: Loss has increased *****
Loss at iteration [284]: 0.002227858000609991
Loss at iteration [285]: 0.0022299164031904825
***** Warning: Loss has increased *****
Loss at iteration [286]: 0.0022266400177307325
Loss at iteration [287]: 0.0022275914784509023
***** Warning: Loss has increased *****
Loss at iteration [288]: 0.0022257876575856386
Loss at iteration [289]: 0.0022261176874937678
***** Warning: Loss has increased *****
Loss at iteration [290]: 0.002227609367462578
***** Warning: Loss has increased *****
Loss at iteration [291]: 0.002223292805063428
Loss at iteration [292]: 0.0022249968475419614
***** Warning: Loss has increased *****
Loss at iteration [293]: 0.0022240909626877613
Loss at iteration [294]: 0.002221014711153282
Loss at iteration [295]: 0.0022203530934609733
Loss at iteration [296]: 0.0022219144511034664
***** Warning: Loss has increased *****
Loss at iteration [297]: 0.00222219915125574
***** Warning: Loss has increased *****
Loss at iteration [298]: 0.002217859650051181
Loss at iteration [299]: 0.002219364187743645
***** Warning: Loss has increased *****
Loss at iteration [300]: 0.00222256398888967
***** Warning: Loss has increased *****
Loss at iteration [301]: 0.002220632147584015
Loss at iteration [302]: 0.0022157718141232333
Loss at iteration [303]: 0.0022177244071255675
***** Warning: Loss has increased *****
Loss at iteration [304]: 0.0022215884088624994
***** Warning: Loss has increased *****
Loss at iteration [305]: 0.002222554448421783
***** Warning: Loss has increased *****
Loss at iteration [306]: 0.0022159332234255253
Loss at iteration [307]: 0.002213247124388095
Loss at iteration [308]: 0.0022164183800595146
***** Warning: Loss has increased *****
Loss at iteration [309]: 0.00221498889141731
Loss at iteration [310]: 0.00221170841350356
Loss at iteration [311]: 0.002211185883093359
Loss at iteration [312]: 0.0022122767069602026
***** Warning: Loss has increased *****
Loss at iteration [313]: 0.0022107344374236097
Loss at iteration [314]: 0.0022094580529686417
Loss at iteration [315]: 0.0022116076200330726
***** Warning: Loss has increased *****
Loss at iteration [316]: 0.002210506134640738
Loss at iteration [317]: 0.0022086471441618654
Loss at iteration [318]: 0.0022101473803989333
***** Warning: Loss has increased *****
Loss at iteration [319]: 0.002210167841668435
***** Warning: Loss has increased *****
Loss at iteration [320]: 0.0022092756354531056
Loss at iteration [321]: 0.0022067867002406347
Loss at iteration [322]: 0.002207391444384547
***** Warning: Loss has increased *****
Loss at iteration [323]: 0.002210434692309844
***** Warning: Loss has increased *****
Loss at iteration [324]: 0.0022158764714920193
***** Warning: Loss has increased *****
Loss at iteration [325]: 0.002213514821559367
Loss at iteration [326]: 0.0022061646322066126
Loss at iteration [327]: 0.002206846640886008
***** Warning: Loss has increased *****
Loss at iteration [328]: 0.0022220184231627506
***** Warning: Loss has increased *****
Loss at iteration [329]: 0.0022255009839980927
***** Warning: Loss has increased *****
Loss at iteration [330]: 0.002204560597982512
Loss at iteration [331]: 0.00221626938396754
***** Warning: Loss has increased *****
Loss at iteration [332]: 0.0022120411004903254
Loss at iteration [333]: 0.002204464185602824
Loss at iteration [334]: 0.0022099095048765337
***** Warning: Loss has increased *****
Loss at iteration [335]: 0.002204849533489835
Loss at iteration [336]: 0.0022046584691283004
Loss at iteration [337]: 0.00220875513052358
***** Warning: Loss has increased *****
Loss at iteration [338]: 0.0022024900573651826
Loss at iteration [339]: 0.002207822802619573
***** Warning: Loss has increased *****
Loss at iteration [340]: 0.0022026092665662442
Loss at iteration [341]: 0.002207563720957391
***** Warning: Loss has increased *****
Loss at iteration [342]: 0.002208329576221822
***** Warning: Loss has increased *****
Loss at iteration [343]: 0.0022030901588259806
Loss at iteration [344]: 0.002206401067251203
***** Warning: Loss has increased *****
Loss at iteration [345]: 0.0022004805351635583
Loss at iteration [346]: 0.0022075394811871523
***** Warning: Loss has increased *****
Loss at iteration [347]: 0.0022007341348489065
Loss at iteration [348]: 0.0022031445585963894
***** Warning: Loss has increased *****
Loss at iteration [349]: 0.0022010031792011955
Loss at iteration [350]: 0.0022004891782085425
Loss at iteration [351]: 0.002202159741808603
***** Warning: Loss has increased *****
Loss at iteration [352]: 0.002200110901319375
Loss at iteration [353]: 0.0022006545812166575
***** Warning: Loss has increased *****
Loss at iteration [354]: 0.0021994752421553403
Loss at iteration [355]: 0.0021996501700779474
***** Warning: Loss has increased *****
Loss at iteration [356]: 0.0021983971924959167
Loss at iteration [357]: 0.0022017567202099833
***** Warning: Loss has increased *****
Loss at iteration [358]: 0.002198095075217513
Loss at iteration [359]: 0.0021970182952573986
Loss at iteration [360]: 0.0021989260719849687
***** Warning: Loss has increased *****
Loss at iteration [361]: 0.0021971924053592485
Loss at iteration [362]: 0.00219768330406307
***** Warning: Loss has increased *****
Loss at iteration [363]: 0.0021974032642204443
Loss at iteration [364]: 0.0021958190768156323
Loss at iteration [365]: 0.0021961351093529925
***** Warning: Loss has increased *****
Loss at iteration [366]: 0.0021959547613640286
Loss at iteration [367]: 0.0021952566085116833
Loss at iteration [368]: 0.002195800204420708
***** Warning: Loss has increased *****
Loss at iteration [369]: 0.002198070228145891
***** Warning: Loss has increased *****
Loss at iteration [370]: 0.0021950251379701047
Loss at iteration [371]: 0.002193496722280774
Loss at iteration [372]: 0.002196893334916079
***** Warning: Loss has increased *****
Loss at iteration [373]: 0.0021941850672732664
Loss at iteration [374]: 0.0021938384275260914
Loss at iteration [375]: 0.002193702188004651
Loss at iteration [376]: 0.0021936738329750003
Loss at iteration [377]: 0.002193081286657701
Loss at iteration [378]: 0.0021948554594271854
***** Warning: Loss has increased *****
Loss at iteration [379]: 0.0021976026624335654
***** Warning: Loss has increased *****
Loss at iteration [380]: 0.0021931458789308535
Loss at iteration [381]: 0.0021928280566673986
Loss at iteration [382]: 0.0021940949046020267
***** Warning: Loss has increased *****
Loss at iteration [383]: 0.002193872246707372
Loss at iteration [384]: 0.0021912769293670744
Loss at iteration [385]: 0.002190952537360504
Loss at iteration [386]: 0.0021940764508052496
***** Warning: Loss has increased *****
Loss at iteration [387]: 0.002194173419500485
***** Warning: Loss has increased *****
Loss at iteration [388]: 0.002192252976552558
Loss at iteration [389]: 0.0021899266931674844
Loss at iteration [390]: 0.0021914534729567687
***** Warning: Loss has increased *****
Loss at iteration [391]: 0.002190822401457227
Loss at iteration [392]: 0.002189989692602331
Loss at iteration [393]: 0.002190734046043581
***** Warning: Loss has increased *****
Loss at iteration [394]: 0.0021910923353136882
***** Warning: Loss has increased *****
Loss at iteration [395]: 0.00218991646488323
Loss at iteration [396]: 0.002191586552709014
***** Warning: Loss has increased *****
Loss at iteration [397]: 0.0021937204054189247
***** Warning: Loss has increased *****
Loss at iteration [398]: 0.0021908903254571725
Loss at iteration [399]: 0.002189095309511225
Loss at iteration [400]: 0.002191620993430297
***** Warning: Loss has increased *****
Loss at iteration [401]: 0.0021988967481201844
***** Warning: Loss has increased *****
Loss at iteration [402]: 0.002189405399177137
Loss at iteration [403]: 0.0021889274607862635
Loss at iteration [404]: 0.002195497615047073
***** Warning: Loss has increased *****
Loss at iteration [405]: 0.0021884005930827566
Loss at iteration [406]: 0.0021874207316092804
Loss at iteration [407]: 0.0021937777618841726
***** Warning: Loss has increased *****
Loss at iteration [408]: 0.002188025679444936
Loss at iteration [409]: 0.002187184805832432
Loss at iteration [410]: 0.002186869945955865
Loss at iteration [411]: 0.0021874604309789613
***** Warning: Loss has increased *****
Loss at iteration [412]: 0.002186933925718429
Loss at iteration [413]: 0.002185713077344986
Loss at iteration [414]: 0.0021855542319996727
Loss at iteration [415]: 0.0021850242759984956
Loss at iteration [416]: 0.0021869344988556423
***** Warning: Loss has increased *****
Loss at iteration [417]: 0.0021915956931305874
***** Warning: Loss has increased *****
Loss at iteration [418]: 0.002187027463016393
Loss at iteration [419]: 0.0021861128575590065
Loss at iteration [420]: 0.00219198130795518
***** Warning: Loss has increased *****
Loss at iteration [421]: 0.0021877066673032563
Loss at iteration [422]: 0.002187581834675385
Loss at iteration [423]: 0.002184551858092242
Loss at iteration [424]: 0.002187567335301548
***** Warning: Loss has increased *****
Loss at iteration [425]: 0.002189212730990796
***** Warning: Loss has increased *****
Loss at iteration [426]: 0.0021833769460336576
Loss at iteration [427]: 0.0021929025028161737
***** Warning: Loss has increased *****
Loss at iteration [428]: 0.002196997236208777
***** Warning: Loss has increased *****
Loss at iteration [429]: 0.0021827738105356363
Loss at iteration [430]: 0.002196500237086769
***** Warning: Loss has increased *****
Loss at iteration [431]: 0.0021962330298487993
Loss at iteration [432]: 0.0021844670677093263
Loss at iteration [433]: 0.002200885183834625
***** Warning: Loss has increased *****
Loss at iteration [434]: 0.002187971094077483
Loss at iteration [435]: 0.0021898924969646007
***** Warning: Loss has increased *****
Loss at iteration [436]: 0.0021930590341960533
***** Warning: Loss has increased *****
Loss at iteration [437]: 0.002182471628277243
Loss at iteration [438]: 0.0021899482486608607
***** Warning: Loss has increased *****
Loss at iteration [439]: 0.0021833135178349733
Loss at iteration [440]: 0.00218511637831444
***** Warning: Loss has increased *****
Loss at iteration [441]: 0.0021848085715112106
Loss at iteration [442]: 0.0021820582798866947
Loss at iteration [443]: 0.0021860814495034183
***** Warning: Loss has increased *****
Loss at iteration [444]: 0.002181176292456374
Loss at iteration [445]: 0.0021836406970380676
***** Warning: Loss has increased *****
Loss at iteration [446]: 0.002183299366136806
Loss at iteration [447]: 0.002180475168464091
Loss at iteration [448]: 0.0021824740131825525
***** Warning: Loss has increased *****
Loss at iteration [449]: 0.0021818683692918423
Loss at iteration [450]: 0.0021799256043048038
Loss at iteration [451]: 0.002180841361097586
***** Warning: Loss has increased *****
Loss at iteration [452]: 0.0021813306064964967
***** Warning: Loss has increased *****
Loss at iteration [453]: 0.002179916713469132
Loss at iteration [454]: 0.0021797335844614923
Loss at iteration [455]: 0.002179869061969234
***** Warning: Loss has increased *****
Loss at iteration [456]: 0.002181219859737931
***** Warning: Loss has increased *****
Loss at iteration [457]: 0.0021796194178455228
Loss at iteration [458]: 0.0021810059510823728
***** Warning: Loss has increased *****
Loss at iteration [459]: 0.002182691458752821
***** Warning: Loss has increased *****
Loss at iteration [460]: 0.002179159907315072
Loss at iteration [461]: 0.0021856390512012126
***** Warning: Loss has increased *****
Loss at iteration [462]: 0.0021835476383125977
Loss at iteration [463]: 0.0021776191362089118
Loss at iteration [464]: 0.0021845453845950884
***** Warning: Loss has increased *****
Loss at iteration [465]: 0.002180921985165927
Loss at iteration [466]: 0.00217826125907869
Loss at iteration [467]: 0.0021844031974332474
***** Warning: Loss has increased *****
Loss at iteration [468]: 0.0021797016388455702
Loss at iteration [469]: 0.0021782508440067426
Loss at iteration [470]: 0.0021837019006354405
***** Warning: Loss has increased *****
Loss at iteration [471]: 0.0021779208420673038
Loss at iteration [472]: 0.002178168036078926
***** Warning: Loss has increased *****
Loss at iteration [473]: 0.002182092313852957
***** Warning: Loss has increased *****
Loss at iteration [474]: 0.0021785492154815064
Loss at iteration [475]: 0.0021766479083042143
Loss at iteration [476]: 0.002183918332107192
***** Warning: Loss has increased *****
Loss at iteration [477]: 0.0021797166261992105
Loss at iteration [478]: 0.0021765132231940116
Loss at iteration [479]: 0.0021803377640623235
***** Warning: Loss has increased *****
Loss at iteration [480]: 0.0021804506683384376
***** Warning: Loss has increased *****
Loss at iteration [481]: 0.0021764317596927683
Loss at iteration [482]: 0.0021840061939143044
***** Warning: Loss has increased *****
Loss at iteration [483]: 0.002181889290479926
Loss at iteration [484]: 0.0021764494760469397
Loss at iteration [485]: 0.002189691851278285
***** Warning: Loss has increased *****
Loss at iteration [486]: 0.0021812172792931746
Loss at iteration [487]: 0.002176422887417457
Loss at iteration [488]: 0.002188290759958775
***** Warning: Loss has increased *****
Loss at iteration [489]: 0.002177584842230281
Loss at iteration [490]: 0.0021781706339877777
***** Warning: Loss has increased *****
Loss at iteration [491]: 0.002183815047605487
***** Warning: Loss has increased *****
Loss at iteration [492]: 0.002175242564186215
Loss at iteration [493]: 0.00217970332120768
***** Warning: Loss has increased *****
Loss at iteration [494]: 0.0021796456304720244
Loss at iteration [495]: 0.002175883627422193
Loss at iteration [496]: 0.002180720888643731
***** Warning: Loss has increased *****
Loss at iteration [497]: 0.0021737514841443737
Loss at iteration [498]: 0.0021778414137149566
***** Warning: Loss has increased *****
Loss at iteration [499]: 0.00217851109046063
***** Warning: Loss has increased *****
Loss at iteration [500]: 0.0021731390485458324
Loss at iteration [501]: 0.0021782304662324057
***** Warning: Loss has increased *****
Loss at iteration [502]: 0.0021769772425874324
Loss at iteration [503]: 0.002173323618274014
Loss at iteration [504]: 0.0021786219578799173
***** Warning: Loss has increased *****
Loss at iteration [505]: 0.0021773728430393067
Loss at iteration [506]: 0.0021743272225655317
Loss at iteration [507]: 0.002181287956145943
***** Warning: Loss has increased *****
Loss at iteration [508]: 0.0021754824238855376
Loss at iteration [509]: 0.002174165567012445
Loss at iteration [510]: 0.0021811496136453397
***** Warning: Loss has increased *****
Loss at iteration [511]: 0.002172286081546929
Loss at iteration [512]: 0.002177578313007731
***** Warning: Loss has increased *****
Loss at iteration [513]: 0.002180567252352373
***** Warning: Loss has increased *****
Loss at iteration [514]: 0.0021727914596463512
Loss at iteration [515]: 0.002179066742796024
***** Warning: Loss has increased *****
Loss at iteration [516]: 0.0021732663811007847
Loss at iteration [517]: 0.00217321835377585
Loss at iteration [518]: 0.0021754642361478294
***** Warning: Loss has increased *****
Loss at iteration [519]: 0.002170382573345475
Loss at iteration [520]: 0.0021733368952115252
***** Warning: Loss has increased *****
Loss at iteration [521]: 0.0021729193018997345
Loss at iteration [522]: 0.0021701521318397998
Loss at iteration [523]: 0.0021740220642111857
***** Warning: Loss has increased *****
Loss at iteration [524]: 0.002175122520164739
***** Warning: Loss has increased *****
Loss at iteration [525]: 0.0021699034899994247
Loss at iteration [526]: 0.0021787112215796065
***** Warning: Loss has increased *****
Loss at iteration [527]: 0.002177732473173292
Loss at iteration [528]: 0.002170823056805902
Loss at iteration [529]: 0.0021823588499314643
***** Warning: Loss has increased *****
Loss at iteration [530]: 0.0021733567641246364
Loss at iteration [531]: 0.0021722789261208685
Loss at iteration [532]: 0.002180124215814233
***** Warning: Loss has increased *****
Loss at iteration [533]: 0.002169389435275761
Loss at iteration [534]: 0.0021742294140626238
***** Warning: Loss has increased *****
Loss at iteration [535]: 0.002174396115290501
***** Warning: Loss has increased *****
Loss at iteration [536]: 0.0021686601659041717
Loss at iteration [537]: 0.002174621349362867
***** Warning: Loss has increased *****
Loss at iteration [538]: 0.0021703317492789854
Loss at iteration [539]: 0.002169282301257683
Loss at iteration [540]: 0.0021737028991854574
***** Warning: Loss has increased *****
Loss at iteration [541]: 0.0021675897365348125
Loss at iteration [542]: 0.0021705021591722448
***** Warning: Loss has increased *****
Loss at iteration [543]: 0.0021701761774974068
Loss at iteration [544]: 0.0021675175152348273
Loss at iteration [545]: 0.002170069456346003
***** Warning: Loss has increased *****
Loss at iteration [546]: 0.002168770125324107
Loss at iteration [547]: 0.0021670577430221
Loss at iteration [548]: 0.002171044680601361
***** Warning: Loss has increased *****
Loss at iteration [549]: 0.002170233123714221
Loss at iteration [550]: 0.0021669846313409325
Loss at iteration [551]: 0.0021718077561490847
***** Warning: Loss has increased *****
Loss at iteration [552]: 0.0021678169974856374
Loss at iteration [553]: 0.0021678325406038227
***** Warning: Loss has increased *****
Loss at iteration [554]: 0.002170848979860033
***** Warning: Loss has increased *****
Loss at iteration [555]: 0.002168518606093403
Loss at iteration [556]: 0.0021674503665565902
Loss at iteration [557]: 0.0021708879120541096
***** Warning: Loss has increased *****
Loss at iteration [558]: 0.002167162671354088
Loss at iteration [559]: 0.0021663726181370235
Loss at iteration [560]: 0.0021700694635989047
***** Warning: Loss has increased *****
Loss at iteration [561]: 0.002168023585750451
Loss at iteration [562]: 0.002165882766450689
Loss at iteration [563]: 0.00216878285093385
***** Warning: Loss has increased *****
Loss at iteration [564]: 0.00216692859333296
Loss at iteration [565]: 0.0021646658433153807
Loss at iteration [566]: 0.0021698054875731333
***** Warning: Loss has increased *****
Loss at iteration [567]: 0.0021682500042837977
Loss at iteration [568]: 0.0021647904173144467
Loss at iteration [569]: 0.002169787612352644
***** Warning: Loss has increased *****
Loss at iteration [570]: 0.0021722781631865044
***** Warning: Loss has increased *****
Loss at iteration [571]: 0.002164936465046701
Loss at iteration [572]: 0.0021722062728400993
***** Warning: Loss has increased *****
Loss at iteration [573]: 0.0021761005667254416
***** Warning: Loss has increased *****
Loss at iteration [574]: 0.0021645023989443754
Loss at iteration [575]: 0.0021794592168828996
***** Warning: Loss has increased *****
Loss at iteration [576]: 0.0021743117337618342
Loss at iteration [577]: 0.0021646767521948905
Loss at iteration [578]: 0.002181386873586097
***** Warning: Loss has increased *****
Loss at iteration [579]: 0.002171133486997996
Loss at iteration [580]: 0.0021663663183069206
Loss at iteration [581]: 0.002179452895528458
***** Warning: Loss has increased *****
Loss at iteration [582]: 0.0021653806871304417
Loss at iteration [583]: 0.002168474032625237
***** Warning: Loss has increased *****
Loss at iteration [584]: 0.002172755776439385
***** Warning: Loss has increased *****
Loss at iteration [585]: 0.002163088236936527
Loss at iteration [586]: 0.0021714894817471695
***** Warning: Loss has increased *****
Loss at iteration [587]: 0.0021681327996448917
Loss at iteration [588]: 0.0021630788117619117
Loss at iteration [589]: 0.0021676864550421917
***** Warning: Loss has increased *****
Loss at iteration [590]: 0.0021636705120202603
Loss at iteration [591]: 0.002164397684278995
***** Warning: Loss has increased *****
Loss at iteration [592]: 0.002166687745665805
***** Warning: Loss has increased *****
Loss at iteration [593]: 0.002161875834059546
Loss at iteration [594]: 0.0021648778619969256
***** Warning: Loss has increased *****
Loss at iteration [595]: 0.002165708083895062
***** Warning: Loss has increased *****
Loss at iteration [596]: 0.002161744241651249
Loss at iteration [597]: 0.0021658220806798968
***** Warning: Loss has increased *****
Loss at iteration [598]: 0.0021637890225128997
Loss at iteration [599]: 0.0021618339734892802
Loss at iteration [600]: 0.0021655262950119832
***** Warning: Loss has increased *****
Loss at iteration [601]: 0.0021636415748319446
Loss at iteration [602]: 0.0021620329974733773
Loss at iteration [603]: 0.002166164125681275
***** Warning: Loss has increased *****
Loss at iteration [604]: 0.0021626816669560373
Loss at iteration [605]: 0.0021618509370280956
Loss at iteration [606]: 0.0021647849496885083
***** Warning: Loss has increased *****
Loss at iteration [607]: 0.002161698242709896
Loss at iteration [608]: 0.002161615107741341
Loss at iteration [609]: 0.0021644269653224146
***** Warning: Loss has increased *****
Loss at iteration [610]: 0.0021613914757429194
Loss at iteration [611]: 0.002160495833420878
Loss at iteration [612]: 0.002161485983956301
***** Warning: Loss has increased *****
Loss at iteration [613]: 0.002160302716317004
Loss at iteration [614]: 0.002160805598366615
***** Warning: Loss has increased *****
Loss at iteration [615]: 0.00216218627592348
***** Warning: Loss has increased *****
Loss at iteration [616]: 0.002160237008540276
Loss at iteration [617]: 0.002159536444553001
Loss at iteration [618]: 0.0021614931318480724
***** Warning: Loss has increased *****
Loss at iteration [619]: 0.0021609728507141135
Loss at iteration [620]: 0.002159077858413696
Loss at iteration [621]: 0.002159978219357554
***** Warning: Loss has increased *****
Loss at iteration [622]: 0.0021615113027617255
***** Warning: Loss has increased *****
Loss at iteration [623]: 0.002159725599330839
Loss at iteration [624]: 0.002159570447669668
Loss at iteration [625]: 0.002160469820698606
***** Warning: Loss has increased *****
Loss at iteration [626]: 0.002159928302403129
Loss at iteration [627]: 0.002158813623625905
Loss at iteration [628]: 0.002159964672373055
***** Warning: Loss has increased *****
Loss at iteration [629]: 0.0021606160574465837
***** Warning: Loss has increased *****
Loss at iteration [630]: 0.002159178471647406
Loss at iteration [631]: 0.002158022576262208
Loss at iteration [632]: 0.002159908216302467
***** Warning: Loss has increased *****
Loss at iteration [633]: 0.0021612662802847843
***** Warning: Loss has increased *****
Loss at iteration [634]: 0.002158473029093091
Loss at iteration [635]: 0.002157673141209474
Loss at iteration [636]: 0.0021582892605584796
***** Warning: Loss has increased *****
Loss at iteration [637]: 0.0021575324395873563
Loss at iteration [638]: 0.0021572305149538604
Loss at iteration [639]: 0.002157062759716821
Loss at iteration [640]: 0.0021582394039969836
***** Warning: Loss has increased *****
Loss at iteration [641]: 0.0021593251309268385
***** Warning: Loss has increased *****
Loss at iteration [642]: 0.0021575979262051168
Loss at iteration [643]: 0.0021571558128470106
Loss at iteration [644]: 0.002158834376507773
***** Warning: Loss has increased *****
Loss at iteration [645]: 0.00215726810211928
Loss at iteration [646]: 0.0021563084031012113
Loss at iteration [647]: 0.0021560071681302724
Loss at iteration [648]: 0.0021565519068282783
***** Warning: Loss has increased *****
Loss at iteration [649]: 0.0021563478879986268
Loss at iteration [650]: 0.002156497210988705
***** Warning: Loss has increased *****
Loss at iteration [651]: 0.002156682972605114
***** Warning: Loss has increased *****
Loss at iteration [652]: 0.0021567917650088223
***** Warning: Loss has increased *****
Loss at iteration [653]: 0.0021561796052333763
Loss at iteration [654]: 0.0021570359743392988
***** Warning: Loss has increased *****
Loss at iteration [655]: 0.0021556423927879933
Loss at iteration [656]: 0.00215645429767429
***** Warning: Loss has increased *****
Loss at iteration [657]: 0.0021581965652170032
***** Warning: Loss has increased *****
Loss at iteration [658]: 0.002156497927154146
Loss at iteration [659]: 0.002155052243416805
Loss at iteration [660]: 0.0021563470860057254
***** Warning: Loss has increased *****
Loss at iteration [661]: 0.002157386165300486
***** Warning: Loss has increased *****
Loss at iteration [662]: 0.002155364408025254
Loss at iteration [663]: 0.00215529802440447
Loss at iteration [664]: 0.002155876560496144
***** Warning: Loss has increased *****
Loss at iteration [665]: 0.0021564540794078184
***** Warning: Loss has increased *****
Loss at iteration [666]: 0.002155569179337078
Loss at iteration [667]: 0.0021555623053913717
Loss at iteration [668]: 0.0021545619076559527
Loss at iteration [669]: 0.002154193058505738
Loss at iteration [670]: 0.002154552749717288
***** Warning: Loss has increased *****
Loss at iteration [671]: 0.0021546406184271296
***** Warning: Loss has increased *****
Loss at iteration [672]: 0.002156059203612412
***** Warning: Loss has increased *****
Loss at iteration [673]: 0.0021568617610741143
***** Warning: Loss has increased *****
Loss at iteration [674]: 0.0021569523397584485
***** Warning: Loss has increased *****
Loss at iteration [675]: 0.002154695336845301
Loss at iteration [676]: 0.002153734701522273
Loss at iteration [677]: 0.002153531554509857
Loss at iteration [678]: 0.0021533795930952803
Loss at iteration [679]: 0.0021546525957564884
***** Warning: Loss has increased *****
Loss at iteration [680]: 0.0021563107933232697
***** Warning: Loss has increased *****
Loss at iteration [681]: 0.002157474040883893
***** Warning: Loss has increased *****
Loss at iteration [682]: 0.0021546122690691335
Loss at iteration [683]: 0.0021529218310945906
Loss at iteration [684]: 0.0021580085137427657
***** Warning: Loss has increased *****
Loss at iteration [685]: 0.0021600623066279
***** Warning: Loss has increased *****
Loss at iteration [686]: 0.002160053122638859
Loss at iteration [687]: 0.0021549122301571444
Loss at iteration [688]: 0.0021532279582611802
Loss at iteration [689]: 0.0021554248429066333
***** Warning: Loss has increased *****
Loss at iteration [690]: 0.0021550083823763163
Loss at iteration [691]: 0.0021543388883748485
Loss at iteration [692]: 0.0021527522157652203
Loss at iteration [693]: 0.002152804419084787
***** Warning: Loss has increased *****
Loss at iteration [694]: 0.0021529130896367657
***** Warning: Loss has increased *****
Loss at iteration [695]: 0.002152159625132199
Loss at iteration [696]: 0.0021515722028612257
Loss at iteration [697]: 0.002152949876222969
***** Warning: Loss has increased *****
Loss at iteration [698]: 0.0021546329460697513
***** Warning: Loss has increased *****
Loss at iteration [699]: 0.002156704744134671
***** Warning: Loss has increased *****
Loss at iteration [700]: 0.0021603991156846085
***** Warning: Loss has increased *****
Loss at iteration [701]: 0.0021554723575737005
Loss at iteration [702]: 0.0021512244376616497
Loss at iteration [703]: 0.0021542809039172043
***** Warning: Loss has increased *****
Loss at iteration [704]: 0.002159754653494177
***** Warning: Loss has increased *****
Loss at iteration [705]: 0.0021648862283300168
***** Warning: Loss has increased *****
Loss at iteration [706]: 0.002165226868447941
***** Warning: Loss has increased *****
Loss at iteration [707]: 0.0021600891731744618
Loss at iteration [708]: 0.0021522201744379353
Loss at iteration [709]: 0.0021531847568653194
***** Warning: Loss has increased *****
Loss at iteration [710]: 0.0021560625973181874
***** Warning: Loss has increased *****
Loss at iteration [711]: 0.002156233161574118
***** Warning: Loss has increased *****
Loss at iteration [712]: 0.002154119095772981
Loss at iteration [713]: 0.002151045216065358
Loss at iteration [714]: 0.002152341808871686
***** Warning: Loss has increased *****
Loss at iteration [715]: 0.002156763956853431
***** Warning: Loss has increased *****
Loss at iteration [716]: 0.0021586751302784773
***** Warning: Loss has increased *****
Loss at iteration [717]: 0.002156436327116266
Loss at iteration [718]: 0.0021510179958506037
Loss at iteration [719]: 0.0021504689601692015
Loss at iteration [720]: 0.0021511858363885954
***** Warning: Loss has increased *****
Loss at iteration [721]: 0.002150502122765869
Loss at iteration [722]: 0.0021498758698027674
Loss at iteration [723]: 0.0021495478937368453
Loss at iteration [724]: 0.002150207747670031
***** Warning: Loss has increased *****
Loss at iteration [725]: 0.0021515208963472387
***** Warning: Loss has increased *****
Loss at iteration [726]: 0.0021516921865761974
***** Warning: Loss has increased *****
Loss at iteration [727]: 0.002153005191592337
***** Warning: Loss has increased *****
Loss at iteration [728]: 0.002150690985437906
Loss at iteration [729]: 0.002150149448772854
Loss at iteration [730]: 0.002151742997592631
***** Warning: Loss has increased *****
Loss at iteration [731]: 0.0021513968421385515
Loss at iteration [732]: 0.002152744558307729
***** Warning: Loss has increased *****
Loss at iteration [733]: 0.002151909512601064
Loss at iteration [734]: 0.002149404131809609
Loss at iteration [735]: 0.00215015941507213
***** Warning: Loss has increased *****
Loss at iteration [736]: 0.0021535474192286613
***** Warning: Loss has increased *****
Loss at iteration [737]: 0.0021604330439625816
***** Warning: Loss has increased *****
Loss at iteration [738]: 0.002159539107096667
Loss at iteration [739]: 0.002159018269184636
Loss at iteration [740]: 0.0021519084061180087
Loss at iteration [741]: 0.0021535827803835874
***** Warning: Loss has increased *****
Loss at iteration [742]: 0.0021644713542075754
***** Warning: Loss has increased *****
Loss at iteration [743]: 0.0021666641530887366
***** Warning: Loss has increased *****
Loss at iteration [744]: 0.002157163236174429
Loss at iteration [745]: 0.0021480565991368007
Loss at iteration [746]: 0.002150037042958166
***** Warning: Loss has increased *****
Loss at iteration [747]: 0.0021527914764151216
***** Warning: Loss has increased *****
Loss at iteration [748]: 0.0021503908864839464
Loss at iteration [749]: 0.0021490081783687542
Loss at iteration [750]: 0.0021473295410661584
Loss at iteration [751]: 0.0021518443824009915
***** Warning: Loss has increased *****
Loss at iteration [752]: 0.002158303615748044
***** Warning: Loss has increased *****
Loss at iteration [753]: 0.0021522524491149352
Loss at iteration [754]: 0.002148618291344481
Loss at iteration [755]: 0.0021478167047118512
Loss at iteration [756]: 0.00215003353791889
***** Warning: Loss has increased *****
Loss at iteration [757]: 0.0021524412402663174
***** Warning: Loss has increased *****
Loss at iteration [758]: 0.0021475194092853353
Loss at iteration [759]: 0.002146425145993391
Loss at iteration [760]: 0.0021476291528900932
***** Warning: Loss has increased *****
Loss at iteration [761]: 0.0021472650437741295
Loss at iteration [762]: 0.002146908761052134
Loss at iteration [763]: 0.0021465090678935714
Loss at iteration [764]: 0.0021462047927808866
Loss at iteration [765]: 0.002147840233933271
***** Warning: Loss has increased *****
Loss at iteration [766]: 0.002148633967789833
***** Warning: Loss has increased *****
Loss at iteration [767]: 0.002150926530229636
***** Warning: Loss has increased *****
Loss at iteration [768]: 0.002148670667932901
Loss at iteration [769]: 0.002146013149240868
Loss at iteration [770]: 0.0021469207201759728
***** Warning: Loss has increased *****
Loss at iteration [771]: 0.0021511788848131946
***** Warning: Loss has increased *****
Loss at iteration [772]: 0.002157445173396119
***** Warning: Loss has increased *****
Loss at iteration [773]: 0.002154881277036236
Loss at iteration [774]: 0.002154466939764279
Loss at iteration [775]: 0.0021474019556962814
Loss at iteration [776]: 0.002149427539957065
***** Warning: Loss has increased *****
Loss at iteration [777]: 0.002157655648225243
***** Warning: Loss has increased *****
Loss at iteration [778]: 0.0021582881871028982
***** Warning: Loss has increased *****
Loss at iteration [779]: 0.0021575664895493894
Loss at iteration [780]: 0.00214806389262742
Loss at iteration [781]: 0.0021466868369104477
Loss at iteration [782]: 0.002150277810284623
***** Warning: Loss has increased *****
Loss at iteration [783]: 0.0021492666154914243
Loss at iteration [784]: 0.002149993480758822
***** Warning: Loss has increased *****
Loss at iteration [785]: 0.0021463778486986496
Loss at iteration [786]: 0.00214515107392749
Loss at iteration [787]: 0.0021444179654614907
Loss at iteration [788]: 0.0021460030055474952
***** Warning: Loss has increased *****
Loss at iteration [789]: 0.0021464564364230714
***** Warning: Loss has increased *****
Loss at iteration [790]: 0.0021449236130836363
Loss at iteration [791]: 0.0021435965987448543
Loss at iteration [792]: 0.0021440592514163003
***** Warning: Loss has increased *****
Loss at iteration [793]: 0.0021461163193672887
***** Warning: Loss has increased *****
Loss at iteration [794]: 0.0021501451662601684
***** Warning: Loss has increased *****
Loss at iteration [795]: 0.0021491838445273207
Loss at iteration [796]: 0.002149237744535764
***** Warning: Loss has increased *****
Loss at iteration [797]: 0.002144878744208258
Loss at iteration [798]: 0.0021444990515709366
Loss at iteration [799]: 0.0021509151366237295
***** Warning: Loss has increased *****
Loss at iteration [800]: 0.0021523159867563044
***** Warning: Loss has increased *****
Loss at iteration [801]: 0.0021540693377951167
***** Warning: Loss has increased *****
Loss at iteration [802]: 0.002147078247401531
Loss at iteration [803]: 0.0021443336096489433
Loss at iteration [804]: 0.0021468257136015687
***** Warning: Loss has increased *****
Loss at iteration [805]: 0.002148459423183537
***** Warning: Loss has increased *****
Loss at iteration [806]: 0.002150518700659401
***** Warning: Loss has increased *****
Loss at iteration [807]: 0.0021462188382810757
Loss at iteration [808]: 0.00214472347359171
Loss at iteration [809]: 0.0021438291654490675
Loss at iteration [810]: 0.0021442854248683993
***** Warning: Loss has increased *****
Loss at iteration [811]: 0.0021454655481347347
***** Warning: Loss has increased *****
Loss at iteration [812]: 0.002144515753685241
Loss at iteration [813]: 0.0021432468547203983
Loss at iteration [814]: 0.002142769256343648
Loss at iteration [815]: 0.0021424951390552564
Loss at iteration [816]: 0.0021423555755306785
Loss at iteration [817]: 0.0021425426442671547
***** Warning: Loss has increased *****
Loss at iteration [818]: 0.0021425252813334172
Loss at iteration [819]: 0.0021424438638239
Loss at iteration [820]: 0.002144422784137289
***** Warning: Loss has increased *****
Loss at iteration [821]: 0.0021438345831119687
Loss at iteration [822]: 0.0021423448641610886
Loss at iteration [823]: 0.0021412732041645377
Loss at iteration [824]: 0.002143377359499613
***** Warning: Loss has increased *****
Loss at iteration [825]: 0.002148091374088521
***** Warning: Loss has increased *****
Loss at iteration [826]: 0.002150186536885557
***** Warning: Loss has increased *****
Loss at iteration [827]: 0.0021600494184622256
***** Warning: Loss has increased *****
Loss at iteration [828]: 0.0021494781420677528
Loss at iteration [829]: 0.0021440939116934314
Loss at iteration [830]: 0.002148474827365341
***** Warning: Loss has increased *****
Loss at iteration [831]: 0.0021565317585282204
***** Warning: Loss has increased *****
Loss at iteration [832]: 0.0021634927951216416
***** Warning: Loss has increased *****
Loss at iteration [833]: 0.002159070816336054
Loss at iteration [834]: 0.0021578583120367453
Loss at iteration [835]: 0.002146688025100374
Loss at iteration [836]: 0.0021482989484698906
***** Warning: Loss has increased *****
Loss at iteration [837]: 0.0021545131359887136
***** Warning: Loss has increased *****
Loss at iteration [838]: 0.002151602523230097
Loss at iteration [839]: 0.0021445822520141456
Loss at iteration [840]: 0.0021418894788771846
Loss at iteration [841]: 0.002144421104802524
***** Warning: Loss has increased *****
Loss at iteration [842]: 0.002147907048373281
***** Warning: Loss has increased *****
Loss at iteration [843]: 0.0021433683514858986
Loss at iteration [844]: 0.002140940931636741
Loss at iteration [845]: 0.0021464991776427708
***** Warning: Loss has increased *****
Loss at iteration [846]: 0.002146215526451847
Loss at iteration [847]: 0.00214520709658983
Loss at iteration [848]: 0.002142294014771008
Loss at iteration [849]: 0.0021433425882008347
***** Warning: Loss has increased *****
Loss at iteration [850]: 0.002144244956104009
***** Warning: Loss has increased *****
Loss at iteration [851]: 0.0021433808349418156
Loss at iteration [852]: 0.0021425412551319837
Loss at iteration [853]: 0.002141498915110757
Loss at iteration [854]: 0.002141706939412811
***** Warning: Loss has increased *****
Loss at iteration [855]: 0.00214077818927017
Loss at iteration [856]: 0.002140462872978758
Loss at iteration [857]: 0.0021394391750020245
Loss at iteration [858]: 0.002141535788603699
***** Warning: Loss has increased *****
Loss at iteration [859]: 0.002146222457650918
***** Warning: Loss has increased *****
Loss at iteration [860]: 0.0021470842662767867
***** Warning: Loss has increased *****
Loss at iteration [861]: 0.002147593205509922
***** Warning: Loss has increased *****
Loss at iteration [862]: 0.0021427030913977823
Loss at iteration [863]: 0.0021392534619499777
Loss at iteration [864]: 0.0021419762720026655
***** Warning: Loss has increased *****
Loss at iteration [865]: 0.0021492252993876673
***** Warning: Loss has increased *****
Loss at iteration [866]: 0.002154124438762991
***** Warning: Loss has increased *****
Loss at iteration [867]: 0.0021519830817775232
Loss at iteration [868]: 0.00214999630262948
Loss at iteration [869]: 0.002142552885343143
Loss at iteration [870]: 0.002142466883535022
Loss at iteration [871]: 0.00214736238442143
***** Warning: Loss has increased *****
Loss at iteration [872]: 0.002149974874851521
***** Warning: Loss has increased *****
Loss at iteration [873]: 0.002147606574783663
Loss at iteration [874]: 0.0021406574240437617
Loss at iteration [875]: 0.002141196110577233
***** Warning: Loss has increased *****
Loss at iteration [876]: 0.0021432950664758018
***** Warning: Loss has increased *****
Loss at iteration [877]: 0.0021443157480629063
***** Warning: Loss has increased *****
Loss at iteration [878]: 0.002147861232057077
***** Warning: Loss has increased *****
Loss at iteration [879]: 0.00214559251788844
Loss at iteration [880]: 0.002143691751855046
Loss at iteration [881]: 0.002139414378636508
Loss at iteration [882]: 0.0021395453092606788
***** Warning: Loss has increased *****
Loss at iteration [883]: 0.0021401209128955392
***** Warning: Loss has increased *****
Loss at iteration [884]: 0.0021415611476973467
***** Warning: Loss has increased *****
Loss at iteration [885]: 0.0021435916819465815
***** Warning: Loss has increased *****
Loss at iteration [886]: 0.0021415134149728003
Loss at iteration [887]: 0.0021392377159444296
Loss at iteration [888]: 0.002137440617334775
Loss at iteration [889]: 0.0021383465982798134
***** Warning: Loss has increased *****
Loss at iteration [890]: 0.002141501938415494
***** Warning: Loss has increased *****
Loss at iteration [891]: 0.002140832760010121
Loss at iteration [892]: 0.002140250478926397
Loss at iteration [893]: 0.002138502847546989
Loss at iteration [894]: 0.002137733750855315
Loss at iteration [895]: 0.002137239285723221
Loss at iteration [896]: 0.002139752859498591
***** Warning: Loss has increased *****
Loss at iteration [897]: 0.0021452732539196
***** Warning: Loss has increased *****
Loss at iteration [898]: 0.0021486014410252896
***** Warning: Loss has increased *****
Loss at iteration [899]: 0.0021525507210337492
***** Warning: Loss has increased *****
Loss at iteration [900]: 0.002142725571578782
Loss at iteration [901]: 0.002138707914704904
Loss at iteration [902]: 0.002145895960318785
***** Warning: Loss has increased *****
Loss at iteration [903]: 0.0021540477111408715
***** Warning: Loss has increased *****
Loss at iteration [904]: 0.002159839857969768
***** Warning: Loss has increased *****
Loss at iteration [905]: 0.0021515975338016096
Loss at iteration [906]: 0.0021430839749932676
Loss at iteration [907]: 0.002138871560819573
Loss at iteration [908]: 0.0021424933109199405
***** Warning: Loss has increased *****
Loss at iteration [909]: 0.0021532212783196107
***** Warning: Loss has increased *****
Loss at iteration [910]: 0.0021505239162102345
Loss at iteration [911]: 0.0021466478611850918
Loss at iteration [912]: 0.0021388786691489887
Loss at iteration [913]: 0.002138698844361633
Loss at iteration [914]: 0.002143098987339014
***** Warning: Loss has increased *****
Loss at iteration [915]: 0.0021428795834083545
Loss at iteration [916]: 0.0021420927639802145
Loss at iteration [917]: 0.0021372355386465214
Loss at iteration [918]: 0.002136162677004426
Loss at iteration [919]: 0.0021359714651921495
Loss at iteration [920]: 0.002135667669244517
Loss at iteration [921]: 0.0021353736696615457
Loss at iteration [922]: 0.002135672924085043
***** Warning: Loss has increased *****
Loss at iteration [923]: 0.0021354716938304674
Loss at iteration [924]: 0.0021351417353930994
Loss at iteration [925]: 0.002135559047354796
***** Warning: Loss has increased *****
Loss at iteration [926]: 0.0021376122933666674
***** Warning: Loss has increased *****
Loss at iteration [927]: 0.0021354240440242833
Loss at iteration [928]: 0.0021343732142388772
Loss at iteration [929]: 0.002135179875072621
***** Warning: Loss has increased *****
Loss at iteration [930]: 0.0021356469483126924
***** Warning: Loss has increased *****
Loss at iteration [931]: 0.002137091478650449
***** Warning: Loss has increased *****
Loss at iteration [932]: 0.002138737872415806
***** Warning: Loss has increased *****
Loss at iteration [933]: 0.002136605944340061
Loss at iteration [934]: 0.0021338241231694355
Loss at iteration [935]: 0.002138219542222271
***** Warning: Loss has increased *****
Loss at iteration [936]: 0.002147869046286469
***** Warning: Loss has increased *****
Loss at iteration [937]: 0.002153323861370477
***** Warning: Loss has increased *****
Loss at iteration [938]: 0.002162379074976018
***** Warning: Loss has increased *****
Loss at iteration [939]: 0.0021430635530479197
Loss at iteration [940]: 0.002146423041194236
***** Warning: Loss has increased *****
Loss at iteration [941]: 0.002159972782557267
***** Warning: Loss has increased *****
Loss at iteration [942]: 0.002155122056447361
Loss at iteration [943]: 0.0021482044118444933
Loss at iteration [944]: 0.0021389983690427803
Loss at iteration [945]: 0.0021357646570569835
Loss at iteration [946]: 0.002137288670729325
***** Warning: Loss has increased *****
Loss at iteration [947]: 0.0021402863214115327
***** Warning: Loss has increased *****
Loss at iteration [948]: 0.002139752201549756
Loss at iteration [949]: 0.0021365387615930177
Loss at iteration [950]: 0.002134492054144369
Loss at iteration [951]: 0.0021342946893394485
Loss at iteration [952]: 0.0021362729152349293
***** Warning: Loss has increased *****
Loss at iteration [953]: 0.0021373953495694457
***** Warning: Loss has increased *****
Loss at iteration [954]: 0.0021354811278788814
Loss at iteration [955]: 0.00213432115042572
Loss at iteration [956]: 0.0021339159171931466
Loss at iteration [957]: 0.002133641008537249
Loss at iteration [958]: 0.0021336455625565586
***** Warning: Loss has increased *****
Loss at iteration [959]: 0.002135394736952652
***** Warning: Loss has increased *****
Loss at iteration [960]: 0.0021398223098167777
***** Warning: Loss has increased *****
Loss at iteration [961]: 0.002138523448076513
Loss at iteration [962]: 0.002136654453131551
Loss at iteration [963]: 0.0021331909305291235
Loss at iteration [964]: 0.002132569911426871
Loss at iteration [965]: 0.0021317205278364973
Loss at iteration [966]: 0.0021316506331879133
Loss at iteration [967]: 0.002132998585511884
***** Warning: Loss has increased *****
Loss at iteration [968]: 0.002134554094034943
***** Warning: Loss has increased *****
Loss at iteration [969]: 0.0021391039715984756
***** Warning: Loss has increased *****
Loss at iteration [970]: 0.0021390238306367537
Loss at iteration [971]: 0.0021402641926010057
***** Warning: Loss has increased *****
Loss at iteration [972]: 0.0021396591466552547
Loss at iteration [973]: 0.002137299975959583
Loss at iteration [974]: 0.002132903693708678
Loss at iteration [975]: 0.002133768379509326
***** Warning: Loss has increased *****
Loss at iteration [976]: 0.002132738233605033
Loss at iteration [977]: 0.002130896291713623
Loss at iteration [978]: 0.0021323139401769023
***** Warning: Loss has increased *****
Loss at iteration [979]: 0.0021330944761464793
***** Warning: Loss has increased *****
Loss at iteration [980]: 0.002132140039500911
Loss at iteration [981]: 0.002131205425670579
Loss at iteration [982]: 0.002132066892898959
***** Warning: Loss has increased *****
Loss at iteration [983]: 0.00213209502123424
***** Warning: Loss has increased *****
Loss at iteration [984]: 0.0021319108996987603
Loss at iteration [985]: 0.0021306561738073694
Loss at iteration [986]: 0.002131356178859665
***** Warning: Loss has increased *****
Loss at iteration [987]: 0.002131270980389589
Loss at iteration [988]: 0.002130278297601816
Loss at iteration [989]: 0.0021320451341145354
***** Warning: Loss has increased *****
Loss at iteration [990]: 0.002135134971181701
***** Warning: Loss has increased *****
Loss at iteration [991]: 0.0021384069141992326
***** Warning: Loss has increased *****
Loss at iteration [992]: 0.002150399571969388
***** Warning: Loss has increased *****
Loss at iteration [993]: 0.002146548352343451
Loss at iteration [994]: 0.002139755906875273
Loss at iteration [995]: 0.0021346549818047973
Loss at iteration [996]: 0.0021332575259308184
Loss at iteration [997]: 0.0021297833430463315
Loss at iteration [998]: 0.002129372197444557
Loss at iteration [999]: 0.0021299456804754393
***** Warning: Loss has increased *****
Loss at iteration [1000]: 0.0021297326478835737
Loss at iteration [1001]: 0.002130286994042394
***** Warning: Loss has increased *****
Loss at iteration [1002]: 0.0021296782215983914
Loss at iteration [1003]: 0.002128989752512929
Loss at iteration [1004]: 0.0021305695494203175
***** Warning: Loss has increased *****
Loss at iteration [1005]: 0.0021331174022316637
***** Warning: Loss has increased *****
Loss at iteration [1006]: 0.002133182280506762
***** Warning: Loss has increased *****
Loss at iteration [1007]: 0.0021376266969795952
***** Warning: Loss has increased *****
Loss at iteration [1008]: 0.0021389623459205052
***** Warning: Loss has increased *****
Loss at iteration [1009]: 0.0021460801638957332
***** Warning: Loss has increased *****
Loss at iteration [1010]: 0.002146357524607847
***** Warning: Loss has increased *****
Loss at iteration [1011]: 0.0021472387715542763
***** Warning: Loss has increased *****
Loss at iteration [1012]: 0.002138062922761834
Loss at iteration [1013]: 0.00213386496054167
Loss at iteration [1014]: 0.002129325703041245
Loss at iteration [1015]: 0.0021272490798532385
Loss at iteration [1016]: 0.0021281689450397536
***** Warning: Loss has increased *****
Loss at iteration [1017]: 0.0021284638656110574
***** Warning: Loss has increased *****
Loss at iteration [1018]: 0.002129458829670005
***** Warning: Loss has increased *****
Loss at iteration [1019]: 0.0021307280115514587
***** Warning: Loss has increased *****
Loss at iteration [1020]: 0.002132696984797231
***** Warning: Loss has increased *****
Loss at iteration [1021]: 0.0021322884807183622
Loss at iteration [1022]: 0.002136589674452974
***** Warning: Loss has increased *****
Loss at iteration [1023]: 0.002136381053744372
Loss at iteration [1024]: 0.002138595900461815
***** Warning: Loss has increased *****
Loss at iteration [1025]: 0.0021368594707548002
Loss at iteration [1026]: 0.0021374306419747278
***** Warning: Loss has increased *****
Loss at iteration [1027]: 0.0021335972274928636
Loss at iteration [1028]: 0.002128572539472269
Loss at iteration [1029]: 0.002127199499562044
Loss at iteration [1030]: 0.0021279759028659394
***** Warning: Loss has increased *****
Loss at iteration [1031]: 0.0021317867893843948
***** Warning: Loss has increased *****
Loss at iteration [1032]: 0.002132996974502933
***** Warning: Loss has increased *****
Loss at iteration [1033]: 0.0021393846765233866
***** Warning: Loss has increased *****
Loss at iteration [1034]: 0.0021409019405418043
***** Warning: Loss has increased *****
Loss at iteration [1035]: 0.0021415281325844306
***** Warning: Loss has increased *****
Loss at iteration [1036]: 0.00213464771657802
Loss at iteration [1037]: 0.002130037865957859
Loss at iteration [1038]: 0.0021264360202530863
Loss at iteration [1039]: 0.002129608430998046
***** Warning: Loss has increased *****
Loss at iteration [1040]: 0.0021392953848598188
***** Warning: Loss has increased *****
Loss at iteration [1041]: 0.002146218240604387
***** Warning: Loss has increased *****
Loss at iteration [1042]: 0.002159773043439894
***** Warning: Loss has increased *****
Loss at iteration [1043]: 0.002145878192443229
Loss at iteration [1044]: 0.00213318697368085
Loss at iteration [1045]: 0.0021309529517050977
Loss at iteration [1046]: 0.002137107642944757
***** Warning: Loss has increased *****
Loss at iteration [1047]: 0.0021571731449441267
***** Warning: Loss has increased *****
Loss at iteration [1048]: 0.0021666537338030497
***** Warning: Loss has increased *****
Loss at iteration [1049]: 0.002172495721135553
***** Warning: Loss has increased *****
Loss at iteration [1050]: 0.0021534380742421286
Loss at iteration [1051]: 0.002169818573071342
***** Warning: Loss has increased *****
Loss at iteration [1052]: 0.0021749971741627672
***** Warning: Loss has increased *****
Loss at iteration [1053]: 0.0021307034061910095
Loss at iteration [1054]: 0.0021607243424485844
***** Warning: Loss has increased *****
Loss at iteration [1055]: 0.002206562739674169
***** Warning: Loss has increased *****
Loss at iteration [1056]: 0.00214714367197674
Loss at iteration [1057]: 0.002190549428113235
***** Warning: Loss has increased *****
Loss at iteration [1058]: 0.002211197965981101
***** Warning: Loss has increased *****
Loss at iteration [1059]: 0.0021319386912272017
Loss at iteration [1060]: 0.002254450163658016
***** Warning: Loss has increased *****
Loss at iteration [1061]: 0.0022597172400570277
***** Warning: Loss has increased *****
Loss at iteration [1062]: 0.00221573203230215
Loss at iteration [1063]: 0.002257522766458385
***** Warning: Loss has increased *****
Loss at iteration [1064]: 0.0022224428992935817
Loss at iteration [1065]: 0.002182387725985673
Loss at iteration [1066]: 0.0021822228408183196
Loss at iteration [1067]: 0.002252478130973841
***** Warning: Loss has increased *****
Loss at iteration [1068]: 0.0021961544441895047
Loss at iteration [1069]: 0.0021904116471630655
Loss at iteration [1070]: 0.0022135128762399087
***** Warning: Loss has increased *****
Loss at iteration [1071]: 0.002161575496101756
Loss at iteration [1072]: 0.002180095704916156
***** Warning: Loss has increased *****
Loss at iteration [1073]: 0.0021519935820277633
Loss at iteration [1074]: 0.0021901807637017315
***** Warning: Loss has increased *****
Loss at iteration [1075]: 0.002152372322126113
Loss at iteration [1076]: 0.002181699353746061
***** Warning: Loss has increased *****
Loss at iteration [1077]: 0.002161422084575626
Loss at iteration [1078]: 0.0021583557789639114
Loss at iteration [1079]: 0.0021443884471573807
Loss at iteration [1080]: 0.002146265527950404
***** Warning: Loss has increased *****
Loss at iteration [1081]: 0.0021564478915956143
***** Warning: Loss has increased *****
Loss at iteration [1082]: 0.002140561360244431
Loss at iteration [1083]: 0.0021602051932979064
***** Warning: Loss has increased *****
Loss at iteration [1084]: 0.0021429674981128626
Loss at iteration [1085]: 0.002148330496829912
***** Warning: Loss has increased *****
Loss at iteration [1086]: 0.0021366544083426336
Loss at iteration [1087]: 0.002136828993954926
***** Warning: Loss has increased *****
Loss at iteration [1088]: 0.0021421010433795838
***** Warning: Loss has increased *****
Loss at iteration [1089]: 0.002133692234831945
Loss at iteration [1090]: 0.0021423174341875096
***** Warning: Loss has increased *****
Loss at iteration [1091]: 0.0021345027731607806
Loss at iteration [1092]: 0.0021387705769933475
***** Warning: Loss has increased *****
Loss at iteration [1093]: 0.0021323513563657887
Loss at iteration [1094]: 0.002130302447148235
Loss at iteration [1095]: 0.0021310669290283205
***** Warning: Loss has increased *****
Loss at iteration [1096]: 0.0021262496958126785
Loss at iteration [1097]: 0.002133079895753217
***** Warning: Loss has increased *****
Loss at iteration [1098]: 0.0021294391184287762
Loss at iteration [1099]: 0.0021296199236191618
***** Warning: Loss has increased *****
Loss at iteration [1100]: 0.0021289028518496275
Loss at iteration [1101]: 0.0021277541715375293
Loss at iteration [1102]: 0.0021274453685605267
Loss at iteration [1103]: 0.0021262711799168174
Loss at iteration [1104]: 0.0021255727629453203
Loss at iteration [1105]: 0.0021250961121240873
Loss at iteration [1106]: 0.0021241196990286967
Loss at iteration [1107]: 0.0021253463419460407
***** Warning: Loss has increased *****
Loss at iteration [1108]: 0.0021268012105339156
***** Warning: Loss has increased *****
Loss at iteration [1109]: 0.002128090325750949
***** Warning: Loss has increased *****
Loss at iteration [1110]: 0.002126781758177623
Loss at iteration [1111]: 0.0021264556385265144
Loss at iteration [1112]: 0.002125303362518114
Loss at iteration [1113]: 0.0021238635705178245
Loss at iteration [1114]: 0.002123962195182137
***** Warning: Loss has increased *****
Loss at iteration [1115]: 0.002125559447969224
***** Warning: Loss has increased *****
Loss at iteration [1116]: 0.0021232785374665805
Loss at iteration [1117]: 0.0021229827496578356
Loss at iteration [1118]: 0.0021244195692904817
***** Warning: Loss has increased *****
Loss at iteration [1119]: 0.00212208845553354
Loss at iteration [1120]: 0.0021234376050300423
***** Warning: Loss has increased *****
Loss at iteration [1121]: 0.0021226793340370135
Loss at iteration [1122]: 0.0021216241128942715
Loss at iteration [1123]: 0.0021223734225555378
***** Warning: Loss has increased *****
Loss at iteration [1124]: 0.002121329300115954
Loss at iteration [1125]: 0.002121610663280319
***** Warning: Loss has increased *****
Loss at iteration [1126]: 0.002120973569898966
Loss at iteration [1127]: 0.002120278618834852
Loss at iteration [1128]: 0.0021206259439413393
***** Warning: Loss has increased *****
Loss at iteration [1129]: 0.0021204283229558888
Loss at iteration [1130]: 0.0021198287942006157
Loss at iteration [1131]: 0.0021201380964973706
***** Warning: Loss has increased *****
Loss at iteration [1132]: 0.0021198143146931524
Loss at iteration [1133]: 0.002119748329648326
Loss at iteration [1134]: 0.00211963776021322
Loss at iteration [1135]: 0.0021199507248651894
***** Warning: Loss has increased *****
Loss at iteration [1136]: 0.002119210706002953
Loss at iteration [1137]: 0.0021196494069287823
***** Warning: Loss has increased *****
Loss at iteration [1138]: 0.0021201066714315424
***** Warning: Loss has increased *****
Loss at iteration [1139]: 0.0021199736328779513
Loss at iteration [1140]: 0.002119239002677337
Loss at iteration [1141]: 0.002118541404977737
Loss at iteration [1142]: 0.00211862845816251
***** Warning: Loss has increased *****
Loss at iteration [1143]: 0.0021187436463053853
***** Warning: Loss has increased *****
Loss at iteration [1144]: 0.002118418501127518
Loss at iteration [1145]: 0.0021179060448480004
Loss at iteration [1146]: 0.002117791325139612
Loss at iteration [1147]: 0.002117832866183042
***** Warning: Loss has increased *****
Loss at iteration [1148]: 0.0021176418515533326
Loss at iteration [1149]: 0.0021180637098226294
***** Warning: Loss has increased *****
Loss at iteration [1150]: 0.002118199877056451
***** Warning: Loss has increased *****
Loss at iteration [1151]: 0.002118531738918307
***** Warning: Loss has increased *****
Loss at iteration [1152]: 0.002120281773419152
***** Warning: Loss has increased *****
Loss at iteration [1153]: 0.0021190779633403754
Loss at iteration [1154]: 0.0021180950999799882
Loss at iteration [1155]: 0.0021177147381726685
Loss at iteration [1156]: 0.0021181608851689308
***** Warning: Loss has increased *****
Loss at iteration [1157]: 0.0021201738638811753
***** Warning: Loss has increased *****
Loss at iteration [1158]: 0.0021220642076143557
***** Warning: Loss has increased *****
Loss at iteration [1159]: 0.002125506931198132
***** Warning: Loss has increased *****
Loss at iteration [1160]: 0.0021297717108244895
***** Warning: Loss has increased *****
Loss at iteration [1161]: 0.0021384636595855413
***** Warning: Loss has increased *****
Loss at iteration [1162]: 0.002155525922951241
***** Warning: Loss has increased *****
Loss at iteration [1163]: 0.0021875275310686646
***** Warning: Loss has increased *****
Loss at iteration [1164]: 0.0022395284448025214
***** Warning: Loss has increased *****
Loss at iteration [1165]: 0.0023339755657420494
***** Warning: Loss has increased *****
Loss at iteration [1166]: 0.00245275881046811
***** Warning: Loss has increased *****
Loss at iteration [1167]: 0.002671484696540071
***** Warning: Loss has increased *****
Loss at iteration [1168]: 0.0029198054227462787
***** Warning: Loss has increased *****
Loss at iteration [1169]: 0.003331114739346447
***** Warning: Loss has increased *****
Loss at iteration [1170]: 0.003656551258245025
***** Warning: Loss has increased *****
Loss at iteration [1171]: 0.004084916548920715
***** Warning: Loss has increased *****
Loss at iteration [1172]: 0.0037053384492665457
Loss at iteration [1173]: 0.0032072695341458933
Loss at iteration [1174]: 0.002351048012662381
Loss at iteration [1175]: 0.0023043009347652054
Loss at iteration [1176]: 0.0027273845314064763
***** Warning: Loss has increased *****
Loss at iteration [1177]: 0.0029625947477682694
***** Warning: Loss has increased *****
Loss at iteration [1178]: 0.0026302845120402814
Loss at iteration [1179]: 0.0022385405079052988
Loss at iteration [1180]: 0.0023765409362496016
***** Warning: Loss has increased *****
Loss at iteration [1181]: 0.002643691318053131
***** Warning: Loss has increased *****
Loss at iteration [1182]: 0.00241760407788163
Loss at iteration [1183]: 0.002197897374682072
Loss at iteration [1184]: 0.002374483609152318
***** Warning: Loss has increased *****
Loss at iteration [1185]: 0.002427210570262594
***** Warning: Loss has increased *****
Loss at iteration [1186]: 0.0022645456348160376
Loss at iteration [1187]: 0.0022133507322150033
Loss at iteration [1188]: 0.002360146147494267
***** Warning: Loss has increased *****
Loss at iteration [1189]: 0.0023063186205994333
Loss at iteration [1190]: 0.0021995676907288217
Loss at iteration [1191]: 0.002253817701180854
***** Warning: Loss has increased *****
Loss at iteration [1192]: 0.0023159028729736104
***** Warning: Loss has increased *****
Loss at iteration [1193]: 0.002210485238046031
Loss at iteration [1194]: 0.0022047536545008466
Loss at iteration [1195]: 0.002265449326905834
***** Warning: Loss has increased *****
Loss at iteration [1196]: 0.002226783061968198
Loss at iteration [1197]: 0.002178417703085143
Loss at iteration [1198]: 0.0022165092826388795
***** Warning: Loss has increased *****
Loss at iteration [1199]: 0.0022126472789230846
Loss at iteration [1200]: 0.0021592880154715356
Loss at iteration [1201]: 0.002166862846906832
***** Warning: Loss has increased *****
Loss at iteration [1202]: 0.002198740100224427
***** Warning: Loss has increased *****
Loss at iteration [1203]: 0.002173253305109186
Loss at iteration [1204]: 0.0021676361231745666
Loss at iteration [1205]: 0.0021745213646414984
***** Warning: Loss has increased *****
Loss at iteration [1206]: 0.002178718739514945
***** Warning: Loss has increased *****
Loss at iteration [1207]: 0.002150706325177061
Loss at iteration [1208]: 0.002155520828603785
***** Warning: Loss has increased *****
Loss at iteration [1209]: 0.0021699114590153458
***** Warning: Loss has increased *****
Loss at iteration [1210]: 0.0021516277032903376
Loss at iteration [1211]: 0.0021441335460818866
Loss at iteration [1212]: 0.00215930595798321
***** Warning: Loss has increased *****
Loss at iteration [1213]: 0.0021540281722935853
Loss at iteration [1214]: 0.002138832975052885
Loss at iteration [1215]: 0.0021450106084573003
***** Warning: Loss has increased *****
Loss at iteration [1216]: 0.0021491666444932653
***** Warning: Loss has increased *****
Loss at iteration [1217]: 0.0021387316952357713
Loss at iteration [1218]: 0.0021366048386247
Loss at iteration [1219]: 0.0021431335811865717
***** Warning: Loss has increased *****
Loss at iteration [1220]: 0.002140005991179936
Loss at iteration [1221]: 0.002132582810982315
Loss at iteration [1222]: 0.0021350543280025135
***** Warning: Loss has increased *****
Loss at iteration [1223]: 0.0021371842455009337
***** Warning: Loss has increased *****
Loss at iteration [1224]: 0.002132414101300473
Loss at iteration [1225]: 0.0021291949579835393
Loss at iteration [1226]: 0.002131376089386855
***** Warning: Loss has increased *****
Loss at iteration [1227]: 0.0021285907012131124
Loss at iteration [1228]: 0.0021244330575830527
Loss at iteration [1229]: 0.0021241914473937354
Loss at iteration [1230]: 0.0021246314633444523
***** Warning: Loss has increased *****
Loss at iteration [1231]: 0.0021234791199669766
Loss at iteration [1232]: 0.002123463662880431
Loss at iteration [1233]: 0.002123864095207336
***** Warning: Loss has increased *****
Loss at iteration [1234]: 0.002123602271382349
Loss at iteration [1235]: 0.0021222618325496285
Loss at iteration [1236]: 0.0021234405115846864
***** Warning: Loss has increased *****
Loss at iteration [1237]: 0.002124681400662682
***** Warning: Loss has increased *****
Loss at iteration [1238]: 0.0021250803885098373
***** Warning: Loss has increased *****
Loss at iteration [1239]: 0.0021233936629262546
Loss at iteration [1240]: 0.002121670598675951
Loss at iteration [1241]: 0.002120450856913738
Loss at iteration [1242]: 0.0021193300544607675
Loss at iteration [1243]: 0.002120931266836932
***** Warning: Loss has increased *****
Loss at iteration [1244]: 0.002120679289718021
Loss at iteration [1245]: 0.002118837469248048
Loss at iteration [1246]: 0.002117773908708873
Loss at iteration [1247]: 0.0021212211945127115
***** Warning: Loss has increased *****
Loss at iteration [1248]: 0.002131151517453297
***** Warning: Loss has increased *****
Loss at iteration [1249]: 0.0021244685349415347
Loss at iteration [1250]: 0.0021207485937494437
Loss at iteration [1251]: 0.002120804706716335
***** Warning: Loss has increased *****
Loss at iteration [1252]: 0.00212723327822657
***** Warning: Loss has increased *****
Loss at iteration [1253]: 0.0021287560544447966
***** Warning: Loss has increased *****
Loss at iteration [1254]: 0.0021232406949551843
Loss at iteration [1255]: 0.002118252268686494
Loss at iteration [1256]: 0.0021281900176956725
***** Warning: Loss has increased *****
Loss at iteration [1257]: 0.002120358508357744
Loss at iteration [1258]: 0.002118323399942535
Loss at iteration [1259]: 0.002126928911085432
***** Warning: Loss has increased *****
Loss at iteration [1260]: 0.0021210122574448005
Loss at iteration [1261]: 0.00211723595747972
Loss at iteration [1262]: 0.0021167758428328946
Loss at iteration [1263]: 0.002117801732256417
***** Warning: Loss has increased *****
Loss at iteration [1264]: 0.0021209179796819267
***** Warning: Loss has increased *****
Loss at iteration [1265]: 0.002118529712117937
Loss at iteration [1266]: 0.002115943981696245
Loss at iteration [1267]: 0.0021199941355986578
***** Warning: Loss has increased *****
Loss at iteration [1268]: 0.0021245393774208744
***** Warning: Loss has increased *****
Loss at iteration [1269]: 0.002140699719524586
***** Warning: Loss has increased *****
Loss at iteration [1270]: 0.0021273843422968067
Loss at iteration [1271]: 0.002127132707044617
Loss at iteration [1272]: 0.002129173308301693
***** Warning: Loss has increased *****
Loss at iteration [1273]: 0.0021242224849036184
Loss at iteration [1274]: 0.0021292912619691523
***** Warning: Loss has increased *****
Loss at iteration [1275]: 0.0021193171073180055
Loss at iteration [1276]: 0.0021240361860949677
***** Warning: Loss has increased *****
Loss at iteration [1277]: 0.0021207376618335573
Loss at iteration [1278]: 0.002115550147477934
Loss at iteration [1279]: 0.0021201794568857187
***** Warning: Loss has increased *****
Loss at iteration [1280]: 0.0021180149237505757
Loss at iteration [1281]: 0.0021138054936168833
Loss at iteration [1282]: 0.0021112199118212523
Loss at iteration [1283]: 0.002114349966890887
***** Warning: Loss has increased *****
Loss at iteration [1284]: 0.00211795659172598
***** Warning: Loss has increased *****
Loss at iteration [1285]: 0.0021158962905390376
Loss at iteration [1286]: 0.0021108850281025167
Loss at iteration [1287]: 0.0021118621489035214
***** Warning: Loss has increased *****
Loss at iteration [1288]: 0.0021138895246824094
***** Warning: Loss has increased *****
Loss at iteration [1289]: 0.0021147386295424994
***** Warning: Loss has increased *****
Loss at iteration [1290]: 0.00211317178863618
Loss at iteration [1291]: 0.0021101657019354392
Loss at iteration [1292]: 0.0021093705287759663
Loss at iteration [1293]: 0.0021106435228370113
***** Warning: Loss has increased *****
Loss at iteration [1294]: 0.0021151968350205915
***** Warning: Loss has increased *****
Loss at iteration [1295]: 0.0021210745169531263
***** Warning: Loss has increased *****
Loss at iteration [1296]: 0.0021117152094759207
Loss at iteration [1297]: 0.002108393319013698
Loss at iteration [1298]: 0.002109478093536995
***** Warning: Loss has increased *****
Loss at iteration [1299]: 0.0021130766171013385
***** Warning: Loss has increased *****
Loss at iteration [1300]: 0.002120388764895516
***** Warning: Loss has increased *****
Loss at iteration [1301]: 0.0021158417875970534
Loss at iteration [1302]: 0.002115067290152476
Loss at iteration [1303]: 0.002109981405597663
Loss at iteration [1304]: 0.0021091136649317097
Loss at iteration [1305]: 0.0021124976774445164
***** Warning: Loss has increased *****
Loss at iteration [1306]: 0.0021199905013210897
***** Warning: Loss has increased *****
Loss at iteration [1307]: 0.002123040688421531
***** Warning: Loss has increased *****
Loss at iteration [1308]: 0.0021167350530868533
Loss at iteration [1309]: 0.0021154757511588013
Loss at iteration [1310]: 0.002108961226618695
Loss at iteration [1311]: 0.0021073580784159414
Loss at iteration [1312]: 0.0021106281780139727
***** Warning: Loss has increased *****
Loss at iteration [1313]: 0.002122280950728072
***** Warning: Loss has increased *****
Loss at iteration [1314]: 0.00213677378153861
***** Warning: Loss has increased *****
Loss at iteration [1315]: 0.0021227323451395498
Loss at iteration [1316]: 0.0021121320735859486
Loss at iteration [1317]: 0.002114935568491804
***** Warning: Loss has increased *****
Loss at iteration [1318]: 0.0021117543761689744
Loss at iteration [1319]: 0.002106067906040349
Loss at iteration [1320]: 0.0021154664956566684
***** Warning: Loss has increased *****
Loss at iteration [1321]: 0.00212923846010966
***** Warning: Loss has increased *****
Loss at iteration [1322]: 0.002157730151616377
***** Warning: Loss has increased *****
Loss at iteration [1323]: 0.0021244760506803195
Loss at iteration [1324]: 0.002129225875937195
***** Warning: Loss has increased *****
Loss at iteration [1325]: 0.0021415968424957276
***** Warning: Loss has increased *****
Loss at iteration [1326]: 0.002112902076632771
Loss at iteration [1327]: 0.002129631247635956
***** Warning: Loss has increased *****
Loss at iteration [1328]: 0.002114613230947209
Loss at iteration [1329]: 0.0021241925852181925
***** Warning: Loss has increased *****
Loss at iteration [1330]: 0.0021149826512087584
Loss at iteration [1331]: 0.0021250332176343378
***** Warning: Loss has increased *****
Loss at iteration [1332]: 0.002119269728768026
Loss at iteration [1333]: 0.0021143419979840184
Loss at iteration [1334]: 0.0021303752424789856
***** Warning: Loss has increased *****
Loss at iteration [1335]: 0.00211997505702147
Loss at iteration [1336]: 0.002116549180965659
Loss at iteration [1337]: 0.002113863882702314
Loss at iteration [1338]: 0.0021154537811234244
***** Warning: Loss has increased *****
Loss at iteration [1339]: 0.0021202435626457784
***** Warning: Loss has increased *****
Loss at iteration [1340]: 0.002106754030247873
Loss at iteration [1341]: 0.0021095102647357006
***** Warning: Loss has increased *****
Loss at iteration [1342]: 0.002124675846886312
***** Warning: Loss has increased *****
Loss at iteration [1343]: 0.0021290173212296223
***** Warning: Loss has increased *****
Loss at iteration [1344]: 0.002149414909690133
***** Warning: Loss has increased *****
Loss at iteration [1345]: 0.0021179436295830213
Loss at iteration [1346]: 0.002121705180706874
***** Warning: Loss has increased *****
Loss at iteration [1347]: 0.002122561042505921
***** Warning: Loss has increased *****
Loss at iteration [1348]: 0.0021084934580352177
Loss at iteration [1349]: 0.0021232658347934714
***** Warning: Loss has increased *****
Loss at iteration [1350]: 0.0021110793225213743
Loss at iteration [1351]: 0.0021048514638311484
Loss at iteration [1352]: 0.002113873695119927
***** Warning: Loss has increased *****
Loss at iteration [1353]: 0.002143481154231378
***** Warning: Loss has increased *****
Loss at iteration [1354]: 0.002122102005398544
Loss at iteration [1355]: 0.002108901579452786
Loss at iteration [1356]: 0.002110825318304068
***** Warning: Loss has increased *****
Loss at iteration [1357]: 0.002114650641484301
***** Warning: Loss has increased *****
Loss at iteration [1358]: 0.0021055101945488294
Loss at iteration [1359]: 0.0021057911184621187
***** Warning: Loss has increased *****
Loss at iteration [1360]: 0.002116370688924183
***** Warning: Loss has increased *****
Loss at iteration [1361]: 0.0021258998188773987
***** Warning: Loss has increased *****
Loss at iteration [1362]: 0.002104704414093595
Loss at iteration [1363]: 0.002108955439183032
***** Warning: Loss has increased *****
Loss at iteration [1364]: 0.0021231342726087373
***** Warning: Loss has increased *****
Loss at iteration [1365]: 0.002103781149636155
Loss at iteration [1366]: 0.002098255677775401
Loss at iteration [1367]: 0.002106969243074043
***** Warning: Loss has increased *****
Loss at iteration [1368]: 0.002111029983560737
***** Warning: Loss has increased *****
Loss at iteration [1369]: 0.00211655061024613
***** Warning: Loss has increased *****
Loss at iteration [1370]: 0.002103289268340983
Loss at iteration [1371]: 0.0021069780984567465
***** Warning: Loss has increased *****
Loss at iteration [1372]: 0.0021362992232374124
***** Warning: Loss has increased *****
Loss at iteration [1373]: 0.0021300782364115766
Loss at iteration [1374]: 0.002137465440139496
***** Warning: Loss has increased *****
Loss at iteration [1375]: 0.002124543107875723
Loss at iteration [1376]: 0.0021328432810405833
***** Warning: Loss has increased *****
Loss at iteration [1377]: 0.0021201626402730647
Loss at iteration [1378]: 0.0021301245300509235
***** Warning: Loss has increased *****
Loss at iteration [1379]: 0.0021065924828542124
Loss at iteration [1380]: 0.0021164336052733423
***** Warning: Loss has increased *****
Loss at iteration [1381]: 0.002105971288031965
Loss at iteration [1382]: 0.002148548481747888
***** Warning: Loss has increased *****
Loss at iteration [1383]: 0.0022559748701068815
***** Warning: Loss has increased *****
Loss at iteration [1384]: 0.0021783348839338064
Loss at iteration [1385]: 0.002186369668572234
***** Warning: Loss has increased *****
Loss at iteration [1386]: 0.002177738848582269
Loss at iteration [1387]: 0.002171497903725687
Loss at iteration [1388]: 0.0021920604164090305
***** Warning: Loss has increased *****
Loss at iteration [1389]: 0.002146444350759498
Loss at iteration [1390]: 0.0021835379399673376
***** Warning: Loss has increased *****
Loss at iteration [1391]: 0.0021429826845904346
Loss at iteration [1392]: 0.0021691359883608003
***** Warning: Loss has increased *****
Loss at iteration [1393]: 0.0021291485691679853
Loss at iteration [1394]: 0.002164239786951215
***** Warning: Loss has increased *****
Loss at iteration [1395]: 0.0021206430503795627
Loss at iteration [1396]: 0.0021546440285562106
***** Warning: Loss has increased *****
Loss at iteration [1397]: 0.0021195982495316656
Loss at iteration [1398]: 0.002142671258815657
***** Warning: Loss has increased *****
Loss at iteration [1399]: 0.0021222773653452186
Loss at iteration [1400]: 0.0021329055112781566
***** Warning: Loss has increased *****
Loss at iteration [1401]: 0.002125704845750107
Loss at iteration [1402]: 0.0021247394681639465
Loss at iteration [1403]: 0.0021252128720262295
***** Warning: Loss has increased *****
Loss at iteration [1404]: 0.0021165153453566897
Loss at iteration [1405]: 0.0021261759222551855
***** Warning: Loss has increased *****
Loss at iteration [1406]: 0.002113207129624356
Loss at iteration [1407]: 0.002123729643352012
***** Warning: Loss has increased *****
Loss at iteration [1408]: 0.002110668426916168
Loss at iteration [1409]: 0.002119495279683762
***** Warning: Loss has increased *****
Loss at iteration [1410]: 0.0021095038741250735
Loss at iteration [1411]: 0.0021148913175579953
***** Warning: Loss has increased *****
Loss at iteration [1412]: 0.0021097513937423334
Loss at iteration [1413]: 0.002109736857994786
Loss at iteration [1414]: 0.002109553220410489
Loss at iteration [1415]: 0.0021066794209182783
Loss at iteration [1416]: 0.0021093678678146175
***** Warning: Loss has increased *****
Loss at iteration [1417]: 0.0021056808369130024
Loss at iteration [1418]: 0.0021065633249130224
***** Warning: Loss has increased *****
Loss at iteration [1419]: 0.0021062126907123654
Loss at iteration [1420]: 0.002104108451306168
Loss at iteration [1421]: 0.0021055358735943943
***** Warning: Loss has increased *****
Loss at iteration [1422]: 0.002102991427422839
Loss at iteration [1423]: 0.002103783571557999
***** Warning: Loss has increased *****
Loss at iteration [1424]: 0.0021019910866402433
Loss at iteration [1425]: 0.0021008367701769574
Loss at iteration [1426]: 0.00210199283942999
***** Warning: Loss has increased *****
Loss at iteration [1427]: 0.002100352642731175
Loss at iteration [1428]: 0.002101217537065331
***** Warning: Loss has increased *****
Loss at iteration [1429]: 0.002100607339669677
Loss at iteration [1430]: 0.0020997373808423017
Loss at iteration [1431]: 0.0021009481896181854
***** Warning: Loss has increased *****
Loss at iteration [1432]: 0.0020991667072510173
Loss at iteration [1433]: 0.0020992250493605427
***** Warning: Loss has increased *****
Loss at iteration [1434]: 0.002098964518918581
Loss at iteration [1435]: 0.002098072654335587
Loss at iteration [1436]: 0.002097676911062055
Loss at iteration [1437]: 0.002098433616155562
***** Warning: Loss has increased *****
Loss at iteration [1438]: 0.002097451498437052
Loss at iteration [1439]: 0.002097714641894216
***** Warning: Loss has increased *****
Loss at iteration [1440]: 0.0020966484768903137
Loss at iteration [1441]: 0.0020968434847213227
***** Warning: Loss has increased *****
Loss at iteration [1442]: 0.0020966582706946125
Loss at iteration [1443]: 0.0020965600117726735
Loss at iteration [1444]: 0.002096408216540768
Loss at iteration [1445]: 0.0020951306920600505
Loss at iteration [1446]: 0.0020957866571406675
***** Warning: Loss has increased *****
Loss at iteration [1447]: 0.0020970904418529087
***** Warning: Loss has increased *****
Loss at iteration [1448]: 0.002096346992167683
Loss at iteration [1449]: 0.0020950194351471788
Loss at iteration [1450]: 0.00209460808865172
Loss at iteration [1451]: 0.0020966594055127326
***** Warning: Loss has increased *****
Loss at iteration [1452]: 0.0020956800073307557
Loss at iteration [1453]: 0.002095253713912955
Loss at iteration [1454]: 0.0020954881401442807
***** Warning: Loss has increased *****
Loss at iteration [1455]: 0.002093409228396623
Loss at iteration [1456]: 0.0020942457253773964
***** Warning: Loss has increased *****
Loss at iteration [1457]: 0.0020946393586191844
***** Warning: Loss has increased *****
Loss at iteration [1458]: 0.002092450085163292
Loss at iteration [1459]: 0.00209417521247198
***** Warning: Loss has increased *****
Loss at iteration [1460]: 0.0020951008876361706
***** Warning: Loss has increased *****
Loss at iteration [1461]: 0.002091946901799237
Loss at iteration [1462]: 0.0020940858704864254
***** Warning: Loss has increased *****
Loss at iteration [1463]: 0.0020958357121729243
***** Warning: Loss has increased *****
Loss at iteration [1464]: 0.002091407873436961
Loss at iteration [1465]: 0.0020937987525469445
***** Warning: Loss has increased *****
Loss at iteration [1466]: 0.002095130617694498
***** Warning: Loss has increased *****
Loss at iteration [1467]: 0.0020903302114690784
Loss at iteration [1468]: 0.002096332752256356
***** Warning: Loss has increased *****
Loss at iteration [1469]: 0.002100060025547937
***** Warning: Loss has increased *****
Loss at iteration [1470]: 0.002090493277555153
Loss at iteration [1471]: 0.002098453645657549
***** Warning: Loss has increased *****
Loss at iteration [1472]: 0.0020945580863232574
Loss at iteration [1473]: 0.0020899437537723665
Loss at iteration [1474]: 0.0020973630866885813
***** Warning: Loss has increased *****
Loss at iteration [1475]: 0.0020923719263525997
Loss at iteration [1476]: 0.002089249582389693
Loss at iteration [1477]: 0.002090055182240327
***** Warning: Loss has increased *****
Loss at iteration [1478]: 0.0020918249097375565
***** Warning: Loss has increased *****
Loss at iteration [1479]: 0.0020890450070461487
Loss at iteration [1480]: 0.002089797019788632
***** Warning: Loss has increased *****
Loss at iteration [1481]: 0.002093944269498666
***** Warning: Loss has increased *****
Loss at iteration [1482]: 0.0020920864932195637
Loss at iteration [1483]: 0.002093757139418941
***** Warning: Loss has increased *****
Loss at iteration [1484]: 0.0020940777082668386
***** Warning: Loss has increased *****
Loss at iteration [1485]: 0.0020876002039822705
Loss at iteration [1486]: 0.0020933764988557667
***** Warning: Loss has increased *****
Loss at iteration [1487]: 0.002100989542221285
***** Warning: Loss has increased *****
Loss at iteration [1488]: 0.0020893063077299825
Loss at iteration [1489]: 0.0020931976032066806
***** Warning: Loss has increased *****
Loss at iteration [1490]: 0.002099173839288764
***** Warning: Loss has increased *****
Loss at iteration [1491]: 0.002088379281788001
Loss at iteration [1492]: 0.002090147214099713
***** Warning: Loss has increased *****
Loss at iteration [1493]: 0.00209544880430552
***** Warning: Loss has increased *****
Loss at iteration [1494]: 0.002091629739725111
Loss at iteration [1495]: 0.0020909550488812535
Loss at iteration [1496]: 0.0020954016204977927
***** Warning: Loss has increased *****
Loss at iteration [1497]: 0.002095023211439598
Loss at iteration [1498]: 0.002098688692176634
***** Warning: Loss has increased *****
Loss at iteration [1499]: 0.0021030137966404423
***** Warning: Loss has increased *****
Loss at iteration [1500]: 0.0021097730326180954
***** Warning: Loss has increased *****
Loss at iteration [1501]: 0.0021189312011134767
***** Warning: Loss has increased *****
Loss at iteration [1502]: 0.0021311418762505565
***** Warning: Loss has increased *****
Loss at iteration [1503]: 0.002147250616769284
***** Warning: Loss has increased *****
Loss at iteration [1504]: 0.002173914364263679
***** Warning: Loss has increased *****
Loss at iteration [1505]: 0.00220206084435217
***** Warning: Loss has increased *****
Loss at iteration [1506]: 0.0022501483432644867
***** Warning: Loss has increased *****
Loss at iteration [1507]: 0.0023002300543423595
***** Warning: Loss has increased *****
Loss at iteration [1508]: 0.002394104812910813
***** Warning: Loss has increased *****
Loss at iteration [1509]: 0.002476658477651855
***** Warning: Loss has increased *****
Loss at iteration [1510]: 0.0026357660627342725
***** Warning: Loss has increased *****
Loss at iteration [1511]: 0.0027521487313997037
***** Warning: Loss has increased *****
Loss at iteration [1512]: 0.002991814602311711
***** Warning: Loss has increased *****
Loss at iteration [1513]: 0.0030909588647364933
***** Warning: Loss has increased *****
Loss at iteration [1514]: 0.0033319245181386435
***** Warning: Loss has increased *****
Loss at iteration [1515]: 0.0032380184338868774
Loss at iteration [1516]: 0.003249672990994885
***** Warning: Loss has increased *****
Loss at iteration [1517]: 0.0028998336345582125
Loss at iteration [1518]: 0.002576147655242558
Loss at iteration [1519]: 0.002283380988746262
Loss at iteration [1520]: 0.002135122061554409
Loss at iteration [1521]: 0.0021227118120746248
Loss at iteration [1522]: 0.0022961356005014293
***** Warning: Loss has increased *****
Loss at iteration [1523]: 0.0024079727944512825
***** Warning: Loss has increased *****
Loss at iteration [1524]: 0.0023652510814134893
Loss at iteration [1525]: 0.002267991847704221
Loss at iteration [1526]: 0.0021099775930750945
Loss at iteration [1527]: 0.0021122805024706014
***** Warning: Loss has increased *****
Loss at iteration [1528]: 0.0022053356899783385
***** Warning: Loss has increased *****
Loss at iteration [1529]: 0.0022395298631270846
***** Warning: Loss has increased *****
Loss at iteration [1530]: 0.0021960034196105258
Loss at iteration [1531]: 0.002109506343657475
Loss at iteration [1532]: 0.002090595428046934
Loss at iteration [1533]: 0.0021468243805737477
***** Warning: Loss has increased *****
Loss at iteration [1534]: 0.002173654207856001
***** Warning: Loss has increased *****
Loss at iteration [1535]: 0.0021432915637442066
Loss at iteration [1536]: 0.0020980604460757064
Loss at iteration [1537]: 0.002085882085696927
Loss at iteration [1538]: 0.002114264626990547
***** Warning: Loss has increased *****
Loss at iteration [1539]: 0.00213472544966866
***** Warning: Loss has increased *****
Loss at iteration [1540]: 0.002123289774433739
Loss at iteration [1541]: 0.0020946279427350627
Loss at iteration [1542]: 0.002084650192060545
Loss at iteration [1543]: 0.0020984144736891485
***** Warning: Loss has increased *****
Loss at iteration [1544]: 0.0021177318786296043
***** Warning: Loss has increased *****
Loss at iteration [1545]: 0.002119995728330213
***** Warning: Loss has increased *****
Loss at iteration [1546]: 0.0020958169271810107
Loss at iteration [1547]: 0.002089453604743378
Loss at iteration [1548]: 0.002103515417539917
***** Warning: Loss has increased *****
Loss at iteration [1549]: 0.0021075085987616667
***** Warning: Loss has increased *****
Loss at iteration [1550]: 0.0021031762685319384
Loss at iteration [1551]: 0.0020961585099052763
Loss at iteration [1552]: 0.0020848466340443925
Loss at iteration [1553]: 0.002081387836864176
Loss at iteration [1554]: 0.002088517240266145
***** Warning: Loss has increased *****
Loss at iteration [1555]: 0.0020940355566480993
***** Warning: Loss has increased *****
Loss at iteration [1556]: 0.0020896883880675314
Loss at iteration [1557]: 0.0020804015442693587
Loss at iteration [1558]: 0.002081757127155279
***** Warning: Loss has increased *****
Loss at iteration [1559]: 0.0020952870963354252
***** Warning: Loss has increased *****
Loss at iteration [1560]: 0.002085868358767376
Loss at iteration [1561]: 0.002088104001795747
***** Warning: Loss has increased *****
Loss at iteration [1562]: 0.0021004115025816577
***** Warning: Loss has increased *****
Loss at iteration [1563]: 0.0020794827480738213
Loss at iteration [1564]: 0.002088124696685339
***** Warning: Loss has increased *****
Loss at iteration [1565]: 0.002115451388167439
***** Warning: Loss has increased *****
Loss at iteration [1566]: 0.002083821965192104
Loss at iteration [1567]: 0.0021059533241082544
***** Warning: Loss has increased *****
Loss at iteration [1568]: 0.002126827723645278
***** Warning: Loss has increased *****
Loss at iteration [1569]: 0.002088034361027236
Loss at iteration [1570]: 0.0021110640293847604
***** Warning: Loss has increased *****
Loss at iteration [1571]: 0.0021038369622933223
Loss at iteration [1572]: 0.0020810670651663886
Loss at iteration [1573]: 0.002115100148618914
***** Warning: Loss has increased *****
Loss at iteration [1574]: 0.0021222939608166102
***** Warning: Loss has increased *****
Loss at iteration [1575]: 0.0020976481533268817
Loss at iteration [1576]: 0.00211944390756197
***** Warning: Loss has increased *****
Loss at iteration [1577]: 0.0021057737329850676
Loss at iteration [1578]: 0.002085685009174793
Loss at iteration [1579]: 0.002128530782717265
***** Warning: Loss has increased *****
Loss at iteration [1580]: 0.002117061617928576
Loss at iteration [1581]: 0.002091381680440973
Loss at iteration [1582]: 0.0021590399264446287
***** Warning: Loss has increased *****
Loss at iteration [1583]: 0.002156570625092551
Loss at iteration [1584]: 0.0021058046687068965
Loss at iteration [1585]: 0.002151656871330088
***** Warning: Loss has increased *****
Loss at iteration [1586]: 0.00210909314752771
Loss at iteration [1587]: 0.002117773525288192
***** Warning: Loss has increased *****
Loss at iteration [1588]: 0.0021158894193451087
Loss at iteration [1589]: 0.0021112533972188338
Loss at iteration [1590]: 0.002099133646628621
Loss at iteration [1591]: 0.0021326096076131154
***** Warning: Loss has increased *****
Loss at iteration [1592]: 0.0021206104172029083
Loss at iteration [1593]: 0.0021358780440989423
***** Warning: Loss has increased *****
Loss at iteration [1594]: 0.0021281717382846737
Loss at iteration [1595]: 0.0021495235576139858
***** Warning: Loss has increased *****
Loss at iteration [1596]: 0.0021358790977365105
Loss at iteration [1597]: 0.002113427215904651
Loss at iteration [1598]: 0.0021201486550229054
***** Warning: Loss has increased *****
Loss at iteration [1599]: 0.0020815516822867935
Loss at iteration [1600]: 0.0021561387325316427
***** Warning: Loss has increased *****
Loss at iteration [1601]: 0.0021785498238766684
***** Warning: Loss has increased *****
Loss at iteration [1602]: 0.002125539884355224
Loss at iteration [1603]: 0.002115841595870922
Loss at iteration [1604]: 0.0021480003683480666
***** Warning: Loss has increased *****
Loss at iteration [1605]: 0.002094349306281839
Loss at iteration [1606]: 0.002125596490930034
***** Warning: Loss has increased *****
Loss at iteration [1607]: 0.0020893736580465745
Loss at iteration [1608]: 0.002093213233381549
***** Warning: Loss has increased *****
Loss at iteration [1609]: 0.0021120107878743496
***** Warning: Loss has increased *****
Loss at iteration [1610]: 0.002134868828278641
***** Warning: Loss has increased *****
Loss at iteration [1611]: 0.0021234139680928795
Loss at iteration [1612]: 0.0020907101754495594
Loss at iteration [1613]: 0.0021106842418632302
***** Warning: Loss has increased *****
Loss at iteration [1614]: 0.0020930570976119894
Loss at iteration [1615]: 0.00210913021477693
***** Warning: Loss has increased *****
Loss at iteration [1616]: 0.0020737434996057737
Loss at iteration [1617]: 0.0020970592275383762
***** Warning: Loss has increased *****
Loss at iteration [1618]: 0.0020842570085236644
Loss at iteration [1619]: 0.0021038900408843634
***** Warning: Loss has increased *****
Loss at iteration [1620]: 0.002093902631120482
Loss at iteration [1621]: 0.002104999396230132
***** Warning: Loss has increased *****
Loss at iteration [1622]: 0.0020807798456298847
Loss at iteration [1623]: 0.002083613991041687
***** Warning: Loss has increased *****
Loss at iteration [1624]: 0.002087157684093047
***** Warning: Loss has increased *****
Loss at iteration [1625]: 0.002084137193971887
Loss at iteration [1626]: 0.002081341596663665
Loss at iteration [1627]: 0.002083449741098202
***** Warning: Loss has increased *****
Loss at iteration [1628]: 0.0020760912682300466
Loss at iteration [1629]: 0.0020845651737045824
***** Warning: Loss has increased *****
Loss at iteration [1630]: 0.002071318337770003
Loss at iteration [1631]: 0.002078513142957101
***** Warning: Loss has increased *****
Loss at iteration [1632]: 0.0020668663452084015
Loss at iteration [1633]: 0.002078884650443419
***** Warning: Loss has increased *****
Loss at iteration [1634]: 0.0020794651772972055
***** Warning: Loss has increased *****
Loss at iteration [1635]: 0.002080040383986002
***** Warning: Loss has increased *****
Loss at iteration [1636]: 0.002073616752447748
Loss at iteration [1637]: 0.002082313433888803
***** Warning: Loss has increased *****
Loss at iteration [1638]: 0.0020770741897103537
Loss at iteration [1639]: 0.00207443323799902
Loss at iteration [1640]: 0.0020692487147230365
Loss at iteration [1641]: 0.0020717913458116813
***** Warning: Loss has increased *****
Loss at iteration [1642]: 0.002073135701527215
***** Warning: Loss has increased *****
Loss at iteration [1643]: 0.0020703110995403443
Loss at iteration [1644]: 0.0020667199636660873
Loss at iteration [1645]: 0.0020782130227519403
***** Warning: Loss has increased *****
Loss at iteration [1646]: 0.0020811517249593914
***** Warning: Loss has increased *****
Loss at iteration [1647]: 0.0020737438943646295
Loss at iteration [1648]: 0.002082917185009193
***** Warning: Loss has increased *****
Loss at iteration [1649]: 0.002111961708366106
***** Warning: Loss has increased *****
Loss at iteration [1650]: 0.0020821192257525966
Loss at iteration [1651]: 0.0020667236873599015
Loss at iteration [1652]: 0.0020832812975749146
***** Warning: Loss has increased *****
Loss at iteration [1653]: 0.0021033425641202442
***** Warning: Loss has increased *****
Loss at iteration [1654]: 0.0020871269548307517
Loss at iteration [1655]: 0.0020771106771937838
Loss at iteration [1656]: 0.002084928525864279
***** Warning: Loss has increased *****
Loss at iteration [1657]: 0.002086326177753306
***** Warning: Loss has increased *****
Loss at iteration [1658]: 0.002067169378974871
Loss at iteration [1659]: 0.0020843923014818517
***** Warning: Loss has increased *****
Loss at iteration [1660]: 0.0020921494801688432
***** Warning: Loss has increased *****
Loss at iteration [1661]: 0.0020648366063098717
Loss at iteration [1662]: 0.0020719872641252577
***** Warning: Loss has increased *****
Loss at iteration [1663]: 0.0020845172167494176
***** Warning: Loss has increased *****
Loss at iteration [1664]: 0.002065883493166552
Loss at iteration [1665]: 0.002099282108587674
***** Warning: Loss has increased *****
Loss at iteration [1666]: 0.002108503926579202
***** Warning: Loss has increased *****
Loss at iteration [1667]: 0.0020892754482608102
Loss at iteration [1668]: 0.0020940772792636428
***** Warning: Loss has increased *****
Loss at iteration [1669]: 0.0021033148104677136
***** Warning: Loss has increased *****
Loss at iteration [1670]: 0.0020938142012119406
Loss at iteration [1671]: 0.0021292182391714678
***** Warning: Loss has increased *****
Loss at iteration [1672]: 0.002074733308179352
Loss at iteration [1673]: 0.002111808465029201
***** Warning: Loss has increased *****
Loss at iteration [1674]: 0.0020889376315780498
Loss at iteration [1675]: 0.0021165879173665474
***** Warning: Loss has increased *****
Loss at iteration [1676]: 0.002111100799027029
Loss at iteration [1677]: 0.0021176803321908914
***** Warning: Loss has increased *****
Loss at iteration [1678]: 0.0021108616404707146
Loss at iteration [1679]: 0.0021189929381478824
***** Warning: Loss has increased *****
Loss at iteration [1680]: 0.0021329411436336533
***** Warning: Loss has increased *****
Loss at iteration [1681]: 0.00214855579589963
***** Warning: Loss has increased *****
Loss at iteration [1682]: 0.0021677447359555835
***** Warning: Loss has increased *****
Loss at iteration [1683]: 0.0022061715146924968
***** Warning: Loss has increased *****
Loss at iteration [1684]: 0.00225063877517774
***** Warning: Loss has increased *****
Loss at iteration [1685]: 0.0022827023709822885
***** Warning: Loss has increased *****
Loss at iteration [1686]: 0.0023488909559805132
***** Warning: Loss has increased *****
Loss at iteration [1687]: 0.0024133318681588264
***** Warning: Loss has increased *****
Loss at iteration [1688]: 0.002513097175662322
***** Warning: Loss has increased *****
Loss at iteration [1689]: 0.0025896578817826794
***** Warning: Loss has increased *****
Loss at iteration [1690]: 0.0027455347209953875
***** Warning: Loss has increased *****
Loss at iteration [1691]: 0.00278215278236314
***** Warning: Loss has increased *****
Loss at iteration [1692]: 0.002910071360360587
***** Warning: Loss has increased *****
Loss at iteration [1693]: 0.002903520291828794
Loss at iteration [1694]: 0.0029345938308965825
***** Warning: Loss has increased *****
Loss at iteration [1695]: 0.0027399200907487933
Loss at iteration [1696]: 0.002615441158993365
Loss at iteration [1697]: 0.002436321634709705
Loss at iteration [1698]: 0.00227120022338348
Loss at iteration [1699]: 0.00212731982177666
Loss at iteration [1700]: 0.0020796387867824833
Loss at iteration [1701]: 0.0021183164119001163
***** Warning: Loss has increased *****
Loss at iteration [1702]: 0.0021490481516694467
***** Warning: Loss has increased *****
Loss at iteration [1703]: 0.002200835228394798
***** Warning: Loss has increased *****
Loss at iteration [1704]: 0.002267801113395318
***** Warning: Loss has increased *****
Loss at iteration [1705]: 0.0023187138104955583
***** Warning: Loss has increased *****
Loss at iteration [1706]: 0.0022737406822137267
Loss at iteration [1707]: 0.002228001637110598
Loss at iteration [1708]: 0.0021564738678410194
Loss at iteration [1709]: 0.002103702074594291
Loss at iteration [1710]: 0.002062493524621218
Loss at iteration [1711]: 0.0020785298485460554
***** Warning: Loss has increased *****
Loss at iteration [1712]: 0.0021155392255707774
***** Warning: Loss has increased *****
Loss at iteration [1713]: 0.0021099423657920985
Loss at iteration [1714]: 0.0021907745446839316
***** Warning: Loss has increased *****
Loss at iteration [1715]: 0.0022124723893074973
***** Warning: Loss has increased *****
Loss at iteration [1716]: 0.0021585416868494605
Loss at iteration [1717]: 0.002142213584384224
Loss at iteration [1718]: 0.0021483752878509914
***** Warning: Loss has increased *****
Loss at iteration [1719]: 0.002077718266862967
Loss at iteration [1720]: 0.0021005118717100897
***** Warning: Loss has increased *****
Loss at iteration [1721]: 0.0020836722992760826
Loss at iteration [1722]: 0.002070730774103468
Loss at iteration [1723]: 0.0020952700456520314
***** Warning: Loss has increased *****
Loss at iteration [1724]: 0.002116448116748633
***** Warning: Loss has increased *****
Loss at iteration [1725]: 0.0021151961264266454
Loss at iteration [1726]: 0.002103941765318774
Loss at iteration [1727]: 0.0021227183816079904
***** Warning: Loss has increased *****
Loss at iteration [1728]: 0.0020841698442386042
Loss at iteration [1729]: 0.002099086669537056
***** Warning: Loss has increased *****
Loss at iteration [1730]: 0.0020673180445733475
Loss at iteration [1731]: 0.0021001007571796017
***** Warning: Loss has increased *****
Loss at iteration [1732]: 0.0020707823456814255
Loss at iteration [1733]: 0.002104952099571241
***** Warning: Loss has increased *****
Loss at iteration [1734]: 0.0020647525852520456
Loss at iteration [1735]: 0.002105797430524774
***** Warning: Loss has increased *****
Loss at iteration [1736]: 0.0020786765128109966
Loss at iteration [1737]: 0.0021016079905107365
***** Warning: Loss has increased *****
Loss at iteration [1738]: 0.0020769760335162192
Loss at iteration [1739]: 0.0021054308024417785
***** Warning: Loss has increased *****
Loss at iteration [1740]: 0.0020871763941677206
Loss at iteration [1741]: 0.0020906442042234854
***** Warning: Loss has increased *****
Loss at iteration [1742]: 0.002072346686240512
Loss at iteration [1743]: 0.002091679628773966
***** Warning: Loss has increased *****
Loss at iteration [1744]: 0.0020804384433311227
Loss at iteration [1745]: 0.0020857635322707314
***** Warning: Loss has increased *****
Loss at iteration [1746]: 0.0020733792563878903
Loss at iteration [1747]: 0.002071449430770808
Loss at iteration [1748]: 0.002077048351383651
***** Warning: Loss has increased *****
Loss at iteration [1749]: 0.0020686946172322924
Loss at iteration [1750]: 0.0020673193700355783
Loss at iteration [1751]: 0.0020620286192408895
Loss at iteration [1752]: 0.002076260030763571
***** Warning: Loss has increased *****
Loss at iteration [1753]: 0.0020620827421268728
Loss at iteration [1754]: 0.0020761383195406257
***** Warning: Loss has increased *****
Loss at iteration [1755]: 0.002088616295772038
***** Warning: Loss has increased *****
Loss at iteration [1756]: 0.0020865078956209316
Loss at iteration [1757]: 0.00205373892557429
Loss at iteration [1758]: 0.002102208266884149
***** Warning: Loss has increased *****
Loss at iteration [1759]: 0.002094219536392005
Loss at iteration [1760]: 0.002082320938043834
Loss at iteration [1761]: 0.002071916624347083
Loss at iteration [1762]: 0.002116229416312399
***** Warning: Loss has increased *****
Loss at iteration [1763]: 0.002126358313849332
***** Warning: Loss has increased *****
Loss at iteration [1764]: 0.002097651414369955
Loss at iteration [1765]: 0.002083916894509736
Loss at iteration [1766]: 0.0021198875841782283
***** Warning: Loss has increased *****
Loss at iteration [1767]: 0.0020646172602112344
Loss at iteration [1768]: 0.002122773979437254
***** Warning: Loss has increased *****
Loss at iteration [1769]: 0.0020818879603430535
Loss at iteration [1770]: 0.0021187708962741836
***** Warning: Loss has increased *****
Loss at iteration [1771]: 0.00209847527189329
Loss at iteration [1772]: 0.002144418410826325
***** Warning: Loss has increased *****
Loss at iteration [1773]: 0.0021640476733312147
***** Warning: Loss has increased *****
Loss at iteration [1774]: 0.002222989483850411
***** Warning: Loss has increased *****
Loss at iteration [1775]: 0.0023010845048620724
***** Warning: Loss has increased *****
Loss at iteration [1776]: 0.0023976643982192634
***** Warning: Loss has increased *****
Loss at iteration [1777]: 0.0025779808543803353
***** Warning: Loss has increased *****
Loss at iteration [1778]: 0.002721489672340593
***** Warning: Loss has increased *****
Loss at iteration [1779]: 0.0030300052884410737
***** Warning: Loss has increased *****
Loss at iteration [1780]: 0.003144194180881205
***** Warning: Loss has increased *****
Loss at iteration [1781]: 0.0035118012439547108
***** Warning: Loss has increased *****
Loss at iteration [1782]: 0.0034890120767188913
Loss at iteration [1783]: 0.0036123871045809215
***** Warning: Loss has increased *****
Loss at iteration [1784]: 0.0032141668624486716
Loss at iteration [1785]: 0.002940970706152821
Loss at iteration [1786]: 0.0025072561948036474
Loss at iteration [1787]: 0.0021840126971907126
Loss at iteration [1788]: 0.002105567758592738
Loss at iteration [1789]: 0.0021387830932338052
***** Warning: Loss has increased *****
Loss at iteration [1790]: 0.0023262002776638967
***** Warning: Loss has increased *****
Loss at iteration [1791]: 0.0024311491377982356
***** Warning: Loss has increased *****
Loss at iteration [1792]: 0.002524954734866766
***** Warning: Loss has increased *****
Loss at iteration [1793]: 0.0024235725873651354
Loss at iteration [1794]: 0.002262806361451541
Loss at iteration [1795]: 0.002178904656380951
Loss at iteration [1796]: 0.0020819986524064655
Loss at iteration [1797]: 0.0021496081580914496
***** Warning: Loss has increased *****
Loss at iteration [1798]: 0.002176445139420289
***** Warning: Loss has increased *****
Loss at iteration [1799]: 0.002222778952111809
***** Warning: Loss has increased *****
Loss at iteration [1800]: 0.0022433311342071704
***** Warning: Loss has increased *****
Loss at iteration [1801]: 0.0022189150552786316
Loss at iteration [1802]: 0.0021729058448305867
Loss at iteration [1803]: 0.0020823169076714765
Loss at iteration [1804]: 0.002129585315308872
***** Warning: Loss has increased *****
Loss at iteration [1805]: 0.002080900472499245
Loss at iteration [1806]: 0.002157570529784148
***** Warning: Loss has increased *****
Loss at iteration [1807]: 0.002137716008117635
Loss at iteration [1808]: 0.0021483185520027436
***** Warning: Loss has increased *****
Loss at iteration [1809]: 0.002090985964362736
Loss at iteration [1810]: 0.0020915787871624127
***** Warning: Loss has increased *****
Loss at iteration [1811]: 0.0020734793018506196
Loss at iteration [1812]: 0.0020727389414842787
Loss at iteration [1813]: 0.002093886216574042
***** Warning: Loss has increased *****
Loss at iteration [1814]: 0.0020796240362583444
Loss at iteration [1815]: 0.0020891861175493134
***** Warning: Loss has increased *****
Loss at iteration [1816]: 0.002082746127898981
Loss at iteration [1817]: 0.0020715869652472023
Loss at iteration [1818]: 0.002053422536398859
Loss at iteration [1819]: 0.002056786756268364
***** Warning: Loss has increased *****
Loss at iteration [1820]: 0.0020596188575461205
***** Warning: Loss has increased *****
Loss at iteration [1821]: 0.002054734395320712
Loss at iteration [1822]: 0.002060940278877597
***** Warning: Loss has increased *****
Loss at iteration [1823]: 0.0020714850214556507
***** Warning: Loss has increased *****
Loss at iteration [1824]: 0.0020628624306387685
Loss at iteration [1825]: 0.002055136980846568
Loss at iteration [1826]: 0.002057000509763579
***** Warning: Loss has increased *****
Loss at iteration [1827]: 0.0020515182000410445
Loss at iteration [1828]: 0.0020477872663146974
Loss at iteration [1829]: 0.0020573885474680876
***** Warning: Loss has increased *****
Loss at iteration [1830]: 0.00205831107311355
***** Warning: Loss has increased *****
Loss at iteration [1831]: 0.0020576722875642744
Loss at iteration [1832]: 0.002055772930593776
Loss at iteration [1833]: 0.002056193093415725
***** Warning: Loss has increased *****
Loss at iteration [1834]: 0.002050433358994885
Loss at iteration [1835]: 0.0020559135727538118
***** Warning: Loss has increased *****
Loss at iteration [1836]: 0.0020713828553248336
***** Warning: Loss has increased *****
Loss at iteration [1837]: 0.002079946065115045
***** Warning: Loss has increased *****
Loss at iteration [1838]: 0.0020678186505616414
Loss at iteration [1839]: 0.002056633375308687
Loss at iteration [1840]: 0.002050874794198932
Loss at iteration [1841]: 0.002056921768398807
***** Warning: Loss has increased *****
Loss at iteration [1842]: 0.0020561885135029663
Loss at iteration [1843]: 0.002048533927153818
Loss at iteration [1844]: 0.0020483938168295427
Loss at iteration [1845]: 0.0020541196658144883
***** Warning: Loss has increased *****
Loss at iteration [1846]: 0.0020539070863581895
Loss at iteration [1847]: 0.0020632852790734247
***** Warning: Loss has increased *****
Loss at iteration [1848]: 0.0020747629026358744
***** Warning: Loss has increased *****
Loss at iteration [1849]: 0.002055949172149384
Loss at iteration [1850]: 0.0020697803061921387
***** Warning: Loss has increased *****
Loss at iteration [1851]: 0.002112209582734213
***** Warning: Loss has increased *****
Loss at iteration [1852]: 0.0020808391178085946
Loss at iteration [1853]: 0.002086888753361423
***** Warning: Loss has increased *****
Loss at iteration [1854]: 0.0021763278706074074
***** Warning: Loss has increased *****
Loss at iteration [1855]: 0.0021803728136181866
***** Warning: Loss has increased *****
Loss at iteration [1856]: 0.002109776809340065
Loss at iteration [1857]: 0.0021305706673073453
***** Warning: Loss has increased *****
Loss at iteration [1858]: 0.0021378636957959875
***** Warning: Loss has increased *****
Loss at iteration [1859]: 0.0021008508674800167
Loss at iteration [1860]: 0.0021489977216837338
***** Warning: Loss has increased *****
Loss at iteration [1861]: 0.0021669400844685186
***** Warning: Loss has increased *****
Loss at iteration [1862]: 0.0020681513232362367
Loss at iteration [1863]: 0.0021155240988809444
***** Warning: Loss has increased *****
Loss at iteration [1864]: 0.0022564708417641076
***** Warning: Loss has increased *****
Loss at iteration [1865]: 0.0022754438079651018
***** Warning: Loss has increased *****
Loss at iteration [1866]: 0.0022101094895839624
Loss at iteration [1867]: 0.002231379924042151
***** Warning: Loss has increased *****
Loss at iteration [1868]: 0.0022577771632659115
***** Warning: Loss has increased *****
Loss at iteration [1869]: 0.0021821408736129692
Loss at iteration [1870]: 0.002196740546581827
***** Warning: Loss has increased *****
Loss at iteration [1871]: 0.002200539944947362
***** Warning: Loss has increased *****
Loss at iteration [1872]: 0.002167598721091415
Loss at iteration [1873]: 0.0022060784585491993
***** Warning: Loss has increased *****
Loss at iteration [1874]: 0.002202422555202174
Loss at iteration [1875]: 0.0021971916455446115
Loss at iteration [1876]: 0.0022316319854774765
***** Warning: Loss has increased *****
Loss at iteration [1877]: 0.0022226327601681996
Loss at iteration [1878]: 0.002232670265587362
***** Warning: Loss has increased *****
Loss at iteration [1879]: 0.002261176387015031
***** Warning: Loss has increased *****
Loss at iteration [1880]: 0.0022572772930051586
Loss at iteration [1881]: 0.0022887244320408625
***** Warning: Loss has increased *****
Loss at iteration [1882]: 0.002313395294711602
***** Warning: Loss has increased *****
Loss at iteration [1883]: 0.002340413342781821
***** Warning: Loss has increased *****
Loss at iteration [1884]: 0.0023806007349247782
***** Warning: Loss has increased *****
Loss at iteration [1885]: 0.002441908342644732
***** Warning: Loss has increased *****
Loss at iteration [1886]: 0.0024778015761192326
***** Warning: Loss has increased *****
Loss at iteration [1887]: 0.002575241258739716
***** Warning: Loss has increased *****
Loss at iteration [1888]: 0.0026188022800343936
***** Warning: Loss has increased *****
Loss at iteration [1889]: 0.0027301868433839828
***** Warning: Loss has increased *****
Loss at iteration [1890]: 0.0027592976994087535
***** Warning: Loss has increased *****
Loss at iteration [1891]: 0.002863630185454245
***** Warning: Loss has increased *****
Loss at iteration [1892]: 0.002832682630797376
Loss at iteration [1893]: 0.002886323560351569
***** Warning: Loss has increased *****
Loss at iteration [1894]: 0.0027742578864568272
Loss at iteration [1895]: 0.0027309775019456395
Loss at iteration [1896]: 0.002566536962912059
Loss at iteration [1897]: 0.0024440814850369063
Loss at iteration [1898]: 0.0023022803054608514
Loss at iteration [1899]: 0.002206039488114139
Loss at iteration [1900]: 0.0021476257603958307
Loss at iteration [1901]: 0.0021372565661362678
Loss at iteration [1902]: 0.002158657182261796
***** Warning: Loss has increased *****
Loss at iteration [1903]: 0.0021979881415064655
***** Warning: Loss has increased *****
Loss at iteration [1904]: 0.0022451341840762805
***** Warning: Loss has increased *****
Loss at iteration [1905]: 0.0022731674912444062
***** Warning: Loss has increased *****
Loss at iteration [1906]: 0.002286311405319051
***** Warning: Loss has increased *****
Loss at iteration [1907]: 0.0022688011427707874
Loss at iteration [1908]: 0.002242161654825131
Loss at iteration [1909]: 0.0022010507130751232
Loss at iteration [1910]: 0.002166913816347416
Loss at iteration [1911]: 0.0021402840034186114
Loss at iteration [1912]: 0.002127925274399497
Loss at iteration [1913]: 0.0021286937248398263
***** Warning: Loss has increased *****
Loss at iteration [1914]: 0.002137814115896776
***** Warning: Loss has increased *****
Loss at iteration [1915]: 0.0021523090239996597
***** Warning: Loss has increased *****
Loss at iteration [1916]: 0.00216507019441609
***** Warning: Loss has increased *****
Loss at iteration [1917]: 0.002174339418187211
***** Warning: Loss has increased *****
Loss at iteration [1918]: 0.002176059036462588
***** Warning: Loss has increased *****
Loss at iteration [1919]: 0.0021737840433891146
Loss at iteration [1920]: 0.002164493281610225
Loss at iteration [1921]: 0.0021551722961390073
Loss at iteration [1922]: 0.0021433113228863433
Loss at iteration [1923]: 0.0021334359915991846
Loss at iteration [1924]: 0.002127083271471049
Loss at iteration [1925]: 0.0021235364931627386
Loss at iteration [1926]: 0.0021225036044201606
Loss at iteration [1927]: 0.002123525815810335
***** Warning: Loss has increased *****
Loss at iteration [1928]: 0.002125494100827743
***** Warning: Loss has increased *****
Loss at iteration [1929]: 0.002128612428044033
***** Warning: Loss has increased *****
Loss at iteration [1930]: 0.0021314485217603977
***** Warning: Loss has increased *****
Loss at iteration [1931]: 0.0021343222742514763
***** Warning: Loss has increased *****
Loss at iteration [1932]: 0.0021389029715976685
***** Warning: Loss has increased *****
Loss at iteration [1933]: 0.0021388833095989913
Loss at iteration [1934]: 0.002140602722668597
***** Warning: Loss has increased *****
Loss at iteration [1935]: 0.0021410452717739257
***** Warning: Loss has increased *****
Loss at iteration [1936]: 0.002141663335055632
***** Warning: Loss has increased *****
Loss at iteration [1937]: 0.0021410599233103724
Loss at iteration [1938]: 0.0021409095343644454
Loss at iteration [1939]: 0.0021411272561639837
***** Warning: Loss has increased *****
Loss at iteration [1940]: 0.0021408634842051775
Loss at iteration [1941]: 0.0021410153991840183
***** Warning: Loss has increased *****
Loss at iteration [1942]: 0.002142106072828227
***** Warning: Loss has increased *****
Loss at iteration [1943]: 0.002142524921625226
***** Warning: Loss has increased *****
Loss at iteration [1944]: 0.0021439474237261503
***** Warning: Loss has increased *****
Loss at iteration [1945]: 0.002145011188459776
***** Warning: Loss has increased *****
Loss at iteration [1946]: 0.0021485351175888965
***** Warning: Loss has increased *****
Loss at iteration [1947]: 0.002150074549777488
***** Warning: Loss has increased *****
Loss at iteration [1948]: 0.002155696276605819
***** Warning: Loss has increased *****
Loss at iteration [1949]: 0.002159814609200394
***** Warning: Loss has increased *****
Loss at iteration [1950]: 0.0021683075339439246
***** Warning: Loss has increased *****
Loss at iteration [1951]: 0.002176694248214588
***** Warning: Loss has increased *****
Loss at iteration [1952]: 0.0021906233264192636
***** Warning: Loss has increased *****
Loss at iteration [1953]: 0.0022041871144288525
***** Warning: Loss has increased *****
Loss at iteration [1954]: 0.0022285394463231675
***** Warning: Loss has increased *****
Loss at iteration [1955]: 0.0022512745440605914
***** Warning: Loss has increased *****
Loss at iteration [1956]: 0.0022922409212284777
***** Warning: Loss has increased *****
Loss at iteration [1957]: 0.0023311563141291875
***** Warning: Loss has increased *****
Loss at iteration [1958]: 0.0023964411629124856
***** Warning: Loss has increased *****
Loss at iteration [1959]: 0.00244902997707173
***** Warning: Loss has increased *****
Loss at iteration [1960]: 0.0025522216285223383
***** Warning: Loss has increased *****
Loss at iteration [1961]: 0.002613929569752846
***** Warning: Loss has increased *****
Loss at iteration [1962]: 0.0027566968693282503
***** Warning: Loss has increased *****
Loss at iteration [1963]: 0.0028034482772660778
***** Warning: Loss has increased *****
Loss at iteration [1964]: 0.0029568741915407216
***** Warning: Loss has increased *****
Loss at iteration [1965]: 0.0029397343989866495
Loss at iteration [1966]: 0.003033539237215903
***** Warning: Loss has increased *****
Loss at iteration [1967]: 0.0029044233857323976
Loss at iteration [1968]: 0.0028626289814370444
Loss at iteration [1969]: 0.002644677972989641
Loss at iteration [1970]: 0.0024900695546263393
Loss at iteration [1971]: 0.002305998499427109
Loss at iteration [1972]: 0.002186862459482542
Loss at iteration [1973]: 0.0021253957866384085
Loss at iteration [1974]: 0.0021256876181888476
***** Warning: Loss has increased *****
Loss at iteration [1975]: 0.0021703943579971167
***** Warning: Loss has increased *****
Loss at iteration [1976]: 0.0022308905532880123
***** Warning: Loss has increased *****
Loss at iteration [1977]: 0.002291799752284366
***** Warning: Loss has increased *****
Loss at iteration [1978]: 0.0023110530264591855
***** Warning: Loss has increased *****
Loss at iteration [1979]: 0.0023129418933310747
***** Warning: Loss has increased *****
Loss at iteration [1980]: 0.0022688100166284694
Loss at iteration [1981]: 0.002219220444872026
Loss at iteration [1982]: 0.002164966268311046
Loss at iteration [1983]: 0.002128654611888704
Loss at iteration [1984]: 0.002113177797610944
Loss at iteration [1985]: 0.002118279302465845
***** Warning: Loss has increased *****
Loss at iteration [1986]: 0.0021367941133477964
***** Warning: Loss has increased *****
Loss at iteration [1987]: 0.0021582605974269234
***** Warning: Loss has increased *****
Loss at iteration [1988]: 0.00217682824799064
***** Warning: Loss has increased *****
Loss at iteration [1989]: 0.0021819755278995132
***** Warning: Loss has increased *****
Loss at iteration [1990]: 0.002179705654085655
Loss at iteration [1991]: 0.0021638602437403127
Loss at iteration [1992]: 0.002147007495782271
Loss at iteration [1993]: 0.002129798286435289
Loss at iteration [1994]: 0.0021178115222319103
Loss at iteration [1995]: 0.002111890082235754
Loss at iteration [1996]: 0.002112262978871766
***** Warning: Loss has increased *****
Loss at iteration [1997]: 0.0021171619095633715
***** Warning: Loss has increased *****
Loss at iteration [1998]: 0.002124100912813379
***** Warning: Loss has increased *****
Loss at iteration [1999]: 0.002131372291680641
***** Warning: Loss has increased *****
Loss at iteration [2000]: 0.0021363936628675255
***** Warning: Loss has increased *****
Loss at iteration [2001]: 0.002139720906202491
***** Warning: Loss has increased *****
Loss at iteration [2002]: 0.002142383652532825
***** Warning: Loss has increased *****
Loss at iteration [2003]: 0.0021376097929826543
Loss at iteration [2004]: 0.0021331600322411
Loss at iteration [2005]: 0.0021283839400849927
Loss at iteration [2006]: 0.002122814389489912
Loss at iteration [2007]: 0.002117910372964965
Loss at iteration [2008]: 0.0021140085817296745
Loss at iteration [2009]: 0.0021108578480379782
Loss at iteration [2010]: 0.002108807027136459
Loss at iteration [2011]: 0.0021078892969670464
Loss at iteration [2012]: 0.0021079615849465168
***** Warning: Loss has increased *****
Loss at iteration [2013]: 0.0021087059239744458
***** Warning: Loss has increased *****
Loss at iteration [2014]: 0.002112274848936112
***** Warning: Loss has increased *****
Loss at iteration [2015]: 0.002111386338600757
Loss at iteration [2016]: 0.0021131285508591805
***** Warning: Loss has increased *****
Loss at iteration [2017]: 0.0021149587166401037
***** Warning: Loss has increased *****
Loss at iteration [2018]: 0.0021169529218416916
***** Warning: Loss has increased *****
Loss at iteration [2019]: 0.0021189037579423743
***** Warning: Loss has increased *****
Loss at iteration [2020]: 0.0021213218191899583
***** Warning: Loss has increased *****
Loss at iteration [2021]: 0.002123642211365729
***** Warning: Loss has increased *****
Loss at iteration [2022]: 0.0021267310368716
***** Warning: Loss has increased *****
Loss at iteration [2023]: 0.002130209360831791
***** Warning: Loss has increased *****
Loss at iteration [2024]: 0.0021348564520689223
***** Warning: Loss has increased *****
Loss at iteration [2025]: 0.002140734475369267
***** Warning: Loss has increased *****
Loss at iteration [2026]: 0.002147938020347757
***** Warning: Loss has increased *****
Loss at iteration [2027]: 0.0021568186439074044
***** Warning: Loss has increased *****
Loss at iteration [2028]: 0.002169798770401042
***** Warning: Loss has increased *****
Loss at iteration [2029]: 0.0021815364058159793
***** Warning: Loss has increased *****
Loss at iteration [2030]: 0.0022001136061973858
***** Warning: Loss has increased *****
Loss at iteration [2031]: 0.002216447340871043
***** Warning: Loss has increased *****
Loss at iteration [2032]: 0.0022457254999139254
***** Warning: Loss has increased *****
Loss at iteration [2033]: 0.0022705314467310147
***** Warning: Loss has increased *****
Loss at iteration [2034]: 0.0023167947646647886
***** Warning: Loss has increased *****
Loss at iteration [2035]: 0.002353866718989913
***** Warning: Loss has increased *****
Loss at iteration [2036]: 0.002424101270823514
***** Warning: Loss has increased *****
Loss at iteration [2037]: 0.0024685227029952275
***** Warning: Loss has increased *****
Loss at iteration [2038]: 0.0025643630881277647
***** Warning: Loss has increased *****
Loss at iteration [2039]: 0.002606747978737915
***** Warning: Loss has increased *****
Loss at iteration [2040]: 0.0027239279184666453
***** Warning: Loss has increased *****
Loss at iteration [2041]: 0.0027461113813952755
***** Warning: Loss has increased *****
Loss at iteration [2042]: 0.002857926674778711
***** Warning: Loss has increased *****
Loss at iteration [2043]: 0.002822160166189759
Loss at iteration [2044]: 0.002871020743057781
***** Warning: Loss has increased *****
Loss at iteration [2045]: 0.0027490081282751434
Loss at iteration [2046]: 0.002690806952010408
Loss at iteration [2047]: 0.0025200229469598216
Loss at iteration [2048]: 0.002395326845339943
Loss at iteration [2049]: 0.0022566153464808427
Loss at iteration [2050]: 0.0021659973754964996
Loss at iteration [2051]: 0.0021157269369652732
Loss at iteration [2052]: 0.002108230931054445
Loss at iteration [2053]: 0.002133007383713749
***** Warning: Loss has increased *****
Loss at iteration [2054]: 0.0021742757632522507
***** Warning: Loss has increased *****
Loss at iteration [2055]: 0.002221084790886823
***** Warning: Loss has increased *****
Loss at iteration [2056]: 0.0022478948896948858
***** Warning: Loss has increased *****
Loss at iteration [2057]: 0.0022657565173924902
***** Warning: Loss has increased *****
Loss at iteration [2058]: 0.0022496345044101087
Loss at iteration [2059]: 0.002228784580280723
Loss at iteration [2060]: 0.0021866535353635887
Loss at iteration [2061]: 0.002151742143402996
Loss at iteration [2062]: 0.0021227370585813637
Loss at iteration [2063]: 0.002106917532444001
Loss at iteration [2064]: 0.0021038221366952207
Loss at iteration [2065]: 0.002110655031848127
***** Warning: Loss has increased *****
Loss at iteration [2066]: 0.0021235569196402845
***** Warning: Loss has increased *****
Loss at iteration [2067]: 0.002136687077785412
***** Warning: Loss has increased *****
Loss at iteration [2068]: 0.002149466472115927
***** Warning: Loss has increased *****
Loss at iteration [2069]: 0.0021559136274850333
***** Warning: Loss has increased *****
Loss at iteration [2070]: 0.002159820675860862
***** Warning: Loss has increased *****
Loss at iteration [2071]: 0.0021556280736247223
Loss at iteration [2072]: 0.002151440996569089
Loss at iteration [2073]: 0.0021381405185092696
Loss at iteration [2074]: 0.0021278701450784285
Loss at iteration [2075]: 0.002118050751943222
Loss at iteration [2076]: 0.002110642540042144
Loss at iteration [2077]: 0.0021053586029292767
Loss at iteration [2078]: 0.0021023394298539384
Loss at iteration [2079]: 0.002101260720440141
Loss at iteration [2080]: 0.0021017524389010233
***** Warning: Loss has increased *****
Loss at iteration [2081]: 0.0021034059995965362
***** Warning: Loss has increased *****
Loss at iteration [2082]: 0.002107662513614848
***** Warning: Loss has increased *****
Loss at iteration [2083]: 0.002109238977248099
***** Warning: Loss has increased *****
Loss at iteration [2084]: 0.0021135451149821177
***** Warning: Loss has increased *****
Loss at iteration [2085]: 0.0021177097527328493
***** Warning: Loss has increased *****
Loss at iteration [2086]: 0.0021218536505200826
***** Warning: Loss has increased *****
Loss at iteration [2087]: 0.0021265565912329523
***** Warning: Loss has increased *****
Loss at iteration [2088]: 0.002130151361150947
***** Warning: Loss has increased *****
Loss at iteration [2089]: 0.002134633680365672
***** Warning: Loss has increased *****
Loss at iteration [2090]: 0.0021381724517515956
***** Warning: Loss has increased *****
Loss at iteration [2091]: 0.002144189989799972
***** Warning: Loss has increased *****
Loss at iteration [2092]: 0.0021494782395107704
***** Warning: Loss has increased *****
Loss at iteration [2093]: 0.002158941489926658
***** Warning: Loss has increased *****
Loss at iteration [2094]: 0.002163821438545208
***** Warning: Loss has increased *****
Loss at iteration [2095]: 0.0021735902549730513
***** Warning: Loss has increased *****
Loss at iteration [2096]: 0.002180119870240999
***** Warning: Loss has increased *****
Loss at iteration [2097]: 0.002192776158147977
***** Warning: Loss has increased *****
Loss at iteration [2098]: 0.002201899985737369
***** Warning: Loss has increased *****
Loss at iteration [2099]: 0.002220339615920204
***** Warning: Loss has increased *****
Loss at iteration [2100]: 0.00223359936021634
***** Warning: Loss has increased *****
Loss at iteration [2101]: 0.0022603769407847996
***** Warning: Loss has increased *****
Loss at iteration [2102]: 0.002278777224742025
***** Warning: Loss has increased *****
Loss at iteration [2103]: 0.0023174561183598613
***** Warning: Loss has increased *****
Loss at iteration [2104]: 0.002341037138750567
***** Warning: Loss has increased *****
Loss at iteration [2105]: 0.0023932445765153846
***** Warning: Loss has increased *****
Loss at iteration [2106]: 0.0024162840505417066
***** Warning: Loss has increased *****
Loss at iteration [2107]: 0.0024788379186306995
***** Warning: Loss has increased *****
Loss at iteration [2108]: 0.0024982368793910925
***** Warning: Loss has increased *****
Loss at iteration [2109]: 0.002565667840855253
***** Warning: Loss has increased *****
Loss at iteration [2110]: 0.002568580369185413
***** Warning: Loss has increased *****
Loss at iteration [2111]: 0.002627209193302743
***** Warning: Loss has increased *****
Loss at iteration [2112]: 0.002598939827276544
Loss at iteration [2113]: 0.0026218666629992254
***** Warning: Loss has increased *****
Loss at iteration [2114]: 0.0025531771417139837
Loss at iteration [2115]: 0.002524534062077905
Loss at iteration [2116]: 0.0024337284250992583
Loss at iteration [2117]: 0.0023692778784135844
Loss at iteration [2118]: 0.002281295907894928
Loss at iteration [2119]: 0.002216620394905046
Loss at iteration [2120]: 0.0021580163577127887
Loss at iteration [2121]: 0.002120574340043422
Loss at iteration [2122]: 0.0021012151548836016
Loss at iteration [2123]: 0.002098472577470952
Loss at iteration [2124]: 0.002107946685147995
***** Warning: Loss has increased *****
Loss at iteration [2125]: 0.0021251481670783987
***** Warning: Loss has increased *****
Loss at iteration [2126]: 0.0021465702849995253
***** Warning: Loss has increased *****
Loss at iteration [2127]: 0.0021678832315552067
***** Warning: Loss has increased *****
Loss at iteration [2128]: 0.0021850602279147946
***** Warning: Loss has increased *****
Loss at iteration [2129]: 0.002194440411589821
***** Warning: Loss has increased *****
Loss at iteration [2130]: 0.002202580903584475
***** Warning: Loss has increased *****
Loss at iteration [2131]: 0.0021976497727110604
Loss at iteration [2132]: 0.002192808814758471
Loss at iteration [2133]: 0.0021782933020910123
Loss at iteration [2134]: 0.002166238700662694
Loss at iteration [2135]: 0.0021501364796310467
Loss at iteration [2136]: 0.0021372133407754337
Loss at iteration [2137]: 0.0021245784925378857
Loss at iteration [2138]: 0.00211515554348378
Loss at iteration [2139]: 0.002106792798776919
Loss at iteration [2140]: 0.0021015917494830423
Loss at iteration [2141]: 0.00209822302535365
Loss at iteration [2142]: 0.002096133210150784
Loss at iteration [2143]: 0.002094943200374353
Loss at iteration [2144]: 0.002094357488577785
Loss at iteration [2145]: 0.0020943601541332005
***** Warning: Loss has increased *****
Loss at iteration [2146]: 0.0020950223825562386
***** Warning: Loss has increased *****
Loss at iteration [2147]: 0.0020965002638801233
***** Warning: Loss has increased *****
Loss at iteration [2148]: 0.002100182539265535
***** Warning: Loss has increased *****
Loss at iteration [2149]: 0.0021014222427151777
***** Warning: Loss has increased *****
Loss at iteration [2150]: 0.002106153710335339
***** Warning: Loss has increased *****
Loss at iteration [2151]: 0.002112460236645025
***** Warning: Loss has increased *****
Loss at iteration [2152]: 0.0021198415739906726
***** Warning: Loss has increased *****
Loss at iteration [2153]: 0.0021298030742710255
***** Warning: Loss has increased *****
Loss at iteration [2154]: 0.0021412010984085575
***** Warning: Loss has increased *****
Loss at iteration [2155]: 0.002159426900010248
***** Warning: Loss has increased *****
Loss at iteration [2156]: 0.002179983808982982
***** Warning: Loss has increased *****
Loss at iteration [2157]: 0.002212154189568116
***** Warning: Loss has increased *****
Loss at iteration [2158]: 0.0022451589081627283
***** Warning: Loss has increased *****
Loss at iteration [2159]: 0.0023007166319285216
***** Warning: Loss has increased *****
Loss at iteration [2160]: 0.0023552507451848853
***** Warning: Loss has increased *****
Loss at iteration [2161]: 0.0024483502027690895
***** Warning: Loss has increased *****
Loss at iteration [2162]: 0.002525319048115477
***** Warning: Loss has increased *****
Loss at iteration [2163]: 0.0026799526070658743
***** Warning: Loss has increased *****
Loss at iteration [2164]: 0.002768540346596328
***** Warning: Loss has increased *****
Loss at iteration [2165]: 0.002974003649415154
***** Warning: Loss has increased *****
Loss at iteration [2166]: 0.0030189155887800742
***** Warning: Loss has increased *****
Loss at iteration [2167]: 0.003212629908860023
***** Warning: Loss has increased *****
Loss at iteration [2168]: 0.0031383788500832953
Loss at iteration [2169]: 0.00320254011261171
***** Warning: Loss has increased *****
Loss at iteration [2170]: 0.0029742175792195026
Loss at iteration [2171]: 0.0028412651605445056
Loss at iteration [2172]: 0.0025555386500701518
Loss at iteration [2173]: 0.0023498018516358568
Loss at iteration [2174]: 0.0021793678787134785
Loss at iteration [2175]: 0.002102144880795695
Loss at iteration [2176]: 0.002108854376197321
***** Warning: Loss has increased *****
Loss at iteration [2177]: 0.002174880711752139
***** Warning: Loss has increased *****
Loss at iteration [2178]: 0.0022704575499999766
***** Warning: Loss has increased *****
Loss at iteration [2179]: 0.0023392256909478683
***** Warning: Loss has increased *****
Loss at iteration [2180]: 0.002390768784948531
***** Warning: Loss has increased *****
Loss at iteration [2181]: 0.002362524684952022
Loss at iteration [2182]: 0.002318170626826575
Loss at iteration [2183]: 0.0022324740464050984
Loss at iteration [2184]: 0.002161017492549556
Loss at iteration [2185]: 0.0021102112217606488
Loss at iteration [2186]: 0.0020931979737091387
Loss at iteration [2187]: 0.0021055844549924658
***** Warning: Loss has increased *****
Loss at iteration [2188]: 0.0021340350311429804
***** Warning: Loss has increased *****
Loss at iteration [2189]: 0.0021671099399280355
***** Warning: Loss has increased *****
Loss at iteration [2190]: 0.00218760638340021
***** Warning: Loss has increased *****
Loss at iteration [2191]: 0.002196597747423424
***** Warning: Loss has increased *****
Loss at iteration [2192]: 0.0021826611021105806
Loss at iteration [2193]: 0.0021615254463378873
Loss at iteration [2194]: 0.002132562147056308
Loss at iteration [2195]: 0.0021098184648156073
Loss at iteration [2196]: 0.0020956826594663073
Loss at iteration [2197]: 0.0020915264663437195
Loss at iteration [2198]: 0.0020954287652556907
***** Warning: Loss has increased *****
Loss at iteration [2199]: 0.002104159722879934
***** Warning: Loss has increased *****
Loss at iteration [2200]: 0.0021145806927065323
***** Warning: Loss has increased *****
Loss at iteration [2201]: 0.002122583030100803
***** Warning: Loss has increased *****
Loss at iteration [2202]: 0.0021278885066892644
***** Warning: Loss has increased *****
Loss at iteration [2203]: 0.002127657872222266
Loss at iteration [2204]: 0.002124620980480549
Loss at iteration [2205]: 0.0021183552938162783
Loss at iteration [2206]: 0.0021122929363939233
Loss at iteration [2207]: 0.002105626130547669
Loss at iteration [2208]: 0.002100031206158657
Loss at iteration [2209]: 0.002094985765585721
Loss at iteration [2210]: 0.0020910924655759616
Loss at iteration [2211]: 0.0020888681745093026
Loss at iteration [2212]: 0.0020882310017836104
Loss at iteration [2213]: 0.0020890126005355027
***** Warning: Loss has increased *****
Loss at iteration [2214]: 0.0020923653525667177
***** Warning: Loss has increased *****
Loss at iteration [2215]: 0.00209265755693622
***** Warning: Loss has increased *****
Loss at iteration [2216]: 0.0020954706042390445
***** Warning: Loss has increased *****
Loss at iteration [2217]: 0.002098938913733353
***** Warning: Loss has increased *****
Loss at iteration [2218]: 0.002102658235678538
***** Warning: Loss has increased *****
Loss at iteration [2219]: 0.0021067087331249077
***** Warning: Loss has increased *****
Loss at iteration [2220]: 0.00211048389505668
***** Warning: Loss has increased *****
Loss at iteration [2221]: 0.002114268395408053
***** Warning: Loss has increased *****
Loss at iteration [2222]: 0.002117702028028555
***** Warning: Loss has increased *****
Loss at iteration [2223]: 0.0021228759088548197
***** Warning: Loss has increased *****
Loss at iteration [2224]: 0.0021273471976081854
***** Warning: Loss has increased *****
Loss at iteration [2225]: 0.002133877493834946
***** Warning: Loss has increased *****
Loss at iteration [2226]: 0.0021399332999366104
***** Warning: Loss has increased *****
Loss at iteration [2227]: 0.0021491924164494277
***** Warning: Loss has increased *****
Loss at iteration [2228]: 0.002157824307589198
***** Warning: Loss has increased *****
Loss at iteration [2229]: 0.002171642416730556
***** Warning: Loss has increased *****
Loss at iteration [2230]: 0.0021834351873073875
***** Warning: Loss has increased *****
Loss at iteration [2231]: 0.0022027085279805255
***** Warning: Loss has increased *****
Loss at iteration [2232]: 0.0022189216328532143
***** Warning: Loss has increased *****
Loss at iteration [2233]: 0.002247085111367185
***** Warning: Loss has increased *****
Loss at iteration [2234]: 0.00226946117966964
***** Warning: Loss has increased *****
Loss at iteration [2235]: 0.002308712085842791
***** Warning: Loss has increased *****
Loss at iteration [2236]: 0.002335355883199909
***** Warning: Loss has increased *****
Loss at iteration [2237]: 0.002387711956370463
***** Warning: Loss has increased *****
Loss at iteration [2238]: 0.002419853885087889
***** Warning: Loss has increased *****
Loss at iteration [2239]: 0.002489959638072914
***** Warning: Loss has increased *****
Loss at iteration [2240]: 0.002524090377889956
***** Warning: Loss has increased *****
Loss at iteration [2241]: 0.002609198424202052
***** Warning: Loss has increased *****
Loss at iteration [2242]: 0.00262576883721237
***** Warning: Loss has increased *****
Loss at iteration [2243]: 0.002702335333284543
***** Warning: Loss has increased *****
Loss at iteration [2244]: 0.0026832159168785996
Loss at iteration [2245]: 0.002721216321254321
***** Warning: Loss has increased *****
Loss at iteration [2246]: 0.0026559787501377733
Loss at iteration [2247]: 0.0026379162993826286
Loss at iteration [2248]: 0.00253903321387242
Loss at iteration [2249]: 0.002469722482696305
Loss at iteration [2250]: 0.002360974216550913
Loss at iteration [2251]: 0.0022791517183343235
Loss at iteration [2252]: 0.002197040325163591
Loss at iteration [2253]: 0.002140296838043735
Loss at iteration [2254]: 0.002103134300710095
Loss at iteration [2255]: 0.0020875203551307835
Loss at iteration [2256]: 0.0020895620857292628
***** Warning: Loss has increased *****
Loss at iteration [2257]: 0.0021036435270593533
***** Warning: Loss has increased *****
Loss at iteration [2258]: 0.0021257591178975507
***** Warning: Loss has increased *****
Loss at iteration [2259]: 0.00214848278309687
***** Warning: Loss has increased *****
Loss at iteration [2260]: 0.002172167312856891
***** Warning: Loss has increased *****
Loss at iteration [2261]: 0.002190231934783553
***** Warning: Loss has increased *****
Loss at iteration [2262]: 0.002204244478045697
***** Warning: Loss has increased *****
Loss at iteration [2263]: 0.002210222647614751
***** Warning: Loss has increased *****
Loss at iteration [2264]: 0.002217002691701849
***** Warning: Loss has increased *****
Loss at iteration [2265]: 0.0022106833826776784
Loss at iteration [2266]: 0.002204647274672783
Loss at iteration [2267]: 0.0021889334066719853
Loss at iteration [2268]: 0.0021760533431225693
Loss at iteration [2269]: 0.0021591721463722065
Loss at iteration [2270]: 0.0021463828038807585
Loss at iteration [2271]: 0.0021341113607667517
Loss at iteration [2272]: 0.0021231994655458706
Loss at iteration [2273]: 0.0021124296437184083
Loss at iteration [2274]: 0.0021043980861212585
Loss at iteration [2275]: 0.0020978141994899913
Loss at iteration [2276]: 0.0020930878760553402
Loss at iteration [2277]: 0.0020897430148351156
Loss at iteration [2278]: 0.0020874638074454303
Loss at iteration [2279]: 0.0020855601998276095
Loss at iteration [2280]: 0.0020849830150669285
Loss at iteration [2281]: 0.002084132135178187
Loss at iteration [2282]: 0.0020842479070834604
***** Warning: Loss has increased *****
Loss at iteration [2283]: 0.0020839481558625242
Loss at iteration [2284]: 0.002083065787718349
Loss at iteration [2285]: 0.002082016594849736
Loss at iteration [2286]: 0.0020813032158515823
Loss at iteration [2287]: 0.0020836709122573314
***** Warning: Loss has increased *****
Loss at iteration [2288]: 0.0020811924048417174
Loss at iteration [2289]: 0.002081563971106592
***** Warning: Loss has increased *****
Loss at iteration [2290]: 0.0020820525164814245
***** Warning: Loss has increased *****
Loss at iteration [2291]: 0.002082493066978348
***** Warning: Loss has increased *****
Loss at iteration [2292]: 0.0020829323344074735
***** Warning: Loss has increased *****
Loss at iteration [2293]: 0.0020836149070428377
***** Warning: Loss has increased *****
Loss at iteration [2294]: 0.002084969615189947
***** Warning: Loss has increased *****
Loss at iteration [2295]: 0.0020885343451901924
***** Warning: Loss has increased *****
Loss at iteration [2296]: 0.002093151613191819
***** Warning: Loss has increased *****
Loss at iteration [2297]: 0.0021000314390512524
***** Warning: Loss has increased *****
Loss at iteration [2298]: 0.002112157165254502
***** Warning: Loss has increased *****
Loss at iteration [2299]: 0.002130816424325908
***** Warning: Loss has increased *****
Loss at iteration [2300]: 0.002161525099077283
***** Warning: Loss has increased *****
Loss at iteration [2301]: 0.0022060931249510193
***** Warning: Loss has increased *****
Loss at iteration [2302]: 0.002282680083199822
***** Warning: Loss has increased *****
Loss at iteration [2303]: 0.0023862760504090056
***** Warning: Loss has increased *****
Loss at iteration [2304]: 0.0025742354385195274
***** Warning: Loss has increased *****
Loss at iteration [2305]: 0.002796002869363886
***** Warning: Loss has increased *****
Loss at iteration [2306]: 0.003228437961834477
***** Warning: Loss has increased *****
Loss at iteration [2307]: 0.0036270273475041124
***** Warning: Loss has increased *****
Loss at iteration [2308]: 0.004454584147878746
***** Warning: Loss has increased *****
Loss at iteration [2309]: 0.004855849641994668
***** Warning: Loss has increased *****
Loss at iteration [2310]: 0.005822322102810211
***** Warning: Loss has increased *****
Loss at iteration [2311]: 0.005521598630447981
Loss at iteration [2312]: 0.0054847658875336915
Loss at iteration [2313]: 0.004247270324933118
Loss at iteration [2314]: 0.0031672235205132044
Loss at iteration [2315]: 0.0023289576206917822
Loss at iteration [2316]: 0.002128976601319106
Loss at iteration [2317]: 0.0025022124904664216
***** Warning: Loss has increased *****
Loss at iteration [2318]: 0.0030073039924423902
***** Warning: Loss has increased *****
Loss at iteration [2319]: 0.0033185887023039185
***** Warning: Loss has increased *****
Loss at iteration [2320]: 0.0030510195233667704
Loss at iteration [2321]: 0.0025609478075562463
Loss at iteration [2322]: 0.0021607320182039425
Loss at iteration [2323]: 0.0021298844371432095
Loss at iteration [2324]: 0.0023846348086060385
***** Warning: Loss has increased *****
Loss at iteration [2325]: 0.002613879233152261
***** Warning: Loss has increased *****
Loss at iteration [2326]: 0.0026207358450988385
***** Warning: Loss has increased *****
Loss at iteration [2327]: 0.0023794672066692747
Loss at iteration [2328]: 0.0021572889898900456
Loss at iteration [2329]: 0.0021068698932782183
Loss at iteration [2330]: 0.0022185571459043055
***** Warning: Loss has increased *****
Loss at iteration [2331]: 0.0023493812279179386
***** Warning: Loss has increased *****
Loss at iteration [2332]: 0.0023548439999546826
***** Warning: Loss has increased *****
Loss at iteration [2333]: 0.002261162980065387
Loss at iteration [2334]: 0.002137445834523751
Loss at iteration [2335]: 0.0020968218604351507
Loss at iteration [2336]: 0.002145865888292161
***** Warning: Loss has increased *****
Loss at iteration [2337]: 0.002222146566654652
***** Warning: Loss has increased *****
Loss at iteration [2338]: 0.0022423425067586854
***** Warning: Loss has increased *****
Loss at iteration [2339]: 0.0021893806703370488
Loss at iteration [2340]: 0.002121080002752522
Loss at iteration [2341]: 0.002088078439041246
Loss at iteration [2342]: 0.002109938120750985
***** Warning: Loss has increased *****
Loss at iteration [2343]: 0.002153617255235944
***** Warning: Loss has increased *****
Loss at iteration [2344]: 0.0021681397345395823
***** Warning: Loss has increased *****
Loss at iteration [2345]: 0.0021474133195507846
Loss at iteration [2346]: 0.0021080900962499763
Loss at iteration [2347]: 0.00208701557456212
Loss at iteration [2348]: 0.002093567446117634
***** Warning: Loss has increased *****
Loss at iteration [2349]: 0.0021146921624469717
***** Warning: Loss has increased *****
Loss at iteration [2350]: 0.0021293988846447552
***** Warning: Loss has increased *****
Loss at iteration [2351]: 0.002123535285710033
Loss at iteration [2352]: 0.0021055028322605153
Loss at iteration [2353]: 0.0020889539781728836
Loss at iteration [2354]: 0.002084455424820067
Loss at iteration [2355]: 0.00209187939085713
***** Warning: Loss has increased *****
Loss at iteration [2356]: 0.0021019119748038944
***** Warning: Loss has increased *****
Loss at iteration [2357]: 0.0021055240414437726
***** Warning: Loss has increased *****
Loss at iteration [2358]: 0.0020996695206090344
Loss at iteration [2359]: 0.0020897462127304747
Loss at iteration [2360]: 0.0020832340173296887
Loss at iteration [2361]: 0.0020827056371402583
Loss at iteration [2362]: 0.0020881978670938372
***** Warning: Loss has increased *****
Loss at iteration [2363]: 0.0020935555840584075
***** Warning: Loss has increased *****
Loss at iteration [2364]: 0.002094433208763947
***** Warning: Loss has increased *****
Loss at iteration [2365]: 0.0020904767976967524
Loss at iteration [2366]: 0.002085100019441789
Loss at iteration [2367]: 0.002081512552052509
Loss at iteration [2368]: 0.0020819493991411434
***** Warning: Loss has increased *****
Loss at iteration [2369]: 0.002083747821259255
***** Warning: Loss has increased *****
Loss at iteration [2370]: 0.0020853855971117493
***** Warning: Loss has increased *****
Loss at iteration [2371]: 0.002086314577300175
***** Warning: Loss has increased *****
Loss at iteration [2372]: 0.002085375165861037
Loss at iteration [2373]: 0.0020833745505366313
Loss at iteration [2374]: 0.0020810619947992247
Loss at iteration [2375]: 0.0020796322394637454
Loss at iteration [2376]: 0.002079587380670125
Loss at iteration [2377]: 0.0020801888500301784
***** Warning: Loss has increased *****
Loss at iteration [2378]: 0.0020826697680042206
***** Warning: Loss has increased *****
Loss at iteration [2379]: 0.0020822414347944095
Loss at iteration [2380]: 0.0020829192738842633
***** Warning: Loss has increased *****
Loss at iteration [2381]: 0.0020825352664987063
Loss at iteration [2382]: 0.0020812020731385016
Loss at iteration [2383]: 0.0020796977418795575
Loss at iteration [2384]: 0.002078561678630414
Loss at iteration [2385]: 0.0020784091437926434
Loss at iteration [2386]: 0.002080252944425968
***** Warning: Loss has increased *****
Loss at iteration [2387]: 0.00207897043924413
Loss at iteration [2388]: 0.0020793565314136085
***** Warning: Loss has increased *****
Loss at iteration [2389]: 0.0020800014874621654
***** Warning: Loss has increased *****
Loss at iteration [2390]: 0.002080332052142852
***** Warning: Loss has increased *****
Loss at iteration [2391]: 0.002079793503825235
Loss at iteration [2392]: 0.0020791534332892715
Loss at iteration [2393]: 0.002078590784839927
Loss at iteration [2394]: 0.0020776965385309407
Loss at iteration [2395]: 0.002077288234394383
Loss at iteration [2396]: 0.002077068961898667
Loss at iteration [2397]: 0.0020770757597789887
***** Warning: Loss has increased *****
Loss at iteration [2398]: 0.0020771892815398462
***** Warning: Loss has increased *****
Loss at iteration [2399]: 0.002077098450078381
Loss at iteration [2400]: 0.00207695671789581
Loss at iteration [2401]: 0.0020767811796601307
Loss at iteration [2402]: 0.002076620414016976
Loss at iteration [2403]: 0.0020763852915471484
Loss at iteration [2404]: 0.002076246754167175
Loss at iteration [2405]: 0.002076089522983913
Loss at iteration [2406]: 0.0020760129954602064
Loss at iteration [2407]: 0.002075773321426443
Loss at iteration [2408]: 0.0020754533524374344
Loss at iteration [2409]: 0.0020753336176059017
Loss at iteration [2410]: 0.0020755063687840705
***** Warning: Loss has increased *****
Loss at iteration [2411]: 0.0020756219866236198
***** Warning: Loss has increased *****
Loss at iteration [2412]: 0.0020765792777132012
***** Warning: Loss has increased *****
Loss at iteration [2413]: 0.002077222360979051
***** Warning: Loss has increased *****
Loss at iteration [2414]: 0.002076992334050652
Loss at iteration [2415]: 0.0020761784197444832
Loss at iteration [2416]: 0.002075465200327667
Loss at iteration [2417]: 0.002075412647038612
Loss at iteration [2418]: 0.0020759123212995383
***** Warning: Loss has increased *****
Loss at iteration [2419]: 0.0020765438460783586
***** Warning: Loss has increased *****
Loss at iteration [2420]: 0.0020767997355012587
***** Warning: Loss has increased *****
Loss at iteration [2421]: 0.0020767007278839054
Loss at iteration [2422]: 0.002076561807485819
Loss at iteration [2423]: 0.002076731784298196
***** Warning: Loss has increased *****
Loss at iteration [2424]: 0.002077181845308739
***** Warning: Loss has increased *****
Loss at iteration [2425]: 0.0020780405074578966
***** Warning: Loss has increased *****
Loss at iteration [2426]: 0.002079016580477637
***** Warning: Loss has increased *****
Loss at iteration [2427]: 0.0020800786027731243
***** Warning: Loss has increased *****
Loss at iteration [2428]: 0.002081352956128263
***** Warning: Loss has increased *****
Loss at iteration [2429]: 0.0020830549062224074
***** Warning: Loss has increased *****
Loss at iteration [2430]: 0.002084922412924386
***** Warning: Loss has increased *****
Loss at iteration [2431]: 0.0020876683464542676
***** Warning: Loss has increased *****
Loss at iteration [2432]: 0.002090820864431632
***** Warning: Loss has increased *****
Loss at iteration [2433]: 0.0020948924924106055
***** Warning: Loss has increased *****
Loss at iteration [2434]: 0.0020995383958322994
***** Warning: Loss has increased *****
Loss at iteration [2435]: 0.002106060361237846
***** Warning: Loss has increased *****
Loss at iteration [2436]: 0.002112776073696501
***** Warning: Loss has increased *****
Loss at iteration [2437]: 0.002122438608964647
***** Warning: Loss has increased *****
Loss at iteration [2438]: 0.0021335422529952194
***** Warning: Loss has increased *****
Loss at iteration [2439]: 0.0021498499077621885
***** Warning: Loss has increased *****
Loss at iteration [2440]: 0.002167577409502876
***** Warning: Loss has increased *****
Loss at iteration [2441]: 0.002194163160184375
***** Warning: Loss has increased *****
Loss at iteration [2442]: 0.002222321682859823
***** Warning: Loss has increased *****
Loss at iteration [2443]: 0.0022660660171596055
***** Warning: Loss has increased *****
Loss at iteration [2444]: 0.002310062309163185
***** Warning: Loss has increased *****
Loss at iteration [2445]: 0.002380460701855963
***** Warning: Loss has increased *****
Loss at iteration [2446]: 0.002442378302763511
***** Warning: Loss has increased *****
Loss at iteration [2447]: 0.002549393399541721
***** Warning: Loss has increased *****
Loss at iteration [2448]: 0.002627865742274116
***** Warning: Loss has increased *****
Loss at iteration [2449]: 0.0027727749128766163
***** Warning: Loss has increased *****
Loss at iteration [2450]: 0.002849375211871276
***** Warning: Loss has increased *****
Loss at iteration [2451]: 0.0030092379819242903
***** Warning: Loss has increased *****
Loss at iteration [2452]: 0.003044507668859051
***** Warning: Loss has increased *****
Loss at iteration [2453]: 0.003168764211608393
***** Warning: Loss has increased *****
Loss at iteration [2454]: 0.003107317480032841
Loss at iteration [2455]: 0.003106330292521384
Loss at iteration [2456]: 0.0029192812752411724
Loss at iteration [2457]: 0.0027676097419109783
Loss at iteration [2458]: 0.0025303167952162534
Loss at iteration [2459]: 0.002342545043876594
Loss at iteration [2460]: 0.0021871482096482795
Loss at iteration [2461]: 0.002100392847115441
Loss at iteration [2462]: 0.0020774905015670648
Loss at iteration [2463]: 0.0021062114221877992
***** Warning: Loss has increased *****
Loss at iteration [2464]: 0.002167154395167557
***** Warning: Loss has increased *****
Loss at iteration [2465]: 0.002235581161143696
***** Warning: Loss has increased *****
Loss at iteration [2466]: 0.0023019492111321623
***** Warning: Loss has increased *****
Loss at iteration [2467]: 0.0023306979787940554
***** Warning: Loss has increased *****
Loss at iteration [2468]: 0.0023398651355387295
***** Warning: Loss has increased *****
Loss at iteration [2469]: 0.0023041376809010564
Loss at iteration [2470]: 0.002259434545839263
Loss at iteration [2471]: 0.0021971412634119855
Loss at iteration [2472]: 0.002144742208339437
Loss at iteration [2473]: 0.002102637355902932
Loss at iteration [2474]: 0.002078717453183136
Loss at iteration [2475]: 0.002071815117847161
Loss at iteration [2476]: 0.0020790979747161544
***** Warning: Loss has increased *****
Loss at iteration [2477]: 0.002096549013906303
***** Warning: Loss has increased *****
Loss at iteration [2478]: 0.00211481393098174
***** Warning: Loss has increased *****
Loss at iteration [2479]: 0.002133956709646513
***** Warning: Loss has increased *****
Loss at iteration [2480]: 0.0021465916279715
***** Warning: Loss has increased *****
Loss at iteration [2481]: 0.0021549263223015415
***** Warning: Loss has increased *****
Loss at iteration [2482]: 0.0021535942307358714
Loss at iteration [2483]: 0.002148808212935084
Loss at iteration [2484]: 0.0021368395356320914
Loss at iteration [2485]: 0.002124384974976583
Loss at iteration [2486]: 0.002110093707361013
Loss at iteration [2487]: 0.002097661318868756
Loss at iteration [2488]: 0.0020866910202496512
Loss at iteration [2489]: 0.0020786293536038964
Loss at iteration [2490]: 0.0020732493986186096
Loss at iteration [2491]: 0.002070383378694297
Loss at iteration [2492]: 0.002069593914079039
Loss at iteration [2493]: 0.0020703038076201902
***** Warning: Loss has increased *****
Loss at iteration [2494]: 0.0020719066663751665
***** Warning: Loss has increased *****
Loss at iteration [2495]: 0.002074345807925714
***** Warning: Loss has increased *****
Loss at iteration [2496]: 0.002077798889979703
***** Warning: Loss has increased *****
Loss at iteration [2497]: 0.0020819078864924246
***** Warning: Loss has increased *****
Loss at iteration [2498]: 0.0020868923231174845
***** Warning: Loss has increased *****
Loss at iteration [2499]: 0.002092604904029214
***** Warning: Loss has increased *****
Loss at iteration [2500]: 0.0020992610915779187
***** Warning: Loss has increased *****
Loss at iteration [2501]: 0.0021068350198761732
***** Warning: Loss has increased *****
Loss at iteration [2502]: 0.0021168215496068566
***** Warning: Loss has increased *****
Loss at iteration [2503]: 0.002128087418168677
***** Warning: Loss has increased *****
Loss at iteration [2504]: 0.0021430548793167086
***** Warning: Loss has increased *****
Loss at iteration [2505]: 0.002159371630977939
***** Warning: Loss has increased *****
Loss at iteration [2506]: 0.0021821087105119586
***** Warning: Loss has increased *****
Loss at iteration [2507]: 0.002206751966724978
***** Warning: Loss has increased *****
Loss at iteration [2508]: 0.002243075619010285
***** Warning: Loss has increased *****
Loss at iteration [2509]: 0.0022798348264385284
***** Warning: Loss has increased *****
Loss at iteration [2510]: 0.0023368645209204918
***** Warning: Loss has increased *****
Loss at iteration [2511]: 0.0023924666661048486
***** Warning: Loss has increased *****
Loss at iteration [2512]: 0.0024833633550722413
***** Warning: Loss has increased *****
Loss at iteration [2513]: 0.002561579012396784
***** Warning: Loss has increased *****
Loss at iteration [2514]: 0.002689295321472704
***** Warning: Loss has increased *****
Loss at iteration [2515]: 0.0027692965298215283
***** Warning: Loss has increased *****
Loss at iteration [2516]: 0.0029123166393489807
***** Warning: Loss has increased *****
Loss at iteration [2517]: 0.002960659238683812
***** Warning: Loss has increased *****
Loss at iteration [2518]: 0.003078014783388549
***** Warning: Loss has increased *****
Loss at iteration [2519]: 0.003049804571979292
Loss at iteration [2520]: 0.0030682661162083056
***** Warning: Loss has increased *****
Loss at iteration [2521]: 0.002930278757164227
Loss at iteration [2522]: 0.0028223329601130207
Loss at iteration [2523]: 0.0026168942034344206
Loss at iteration [2524]: 0.0024471248437154774
Loss at iteration [2525]: 0.0022730095522084147
Loss at iteration [2526]: 0.002153497518966314
Loss at iteration [2527]: 0.0020849622020096045
Loss at iteration [2528]: 0.0020704648895462866
Loss at iteration [2529]: 0.002097375901169549
***** Warning: Loss has increased *****
Loss at iteration [2530]: 0.0021488752590702294
***** Warning: Loss has increased *****
Loss at iteration [2531]: 0.0022120915132239735
***** Warning: Loss has increased *****
Loss at iteration [2532]: 0.0022650954056924267
***** Warning: Loss has increased *****
Loss at iteration [2533]: 0.002306498781821822
***** Warning: Loss has increased *****
Loss at iteration [2534]: 0.0023123079856669708
***** Warning: Loss has increased *****
Loss at iteration [2535]: 0.0023071528282021707
Loss at iteration [2536]: 0.002264903066473112
Loss at iteration [2537]: 0.002219757345911797
Loss at iteration [2538]: 0.0021659217387096905
Loss at iteration [2539]: 0.00212316331090472
Loss at iteration [2540]: 0.0020917283217790033
Loss at iteration [2541]: 0.0020750164113296343
Loss at iteration [2542]: 0.00206872231768663
Loss at iteration [2543]: 0.00207203945549671
***** Warning: Loss has increased *****
Loss at iteration [2544]: 0.002082619808776218
***** Warning: Loss has increased *****
Loss at iteration [2545]: 0.002097254564563594
***** Warning: Loss has increased *****
Loss at iteration [2546]: 0.002113295343591578
***** Warning: Loss has increased *****
Loss at iteration [2547]: 0.002126395992679074
***** Warning: Loss has increased *****
Loss at iteration [2548]: 0.002137211321138233
***** Warning: Loss has increased *****
Loss at iteration [2549]: 0.002142181860465809
***** Warning: Loss has increased *****
Loss at iteration [2550]: 0.0021449321202192057
***** Warning: Loss has increased *****
Loss at iteration [2551]: 0.0021425954984525823
Loss at iteration [2552]: 0.002142707777273408
***** Warning: Loss has increased *****
Loss at iteration [2553]: 0.002132169707459803
Loss at iteration [2554]: 0.002126111333095291
Loss at iteration [2555]: 0.0021176210963786105
Loss at iteration [2556]: 0.0021096102628716177
Loss at iteration [2557]: 0.0021006374033469763
Loss at iteration [2558]: 0.002093079155245512
Loss at iteration [2559]: 0.0020869458792332177
Loss at iteration [2560]: 0.0020824311193831323
Loss at iteration [2561]: 0.002078932767953495
Loss at iteration [2562]: 0.0020762728782042265
Loss at iteration [2563]: 0.0020738069507237724
Loss at iteration [2564]: 0.0020721608355491277
Loss at iteration [2565]: 0.002071174022888551
Loss at iteration [2566]: 0.0020708511923871287
Loss at iteration [2567]: 0.0020712695878146174
***** Warning: Loss has increased *****
Loss at iteration [2568]: 0.002072202905559014
***** Warning: Loss has increased *****
Loss at iteration [2569]: 0.0020738657424995387
***** Warning: Loss has increased *****
Loss at iteration [2570]: 0.002076648729793284
***** Warning: Loss has increased *****
Loss at iteration [2571]: 0.002080751126448591
***** Warning: Loss has increased *****
Loss at iteration [2572]: 0.002087233151238327
***** Warning: Loss has increased *****
Loss at iteration [2573]: 0.0020963568064953355
***** Warning: Loss has increased *****
Loss at iteration [2574]: 0.002110047459532696
***** Warning: Loss has increased *****
Loss at iteration [2575]: 0.0021291024341423695
***** Warning: Loss has increased *****
Loss at iteration [2576]: 0.002157432324139331
***** Warning: Loss has increased *****
Loss at iteration [2577]: 0.002194079571116751
***** Warning: Loss has increased *****
Loss at iteration [2578]: 0.0022518233827828866
***** Warning: Loss has increased *****
Loss at iteration [2579]: 0.0023232263602378997
***** Warning: Loss has increased *****
Loss at iteration [2580]: 0.0024387584953670096
***** Warning: Loss has increased *****
Loss at iteration [2581]: 0.002566925040829992
***** Warning: Loss has increased *****
Loss at iteration [2582]: 0.0027811159839580967
***** Warning: Loss has increased *****
Loss at iteration [2583]: 0.002980394842281121
***** Warning: Loss has increased *****
Loss at iteration [2584]: 0.0033226507786040774
***** Warning: Loss has increased *****
Loss at iteration [2585]: 0.0035380861956599285
***** Warning: Loss has increased *****
Loss at iteration [2586]: 0.003951429694344332
***** Warning: Loss has increased *****
Loss at iteration [2587]: 0.00401899946252245
***** Warning: Loss has increased *****
Loss at iteration [2588]: 0.004239930755649994
***** Warning: Loss has increased *****
Loss at iteration [2589]: 0.003914899997826205
Loss at iteration [2590]: 0.0036343079359138506
Loss at iteration [2591]: 0.003026703330695603
Loss at iteration [2592]: 0.002530424359662661
Loss at iteration [2593]: 0.0021757179188470776
Loss at iteration [2594]: 0.0020709183743431417
Loss at iteration [2595]: 0.0021830449284270695
***** Warning: Loss has increased *****
Loss at iteration [2596]: 0.002401644092947266
***** Warning: Loss has increased *****
Loss at iteration [2597]: 0.00262909289401683
***** Warning: Loss has increased *****
Loss at iteration [2598]: 0.002705271368872678
***** Warning: Loss has increased *****
Loss at iteration [2599]: 0.0026788883828702324
Loss at iteration [2600]: 0.002482165589525744
Loss at iteration [2601]: 0.002279881974413494
Loss at iteration [2602]: 0.002119227610025999
Loss at iteration [2603]: 0.0020686601178297325
Loss at iteration [2604]: 0.002117664006994068
***** Warning: Loss has increased *****
Loss at iteration [2605]: 0.0022151172213722326
***** Warning: Loss has increased *****
Loss at iteration [2606]: 0.002305862622192164
***** Warning: Loss has increased *****
Loss at iteration [2607]: 0.0023260205587620074
***** Warning: Loss has increased *****
Loss at iteration [2608]: 0.0022976288857634755
Loss at iteration [2609]: 0.0022112258539534416
Loss at iteration [2610]: 0.002129794983036898
Loss at iteration [2611]: 0.0020758487227552553
Loss at iteration [2612]: 0.002066858565873681
Loss at iteration [2613]: 0.0020942982281425947
***** Warning: Loss has increased *****
Loss at iteration [2614]: 0.0021353864665182376
***** Warning: Loss has increased *****
Loss at iteration [2615]: 0.002170630627724395
***** Warning: Loss has increased *****
Loss at iteration [2616]: 0.002175554034313276
***** Warning: Loss has increased *****
Loss at iteration [2617]: 0.0021598232023444853
Loss at iteration [2618]: 0.002123784415099106
Loss at iteration [2619]: 0.002090113156280902
Loss at iteration [2620]: 0.0020679066532022887
Loss at iteration [2621]: 0.0020629857997863208
Loss at iteration [2622]: 0.002072536868428816
***** Warning: Loss has increased *****
Loss at iteration [2623]: 0.0020882387014598816
***** Warning: Loss has increased *****
Loss at iteration [2624]: 0.002102688455135867
***** Warning: Loss has increased *****
Loss at iteration [2625]: 0.0021085603398731615
***** Warning: Loss has increased *****
Loss at iteration [2626]: 0.002106979991754913
Loss at iteration [2627]: 0.002097445243387335
Loss at iteration [2628]: 0.002085138476178721
Loss at iteration [2629]: 0.0020732169238391373
Loss at iteration [2630]: 0.002064718076025916
Loss at iteration [2631]: 0.002060843748072113
Loss at iteration [2632]: 0.0020614719430409652
***** Warning: Loss has increased *****
Loss at iteration [2633]: 0.002065619798678543
***** Warning: Loss has increased *****
Loss at iteration [2634]: 0.002070862151969985
***** Warning: Loss has increased *****
Loss at iteration [2635]: 0.002075423062771236
***** Warning: Loss has increased *****
Loss at iteration [2636]: 0.0020782846808528274
***** Warning: Loss has increased *****
Loss at iteration [2637]: 0.002079239484898678
***** Warning: Loss has increased *****
Loss at iteration [2638]: 0.0020779357476726247
Loss at iteration [2639]: 0.0020753273673280795
Loss at iteration [2640]: 0.002072095090999253
Loss at iteration [2641]: 0.0020693597431596443
Loss at iteration [2642]: 0.0020678003057563975
Loss at iteration [2643]: 0.002065909320083955
Loss at iteration [2644]: 0.0020629420411163485
Loss at iteration [2645]: 0.0020602130153389783
Loss at iteration [2646]: 0.002058687905138065
Loss at iteration [2647]: 0.002058857776592516
***** Warning: Loss has increased *****
Loss at iteration [2648]: 0.0020600228462734423
***** Warning: Loss has increased *****
Loss at iteration [2649]: 0.002060971419409496
***** Warning: Loss has increased *****
Loss at iteration [2650]: 0.0020614169273988573
***** Warning: Loss has increased *****
Loss at iteration [2651]: 0.0020618925115905348
***** Warning: Loss has increased *****
Loss at iteration [2652]: 0.0020626615423217118
***** Warning: Loss has increased *****
Loss at iteration [2653]: 0.002064233622137908
***** Warning: Loss has increased *****
Loss at iteration [2654]: 0.0020661334999337016
***** Warning: Loss has increased *****
Loss at iteration [2655]: 0.0020681271786335033
***** Warning: Loss has increased *****
Loss at iteration [2656]: 0.002070542187139078
***** Warning: Loss has increased *****
Loss at iteration [2657]: 0.0020729723798111263
***** Warning: Loss has increased *****
Loss at iteration [2658]: 0.0020760784232928043
***** Warning: Loss has increased *****
Loss at iteration [2659]: 0.002079913541895285
***** Warning: Loss has increased *****
Loss at iteration [2660]: 0.0020847987297383545
***** Warning: Loss has increased *****
Loss at iteration [2661]: 0.0020906625850941754
***** Warning: Loss has increased *****
Loss at iteration [2662]: 0.0020981527911059505
***** Warning: Loss has increased *****
Loss at iteration [2663]: 0.0021063540679802173
***** Warning: Loss has increased *****
Loss at iteration [2664]: 0.0021177557693392154
***** Warning: Loss has increased *****
Loss at iteration [2665]: 0.0021295864885948904
***** Warning: Loss has increased *****
Loss at iteration [2666]: 0.0021470304111735677
***** Warning: Loss has increased *****
Loss at iteration [2667]: 0.0021651031223300582
***** Warning: Loss has increased *****
Loss at iteration [2668]: 0.002192024888004271
***** Warning: Loss has increased *****
Loss at iteration [2669]: 0.0022183248311735126
***** Warning: Loss has increased *****
Loss at iteration [2670]: 0.002259020439260007
***** Warning: Loss has increased *****
Loss at iteration [2671]: 0.002297233621224377
***** Warning: Loss has increased *****
Loss at iteration [2672]: 0.0023597970246247123
***** Warning: Loss has increased *****
Loss at iteration [2673]: 0.0024138292712994752
***** Warning: Loss has increased *****
Loss at iteration [2674]: 0.002502780099945664
***** Warning: Loss has increased *****
Loss at iteration [2675]: 0.0025647758718997395
***** Warning: Loss has increased *****
Loss at iteration [2676]: 0.002673925286027031
***** Warning: Loss has increased *****
Loss at iteration [2677]: 0.0027219829146341175
***** Warning: Loss has increased *****
Loss at iteration [2678]: 0.00282386405566922
***** Warning: Loss has increased *****
Loss at iteration [2679]: 0.002832217163595857
***** Warning: Loss has increased *****
Loss at iteration [2680]: 0.002883696678235224
***** Warning: Loss has increased *****
Loss at iteration [2681]: 0.002817563303804837
Loss at iteration [2682]: 0.0027813267284030135
Loss at iteration [2683]: 0.002649037724567339
Loss at iteration [2684]: 0.002534182154995209
Loss at iteration [2685]: 0.0023810389559506726
Loss at iteration [2686]: 0.0022569270157759137
Loss at iteration [2687]: 0.0021531189857655997
Loss at iteration [2688]: 0.002088815072136166
Loss at iteration [2689]: 0.0020595435918389854
Loss at iteration [2690]: 0.002059834855832438
***** Warning: Loss has increased *****
Loss at iteration [2691]: 0.002081676936459956
***** Warning: Loss has increased *****
Loss at iteration [2692]: 0.002116620772313165
***** Warning: Loss has increased *****
Loss at iteration [2693]: 0.0021583921527963397
***** Warning: Loss has increased *****
Loss at iteration [2694]: 0.0021954653811825987
***** Warning: Loss has increased *****
Loss at iteration [2695]: 0.002230606953389122
***** Warning: Loss has increased *****
Loss at iteration [2696]: 0.002248859174997091
***** Warning: Loss has increased *****
Loss at iteration [2697]: 0.0022659358489856077
***** Warning: Loss has increased *****
Loss at iteration [2698]: 0.0022604238298829485
Loss at iteration [2699]: 0.0022542696056764054
Loss at iteration [2700]: 0.002227378853480525
Loss at iteration [2701]: 0.002205493999781213
Loss at iteration [2702]: 0.002175334878447005
Loss at iteration [2703]: 0.002152173705916485
Loss at iteration [2704]: 0.0021272355821402664
Loss at iteration [2705]: 0.002106937704886285
Loss at iteration [2706]: 0.002088099026310807
Loss at iteration [2707]: 0.0020732391657758624
Loss at iteration [2708]: 0.002062904568542389
Loss at iteration [2709]: 0.0020570639284957742
Loss at iteration [2710]: 0.002054686482742675
Loss at iteration [2711]: 0.002054634206840062
Loss at iteration [2712]: 0.002056048883474161
***** Warning: Loss has increased *****
Loss at iteration [2713]: 0.0020587290523367986
***** Warning: Loss has increased *****
Loss at iteration [2714]: 0.0020627287023794886
***** Warning: Loss has increased *****
Loss at iteration [2715]: 0.002068309263271294
***** Warning: Loss has increased *****
Loss at iteration [2716]: 0.002075097219572362
***** Warning: Loss has increased *****
Loss at iteration [2717]: 0.002082973774363474
***** Warning: Loss has increased *****
Loss at iteration [2718]: 0.002092734848226049
***** Warning: Loss has increased *****
Loss at iteration [2719]: 0.0021035982986433092
***** Warning: Loss has increased *****
Loss at iteration [2720]: 0.002117883715618422
***** Warning: Loss has increased *****
Loss at iteration [2721]: 0.0021338981733799215
***** Warning: Loss has increased *****
Loss at iteration [2722]: 0.0021569260218328046
***** Warning: Loss has increased *****
Loss at iteration [2723]: 0.002182605170784622
***** Warning: Loss has increased *****
Loss at iteration [2724]: 0.0022201519110502037
***** Warning: Loss has increased *****
Loss at iteration [2725]: 0.002261261743256359
***** Warning: Loss has increased *****
Loss at iteration [2726]: 0.0023244443382644544
***** Warning: Loss has increased *****
Loss at iteration [2727]: 0.0023868869670441105
***** Warning: Loss has increased *****
Loss at iteration [2728]: 0.0024836837380262616
***** Warning: Loss has increased *****
Loss at iteration [2729]: 0.002567563806949097
***** Warning: Loss has increased *****
Loss at iteration [2730]: 0.0027070176709846246
***** Warning: Loss has increased *****
Loss at iteration [2731]: 0.0027992182859146756
***** Warning: Loss has increased *****
Loss at iteration [2732]: 0.0029651522709376625
***** Warning: Loss has increased *****
Loss at iteration [2733]: 0.0030242935768835665
***** Warning: Loss has increased *****
Loss at iteration [2734]: 0.0031516407710862394
***** Warning: Loss has increased *****
Loss at iteration [2735]: 0.0031062067047685385
Loss at iteration [2736]: 0.003102236170366661
Loss at iteration [2737]: 0.0029257794528011294
Loss at iteration [2738]: 0.0027715371252223886
Loss at iteration [2739]: 0.0025407045099511963
Loss at iteration [2740]: 0.002350437168245696
Loss at iteration [2741]: 0.0021887286519676782
Loss at iteration [2742]: 0.002090300133614657
Loss at iteration [2743]: 0.0020534230727919434
Loss at iteration [2744]: 0.0020694302006409237
***** Warning: Loss has increased *****
Loss at iteration [2745]: 0.0021207398682948197
***** Warning: Loss has increased *****
Loss at iteration [2746]: 0.0021869114943318456
***** Warning: Loss has increased *****
Loss at iteration [2747]: 0.002255869091269016
***** Warning: Loss has increased *****
Loss at iteration [2748]: 0.0023011072927862433
***** Warning: Loss has increased *****
Loss at iteration [2749]: 0.0023322317748556034
***** Warning: Loss has increased *****
Loss at iteration [2750]: 0.0023224309147216425
Loss at iteration [2751]: 0.0023011927847678315
Loss at iteration [2752]: 0.0022486213692800355
Loss at iteration [2753]: 0.002196875715986168
Loss at iteration [2754]: 0.002139105428071081
Loss at iteration [2755]: 0.002095172306846825
Loss at iteration [2756]: 0.0020660283425907562
Loss at iteration [2757]: 0.0020538147729029957
Loss at iteration [2758]: 0.0020542457649966534
***** Warning: Loss has increased *****
Loss at iteration [2759]: 0.0020630067428883117
***** Warning: Loss has increased *****
Loss at iteration [2760]: 0.0020771007600658606
***** Warning: Loss has increased *****
Loss at iteration [2761]: 0.002092828295041981
***** Warning: Loss has increased *****
Loss at iteration [2762]: 0.002109664249730464
***** Warning: Loss has increased *****
Loss at iteration [2763]: 0.0021219463307179196
***** Warning: Loss has increased *****
Loss at iteration [2764]: 0.002131486139886804
***** Warning: Loss has increased *****
Loss at iteration [2765]: 0.002132915681755968
***** Warning: Loss has increased *****
Loss at iteration [2766]: 0.002131975664924374
Loss at iteration [2767]: 0.0021264261525401353
Loss at iteration [2768]: 0.002120756860159355
Loss at iteration [2769]: 0.0021120481745859902
Loss at iteration [2770]: 0.00210459452782565
Loss at iteration [2771]: 0.0020960389850806396
Loss at iteration [2772]: 0.0020887250894966497
Loss at iteration [2773]: 0.002081401121708328
Loss at iteration [2774]: 0.0020757095644843623
Loss at iteration [2775]: 0.002070783515763147
Loss at iteration [2776]: 0.002067096300214956
Loss at iteration [2777]: 0.002063329509614995
Loss at iteration [2778]: 0.0020606078296396854
Loss at iteration [2779]: 0.0020587515586146595
Loss at iteration [2780]: 0.0020578462900546764
Loss at iteration [2781]: 0.002057630826251645
Loss at iteration [2782]: 0.00205791656576207
***** Warning: Loss has increased *****
Loss at iteration [2783]: 0.002058537623148289
***** Warning: Loss has increased *****
Loss at iteration [2784]: 0.002060527795087448
***** Warning: Loss has increased *****
Loss at iteration [2785]: 0.0020635335038801505
***** Warning: Loss has increased *****
Loss at iteration [2786]: 0.0020681434081609032
***** Warning: Loss has increased *****
Loss at iteration [2787]: 0.0020742862597803195
***** Warning: Loss has increased *****
Loss at iteration [2788]: 0.002083644410984184
***** Warning: Loss has increased *****
Loss at iteration [2789]: 0.0020975077038791687
***** Warning: Loss has increased *****
Loss at iteration [2790]: 0.0021190308202850877
***** Warning: Loss has increased *****
Loss at iteration [2791]: 0.0021471758221385338
***** Warning: Loss has increased *****
Loss at iteration [2792]: 0.0021917941898508698
***** Warning: Loss has increased *****
Loss at iteration [2793]: 0.0022484380101159473
***** Warning: Loss has increased *****
Loss at iteration [2794]: 0.0023396557872827827
***** Warning: Loss has increased *****
Loss at iteration [2795]: 0.0024447118613511462
***** Warning: Loss has increased *****
Loss at iteration [2796]: 0.002621735383593956
***** Warning: Loss has increased *****
Loss at iteration [2797]: 0.002794352112118788
***** Warning: Loss has increased *****
Loss at iteration [2798]: 0.003090988568706506
***** Warning: Loss has increased *****
Loss at iteration [2799]: 0.003296308504545498
***** Warning: Loss has increased *****
Loss at iteration [2800]: 0.0036673947045300966
***** Warning: Loss has increased *****
Loss at iteration [2801]: 0.0037821266853714274
***** Warning: Loss has increased *****
Loss at iteration [2802]: 0.004039403120190829
***** Warning: Loss has increased *****
Loss at iteration [2803]: 0.0038696939534653835
Loss at iteration [2804]: 0.003727106293921225
Loss at iteration [2805]: 0.003223332835401282
Loss at iteration [2806]: 0.002748052904659612
Loss at iteration [2807]: 0.0023046480697169348
Loss at iteration [2808]: 0.0020850674145392986
Loss at iteration [2809]: 0.002084361058578226
Loss at iteration [2810]: 0.0022402061252055493
***** Warning: Loss has increased *****
Loss at iteration [2811]: 0.0024465836389977688
***** Warning: Loss has increased *****
Loss at iteration [2812]: 0.0025905178522240492
***** Warning: Loss has increased *****
Loss at iteration [2813]: 0.0026702106179148687
***** Warning: Loss has increased *****
Loss at iteration [2814]: 0.002563233154769007
Loss at iteration [2815]: 0.0024397580996194965
Loss at iteration [2816]: 0.0022441829797178064
Loss at iteration [2817]: 0.0021122971571589685
Loss at iteration [2818]: 0.0020597539178359276
Loss at iteration [2819]: 0.0020938560556109155
***** Warning: Loss has increased *****
Loss at iteration [2820]: 0.002181134154801332
***** Warning: Loss has increased *****
Loss at iteration [2821]: 0.002258761854066513
***** Warning: Loss has increased *****
Loss at iteration [2822]: 0.0023112934543823966
***** Warning: Loss has increased *****
Loss at iteration [2823]: 0.0022753202735964627
Loss at iteration [2824]: 0.0022135342732664542
Loss at iteration [2825]: 0.0021274513389600656
Loss at iteration [2826]: 0.0020698131025448647
Loss at iteration [2827]: 0.002052035550018122
Loss at iteration [2828]: 0.002069230671905121
***** Warning: Loss has increased *****
Loss at iteration [2829]: 0.002103938090978191
***** Warning: Loss has increased *****
Loss at iteration [2830]: 0.002133883318952706
***** Warning: Loss has increased *****
Loss at iteration [2831]: 0.002155142016918851
***** Warning: Loss has increased *****
Loss at iteration [2832]: 0.0021514844551453398
Loss at iteration [2833]: 0.0021358466983423217
Loss at iteration [2834]: 0.002106928252921038
Loss at iteration [2835]: 0.002077632290756751
Loss at iteration [2836]: 0.0020564117704413042
Loss at iteration [2837]: 0.0020477085969165796
Loss at iteration [2838]: 0.002051597101693665
***** Warning: Loss has increased *****
Loss at iteration [2839]: 0.002063643715164996
***** Warning: Loss has increased *****
Loss at iteration [2840]: 0.0020781205532936363
***** Warning: Loss has increased *****
Loss at iteration [2841]: 0.0020886842735639115
***** Warning: Loss has increased *****
Loss at iteration [2842]: 0.002092361144587256
***** Warning: Loss has increased *****
Loss at iteration [2843]: 0.002087806169654281
Loss at iteration [2844]: 0.0020783565545493795
Loss at iteration [2845]: 0.0020668444828054497
Loss at iteration [2846]: 0.0020566795243934432
Loss at iteration [2847]: 0.0020493633519346756
Loss at iteration [2848]: 0.002045630004647784
Loss at iteration [2849]: 0.0020447541056844154
Loss at iteration [2850]: 0.00204619874295404
***** Warning: Loss has increased *****
Loss at iteration [2851]: 0.0020491806464066373
***** Warning: Loss has increased *****
Loss at iteration [2852]: 0.002052720889930438
***** Warning: Loss has increased *****
Loss at iteration [2853]: 0.0020563562700760757
***** Warning: Loss has increased *****
Loss at iteration [2854]: 0.0020590844341405326
***** Warning: Loss has increased *****
Loss at iteration [2855]: 0.0020613360357360855
***** Warning: Loss has increased *****
Loss at iteration [2856]: 0.0020617963982160316
***** Warning: Loss has increased *****
Loss at iteration [2857]: 0.0020618831142085498
***** Warning: Loss has increased *****
Loss at iteration [2858]: 0.0020606859511592196
Loss at iteration [2859]: 0.002059224566679165
Loss at iteration [2860]: 0.0020573951612563855
Loss at iteration [2861]: 0.0020555833101839525
Loss at iteration [2862]: 0.0020534953373216587
Loss at iteration [2863]: 0.0020517881381894258
Loss at iteration [2864]: 0.0020500931875842236
Loss at iteration [2865]: 0.0020488044676466827
Loss at iteration [2866]: 0.0020479983291039543
Loss at iteration [2867]: 0.002047566661698285
Loss at iteration [2868]: 0.0020469479160160355
Loss at iteration [2869]: 0.0020470151779263227
***** Warning: Loss has increased *****
Loss at iteration [2870]: 0.0020474523891872765
***** Warning: Loss has increased *****
Loss at iteration [2871]: 0.002048129369004205
***** Warning: Loss has increased *****
Loss at iteration [2872]: 0.0020491693524947014
***** Warning: Loss has increased *****
Loss at iteration [2873]: 0.0020509052227231897
***** Warning: Loss has increased *****
Loss at iteration [2874]: 0.002052821020873421
***** Warning: Loss has increased *****
Loss at iteration [2875]: 0.0020563623070327633
***** Warning: Loss has increased *****
Loss at iteration [2876]: 0.0020611947396374135
***** Warning: Loss has increased *****
Loss at iteration [2877]: 0.0020685647029652205
***** Warning: Loss has increased *****
Loss at iteration [2878]: 0.002078473683742412
***** Warning: Loss has increased *****
Loss at iteration [2879]: 0.00209246404300925
***** Warning: Loss has increased *****
Loss at iteration [2880]: 0.002110738654843151
***** Warning: Loss has increased *****
Loss at iteration [2881]: 0.002140078626849354
***** Warning: Loss has increased *****
Loss at iteration [2882]: 0.0021758703652016067
***** Warning: Loss has increased *****
Loss at iteration [2883]: 0.0022261740818701396
***** Warning: Loss has increased *****
Loss at iteration [2884]: 0.0022808049522365465
***** Warning: Loss has increased *****
Loss at iteration [2885]: 0.002370294557443772
***** Warning: Loss has increased *****
Loss at iteration [2886]: 0.0024630754616248862
***** Warning: Loss has increased *****
Loss at iteration [2887]: 0.0026177555657707716
***** Warning: Loss has increased *****
Loss at iteration [2888]: 0.0027388787919058214
***** Warning: Loss has increased *****
Loss at iteration [2889]: 0.002949969122991254
***** Warning: Loss has increased *****
Loss at iteration [2890]: 0.0030461553851386302
***** Warning: Loss has increased *****
Loss at iteration [2891]: 0.0032403553411674427
***** Warning: Loss has increased *****
Loss at iteration [2892]: 0.003228471930766049
Loss at iteration [2893]: 0.003291565149918533
***** Warning: Loss has increased *****
Loss at iteration [2894]: 0.0031260347194331808
Loss at iteration [2895]: 0.0029851698218018863
Loss at iteration [2896]: 0.002720088814803516
Loss at iteration [2897]: 0.002481219425590895
Loss at iteration [2898]: 0.002259976750590309
Loss at iteration [2899]: 0.002112775930564785
Loss at iteration [2900]: 0.002046977472085549
Loss at iteration [2901]: 0.002055223114656513
***** Warning: Loss has increased *****
Loss at iteration [2902]: 0.002116918204915359
***** Warning: Loss has increased *****
Loss at iteration [2903]: 0.002202853074882249
***** Warning: Loss has increased *****
Loss at iteration [2904]: 0.002291539139359765
***** Warning: Loss has increased *****
Loss at iteration [2905]: 0.002350153378589212
***** Warning: Loss has increased *****
Loss at iteration [2906]: 0.002385560984055528
***** Warning: Loss has increased *****
Loss at iteration [2907]: 0.002363935007354453
Loss at iteration [2908]: 0.0023281902142508097
Loss at iteration [2909]: 0.002250432159570129
Loss at iteration [2910]: 0.00217966915339026
Loss at iteration [2911]: 0.002108789075421236
Loss at iteration [2912]: 0.0020625905314046785
Loss at iteration [2913]: 0.0020410438843379954
Loss at iteration [2914]: 0.002043177533729019
***** Warning: Loss has increased *****
Loss at iteration [2915]: 0.002062443513852702
***** Warning: Loss has increased *****
Loss at iteration [2916]: 0.0020895993327921685
***** Warning: Loss has increased *****
Loss at iteration [2917]: 0.0021205822704776023
***** Warning: Loss has increased *****
Loss at iteration [2918]: 0.002144397413425501
***** Warning: Loss has increased *****
Loss at iteration [2919]: 0.0021663420307326924
***** Warning: Loss has increased *****
Loss at iteration [2920]: 0.0021701042867621314
***** Warning: Loss has increased *****
Loss at iteration [2921]: 0.002169982526169443
Loss at iteration [2922]: 0.0021534508277049062
Loss at iteration [2923]: 0.0021347287270069215
Loss at iteration [2924]: 0.002110135512712797
Loss at iteration [2925]: 0.0020878321228923764
Loss at iteration [2926]: 0.0020684315703830645
Loss at iteration [2927]: 0.002053698697690634
Loss at iteration [2928]: 0.0020438392005560396
Loss at iteration [2929]: 0.0020381778762058977
Loss at iteration [2930]: 0.002036358524508132
Loss at iteration [2931]: 0.0020375799765074967
***** Warning: Loss has increased *****
Loss at iteration [2932]: 0.0020410612835823094
***** Warning: Loss has increased *****
Loss at iteration [2933]: 0.0020460904556157803
***** Warning: Loss has increased *****
Loss at iteration [2934]: 0.0020522487485082133
***** Warning: Loss has increased *****
Loss at iteration [2935]: 0.0020594594852202377
***** Warning: Loss has increased *****
Loss at iteration [2936]: 0.0020671589789399165
***** Warning: Loss has increased *****
Loss at iteration [2937]: 0.0020756552215696147
***** Warning: Loss has increased *****
Loss at iteration [2938]: 0.002085050441843244
***** Warning: Loss has increased *****
Loss at iteration [2939]: 0.002095131385579106
***** Warning: Loss has increased *****
Loss at iteration [2940]: 0.002107949390668613
***** Warning: Loss has increased *****
Loss at iteration [2941]: 0.002121113651795631
***** Warning: Loss has increased *****
Loss at iteration [2942]: 0.0021404423941062418
***** Warning: Loss has increased *****
Loss at iteration [2943]: 0.002160586777032648
***** Warning: Loss has increased *****
Loss at iteration [2944]: 0.002193970410880543
***** Warning: Loss has increased *****
Loss at iteration [2945]: 0.00222694754511178
***** Warning: Loss has increased *****
Loss at iteration [2946]: 0.002275744648022225
***** Warning: Loss has increased *****
Loss at iteration [2947]: 0.002313512918067582
***** Warning: Loss has increased *****
Loss at iteration [2948]: 0.002377967289682434
***** Warning: Loss has increased *****
Loss at iteration [2949]: 0.0024281338004110667
***** Warning: Loss has increased *****
Loss at iteration [2950]: 0.002518994050641916
***** Warning: Loss has increased *****
Loss at iteration [2951]: 0.0025746845418536243
***** Warning: Loss has increased *****
Loss at iteration [2952]: 0.0026681378707859413
***** Warning: Loss has increased *****
Loss at iteration [2953]: 0.0027034694184143782
***** Warning: Loss has increased *****
Loss at iteration [2954]: 0.002765258651818087
***** Warning: Loss has increased *****
Loss at iteration [2955]: 0.0027458122115008705
Loss at iteration [2956]: 0.00274516456424195
Loss at iteration [2957]: 0.0026629823705085955
Loss at iteration [2958]: 0.002594653826342137
Loss at iteration [2959]: 0.00246903425892924
Loss at iteration [2960]: 0.002361893253851597
Loss at iteration [2961]: 0.00224372656418747
Loss at iteration [2962]: 0.00215479450383574
Loss at iteration [2963]: 0.002087091343782142
Loss at iteration [2964]: 0.002049143363507961
Loss at iteration [2965]: 0.002035735076501599
Loss at iteration [2966]: 0.0020419024227131886
***** Warning: Loss has increased *****
Loss at iteration [2967]: 0.002061709626323471
***** Warning: Loss has increased *****
Loss at iteration [2968]: 0.0020901294693346944
***** Warning: Loss has increased *****
Loss at iteration [2969]: 0.002124182818565826
***** Warning: Loss has increased *****
Loss at iteration [2970]: 0.0021562715840741835
***** Warning: Loss has increased *****
Loss at iteration [2971]: 0.002191047939076291
***** Warning: Loss has increased *****
Loss at iteration [2972]: 0.0022133619407267513
***** Warning: Loss has increased *****
Loss at iteration [2973]: 0.0022384197602721653
***** Warning: Loss has increased *****
Loss at iteration [2974]: 0.0022454680077677053
***** Warning: Loss has increased *****
Loss at iteration [2975]: 0.002257358468760668
***** Warning: Loss has increased *****
Loss at iteration [2976]: 0.0022495165660465655
Loss at iteration [2977]: 0.0022486026578150448
Loss at iteration [2978]: 0.0022308303020175907
Loss at iteration [2979]: 0.0022204367097508246
Loss at iteration [2980]: 0.0021980907344451134
Loss at iteration [2981]: 0.0021817118937932523
Loss at iteration [2982]: 0.0021583538020081617
Loss at iteration [2983]: 0.0021406567886597273
Loss at iteration [2984]: 0.0021211158980526047
Loss at iteration [2985]: 0.002105621827369625
Loss at iteration [2986]: 0.0020908389127737197
Loss at iteration [2987]: 0.0020791037663113695
Loss at iteration [2988]: 0.0020682608137695202
Loss at iteration [2989]: 0.00206045164114202
Loss at iteration [2990]: 0.0020543800329034996
Loss at iteration [2991]: 0.002050315816619069
Loss at iteration [2992]: 0.0020476890117583605
Loss at iteration [2993]: 0.0020465059646257206
Loss at iteration [2994]: 0.002045793476347425
Loss at iteration [2995]: 0.002046540433996774
***** Warning: Loss has increased *****
Loss at iteration [2996]: 0.00204786391987507
***** Warning: Loss has increased *****
Loss at iteration [2997]: 0.0020505927605198844
***** Warning: Loss has increased *****
Loss at iteration [2998]: 0.002055063480197558
***** Warning: Loss has increased *****
Loss at iteration [2999]: 0.0020622439575364504
***** Warning: Loss has increased *****
Loss at iteration [3000]: 0.002072112421230804
***** Warning: Loss has increased *****
