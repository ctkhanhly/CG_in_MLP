Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : Adam
Learning rate                         : 0.001
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 12.78195595741272
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 50.9652961062286%
Percentage of parameters < 1e-7       : 50.965544861828704%
Percentage of parameters < 1e-6       : 50.96653988422914%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 0.9656319032772888
Loss at iteration [2]: 0.321440982990548
Loss at iteration [3]: 0.06371338400996335
Loss at iteration [4]: 0.06733914239820084
***** Warning: Loss has increased *****
Loss at iteration [5]: 0.11299363233111251
***** Warning: Loss has increased *****
Loss at iteration [6]: 0.10113003923781681
Loss at iteration [7]: 0.060981307988981874
Loss at iteration [8]: 0.03085203969246285
Loss at iteration [9]: 0.020267669187641876
Loss at iteration [10]: 0.03368575462744815
***** Warning: Loss has increased *****
Loss at iteration [11]: 0.04890565197554595
***** Warning: Loss has increased *****
Loss at iteration [12]: 0.04700138057328248
Loss at iteration [13]: 0.029989840423577146
Loss at iteration [14]: 0.011009835857363849
Loss at iteration [15]: 0.0032115334386053126
Loss at iteration [16]: 0.009183265695747499
***** Warning: Loss has increased *****
Loss at iteration [17]: 0.01936586513951858
***** Warning: Loss has increased *****
Loss at iteration [18]: 0.02226735570639737
***** Warning: Loss has increased *****
Loss at iteration [19]: 0.016700835324752153
Loss at iteration [20]: 0.009370760745389827
Loss at iteration [21]: 0.005127111830598063
Loss at iteration [22]: 0.004461792002229191
Loss at iteration [23]: 0.00625096443407357
***** Warning: Loss has increased *****
Loss at iteration [24]: 0.009054383216758272
***** Warning: Loss has increased *****
Loss at iteration [25]: 0.010633101534721277
***** Warning: Loss has increased *****
Loss at iteration [26]: 0.009369050499921278
Loss at iteration [27]: 0.005968821536937556
Loss at iteration [28]: 0.0030791777841350397
Loss at iteration [29]: 0.0028646639769759723
Loss at iteration [30]: 0.004765276792337926
***** Warning: Loss has increased *****
Loss at iteration [31]: 0.006346245148661623
***** Warning: Loss has increased *****
Loss at iteration [32]: 0.006176077907693223
Loss at iteration [33]: 0.00486790930985259
Loss at iteration [34]: 0.0036335694002034347
Loss at iteration [35]: 0.003035982553253319
Loss at iteration [36]: 0.0031110098702078824
***** Warning: Loss has increased *****
Loss at iteration [37]: 0.0037080176474016467
***** Warning: Loss has increased *****
Loss at iteration [38]: 0.004371834130105489
***** Warning: Loss has increased *****
Loss at iteration [39]: 0.0044389596551291455
***** Warning: Loss has increased *****
Loss at iteration [40]: 0.0037146434022508066
Loss at iteration [41]: 0.0028281146716877185
Loss at iteration [42]: 0.002551133743316223
Loss at iteration [43]: 0.0029292307944003885
***** Warning: Loss has increased *****
Loss at iteration [44]: 0.00337521035649236
***** Warning: Loss has increased *****
Loss at iteration [45]: 0.0034527200550693174
***** Warning: Loss has increased *****
Loss at iteration [46]: 0.0032214305464711777
Loss at iteration [47]: 0.0029091720817479693
Loss at iteration [48]: 0.002659004577620744
Loss at iteration [49]: 0.002596052811120692
Loss at iteration [50]: 0.0027759451003841404
***** Warning: Loss has increased *****
Loss at iteration [51]: 0.0030130159518887405
***** Warning: Loss has increased *****
Loss at iteration [52]: 0.003028890031485683
***** Warning: Loss has increased *****
Loss at iteration [53]: 0.0028009330969148943
Loss at iteration [54]: 0.002580165733648023
Loss at iteration [55]: 0.00255014272902988
Loss at iteration [56]: 0.0026466864479690517
***** Warning: Loss has increased *****
Loss at iteration [57]: 0.002730545110055528
***** Warning: Loss has increased *****
Loss at iteration [58]: 0.002749443913200792
***** Warning: Loss has increased *****
Loss at iteration [59]: 0.0026989162445325557
Loss at iteration [60]: 0.0025940121942109318
Loss at iteration [61]: 0.002512300507152932
Loss at iteration [62]: 0.0025331919040789716
***** Warning: Loss has increased *****
Loss at iteration [63]: 0.0026175949311207326
***** Warning: Loss has increased *****
Loss at iteration [64]: 0.002651017055290814
***** Warning: Loss has increased *****
Loss at iteration [65]: 0.0026007880760724666
Loss at iteration [66]: 0.0025362223104117283
Loss at iteration [67]: 0.0025118432700294412
Loss at iteration [68]: 0.0025217722123654523
***** Warning: Loss has increased *****
Loss at iteration [69]: 0.002546571773123141
***** Warning: Loss has increased *****
Loss at iteration [70]: 0.0025678132865254335
***** Warning: Loss has increased *****
Loss at iteration [71]: 0.0025585285630190984
Loss at iteration [72]: 0.002518787143882036
Loss at iteration [73]: 0.0024918612733801676
Loss at iteration [74]: 0.0025040024090868204
***** Warning: Loss has increased *****
Loss at iteration [75]: 0.0025277277176697544
***** Warning: Loss has increased *****
Loss at iteration [76]: 0.002530654583999739
***** Warning: Loss has increased *****
Loss at iteration [77]: 0.002516252990837981
Loss at iteration [78]: 0.0025008585180806967
Loss at iteration [79]: 0.0024915785673977914
Loss at iteration [80]: 0.002493542562790309
***** Warning: Loss has increased *****
Loss at iteration [81]: 0.0025050512656711807
***** Warning: Loss has increased *****
Loss at iteration [82]: 0.0025098998035614304
***** Warning: Loss has increased *****
Loss at iteration [83]: 0.0024991521657299082
Loss at iteration [84]: 0.0024864920276459613
Loss at iteration [85]: 0.0024854906778258275
Loss at iteration [86]: 0.0024911697780010088
***** Warning: Loss has increased *****
Loss at iteration [87]: 0.0024941734550620602
***** Warning: Loss has increased *****
Loss at iteration [88]: 0.0024928407053803514
Loss at iteration [89]: 0.0024879891146646627
Loss at iteration [90]: 0.0024821547120011207
Loss at iteration [91]: 0.002481227249802901
Loss at iteration [92]: 0.002485393535370327
***** Warning: Loss has increased *****
Loss at iteration [93]: 0.0024871613971731285
***** Warning: Loss has increased *****
Loss at iteration [94]: 0.002483604925334815
Loss at iteration [95]: 0.00247967129128637
Loss at iteration [96]: 0.0024785897930304918
Loss at iteration [97]: 0.0024792278895787146
***** Warning: Loss has increased *****
Loss at iteration [98]: 0.002480267350018552
***** Warning: Loss has increased *****
Loss at iteration [99]: 0.0024802007201845154
Loss at iteration [100]: 0.0024778305224547395
Loss at iteration [101]: 0.002475318411771975
Loss at iteration [102]: 0.0024753133710575933
Loss at iteration [103]: 0.002476396569273089
***** Warning: Loss has increased *****
Loss at iteration [104]: 0.0024761332775854266
Loss at iteration [105]: 0.0024748023789645324
Loss at iteration [106]: 0.0024735217825855816
Loss at iteration [107]: 0.0024727416860294174
Loss at iteration [108]: 0.0024728019200355306
***** Warning: Loss has increased *****
Loss at iteration [109]: 0.002473123276270357
***** Warning: Loss has increased *****
Loss at iteration [110]: 0.0024724573797080913
Loss at iteration [111]: 0.002471142934758388
Loss at iteration [112]: 0.0024704421916700897
Loss at iteration [113]: 0.0024703457784314535
Loss at iteration [114]: 0.002470174666069036
Loss at iteration [115]: 0.0024697120624094003
Loss at iteration [116]: 0.0024689893482074308
Loss at iteration [117]: 0.0024682236692101997
Loss at iteration [118]: 0.0024678672245522907
Loss at iteration [119]: 0.00246776640645068
Loss at iteration [120]: 0.0024673496180405777
Loss at iteration [121]: 0.002466680167572987
Loss at iteration [122]: 0.002466123998015037
Loss at iteration [123]: 0.002465737200872534
Loss at iteration [124]: 0.002465448307875904
Loss at iteration [125]: 0.002465118673643266
Loss at iteration [126]: 0.002464630528678616
Loss at iteration [127]: 0.002464089539859807
Loss at iteration [128]: 0.0024637220646920653
Loss at iteration [129]: 0.002463438555129569
Loss at iteration [130]: 0.002463076674124222
Loss at iteration [131]: 0.0024626401295922734
Loss at iteration [132]: 0.0024622103136275916
Loss at iteration [133]: 0.0024618224166670453
Loss at iteration [134]: 0.002461510015519143
Loss at iteration [135]: 0.0024611688123065125
Loss at iteration [136]: 0.002460752380566606
Loss at iteration [137]: 0.0024603551915272125
Loss at iteration [138]: 0.002460015715174744
Loss at iteration [139]: 0.0024596975896493713
Loss at iteration [140]: 0.002459361845833679
Loss at iteration [141]: 0.0024589917827396637
Loss at iteration [142]: 0.002458612425046686
Loss at iteration [143]: 0.0024582671692151953
Loss at iteration [144]: 0.002457940852978997
Loss at iteration [145]: 0.0024575888926780674
Loss at iteration [146]: 0.0024572220556033286
Loss at iteration [147]: 0.002456860600899783
Loss at iteration [148]: 0.0024565185591455493
Loss at iteration [149]: 0.0024561732963137126
Loss at iteration [150]: 0.002455810812833944
Loss at iteration [151]: 0.0024554464674434538
Loss at iteration [152]: 0.0024550939544386945
Loss at iteration [153]: 0.0024547523205726627
Loss at iteration [154]: 0.0024544020176350202
Loss at iteration [155]: 0.0024540474201504496
Loss at iteration [156]: 0.002453692235787916
Loss at iteration [157]: 0.002453344181740375
Loss at iteration [158]: 0.0024530040561006913
Loss at iteration [159]: 0.0024526627308717923
Loss at iteration [160]: 0.002452308732354064
Loss at iteration [161]: 0.00245195358026444
Loss at iteration [162]: 0.00245161905066046
Loss at iteration [163]: 0.0024512985195216824
Loss at iteration [164]: 0.0024509731750666807
Loss at iteration [165]: 0.002450637613675832
Loss at iteration [166]: 0.0024502943840125396
Loss at iteration [167]: 0.0024499482093001375
Loss at iteration [168]: 0.002449601915600814
Loss at iteration [169]: 0.0024492465111813667
Loss at iteration [170]: 0.002448883294576179
Loss at iteration [171]: 0.002448533939193199
Loss at iteration [172]: 0.002448202326877593
Loss at iteration [173]: 0.0024478747264070257
Loss at iteration [174]: 0.0024475538201820363
Loss at iteration [175]: 0.0024472323797502422
Loss at iteration [176]: 0.002446906656969745
Loss at iteration [177]: 0.0024465767349285874
Loss at iteration [178]: 0.0024462500922876255
Loss at iteration [179]: 0.0024459132334915625
Loss at iteration [180]: 0.0024455850746604776
Loss at iteration [181]: 0.0024452525733461693
Loss at iteration [182]: 0.002444928726202882
Loss at iteration [183]: 0.002444600590241158
Loss at iteration [184]: 0.0024442888316110255
Loss at iteration [185]: 0.0024440066393860488
Loss at iteration [186]: 0.0024437374889899814
Loss at iteration [187]: 0.0024434645004988887
Loss at iteration [188]: 0.002443190324272746
Loss at iteration [189]: 0.0024429000461393246
Loss at iteration [190]: 0.002442597986794036
Loss at iteration [191]: 0.002442280925960898
Loss at iteration [192]: 0.002441965138201267
Loss at iteration [193]: 0.002441662130966799
Loss at iteration [194]: 0.0024413718232432938
Loss at iteration [195]: 0.002441095271842658
Loss at iteration [196]: 0.002440823585880515
Loss at iteration [197]: 0.0024405447316988176
Loss at iteration [198]: 0.002440263268384221
Loss at iteration [199]: 0.0024399723477038257
Loss at iteration [200]: 0.0024396846534884913
Loss at iteration [201]: 0.0024393898754919738
Loss at iteration [202]: 0.0024391183070048813
Loss at iteration [203]: 0.002438838271630452
Loss at iteration [204]: 0.002438558267623273
Loss at iteration [205]: 0.0024382860750665797
Loss at iteration [206]: 0.0024380109284926695
Loss at iteration [207]: 0.0024377446256804345
Loss at iteration [208]: 0.002437466802760428
Loss at iteration [209]: 0.002437190981623857
Loss at iteration [210]: 0.0024369137674138497
Loss at iteration [211]: 0.0024366452064752265
Loss at iteration [212]: 0.0024363772872999455
Loss at iteration [213]: 0.002436111907201359
Loss at iteration [214]: 0.0024358312775418197
Loss at iteration [215]: 0.002435570554034107
Loss at iteration [216]: 0.0024353114057989564
Loss at iteration [217]: 0.0024350521875860826
Loss at iteration [218]: 0.002434780221742784
Loss at iteration [219]: 0.002434505251114907
Loss at iteration [220]: 0.002434235111381773
Loss at iteration [221]: 0.002433974340905127
Loss at iteration [222]: 0.0024337121241725292
Loss at iteration [223]: 0.0024334453252046746
Loss at iteration [224]: 0.0024331668270971265
Loss at iteration [225]: 0.0024328973092958826
Loss at iteration [226]: 0.002432630406597696
Loss at iteration [227]: 0.0024323604081137188
Loss at iteration [228]: 0.002432090045650784
Loss at iteration [229]: 0.0024318265515482756
Loss at iteration [230]: 0.0024315585206560694
Loss at iteration [231]: 0.00243129486244511
Loss at iteration [232]: 0.0024310329546715693
Loss at iteration [233]: 0.0024307703015568976
Loss at iteration [234]: 0.002430508612777134
Loss at iteration [235]: 0.002430244591380799
Loss at iteration [236]: 0.0024299737072026317
Loss at iteration [237]: 0.0024296990550958394
Loss at iteration [238]: 0.0024294478084610424
Loss at iteration [239]: 0.002429181960828023
Loss at iteration [240]: 0.0024289055492721576
Loss at iteration [241]: 0.002428643584815202
Loss at iteration [242]: 0.002428378884061263
Loss at iteration [243]: 0.002428112206648354
Loss at iteration [244]: 0.0024278540064251227
Loss at iteration [245]: 0.0024275853929579764
Loss at iteration [246]: 0.002427318145631623
Loss at iteration [247]: 0.00242704931051157
Loss at iteration [248]: 0.0024267944513055467
Loss at iteration [249]: 0.0024265318097901017
Loss at iteration [250]: 0.002426273865986707
Loss at iteration [251]: 0.0024260029155289855
Loss at iteration [252]: 0.0024257359394309875
Loss at iteration [253]: 0.002425485262637775
Loss at iteration [254]: 0.0024252083611039604
Loss at iteration [255]: 0.00242495853326295
Loss at iteration [256]: 0.002424706969669808
Loss at iteration [257]: 0.0024244518776092554
Loss at iteration [258]: 0.0024241829460652864
Loss at iteration [259]: 0.002423913855332617
Loss at iteration [260]: 0.0024236427478078735
Loss at iteration [261]: 0.0024233700900965194
Loss at iteration [262]: 0.002423118232368791
Loss at iteration [263]: 0.00242285833998837
Loss at iteration [264]: 0.0024225810982747655
Loss at iteration [265]: 0.002422329383488299
Loss at iteration [266]: 0.002422076296394926
Loss at iteration [267]: 0.0024218110965955023
Loss at iteration [268]: 0.00242154345035574
Loss at iteration [269]: 0.0024212837064602113
Loss at iteration [270]: 0.002421026179988991
Loss at iteration [271]: 0.002420766890292338
Loss at iteration [272]: 0.002420501379648704
Loss at iteration [273]: 0.0024202450784811666
Loss at iteration [274]: 0.00241998721042412
Loss at iteration [275]: 0.002419736024826886
Loss at iteration [276]: 0.0024194752484611153
Loss at iteration [277]: 0.0024192118952100932
Loss at iteration [278]: 0.002418949052695324
Loss at iteration [279]: 0.0024187054276227325
Loss at iteration [280]: 0.0024184259303201452
Loss at iteration [281]: 0.0024181694204006023
Loss at iteration [282]: 0.0024179197967908487
Loss at iteration [283]: 0.002417660694823308
Loss at iteration [284]: 0.002417403878924871
Loss at iteration [285]: 0.0024171398140463984
Loss at iteration [286]: 0.0024168745747459924
Loss at iteration [287]: 0.002416605723460606
Loss at iteration [288]: 0.0024163350842541856
Loss at iteration [289]: 0.0024160910276945177
Loss at iteration [290]: 0.002415838021249963
Loss at iteration [291]: 0.0024155685812611515
Loss at iteration [292]: 0.002415327758139335
Loss at iteration [293]: 0.0024150699368863833
Loss at iteration [294]: 0.0024148229424194446
Loss at iteration [295]: 0.0024145654458118935
Loss at iteration [296]: 0.002414303055519257
Loss at iteration [297]: 0.0024140602018339497
Loss at iteration [298]: 0.0024137944481038487
Loss at iteration [299]: 0.0024135357666668633
Loss at iteration [300]: 0.002413289532791947
Loss at iteration [301]: 0.0024130384269817557
Loss at iteration [302]: 0.002412779557024143
Loss at iteration [303]: 0.002412519874633674
Loss at iteration [304]: 0.0024122567743276534
Loss at iteration [305]: 0.002411987847481534
Loss at iteration [306]: 0.002411716047757668
Loss at iteration [307]: 0.0024114767381605085
Loss at iteration [308]: 0.002411208874584649
Loss at iteration [309]: 0.0024109372527796047
Loss at iteration [310]: 0.0024106836502704758
Loss at iteration [311]: 0.0024104281531744268
Loss at iteration [312]: 0.002410169622456367
Loss at iteration [313]: 0.0024099057124909932
Loss at iteration [314]: 0.0024096415013716344
Loss at iteration [315]: 0.002409375416099505
Loss at iteration [316]: 0.0024091037360082944
Loss at iteration [317]: 0.002408844038153183
Loss at iteration [318]: 0.0024085860370919223
Loss at iteration [319]: 0.0024083159777053115
Loss at iteration [320]: 0.0024080777348834416
Loss at iteration [321]: 0.0024078229716495746
Loss at iteration [322]: 0.00240757120113929
Loss at iteration [323]: 0.002407323002489136
Loss at iteration [324]: 0.0024070662248768756
Loss at iteration [325]: 0.0024068083967612557
Loss at iteration [326]: 0.0024065443354682127
Loss at iteration [327]: 0.002406306566719811
Loss at iteration [328]: 0.002406032808622738
Loss at iteration [329]: 0.0024057864580081137
Loss at iteration [330]: 0.002405540478522301
Loss at iteration [331]: 0.0024053016517844366
Loss at iteration [332]: 0.002405052922286883
Loss at iteration [333]: 0.0024047974187353747
Loss at iteration [334]: 0.002404537576118119
Loss at iteration [335]: 0.002404278696277954
Loss at iteration [336]: 0.002404017892219669
Loss at iteration [337]: 0.00240375630248885
Loss at iteration [338]: 0.0024035021454709466
Loss at iteration [339]: 0.002403296729484134
Loss at iteration [340]: 0.0024030277622580265
Loss at iteration [341]: 0.002402754535489962
Loss at iteration [342]: 0.0024025202436209646
Loss at iteration [343]: 0.0024022685475712643
Loss at iteration [344]: 0.0024020240314391278
Loss at iteration [345]: 0.0024017785913388037
Loss at iteration [346]: 0.0024015169216229015
Loss at iteration [347]: 0.002401247838453669
Loss at iteration [348]: 0.0024009702814052594
Loss at iteration [349]: 0.0024007092263540557
Loss at iteration [350]: 0.0024004364525034598
Loss at iteration [351]: 0.002400197792425997
Loss at iteration [352]: 0.002399906499055734
Loss at iteration [353]: 0.002399616587790947
Loss at iteration [354]: 0.002399357538805561
Loss at iteration [355]: 0.002399100943638148
Loss at iteration [356]: 0.002398845413271066
Loss at iteration [357]: 0.0023985851875733487
Loss at iteration [358]: 0.002398331054813573
Loss at iteration [359]: 0.0023980690718123283
Loss at iteration [360]: 0.002397796239305114
Loss at iteration [361]: 0.002397521692382243
Loss at iteration [362]: 0.0023972550019840683
Loss at iteration [363]: 0.0023970118907992383
Loss at iteration [364]: 0.002396740154003351
Loss at iteration [365]: 0.0023964771815104076
Loss at iteration [366]: 0.00239621026429418
Loss at iteration [367]: 0.002395947491505394
Loss at iteration [368]: 0.002395693785729723
Loss at iteration [369]: 0.0023954295499975945
Loss at iteration [370]: 0.002395155739882906
Loss at iteration [371]: 0.002394874375277506
Loss at iteration [372]: 0.0023946116993600955
Loss at iteration [373]: 0.002394353098295919
Loss at iteration [374]: 0.0023940807220539377
Loss at iteration [375]: 0.0023938028609961346
Loss at iteration [376]: 0.002393586665876685
Loss at iteration [377]: 0.002393297700440538
Loss at iteration [378]: 0.0023930203649043234
Loss at iteration [379]: 0.0023927547298268972
Loss at iteration [380]: 0.0023924995034357134
Loss at iteration [381]: 0.0023922470925676494
Loss at iteration [382]: 0.0023919880181639866
Loss at iteration [383]: 0.0023917281775581657
Loss at iteration [384]: 0.0023914485870237434
Loss at iteration [385]: 0.002391178203840244
Loss at iteration [386]: 0.0023909079699623533
Loss at iteration [387]: 0.002390649630303709
Loss at iteration [388]: 0.002390377626351336
Loss at iteration [389]: 0.002390093534391354
Loss at iteration [390]: 0.002389874005947316
Loss at iteration [391]: 0.0023896009659838853
Loss at iteration [392]: 0.002389307802100336
Loss at iteration [393]: 0.0023890550522380496
Loss at iteration [394]: 0.002388785090784635
Loss at iteration [395]: 0.0023885257598146186
Loss at iteration [396]: 0.002388267171055853
Loss at iteration [397]: 0.002387993758560676
Loss at iteration [398]: 0.002387702580986311
Loss at iteration [399]: 0.002387425805181549
Loss at iteration [400]: 0.0023871566058202057
Loss at iteration [401]: 0.0023868717107167095
Loss at iteration [402]: 0.0023866043968088467
Loss at iteration [403]: 0.0023863210330976464
Loss at iteration [404]: 0.002386066325682871
Loss at iteration [405]: 0.002385746216128887
Loss at iteration [406]: 0.002385455638169832
Loss at iteration [407]: 0.0023851769078952257
Loss at iteration [408]: 0.002384872735304005
Loss at iteration [409]: 0.002384616062883472
Loss at iteration [410]: 0.0023843698070296414
Loss at iteration [411]: 0.002384112977117196
Loss at iteration [412]: 0.002383844246292918
Loss at iteration [413]: 0.002383569219192551
Loss at iteration [414]: 0.002383289639539191
Loss at iteration [415]: 0.002382998738527203
Loss at iteration [416]: 0.0023827100466536928
Loss at iteration [417]: 0.0023824593145898585
Loss at iteration [418]: 0.0023821561666355824
Loss at iteration [419]: 0.002381861441904673
Loss at iteration [420]: 0.00238162248467457
Loss at iteration [421]: 0.0023812972708721707
Loss at iteration [422]: 0.002381018192811803
Loss at iteration [423]: 0.0023807220649952315
Loss at iteration [424]: 0.0023804114281035654
Loss at iteration [425]: 0.002380114432270308
Loss at iteration [426]: 0.002379821006505979
Loss at iteration [427]: 0.0023795363812911734
Loss at iteration [428]: 0.002379226937982112
Loss at iteration [429]: 0.002378932907358175
Loss at iteration [430]: 0.0023786381669251627
Loss at iteration [431]: 0.002378355341657986
Loss at iteration [432]: 0.002378071125930208
Loss at iteration [433]: 0.0023777926569159323
Loss at iteration [434]: 0.0023774947307875134
Loss at iteration [435]: 0.0023771672408017994
Loss at iteration [436]: 0.002376837035059525
Loss at iteration [437]: 0.0023765019743136845
Loss at iteration [438]: 0.0023761507332140217
Loss at iteration [439]: 0.0023758307078085915
Loss at iteration [440]: 0.002375539415343956
Loss at iteration [441]: 0.0023752736241466972
Loss at iteration [442]: 0.002374976657202074
Loss at iteration [443]: 0.0023747073585567956
Loss at iteration [444]: 0.0023744456097187176
Loss at iteration [445]: 0.0023741807835017015
Loss at iteration [446]: 0.002373909584005916
Loss at iteration [447]: 0.0023736295424878263
Loss at iteration [448]: 0.00237336306874901
Loss at iteration [449]: 0.0023730746994905807
Loss at iteration [450]: 0.0023727990577296306
Loss at iteration [451]: 0.002372494611328788
Loss at iteration [452]: 0.0023722013236163924
Loss at iteration [453]: 0.002371902432935558
Loss at iteration [454]: 0.00237160447874687
Loss at iteration [455]: 0.0023713299703742776
Loss at iteration [456]: 0.0023710484099423016
Loss at iteration [457]: 0.002370743218690199
Loss at iteration [458]: 0.0023704410900550685
Loss at iteration [459]: 0.0023701734274544357
Loss at iteration [460]: 0.0023698840781157366
Loss at iteration [461]: 0.0023695930623493615
Loss at iteration [462]: 0.0023693236673427947
Loss at iteration [463]: 0.002369035262707416
Loss at iteration [464]: 0.0023687322691453238
Loss at iteration [465]: 0.0023684395488575683
Loss at iteration [466]: 0.002368140294906194
Loss at iteration [467]: 0.002367854139968906
Loss at iteration [468]: 0.0023675495848820265
Loss at iteration [469]: 0.002367242796363941
Loss at iteration [470]: 0.002366927520955139
Loss at iteration [471]: 0.002366623779731335
Loss at iteration [472]: 0.002366275645487541
Loss at iteration [473]: 0.0023659275136079677
Loss at iteration [474]: 0.0023655804374771486
Loss at iteration [475]: 0.002365230568159428
Loss at iteration [476]: 0.0023648746508745625
Loss at iteration [477]: 0.0023645420183703207
Loss at iteration [478]: 0.0023642086845770496
Loss at iteration [479]: 0.0023638721152979144
Loss at iteration [480]: 0.002363558899067018
Loss at iteration [481]: 0.0023632204115207117
Loss at iteration [482]: 0.0023628741449784867
Loss at iteration [483]: 0.0023625440072736767
Loss at iteration [484]: 0.0023622340968698177
Loss at iteration [485]: 0.0023619265229242025
Loss at iteration [486]: 0.0023616869940416237
Loss at iteration [487]: 0.0023613424124966464
Loss at iteration [488]: 0.002361091349807602
Loss at iteration [489]: 0.002360810904828285
Loss at iteration [490]: 0.0023605118559648845
Loss at iteration [491]: 0.0023601951672602914
Loss at iteration [492]: 0.002359893664159969
Loss at iteration [493]: 0.002359573269571857
Loss at iteration [494]: 0.002359255332103276
Loss at iteration [495]: 0.0023589234379243376
Loss at iteration [496]: 0.0023585916583352537
Loss at iteration [497]: 0.0023582653078284476
Loss at iteration [498]: 0.0023579347058401566
Loss at iteration [499]: 0.002357593842268954
Loss at iteration [500]: 0.0023572943216962645
Loss at iteration [501]: 0.0023569463486371256
Loss at iteration [502]: 0.002356612876184529
Loss at iteration [503]: 0.002356301516438929
Loss at iteration [504]: 0.0023559884199491456
Loss at iteration [505]: 0.0023556380117089855
Loss at iteration [506]: 0.0023553605861798793
Loss at iteration [507]: 0.0023550149281946564
Loss at iteration [508]: 0.002354712072027012
Loss at iteration [509]: 0.002354350596329946
Loss at iteration [510]: 0.0023540568400959917
Loss at iteration [511]: 0.002353774528688939
Loss at iteration [512]: 0.0023533937277606556
Loss at iteration [513]: 0.002353062953803925
Loss at iteration [514]: 0.0023527289569049707
Loss at iteration [515]: 0.0023523956069130595
Loss at iteration [516]: 0.002352063368844906
Loss at iteration [517]: 0.002351725588531836
Loss at iteration [518]: 0.0023514048376435492
Loss at iteration [519]: 0.0023510728486361987
Loss at iteration [520]: 0.002350728904192939
Loss at iteration [521]: 0.0023504238130011615
Loss at iteration [522]: 0.0023500740554701642
Loss at iteration [523]: 0.002349704669155653
Loss at iteration [524]: 0.0023493801382543727
Loss at iteration [525]: 0.002349047685863586
Loss at iteration [526]: 0.0023486979491134543
Loss at iteration [527]: 0.0023483231070884508
Loss at iteration [528]: 0.0023480016262200803
Loss at iteration [529]: 0.0023476490766748278
Loss at iteration [530]: 0.002347300632022631
Loss at iteration [531]: 0.0023469586768874668
Loss at iteration [532]: 0.002346606099716111
Loss at iteration [533]: 0.002346310849726382
Loss at iteration [534]: 0.002345922307954221
Loss at iteration [535]: 0.002345585431782587
Loss at iteration [536]: 0.002345250614129416
Loss at iteration [537]: 0.0023449131124801043
Loss at iteration [538]: 0.0023445749284674083
Loss at iteration [539]: 0.0023442147958926177
Loss at iteration [540]: 0.0023438473838054636
Loss at iteration [541]: 0.0023434735606662027
Loss at iteration [542]: 0.002343095106902983
Loss at iteration [543]: 0.0023427448501650004
Loss at iteration [544]: 0.002342375112020034
Loss at iteration [545]: 0.002341999680800221
Loss at iteration [546]: 0.00234170435486564
Loss at iteration [547]: 0.002341386228509038
Loss at iteration [548]: 0.002341035635109024
Loss at iteration [549]: 0.0023406980198988183
Loss at iteration [550]: 0.002340354035532712
Loss at iteration [551]: 0.0023400300898117382
Loss at iteration [552]: 0.002339655282618453
Loss at iteration [553]: 0.00233929486311841
Loss at iteration [554]: 0.0023389242955178177
Loss at iteration [555]: 0.0023386233409241545
Loss at iteration [556]: 0.0023382098622160242
Loss at iteration [557]: 0.002337859645831852
Loss at iteration [558]: 0.00233750981665031
Loss at iteration [559]: 0.0023371484006807627
Loss at iteration [560]: 0.002336781339608387
Loss at iteration [561]: 0.0023364102864646993
Loss at iteration [562]: 0.002336060746459035
Loss at iteration [563]: 0.002335700520935735
Loss at iteration [564]: 0.0023353686494337962
Loss at iteration [565]: 0.002335029866672992
Loss at iteration [566]: 0.002334666292704374
Loss at iteration [567]: 0.0023343191374113623
Loss at iteration [568]: 0.002333975260191434
Loss at iteration [569]: 0.002333576081743597
Loss at iteration [570]: 0.0023332801897202092
Loss at iteration [571]: 0.002332870591612097
Loss at iteration [572]: 0.002332419944690142
Loss at iteration [573]: 0.0023320536430655823
Loss at iteration [574]: 0.0023315950762113217
Loss at iteration [575]: 0.0023311568428942635
Loss at iteration [576]: 0.0023306938678701035
Loss at iteration [577]: 0.0023302467216326666
Loss at iteration [578]: 0.0023298919927260404
Loss at iteration [579]: 0.0023294748945230856
Loss at iteration [580]: 0.002329072464807912
Loss at iteration [581]: 0.0023286153616673007
Loss at iteration [582]: 0.002328114584311781
Loss at iteration [583]: 0.0023276663068909167
Loss at iteration [584]: 0.0023272298451588827
Loss at iteration [585]: 0.0023268098119742864
Loss at iteration [586]: 0.0023263980946339586
Loss at iteration [587]: 0.0023259067080295005
Loss at iteration [588]: 0.0023254587383149043
Loss at iteration [589]: 0.002325013848046528
Loss at iteration [590]: 0.002324523080601117
Loss at iteration [591]: 0.0023240782012411837
Loss at iteration [592]: 0.0023236085687689803
Loss at iteration [593]: 0.0023231294827455633
Loss at iteration [594]: 0.002322670146234265
Loss at iteration [595]: 0.002322234225412064
Loss at iteration [596]: 0.002321759348145996
Loss at iteration [597]: 0.002321356030063923
Loss at iteration [598]: 0.0023208164200358657
Loss at iteration [599]: 0.00232039329006656
Loss at iteration [600]: 0.0023199000533771453
Loss at iteration [601]: 0.0023193977485313692
Loss at iteration [602]: 0.0023189196271590318
Loss at iteration [603]: 0.002318386557520991
Loss at iteration [604]: 0.002317939221682597
Loss at iteration [605]: 0.0023174490419658623
Loss at iteration [606]: 0.0023170004035984147
Loss at iteration [607]: 0.0023164634649672595
Loss at iteration [608]: 0.0023160011606633713
Loss at iteration [609]: 0.002315497131049634
Loss at iteration [610]: 0.002315042391797583
Loss at iteration [611]: 0.002314542029362626
Loss at iteration [612]: 0.0023140440557136975
Loss at iteration [613]: 0.002313552315187169
Loss at iteration [614]: 0.0023130705143307733
Loss at iteration [615]: 0.0023126121565107175
Loss at iteration [616]: 0.0023121281048757597
Loss at iteration [617]: 0.002311656320383442
Loss at iteration [618]: 0.0023111264456474696
Loss at iteration [619]: 0.002310670885797038
Loss at iteration [620]: 0.0023102043834136296
Loss at iteration [621]: 0.0023096325334566316
Loss at iteration [622]: 0.002309136436275607
Loss at iteration [623]: 0.002308676278694681
Loss at iteration [624]: 0.0023081021440795285
Loss at iteration [625]: 0.00230763194061448
Loss at iteration [626]: 0.0023071136269257613
Loss at iteration [627]: 0.002306572038167901
Loss at iteration [628]: 0.0023060785001073536
Loss at iteration [629]: 0.0023055405243065795
Loss at iteration [630]: 0.002304976278255295
Loss at iteration [631]: 0.0023045424675059408
Loss at iteration [632]: 0.0023039933804869044
Loss at iteration [633]: 0.0023034855561558628
Loss at iteration [634]: 0.002303059087511488
Loss at iteration [635]: 0.0023025000030142658
Loss at iteration [636]: 0.0023020745661915575
Loss at iteration [637]: 0.0023015459186067413
Loss at iteration [638]: 0.0023011553885294665
Loss at iteration [639]: 0.0023006790056498204
Loss at iteration [640]: 0.0023001738848085537
Loss at iteration [641]: 0.0022997344415866964
Loss at iteration [642]: 0.002299234268460197
Loss at iteration [643]: 0.0022986690103304025
Loss at iteration [644]: 0.0022983356902323625
Loss at iteration [645]: 0.0022978297390124818
Loss at iteration [646]: 0.0022973552940470845
Loss at iteration [647]: 0.0022968467429866786
Loss at iteration [648]: 0.002296416893104242
Loss at iteration [649]: 0.0022959026620640805
Loss at iteration [650]: 0.002295396458879267
Loss at iteration [651]: 0.002295001756969777
Loss at iteration [652]: 0.0022944752592395346
Loss at iteration [653]: 0.0022938875904540638
Loss at iteration [654]: 0.002293481780982526
Loss at iteration [655]: 0.002292938692124964
Loss at iteration [656]: 0.0022924648851447896
Loss at iteration [657]: 0.00229203621338512
Loss at iteration [658]: 0.0022915218094302302
Loss at iteration [659]: 0.0022910510780267995
Loss at iteration [660]: 0.0022905450560450564
Loss at iteration [661]: 0.0022900863457662223
Loss at iteration [662]: 0.002289584503683499
Loss at iteration [663]: 0.0022890626905935833
Loss at iteration [664]: 0.002288534535845267
Loss at iteration [665]: 0.0022880487769842634
Loss at iteration [666]: 0.002287500672736332
Loss at iteration [667]: 0.0022870183222055474
Loss at iteration [668]: 0.002286561042898069
Loss at iteration [669]: 0.0022860728064818623
Loss at iteration [670]: 0.002285571584679035
Loss at iteration [671]: 0.002285159809050318
Loss at iteration [672]: 0.0022845193905576947
Loss at iteration [673]: 0.0022841531710919983
Loss at iteration [674]: 0.0022836234962886922
Loss at iteration [675]: 0.002282893031530745
Loss at iteration [676]: 0.002282375345463817
Loss at iteration [677]: 0.0022818081959768105
Loss at iteration [678]: 0.0022812038414679603
Loss at iteration [679]: 0.0022805866147319297
Loss at iteration [680]: 0.002280103080156747
Loss at iteration [681]: 0.0022795336466465647
Loss at iteration [682]: 0.00227908558465082
Loss at iteration [683]: 0.002278538438367718
Loss at iteration [684]: 0.0022780246483386254
Loss at iteration [685]: 0.002277631904147751
Loss at iteration [686]: 0.0022770091883343477
Loss at iteration [687]: 0.002276546790818179
Loss at iteration [688]: 0.002276070618307073
Loss at iteration [689]: 0.002275538715760041
Loss at iteration [690]: 0.002274990980026212
Loss at iteration [691]: 0.0022745166181640766
Loss at iteration [692]: 0.00227389030925287
Loss at iteration [693]: 0.002273406606212333
Loss at iteration [694]: 0.0022727987010678916
Loss at iteration [695]: 0.0022721629028555827
Loss at iteration [696]: 0.0022716807511127825
Loss at iteration [697]: 0.0022711240182217186
Loss at iteration [698]: 0.002270582493183491
Loss at iteration [699]: 0.0022702892398551457
Loss at iteration [700]: 0.002269697068200611
Loss at iteration [701]: 0.002269040946351901
Loss at iteration [702]: 0.00226863975718265
Loss at iteration [703]: 0.002268045170856823
Loss at iteration [704]: 0.0022674420984987787
Loss at iteration [705]: 0.0022670180131104723
Loss at iteration [706]: 0.0022664447179257708
Loss at iteration [707]: 0.0022659200589148177
Loss at iteration [708]: 0.0022654179592888077
Loss at iteration [709]: 0.0022648310081125208
Loss at iteration [710]: 0.0022642648215303244
Loss at iteration [711]: 0.002263812575268164
Loss at iteration [712]: 0.002263269518541849
Loss at iteration [713]: 0.0022627671691947395
Loss at iteration [714]: 0.0022621772035075634
Loss at iteration [715]: 0.0022617656531683807
Loss at iteration [716]: 0.0022612556293849775
Loss at iteration [717]: 0.0022606565265965426
Loss at iteration [718]: 0.0022601916618839525
Loss at iteration [719]: 0.002259634819081905
Loss at iteration [720]: 0.0022591419955790763
Loss at iteration [721]: 0.0022585739199150936
Loss at iteration [722]: 0.002258350813246906
Loss at iteration [723]: 0.0022577569672295023
Loss at iteration [724]: 0.002257252116135513
Loss at iteration [725]: 0.002256819079256755
Loss at iteration [726]: 0.002256349543058766
Loss at iteration [727]: 0.0022557457954004902
Loss at iteration [728]: 0.0022551551715752514
Loss at iteration [729]: 0.002254878610688806
Loss at iteration [730]: 0.002254326323601343
Loss at iteration [731]: 0.0022536679833158866
Loss at iteration [732]: 0.0022532681494464185
Loss at iteration [733]: 0.0022527310920063114
Loss at iteration [734]: 0.0022521950892437103
Loss at iteration [735]: 0.0022516605296120876
Loss at iteration [736]: 0.0022513337010019218
Loss at iteration [737]: 0.00225079626777153
Loss at iteration [738]: 0.0022502753272988195
Loss at iteration [739]: 0.0022497810552101812
Loss at iteration [740]: 0.002249288601942516
Loss at iteration [741]: 0.0022489086717951507
Loss at iteration [742]: 0.0022483330095257503
Loss at iteration [743]: 0.0022478463291826616
Loss at iteration [744]: 0.0022474613224565646
Loss at iteration [745]: 0.002246980908451794
Loss at iteration [746]: 0.002246523846702427
Loss at iteration [747]: 0.0022460228274862404
Loss at iteration [748]: 0.002245470772301958
Loss at iteration [749]: 0.00224504748578785
Loss at iteration [750]: 0.0022445758186904383
Loss at iteration [751]: 0.002244087417152108
Loss at iteration [752]: 0.002243671200554
Loss at iteration [753]: 0.0022431084055657686
Loss at iteration [754]: 0.0022427422724186088
Loss at iteration [755]: 0.002242183331567933
Loss at iteration [756]: 0.0022418962293842453
Loss at iteration [757]: 0.002241507516550941
Loss at iteration [758]: 0.0022409057003301785
Loss at iteration [759]: 0.0022404011637387113
Loss at iteration [760]: 0.002240108747555119
Loss at iteration [761]: 0.0022395194536004707
Loss at iteration [762]: 0.0022389077987962193
Loss at iteration [763]: 0.0022385419299345375
Loss at iteration [764]: 0.0022380441643208644
Loss at iteration [765]: 0.002237485463186498
Loss at iteration [766]: 0.0022370548510818712
Loss at iteration [767]: 0.0022366073539934694
Loss at iteration [768]: 0.0022361444403890315
Loss at iteration [769]: 0.002235720783513977
Loss at iteration [770]: 0.0022352990380253906
Loss at iteration [771]: 0.0022347127294376126
Loss at iteration [772]: 0.002234222772061401
Loss at iteration [773]: 0.0022337470698285014
Loss at iteration [774]: 0.0022333780307766528
Loss at iteration [775]: 0.002232850645260031
Loss at iteration [776]: 0.00223232681554647
Loss at iteration [777]: 0.002231887906421392
Loss at iteration [778]: 0.0022314319952370203
Loss at iteration [779]: 0.0022309298099762133
Loss at iteration [780]: 0.002230502153129009
Loss at iteration [781]: 0.002229938993297222
Loss at iteration [782]: 0.0022296512622842013
Loss at iteration [783]: 0.0022291467519726716
Loss at iteration [784]: 0.0022286029708556004
Loss at iteration [785]: 0.002228275733661408
Loss at iteration [786]: 0.00222781862027791
Loss at iteration [787]: 0.0022271970927722922
Loss at iteration [788]: 0.0022268693651838678
Loss at iteration [789]: 0.0022263289408258786
Loss at iteration [790]: 0.0022258171679744414
Loss at iteration [791]: 0.0022253870331303514
Loss at iteration [792]: 0.0022250147230756375
Loss at iteration [793]: 0.002224330962578477
Loss at iteration [794]: 0.0022238964052597628
Loss at iteration [795]: 0.00222341518976805
Loss at iteration [796]: 0.002222961479298199
Loss at iteration [797]: 0.002222408987398663
Loss at iteration [798]: 0.0022219905845437533
Loss at iteration [799]: 0.00222153727433355
Loss at iteration [800]: 0.002221081113731096
Loss at iteration [801]: 0.0022205421240477116
Loss at iteration [802]: 0.0022201713421932
Loss at iteration [803]: 0.0022197574915352467
Loss at iteration [804]: 0.0022192099579218735
Loss at iteration [805]: 0.0022187001643976015
Loss at iteration [806]: 0.0022183730539697094
Loss at iteration [807]: 0.0022178142075171426
Loss at iteration [808]: 0.0022174545570956064
Loss at iteration [809]: 0.002217042577577738
Loss at iteration [810]: 0.0022165319398110388
Loss at iteration [811]: 0.002215948566137983
Loss at iteration [812]: 0.0022156249568056506
Loss at iteration [813]: 0.0022150804041539784
Loss at iteration [814]: 0.00221464194225082
Loss at iteration [815]: 0.0022143681212612263
Loss at iteration [816]: 0.002213939070094504
Loss at iteration [817]: 0.00221340111656296
Loss at iteration [818]: 0.002212972959562714
Loss at iteration [819]: 0.002212641789018759
Loss at iteration [820]: 0.0022121552699369755
Loss at iteration [821]: 0.0022118168493564377
Loss at iteration [822]: 0.002211529716321116
Loss at iteration [823]: 0.0022109858605657644
Loss at iteration [824]: 0.0022105515430363106
Loss at iteration [825]: 0.0022102146444852
Loss at iteration [826]: 0.002209584414501709
Loss at iteration [827]: 0.002209219828117098
Loss at iteration [828]: 0.002208914354911229
Loss at iteration [829]: 0.002208422629768816
Loss at iteration [830]: 0.0022080175066150807
Loss at iteration [831]: 0.002207584739239184
Loss at iteration [832]: 0.002207031188569862
Loss at iteration [833]: 0.002206725180699367
Loss at iteration [834]: 0.002206392907337916
Loss at iteration [835]: 0.002205953237453065
Loss at iteration [836]: 0.0022055252141178587
Loss at iteration [837]: 0.0022050436194900945
Loss at iteration [838]: 0.0022047427089197397
Loss at iteration [839]: 0.002204249339218143
Loss at iteration [840]: 0.002203928565106135
Loss at iteration [841]: 0.0022035529526433944
Loss at iteration [842]: 0.002202990999778249
Loss at iteration [843]: 0.0022027108420443185
Loss at iteration [844]: 0.002202223111764241
Loss at iteration [845]: 0.0022018439024765334
Loss at iteration [846]: 0.0022013157060389544
Loss at iteration [847]: 0.002201088299486608
Loss at iteration [848]: 0.0022007345300299897
Loss at iteration [849]: 0.0022001335839645923
Loss at iteration [850]: 0.0021997432445888684
Loss at iteration [851]: 0.0021993242150904996
Loss at iteration [852]: 0.0021989108461873247
Loss at iteration [853]: 0.0021985839488129055
Loss at iteration [854]: 0.002198209341712552
Loss at iteration [855]: 0.002197754299282522
Loss at iteration [856]: 0.0021972895333488944
Loss at iteration [857]: 0.002197098015775338
Loss at iteration [858]: 0.0021965254595255885
Loss at iteration [859]: 0.002196345220107675
Loss at iteration [860]: 0.002195932734015433
Loss at iteration [861]: 0.0021954556457875057
Loss at iteration [862]: 0.002195038899776071
Loss at iteration [863]: 0.0021945743616385285
Loss at iteration [864]: 0.0021941478179391613
Loss at iteration [865]: 0.0021939549385539708
Loss at iteration [866]: 0.0021934888258815875
Loss at iteration [867]: 0.0021932591135763687
Loss at iteration [868]: 0.0021927620692173177
Loss at iteration [869]: 0.0021923739643064217
Loss at iteration [870]: 0.002191968561849984
Loss at iteration [871]: 0.002191586114089317
Loss at iteration [872]: 0.0021912573579692973
Loss at iteration [873]: 0.002190914949668961
Loss at iteration [874]: 0.0021903566045576857
Loss at iteration [875]: 0.002190082581862226
Loss at iteration [876]: 0.0021897457102505275
Loss at iteration [877]: 0.002189364837313488
Loss at iteration [878]: 0.002189080395158404
Loss at iteration [879]: 0.0021886347989058456
Loss at iteration [880]: 0.0021881006335534225
Loss at iteration [881]: 0.0021879560640438596
Loss at iteration [882]: 0.002187463617327257
Loss at iteration [883]: 0.002187071373788187
Loss at iteration [884]: 0.0021865359672550125
Loss at iteration [885]: 0.0021863795887374954
Loss at iteration [886]: 0.0021860797757804495
Loss at iteration [887]: 0.002185624986578691
Loss at iteration [888]: 0.002185248774930175
Loss at iteration [889]: 0.002184965090281259
Loss at iteration [890]: 0.0021845099387997283
Loss at iteration [891]: 0.002184052578915383
Loss at iteration [892]: 0.0021836720371373242
Loss at iteration [893]: 0.002183479953345333
Loss at iteration [894]: 0.0021829936379494804
Loss at iteration [895]: 0.002182682008877392
Loss at iteration [896]: 0.0021823288197195937
Loss at iteration [897]: 0.0021819842499907712
Loss at iteration [898]: 0.0021816442161771664
Loss at iteration [899]: 0.0021813278585650453
Loss at iteration [900]: 0.002181021622827095
Loss at iteration [901]: 0.0021805961267068624
Loss at iteration [902]: 0.0021802728992695187
Loss at iteration [903]: 0.002179936826604295
Loss at iteration [904]: 0.002179808772926478
Loss at iteration [905]: 0.002179517671783707
Loss at iteration [906]: 0.002179263651750495
Loss at iteration [907]: 0.0021786718124627608
Loss at iteration [908]: 0.002178296782610795
Loss at iteration [909]: 0.002177892986867451
Loss at iteration [910]: 0.0021774887614687886
Loss at iteration [911]: 0.002177080807467515
Loss at iteration [912]: 0.002176796335037935
Loss at iteration [913]: 0.0021763736833621964
Loss at iteration [914]: 0.002176128875158377
Loss at iteration [915]: 0.0021759547978446804
Loss at iteration [916]: 0.0021756319308907407
Loss at iteration [917]: 0.0021752467967317084
Loss at iteration [918]: 0.002174894164557486
Loss at iteration [919]: 0.0021743461643101403
Loss at iteration [920]: 0.0021740816319726327
Loss at iteration [921]: 0.0021738176136312233
Loss at iteration [922]: 0.002173495123260572
Loss at iteration [923]: 0.002173092592586247
Loss at iteration [924]: 0.0021727152946928625
Loss at iteration [925]: 0.002172634586415261
Loss at iteration [926]: 0.00217227477358728
Loss at iteration [927]: 0.00217201531770867
Loss at iteration [928]: 0.0021717403504786675
Loss at iteration [929]: 0.0021715026528048367
Loss at iteration [930]: 0.0021711490778813553
Loss at iteration [931]: 0.0021711357750660563
Loss at iteration [932]: 0.002171044942759629
Loss at iteration [933]: 0.002170477958485113
Loss at iteration [934]: 0.002170062734869697
Loss at iteration [935]: 0.0021695835598668028
Loss at iteration [936]: 0.0021691834604673564
Loss at iteration [937]: 0.0021688872996697968
Loss at iteration [938]: 0.0021686602262950072
Loss at iteration [939]: 0.002168352592764757
Loss at iteration [940]: 0.0021680530357386803
Loss at iteration [941]: 0.002167600433524755
Loss at iteration [942]: 0.0021675091566407258
Loss at iteration [943]: 0.0021674598875988196
Loss at iteration [944]: 0.00216694174088719
Loss at iteration [945]: 0.002167072275675692
***** Warning: Loss has increased *****
Loss at iteration [946]: 0.0021669059183801135
Loss at iteration [947]: 0.00216660194595385
Loss at iteration [948]: 0.0021661512886893977
Loss at iteration [949]: 0.002166115004988299
Loss at iteration [950]: 0.002165951250658669
Loss at iteration [951]: 0.002165307351928936
Loss at iteration [952]: 0.002164958963893004
Loss at iteration [953]: 0.002164959487339677
***** Warning: Loss has increased *****
