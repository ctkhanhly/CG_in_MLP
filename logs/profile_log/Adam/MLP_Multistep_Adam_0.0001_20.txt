Model name                            : MLP_Multistep
The number of input features          : 20
The number of output features         : 2
Optimizer name                        : Adam
Learning rate                         : 0.0001
Max number of iterations              : 3000
Number of samples in training data    : 39
Number of samples in tests data       : 16
Total training time                   : 0.2698495388031006
Total number of parameters            : 205302
Percentage of parameters < 1e-9       : 48.98978090812559%
Percentage of parameters < 1e-7       : 48.98978090812559%
Percentage of parameters < 1e-6       : 48.98978090812559%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.5464332459118517
Loss at iteration [2]: 1.5313548529249918
Loss at iteration [3]: 1.5173296000803482
Loss at iteration [4]: 1.5041254207796857
Loss at iteration [5]: 1.4917347147265052
Loss at iteration [6]: 1.4795552588486574
Loss at iteration [7]: 1.4674107009586146
Loss at iteration [8]: 1.4555180172559987
Loss at iteration [9]: 1.4437117792400005
Loss at iteration [10]: 1.4318648039642305
Loss at iteration [11]: 1.419968130751136
Loss at iteration [12]: 1.4079120510236491
Loss at iteration [13]: 1.3958181950774597
Loss at iteration [14]: 1.3837388877202388
Loss at iteration [15]: 1.3716746627805902
Loss at iteration [16]: 1.3595458799099513
Loss at iteration [17]: 1.3472837537443574
Loss at iteration [18]: 1.334996476302205
Loss at iteration [19]: 1.3226532129597357
Loss at iteration [20]: 1.3102338015311328
Loss at iteration [21]: 1.2975908988575002
Loss at iteration [22]: 1.284945421133333
Loss at iteration [23]: 1.2721264066368174
Loss at iteration [24]: 1.2588908082001862
Loss at iteration [25]: 1.2454737804002973
Loss at iteration [26]: 1.231830479984912
Loss at iteration [27]: 1.2182065898594845
Loss at iteration [28]: 1.2044481981008883
Loss at iteration [29]: 1.1904380676781101
Loss at iteration [30]: 1.1762138378189144
Loss at iteration [31]: 1.1617716597666863
Loss at iteration [32]: 1.1472750645500447
Loss at iteration [33]: 1.132661633930442
Loss at iteration [34]: 1.117838822620612
Loss at iteration [35]: 1.1028324848772335
Loss at iteration [36]: 1.0876601334513434
Loss at iteration [37]: 1.072276176813083
Loss at iteration [38]: 1.056794217518515
Loss at iteration [39]: 1.041172817206683
Loss at iteration [40]: 1.025292539349837
Loss at iteration [41]: 1.0091630614896963
Loss at iteration [42]: 0.9927345703589185
Loss at iteration [43]: 0.9760641386903751
Loss at iteration [44]: 0.9593423735776924
Loss at iteration [45]: 0.9424880573716637
Loss at iteration [46]: 0.9254945713250005
Loss at iteration [47]: 0.9083724047675875
Loss at iteration [48]: 0.8910161800268668
Loss at iteration [49]: 0.8735089054213179
Loss at iteration [50]: 0.8558879052338053
Loss at iteration [51]: 0.838032521834463
Loss at iteration [52]: 0.8200248902069814
Loss at iteration [53]: 0.8019242671002621
Loss at iteration [54]: 0.783811797912764
Loss at iteration [55]: 0.765726056564838
Loss at iteration [56]: 0.7477047903745325
Loss at iteration [57]: 0.7298884394345118
Loss at iteration [58]: 0.7122770834516503
Loss at iteration [59]: 0.6948942782494549
Loss at iteration [60]: 0.6777638395127144
Loss at iteration [61]: 0.6609303602769284
Loss at iteration [62]: 0.6444214488875488
Loss at iteration [63]: 0.6283130834463129
Loss at iteration [64]: 0.612564273126078
Loss at iteration [65]: 0.5972958743116091
Loss at iteration [66]: 0.582474055298956
Loss at iteration [67]: 0.568121464698782
Loss at iteration [68]: 0.5543433111250402
Loss at iteration [69]: 0.5410865286078762
Loss at iteration [70]: 0.5283934339912495
Loss at iteration [71]: 0.5162735974100334
Loss at iteration [72]: 0.5047130415617849
Loss at iteration [73]: 0.4937624431608523
Loss at iteration [74]: 0.48339711424024745
Loss at iteration [75]: 0.4736175026176173
Loss at iteration [76]: 0.4644298055484686
Loss at iteration [77]: 0.4558492999719954
Loss at iteration [78]: 0.44774439137302924
Loss at iteration [79]: 0.4402077097308553
Loss at iteration [80]: 0.4332220805820229
Loss at iteration [81]: 0.42676780124492997
Loss at iteration [82]: 0.42084775811380254
Loss at iteration [83]: 0.4154675673131918
Loss at iteration [84]: 0.4106360824226778
Loss at iteration [85]: 0.4063590290996643
Loss at iteration [86]: 0.4026383810415628
Loss at iteration [87]: 0.3994635253876799
Loss at iteration [88]: 0.3968180601400708
Loss at iteration [89]: 0.39467098788152144
Loss at iteration [90]: 0.392981997357504
Loss at iteration [91]: 0.3917034795269341
Loss at iteration [92]: 0.3907732646526036
Loss at iteration [93]: 0.39014569395098464
Loss at iteration [94]: 0.389764682518545
Loss at iteration [95]: 0.3895820165736968
Loss at iteration [96]: 0.3895535887688102
Loss at iteration [97]: 0.38963902632399655
***** Warning: Loss has increased *****
Loss at iteration [98]: 0.3898041874883667
***** Warning: Loss has increased *****
Loss at iteration [99]: 0.3900190793223357
***** Warning: Loss has increased *****
Loss at iteration [100]: 0.3902586687638765
***** Warning: Loss has increased *****
Loss at iteration [101]: 0.3905020649223036
***** Warning: Loss has increased *****
Loss at iteration [102]: 0.3907314849095043
***** Warning: Loss has increased *****
Loss at iteration [103]: 0.39093151126551134
***** Warning: Loss has increased *****
Loss at iteration [104]: 0.39109026754905296
***** Warning: Loss has increased *****
Loss at iteration [105]: 0.39119987925214955
***** Warning: Loss has increased *****
Loss at iteration [106]: 0.39125581847199514
***** Warning: Loss has increased *****
Loss at iteration [107]: 0.3912572774361624
***** Warning: Loss has increased *****
Loss at iteration [108]: 0.39120648919799955
Loss at iteration [109]: 0.3911087086419285
Loss at iteration [110]: 0.39097268483884845
Loss at iteration [111]: 0.3908080254964575
Loss at iteration [112]: 0.3906248948245944
Loss at iteration [113]: 0.39043302092433674
Loss at iteration [114]: 0.3902409797040571
Loss at iteration [115]: 0.3900558081936957
Loss at iteration [116]: 0.3898828306146435
Loss at iteration [117]: 0.3897256967056087
Loss at iteration [118]: 0.3895865744196392
Loss at iteration [119]: 0.3894664280441393
Loss at iteration [120]: 0.3893653159481977
Loss at iteration [121]: 0.38928265679516816
Loss at iteration [122]: 0.38921743430916494
Loss at iteration [123]: 0.3891683328631561
Loss at iteration [124]: 0.3891338142825629
Loss at iteration [125]: 0.38911215722789905
Loss at iteration [126]: 0.38910148351812024
Loss at iteration [127]: 0.3890997919354621
Loss at iteration [128]: 0.3891050117982033
***** Warning: Loss has increased *****
Loss at iteration [129]: 0.38911509964039165
***** Warning: Loss has increased *****
Loss at iteration [130]: 0.3891280914952425
***** Warning: Loss has increased *****
Loss at iteration [131]: 0.38914216921583106
***** Warning: Loss has increased *****
Loss at iteration [132]: 0.38915580171762554
***** Warning: Loss has increased *****
Loss at iteration [133]: 0.3891678276662564
***** Warning: Loss has increased *****
Loss at iteration [134]: 0.38917752988320986
***** Warning: Loss has increased *****
Loss at iteration [135]: 0.38918443024430516
***** Warning: Loss has increased *****
Loss at iteration [136]: 0.38918839691252505
***** Warning: Loss has increased *****
Loss at iteration [137]: 0.38918952920946526
***** Warning: Loss has increased *****
Loss at iteration [138]: 0.3891881146140202
Loss at iteration [139]: 0.3891845459094206
Loss at iteration [140]: 0.38917925975107603
Loss at iteration [141]: 0.38917269078498157
Loss at iteration [142]: 0.3891652430073896
Loss at iteration [143]: 0.389157276565374
Loss at iteration [144]: 0.3891491058032775
Loss at iteration [145]: 0.3891410033746091
Loss at iteration [146]: 0.38913320561572373
Loss at iteration [147]: 0.3891259157727034
Loss at iteration [148]: 0.3891193035682247
Loss at iteration [149]: 0.38911350144481777
Loss at iteration [150]: 0.3891085991822902
Loss at iteration [151]: 0.38910463920824795
Loss at iteration [152]: 0.3891016147689503
Loss at iteration [153]: 0.38909947236303244
Loss at iteration [154]: 0.3890981187477842
Loss at iteration [155]: 0.3890974317300872
Loss at iteration [156]: 0.38909727313061554
Loss at iteration [157]: 0.38909750193346104
***** Warning: Loss has increased *****
Loss at iteration [158]: 0.389097985743372
***** Warning: Loss has increased *****
Loss at iteration [159]: 0.3890986091837328
***** Warning: Loss has increased *****
Loss at iteration [160]: 0.38909927860805343
***** Warning: Loss has increased *****
Loss at iteration [161]: 0.3890999232623055
***** Warning: Loss has increased *****
Loss at iteration [162]: 0.3891004936449079
***** Warning: Loss has increased *****
Loss at iteration [163]: 0.3891009581506286
***** Warning: Loss has increased *****
Loss at iteration [164]: 0.3891012991229761
***** Warning: Loss has increased *****
Loss at iteration [165]: 0.3891015092238247
***** Warning: Loss has increased *****
Loss at iteration [166]: 0.38910158865975464
***** Warning: Loss has increased *****
Loss at iteration [167]: 0.38910154340212993
Loss at iteration [168]: 0.3891013842085828
Loss at iteration [169]: 0.3891011260652947
Loss at iteration [170]: 0.38910078764129946
Loss at iteration [171]: 0.3891003904518717
Loss at iteration [172]: 0.38909995761122607
Loss at iteration [173]: 0.3890995122482859
Loss at iteration [174]: 0.38909907580527275
Loss at iteration [175]: 0.38909866650237246
Loss at iteration [176]: 0.3890982982261922
Loss at iteration [177]: 0.38909798000407136
Loss at iteration [178]: 0.38909771609570526
Loss at iteration [179]: 0.38909750660738884
Loss at iteration [180]: 0.38909734844482474
Loss at iteration [181]: 0.3890972363856311
Loss at iteration [182]: 0.38909716407310896
Loss at iteration [183]: 0.3890971247946193
Loss at iteration [184]: 0.38909711198861785
Loss at iteration [185]: 0.38909711950035536
***** Warning: Loss has increased *****
Loss at iteration [186]: 0.3890971416590895
***** Warning: Loss has increased *****
Loss at iteration [187]: 0.3890971732703304
***** Warning: Loss has increased *****
Loss at iteration [188]: 0.38909720960629324
***** Warning: Loss has increased *****
Loss at iteration [189]: 0.38909724644565674
***** Warning: Loss has increased *****
Loss at iteration [190]: 0.38909728017346384
***** Warning: Loss has increased *****
Loss at iteration [191]: 0.3890973079169118
***** Warning: Loss has increased *****
Loss at iteration [192]: 0.38909732767237304
***** Warning: Loss has increased *****
Loss at iteration [193]: 0.38909733837694294
***** Warning: Loss has increased *****
Loss at iteration [194]: 0.38909733989190853
***** Warning: Loss has increased *****
Loss at iteration [195]: 0.38909733288923104
Loss at iteration [196]: 0.3890973186571226
Loss at iteration [197]: 0.3890972988596165
Loss at iteration [198]: 0.3890972752930465
Loss at iteration [199]: 0.389097249678654
Loss at iteration [200]: 0.38909722351762593
Loss at iteration [201]: 0.38909719801743
Loss at iteration [202]: 0.3890971740816148
Loss at iteration [203]: 0.38909715234353104
Loss at iteration [204]: 0.38909713321993833
Loss at iteration [205]: 0.38909711696307764
Loss at iteration [206]: 0.38909710369746364
Loss at iteration [207]: 0.38909709343728477
Loss at iteration [208]: 0.3890970860887883
Loss at iteration [209]: 0.3890970814471305
Loss at iteration [210]: 0.38909707919802466
Loss at iteration [211]: 0.38909707893163714
