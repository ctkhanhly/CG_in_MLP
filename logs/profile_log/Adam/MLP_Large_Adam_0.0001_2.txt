Model name                            : MLP_Large
The number of input features          : 2
The number of output features         : 1
Optimizer name                        : Adam
Learning rate                         : 0.0001
Max number of iterations              : 3000
Number of samples in training data    : 858
Number of samples in tests data       : 368
Total training time                   : 27.10777497291565
Total number of parameters            : 402001
Percentage of parameters < 1e-9       : 49.679975920457906%
Percentage of parameters < 1e-7       : 49.679975920457906%
Percentage of parameters < 1e-6       : 49.68047343165813%

=============================================
=============================================
===================Losses====================
=============================================
=============================================


Loss at iteration [1]: 1.0821790046185182
Loss at iteration [2]: 0.9810548794761123
Loss at iteration [3]: 0.8878172840017611
Loss at iteration [4]: 0.8035161505898948
Loss at iteration [5]: 0.7283426768938911
Loss at iteration [6]: 0.6588924678827884
Loss at iteration [7]: 0.595314635171249
Loss at iteration [8]: 0.5370623798674528
Loss at iteration [9]: 0.4830734189353959
Loss at iteration [10]: 0.43355735276332047
Loss at iteration [11]: 0.3878943384125007
Loss at iteration [12]: 0.34572987340672584
Loss at iteration [13]: 0.30647650877867455
Loss at iteration [14]: 0.2698127066235204
Loss at iteration [15]: 0.2354085787911734
Loss at iteration [16]: 0.20325455350967123
Loss at iteration [17]: 0.17343474052909658
Loss at iteration [18]: 0.14620367169895657
Loss at iteration [19]: 0.12168873747740852
Loss at iteration [20]: 0.09979396090792532
Loss at iteration [21]: 0.08068806786363591
Loss at iteration [22]: 0.0641693835734817
Loss at iteration [23]: 0.05016868419144762
Loss at iteration [24]: 0.038638739334878165
Loss at iteration [25]: 0.029456080195742914
Loss at iteration [26]: 0.022516092800523777
Loss at iteration [27]: 0.017683590406870335
Loss at iteration [28]: 0.014713737622322429
Loss at iteration [29]: 0.013309266397362424
Loss at iteration [30]: 0.01314688006561411
Loss at iteration [31]: 0.013882791955037468
***** Warning: Loss has increased *****
Loss at iteration [32]: 0.015162470710226753
***** Warning: Loss has increased *****
Loss at iteration [33]: 0.016648679088259184
***** Warning: Loss has increased *****
Loss at iteration [34]: 0.018075603596208602
***** Warning: Loss has increased *****
Loss at iteration [35]: 0.019224384698858125
***** Warning: Loss has increased *****
Loss at iteration [36]: 0.019926224709222215
***** Warning: Loss has increased *****
Loss at iteration [37]: 0.020064367634660584
***** Warning: Loss has increased *****
Loss at iteration [38]: 0.01958158891591647
Loss at iteration [39]: 0.018507902153497496
Loss at iteration [40]: 0.016923543419384585
Loss at iteration [41]: 0.014964004308059489
Loss at iteration [42]: 0.012803704905964884
Loss at iteration [43]: 0.010657459745820318
Loss at iteration [44]: 0.008719187857291543
Loss at iteration [45]: 0.007135570048469079
Loss at iteration [46]: 0.005999640873734848
Loss at iteration [47]: 0.005333969173142809
Loss at iteration [48]: 0.005095531928280597
Loss at iteration [49]: 0.005192713662720517
***** Warning: Loss has increased *****
Loss at iteration [50]: 0.005481894095263256
***** Warning: Loss has increased *****
Loss at iteration [51]: 0.005819975154495506
***** Warning: Loss has increased *****
Loss at iteration [52]: 0.006097991852811782
***** Warning: Loss has increased *****
Loss at iteration [53]: 0.0062356691216662275
***** Warning: Loss has increased *****
Loss at iteration [54]: 0.0061991051168227155
Loss at iteration [55]: 0.005993251546233074
Loss at iteration [56]: 0.005657142065418437
Loss at iteration [57]: 0.005244411399233727
Loss at iteration [58]: 0.004800356586514206
Loss at iteration [59]: 0.004370796679811055
Loss at iteration [60]: 0.003990151250637607
Loss at iteration [61]: 0.003678110567369507
Loss at iteration [62]: 0.003445881564564947
Loss at iteration [63]: 0.003294698651621657
Loss at iteration [64]: 0.003218280231426671
Loss at iteration [65]: 0.0032014478128548117
Loss at iteration [66]: 0.0032217977684746877
***** Warning: Loss has increased *****
Loss at iteration [67]: 0.003256900459989144
***** Warning: Loss has increased *****
Loss at iteration [68]: 0.0032871363948417825
***** Warning: Loss has increased *****
Loss at iteration [69]: 0.003297774050349176
***** Warning: Loss has increased *****
Loss at iteration [70]: 0.003280468637727942
Loss at iteration [71]: 0.003233081818329903
Loss at iteration [72]: 0.003159747397736242
Loss at iteration [73]: 0.003069456639823854
Loss at iteration [74]: 0.002973926163170094
Loss at iteration [75]: 0.002884167236920765
Loss at iteration [76]: 0.002809697170715426
Loss at iteration [77]: 0.002756426106870337
Loss at iteration [78]: 0.0027261255530337566
Loss at iteration [79]: 0.0027163592034191645
Loss at iteration [80]: 0.0027212229294497656
***** Warning: Loss has increased *****
Loss at iteration [81]: 0.0027338769874714294
***** Warning: Loss has increased *****
Loss at iteration [82]: 0.002747195359526573
***** Warning: Loss has increased *****
Loss at iteration [83]: 0.002755763016342337
***** Warning: Loss has increased *****
Loss at iteration [84]: 0.00275617376821629
***** Warning: Loss has increased *****
Loss at iteration [85]: 0.0027475747743294647
Loss at iteration [86]: 0.002731143903363574
Loss at iteration [87]: 0.0027096572869157887
Loss at iteration [88]: 0.002686429906170563
Loss at iteration [89]: 0.002664905863243166
Loss at iteration [90]: 0.0026474401459114956
Loss at iteration [91]: 0.0026356808310215776
Loss at iteration [92]: 0.0026296642139232236
Loss at iteration [93]: 0.0026292361947839104
Loss at iteration [94]: 0.002631924247566252
***** Warning: Loss has increased *****
Loss at iteration [95]: 0.0026351722689507123
***** Warning: Loss has increased *****
Loss at iteration [96]: 0.0026367467885252765
***** Warning: Loss has increased *****
Loss at iteration [97]: 0.002635656459741874
Loss at iteration [98]: 0.0026318870791794545
Loss at iteration [99]: 0.0026263609438577653
Loss at iteration [100]: 0.0026197427900282555
Loss at iteration [101]: 0.002612577308564148
Loss at iteration [102]: 0.002605964756911672
Loss at iteration [103]: 0.0026006396841273378
Loss at iteration [104]: 0.0025968031923502615
Loss at iteration [105]: 0.002594374646837747
Loss at iteration [106]: 0.0025930197174760384
Loss at iteration [107]: 0.0025922993445694005
Loss at iteration [108]: 0.002591575444418351
Loss at iteration [109]: 0.0025904155474398972
Loss at iteration [110]: 0.002588558959979042
Loss at iteration [111]: 0.0025860082992471465
Loss at iteration [112]: 0.002582945713461072
Loss at iteration [113]: 0.0025796157930092272
Loss at iteration [114]: 0.002576302940879899
Loss at iteration [115]: 0.0025733411261000525
Loss at iteration [116]: 0.0025708849500759297
Loss at iteration [117]: 0.0025689629345705778
Loss at iteration [118]: 0.0025674564821648524
Loss at iteration [119]: 0.0025662851316700704
Loss at iteration [120]: 0.0025652292772813546
Loss at iteration [121]: 0.0025640830189728337
Loss at iteration [122]: 0.0025627506777410086
Loss at iteration [123]: 0.0025612347200221156
Loss at iteration [124]: 0.0025595835134049373
Loss at iteration [125]: 0.002557870078342007
Loss at iteration [126]: 0.002556200019834256
Loss at iteration [127]: 0.002554673321834423
Loss at iteration [128]: 0.002553330800956254
Loss at iteration [129]: 0.0025521080664350874
Loss at iteration [130]: 0.0025509894240978268
Loss at iteration [131]: 0.0025499616056764893
Loss at iteration [132]: 0.002548974542667239
Loss at iteration [133]: 0.0025479777096340194
Loss at iteration [134]: 0.0025469427160470237
Loss at iteration [135]: 0.002545885799662341
Loss at iteration [136]: 0.002544792891079206
Loss at iteration [137]: 0.0025436930079087037
Loss at iteration [138]: 0.002542615618952874
Loss at iteration [139]: 0.0025415924130776897
Loss at iteration [140]: 0.002540636020333429
Loss at iteration [141]: 0.0025397352312038273
Loss at iteration [142]: 0.0025388877852113166
Loss at iteration [143]: 0.002538062519770858
Loss at iteration [144]: 0.0025372300463053295
Loss at iteration [145]: 0.0025363916238873674
Loss at iteration [146]: 0.002535528264272486
Loss at iteration [147]: 0.0025346433119139465
Loss at iteration [148]: 0.0025337629478453592
Loss at iteration [149]: 0.002532903948341408
Loss at iteration [150]: 0.002532065993971436
Loss at iteration [151]: 0.0025312508481942367
Loss at iteration [152]: 0.0025304660949837096
Loss at iteration [153]: 0.0025297115621449964
Loss at iteration [154]: 0.002528990976365243
Loss at iteration [155]: 0.0025282667014501106
Loss at iteration [156]: 0.0025275412107732187
Loss at iteration [157]: 0.0025268280223163042
Loss at iteration [158]: 0.0025261133748464676
Loss at iteration [159]: 0.0025254115674390675
Loss at iteration [160]: 0.0025247291083457493
Loss at iteration [161]: 0.0025240604885471453
Loss at iteration [162]: 0.0025234147969130654
Loss at iteration [163]: 0.0025227881068624167
Loss at iteration [164]: 0.00252216907300925
Loss at iteration [165]: 0.0025215627055365972
Loss at iteration [166]: 0.002520970679640522
Loss at iteration [167]: 0.002520378818502008
Loss at iteration [168]: 0.0025197915527086297
Loss at iteration [169]: 0.0025192134223262796
Loss at iteration [170]: 0.002518633816327048
Loss at iteration [171]: 0.002518056978451544
Loss at iteration [172]: 0.0025174887319830454
Loss at iteration [173]: 0.002516926985951295
Loss at iteration [174]: 0.0025163714540509764
Loss at iteration [175]: 0.0025158283072628583
Loss at iteration [176]: 0.00251528844208318
Loss at iteration [177]: 0.00251474948969799
Loss at iteration [178]: 0.0025142180622146097
Loss at iteration [179]: 0.0025136972705917513
Loss at iteration [180]: 0.0025131844971434426
Loss at iteration [181]: 0.0025126761680602876
Loss at iteration [182]: 0.002512171485047896
Loss at iteration [183]: 0.0025116703210020606
Loss at iteration [184]: 0.0025111751661737935
Loss at iteration [185]: 0.002510680232350079
Loss at iteration [186]: 0.002510185742562414
Loss at iteration [187]: 0.002509697401628104
Loss at iteration [188]: 0.002509215390485666
Loss at iteration [189]: 0.002508739370056688
Loss at iteration [190]: 0.002508266388249679
Loss at iteration [191]: 0.0025077925975038958
Loss at iteration [192]: 0.002507328669077932
Loss at iteration [193]: 0.0025068759844724758
Loss at iteration [194]: 0.0025064241303088945
Loss at iteration [195]: 0.0025059903506613884
Loss at iteration [196]: 0.0025055590209515934
Loss at iteration [197]: 0.002505121478825543
Loss at iteration [198]: 0.0025046856478303757
Loss at iteration [199]: 0.0025042605306765347
Loss at iteration [200]: 0.00250382605943997
Loss at iteration [201]: 0.002503399348872089
Loss at iteration [202]: 0.0025029927006517383
Loss at iteration [203]: 0.002502586302063174
Loss at iteration [204]: 0.0025021856797426112
Loss at iteration [205]: 0.002501793072321162
Loss at iteration [206]: 0.0025014011070910434
Loss at iteration [207]: 0.0025010031028895054
Loss at iteration [208]: 0.002500597397725087
Loss at iteration [209]: 0.002500197879823708
Loss at iteration [210]: 0.002499809361828512
Loss at iteration [211]: 0.002499430328070632
Loss at iteration [212]: 0.002499056684834578
Loss at iteration [213]: 0.0024986908128925883
Loss at iteration [214]: 0.002498327170860571
Loss at iteration [215]: 0.0024979657920863813
Loss at iteration [216]: 0.0024976040905545266
Loss at iteration [217]: 0.002497253115286533
Loss at iteration [218]: 0.002496901745695277
Loss at iteration [219]: 0.0024965510487998496
Loss at iteration [220]: 0.002496201059309702
Loss at iteration [221]: 0.002495853134122999
Loss at iteration [222]: 0.002495504546718196
Loss at iteration [223]: 0.0024951608945545716
Loss at iteration [224]: 0.002494826536309172
Loss at iteration [225]: 0.002494494720213997
Loss at iteration [226]: 0.002494165403962253
Loss at iteration [227]: 0.002493834089856571
Loss at iteration [228]: 0.002493501063136792
Loss at iteration [229]: 0.0024931735144578123
Loss at iteration [230]: 0.002492850420987966
Loss at iteration [231]: 0.0024925306372181517
Loss at iteration [232]: 0.0024922136091380057
Loss at iteration [233]: 0.002491894635028
Loss at iteration [234]: 0.0024915788478785204
Loss at iteration [235]: 0.002491263033427767
Loss at iteration [236]: 0.002490949194240821
Loss at iteration [237]: 0.002490637444852821
Loss at iteration [238]: 0.0024903261928051544
Loss at iteration [239]: 0.0024900127960757108
Loss at iteration [240]: 0.0024896965772295927
Loss at iteration [241]: 0.002489374622506985
Loss at iteration [242]: 0.0024890587190206743
Loss at iteration [243]: 0.0024887435439399975
Loss at iteration [244]: 0.0024884299254530535
Loss at iteration [245]: 0.0024881237485172873
Loss at iteration [246]: 0.0024878104408058654
Loss at iteration [247]: 0.0024874946802490453
Loss at iteration [248]: 0.002487181877685784
Loss at iteration [249]: 0.0024868715526139297
Loss at iteration [250]: 0.0024865657667638924
Loss at iteration [251]: 0.0024862604040072835
Loss at iteration [252]: 0.0024859584972201873
Loss at iteration [253]: 0.002485656542282979
Loss at iteration [254]: 0.002485357586062211
Loss at iteration [255]: 0.002485060012954024
Loss at iteration [256]: 0.002484759211315582
Loss at iteration [257]: 0.0024844656654953914
Loss at iteration [258]: 0.002484171747252331
Loss at iteration [259]: 0.002483879077300597
Loss at iteration [260]: 0.0024835892200782248
Loss at iteration [261]: 0.0024832994601689585
Loss at iteration [262]: 0.002483009693148377
Loss at iteration [263]: 0.002482719363267657
Loss at iteration [264]: 0.002482430298119106
Loss at iteration [265]: 0.0024821422787851557
Loss at iteration [266]: 0.0024818515119839654
Loss at iteration [267]: 0.0024815490870685906
Loss at iteration [268]: 0.00248124660528913
Loss at iteration [269]: 0.002480939212283108
Loss at iteration [270]: 0.002480631831617199
Loss at iteration [271]: 0.0024803350787795993
Loss at iteration [272]: 0.00248004246754551
Loss at iteration [273]: 0.0024797537384013708
Loss at iteration [274]: 0.0024794660081923726
Loss at iteration [275]: 0.002479186028915913
Loss at iteration [276]: 0.0024789050959198797
Loss at iteration [277]: 0.002478638008919128
Loss at iteration [278]: 0.0024783838090010587
Loss at iteration [279]: 0.0024781334901820564
Loss at iteration [280]: 0.002477878928854801
Loss at iteration [281]: 0.0024776298706129392
Loss at iteration [282]: 0.0024773885346985325
Loss at iteration [283]: 0.0024771454921924366
Loss at iteration [284]: 0.002476901945041467
Loss at iteration [285]: 0.002476657232620533
Loss at iteration [286]: 0.0024764119289948728
Loss at iteration [287]: 0.002476166322287116
Loss at iteration [288]: 0.002475919833847528
Loss at iteration [289]: 0.0024756751810516946
Loss at iteration [290]: 0.0024754296593022056
Loss at iteration [291]: 0.0024751872235563425
Loss at iteration [292]: 0.002474948921574978
Loss at iteration [293]: 0.0024747053668026744
Loss at iteration [294]: 0.0024744652918627734
Loss at iteration [295]: 0.002474230529941999
Loss at iteration [296]: 0.002473991438804049
Loss at iteration [297]: 0.002473752040612802
Loss at iteration [298]: 0.0024735166848677556
Loss at iteration [299]: 0.002473286105558721
Loss at iteration [300]: 0.002473057186930834
Loss at iteration [301]: 0.0024728270228904304
Loss at iteration [302]: 0.0024725986138780342
Loss at iteration [303]: 0.002472367910511848
Loss at iteration [304]: 0.0024721309655134833
Loss at iteration [305]: 0.0024718980890530034
Loss at iteration [306]: 0.0024716592054972427
Loss at iteration [307]: 0.0024714300213906698
Loss at iteration [308]: 0.002471195813431745
Loss at iteration [309]: 0.0024709635088094066
Loss at iteration [310]: 0.0024707321001930242
Loss at iteration [311]: 0.002470501541246972
Loss at iteration [312]: 0.002470267570720016
Loss at iteration [313]: 0.0024700305644525613
Loss at iteration [314]: 0.0024697934573257177
Loss at iteration [315]: 0.0024695543784790854
Loss at iteration [316]: 0.0024693153617993844
Loss at iteration [317]: 0.002469076926813504
Loss at iteration [318]: 0.0024688367433517203
Loss at iteration [319]: 0.0024686004483013956
Loss at iteration [320]: 0.0024683624532652686
Loss at iteration [321]: 0.0024681285987568475
Loss at iteration [322]: 0.002467886654942415
Loss at iteration [323]: 0.002467648822966301
Loss at iteration [324]: 0.002467423626129798
Loss at iteration [325]: 0.00246720406755544
Loss at iteration [326]: 0.0024669904086141755
Loss at iteration [327]: 0.002466768624878439
Loss at iteration [328]: 0.0024665523397390016
Loss at iteration [329]: 0.0024663361323163115
Loss at iteration [330]: 0.002466120671756596
Loss at iteration [331]: 0.002465902589220776
Loss at iteration [332]: 0.0024656881531360747
Loss at iteration [333]: 0.0024654748129123863
Loss at iteration [334]: 0.0024652620705740507
Loss at iteration [335]: 0.0024650503593112547
Loss at iteration [336]: 0.0024648394987094725
Loss at iteration [337]: 0.0024646279965125947
Loss at iteration [338]: 0.00246441748996805
Loss at iteration [339]: 0.0024642054132169066
Loss at iteration [340]: 0.0024639942243796322
Loss at iteration [341]: 0.0024637874119893283
Loss at iteration [342]: 0.0024635820431457064
Loss at iteration [343]: 0.0024633742972741957
Loss at iteration [344]: 0.0024631693350943666
Loss at iteration [345]: 0.002462965036605744
Loss at iteration [346]: 0.0024627685583734917
Loss at iteration [347]: 0.00246255996867498
Loss at iteration [348]: 0.002462357053216366
Loss at iteration [349]: 0.0024621559376515035
Loss at iteration [350]: 0.0024619526836274996
Loss at iteration [351]: 0.002461752010874551
Loss at iteration [352]: 0.002461551432488535
Loss at iteration [353]: 0.0024613513546757173
Loss at iteration [354]: 0.0024611502590602386
Loss at iteration [355]: 0.0024609518750139266
Loss at iteration [356]: 0.0024607569564994262
Loss at iteration [357]: 0.002460560520600527
Loss at iteration [358]: 0.0024603644360482466
Loss at iteration [359]: 0.002460168584793937
Loss at iteration [360]: 0.0024599731757514666
Loss at iteration [361]: 0.0024597787968710905
Loss at iteration [362]: 0.002459582780167476
Loss at iteration [363]: 0.002459389326379933
Loss at iteration [364]: 0.002459197032425794
Loss at iteration [365]: 0.0024590034030597925
Loss at iteration [366]: 0.0024588102364323555
Loss at iteration [367]: 0.0024586172510092917
Loss at iteration [368]: 0.0024584270196678416
Loss at iteration [369]: 0.0024582346992197417
Loss at iteration [370]: 0.0024580406039850706
Loss at iteration [371]: 0.002457847249250182
Loss at iteration [372]: 0.0024576549854263335
Loss at iteration [373]: 0.00245746484874407
Loss at iteration [374]: 0.0024572763866441056
Loss at iteration [375]: 0.0024570839726096422
Loss at iteration [376]: 0.0024568906404700687
Loss at iteration [377]: 0.002456698733963567
Loss at iteration [378]: 0.002456506349550994
Loss at iteration [379]: 0.002456315071984668
Loss at iteration [380]: 0.002456122008903687
Loss at iteration [381]: 0.002455929912708914
Loss at iteration [382]: 0.0024557412431166956
Loss at iteration [383]: 0.002455553759448952
Loss at iteration [384]: 0.002455364371304407
Loss at iteration [385]: 0.002455175970141006
Loss at iteration [386]: 0.0024549884598123596
Loss at iteration [387]: 0.002454801852974946
Loss at iteration [388]: 0.002454613924541977
Loss at iteration [389]: 0.0024544275119238856
Loss at iteration [390]: 0.002454242586893789
Loss at iteration [391]: 0.0024540566309305604
Loss at iteration [392]: 0.002453871745054237
Loss at iteration [393]: 0.002453687320871632
Loss at iteration [394]: 0.0024535000519268677
Loss at iteration [395]: 0.0024533122739747414
Loss at iteration [396]: 0.0024531262830902695
Loss at iteration [397]: 0.0024529398522793147
Loss at iteration [398]: 0.0024527550848368915
Loss at iteration [399]: 0.0024525681104868424
Loss at iteration [400]: 0.002452382023024541
Loss at iteration [401]: 0.00245219519655286
Loss at iteration [402]: 0.0024520123170844754
Loss at iteration [403]: 0.0024518269050053196
Loss at iteration [404]: 0.0024516379122452784
Loss at iteration [405]: 0.002451450578078709
Loss at iteration [406]: 0.002451260821785093
Loss at iteration [407]: 0.0024510731496575357
Loss at iteration [408]: 0.0024508872710532286
Loss at iteration [409]: 0.0024507020653764216
Loss at iteration [410]: 0.0024505132657854384
Loss at iteration [411]: 0.0024503249793063134
Loss at iteration [412]: 0.002450136076880595
Loss at iteration [413]: 0.0024499479109969043
Loss at iteration [414]: 0.002449759022879198
Loss at iteration [415]: 0.0024495701841600777
Loss at iteration [416]: 0.0024493852546606577
Loss at iteration [417]: 0.0024491979010168832
Loss at iteration [418]: 0.0024490075146616492
Loss at iteration [419]: 0.0024488215714179177
Loss at iteration [420]: 0.0024486353462208265
Loss at iteration [421]: 0.002448448841850024
Loss at iteration [422]: 0.002448258873966471
Loss at iteration [423]: 0.0024480756942169644
Loss at iteration [424]: 0.002447891349911389
Loss at iteration [425]: 0.0024477078999247935
Loss at iteration [426]: 0.0024475181811142665
Loss at iteration [427]: 0.002447330180263173
Loss at iteration [428]: 0.0024471439728095666
Loss at iteration [429]: 0.0024469597080954155
Loss at iteration [430]: 0.0024467752911211545
Loss at iteration [431]: 0.0024465925277475467
Loss at iteration [432]: 0.0024464098617692056
Loss at iteration [433]: 0.0024462312208143808
Loss at iteration [434]: 0.002446046048994747
Loss at iteration [435]: 0.002445861305678755
Loss at iteration [436]: 0.0024456794899902125
Loss at iteration [437]: 0.0024454956580432854
Loss at iteration [438]: 0.0024453110426531314
Loss at iteration [439]: 0.0024451235694011703
Loss at iteration [440]: 0.0024449365419482068
Loss at iteration [441]: 0.0024447541074649787
Loss at iteration [442]: 0.002444572049466786
Loss at iteration [443]: 0.002444387365753986
Loss at iteration [444]: 0.0024442050271413613
Loss at iteration [445]: 0.0024440241256557015
Loss at iteration [446]: 0.0024438351364679304
Loss at iteration [447]: 0.002443652326578432
Loss at iteration [448]: 0.0024434756213915125
Loss at iteration [449]: 0.0024432978774674525
Loss at iteration [450]: 0.0024431179738362576
Loss at iteration [451]: 0.002442935935860081
Loss at iteration [452]: 0.0024427537457966733
Loss at iteration [453]: 0.0024425708608638995
Loss at iteration [454]: 0.002442391183761235
Loss at iteration [455]: 0.002442211207551544
Loss at iteration [456]: 0.0024420323730306322
Loss at iteration [457]: 0.0024418488555216075
Loss at iteration [458]: 0.0024416686881572076
Loss at iteration [459]: 0.0024414848196909946
Loss at iteration [460]: 0.0024413037117370193
Loss at iteration [461]: 0.002441122880702698
Loss at iteration [462]: 0.002440941975296489
Loss at iteration [463]: 0.0024407615615331525
Loss at iteration [464]: 0.002440574839059898
Loss at iteration [465]: 0.0024403885545618105
Loss at iteration [466]: 0.0024402092210197754
Loss at iteration [467]: 0.002440027109463195
Loss at iteration [468]: 0.0024398452528899646
Loss at iteration [469]: 0.002439659004527888
Loss at iteration [470]: 0.0024394765808335543
Loss at iteration [471]: 0.002439294343258436
Loss at iteration [472]: 0.00243911780046494
Loss at iteration [473]: 0.00243894035060513
Loss at iteration [474]: 0.002438760434869627
Loss at iteration [475]: 0.0024385792685920425
Loss at iteration [476]: 0.0024383977173964883
Loss at iteration [477]: 0.0024382226010678464
Loss at iteration [478]: 0.0024380413315152113
Loss at iteration [479]: 0.0024378553778761293
Loss at iteration [480]: 0.0024376816971547755
Loss at iteration [481]: 0.002437507639173571
Loss at iteration [482]: 0.0024373302006173336
Loss at iteration [483]: 0.0024371542093168464
Loss at iteration [484]: 0.0024369725431796475
Loss at iteration [485]: 0.0024367898954432
Loss at iteration [486]: 0.0024366117970594185
Loss at iteration [487]: 0.0024364353509260064
Loss at iteration [488]: 0.0024362542350868824
Loss at iteration [489]: 0.0024360764845559107
Loss at iteration [490]: 0.0024358956544909085
Loss at iteration [491]: 0.002435716568521126
Loss at iteration [492]: 0.0024355372507386497
Loss at iteration [493]: 0.0024353516481217196
Loss at iteration [494]: 0.0024351751137737894
Loss at iteration [495]: 0.0024349998880054085
Loss at iteration [496]: 0.0024348127375789412
Loss at iteration [497]: 0.0024346306423938665
Loss at iteration [498]: 0.0024344511865318614
Loss at iteration [499]: 0.0024342744527063847
Loss at iteration [500]: 0.0024340946615069503
Loss at iteration [501]: 0.0024339144685671054
Loss at iteration [502]: 0.002433735733644014
Loss at iteration [503]: 0.0024335560805685374
Loss at iteration [504]: 0.0024333723754741935
Loss at iteration [505]: 0.0024331929694668196
Loss at iteration [506]: 0.0024330072751808174
Loss at iteration [507]: 0.002432828677094714
Loss at iteration [508]: 0.002432649180934142
Loss at iteration [509]: 0.002432468140472831
Loss at iteration [510]: 0.0024322866501163914
Loss at iteration [511]: 0.002432106812776557
Loss at iteration [512]: 0.002431928104956967
Loss at iteration [513]: 0.002431750077728693
Loss at iteration [514]: 0.0024315714633304846
Loss at iteration [515]: 0.002431391154682737
Loss at iteration [516]: 0.0024312088120344156
Loss at iteration [517]: 0.002431031382047697
Loss at iteration [518]: 0.0024308480347921386
Loss at iteration [519]: 0.002430663148121627
Loss at iteration [520]: 0.002430483922774949
Loss at iteration [521]: 0.002430305512543654
Loss at iteration [522]: 0.0024301289599346614
Loss at iteration [523]: 0.0024299476787517493
Loss at iteration [524]: 0.0024297729093469443
Loss at iteration [525]: 0.0024295957224605967
Loss at iteration [526]: 0.0024294162982278827
Loss at iteration [527]: 0.002429238284875587
Loss at iteration [528]: 0.002429050929814496
Loss at iteration [529]: 0.0024288854632465353
Loss at iteration [530]: 0.002428710342226452
Loss at iteration [531]: 0.002428523707060762
Loss at iteration [532]: 0.002428338495857718
Loss at iteration [533]: 0.00242815849255582
Loss at iteration [534]: 0.002427983101874496
Loss at iteration [535]: 0.002427805288931952
Loss at iteration [536]: 0.00242762271593311
Loss at iteration [537]: 0.0024274470805265513
Loss at iteration [538]: 0.0024272720599434576
Loss at iteration [539]: 0.0024270887933470655
Loss at iteration [540]: 0.0024269122542711946
Loss at iteration [541]: 0.0024267345606256807
Loss at iteration [542]: 0.0024265547630324407
Loss at iteration [543]: 0.0024263631698970877
Loss at iteration [544]: 0.0024261653847997607
Loss at iteration [545]: 0.0024259755558533615
Loss at iteration [546]: 0.0024257807530053454
Loss at iteration [547]: 0.002425581107489438
Loss at iteration [548]: 0.00242537818658799
Loss at iteration [549]: 0.0024251810187582534
Loss at iteration [550]: 0.0024249702944402437
Loss at iteration [551]: 0.002424723561685891
Loss at iteration [552]: 0.0024244663113322036
Loss at iteration [553]: 0.002424212667357316
Loss at iteration [554]: 0.0024239542104776087
Loss at iteration [555]: 0.0024236922978330975
Loss at iteration [556]: 0.0024234050489673944
Loss at iteration [557]: 0.0024231303025699846
Loss at iteration [558]: 0.002422866114801763
Loss at iteration [559]: 0.002422614422589852
Loss at iteration [560]: 0.0024223520484912575
Loss at iteration [561]: 0.0024220894403155888
Loss at iteration [562]: 0.0024218126232833878
Loss at iteration [563]: 0.0024215220899062506
Loss at iteration [564]: 0.002421232350515273
Loss at iteration [565]: 0.0024209625433628417
Loss at iteration [566]: 0.002420668466180837
Loss at iteration [567]: 0.0024202743617630896
Loss at iteration [568]: 0.002419836427267177
Loss at iteration [569]: 0.0024195644590813145
Loss at iteration [570]: 0.002419286878769505
Loss at iteration [571]: 0.002419033244812444
Loss at iteration [572]: 0.0024186739306409437
Loss at iteration [573]: 0.0024182781394848176
Loss at iteration [574]: 0.002417987926954682
Loss at iteration [575]: 0.0024177009726288365
Loss at iteration [576]: 0.0024174028764435006
Loss at iteration [577]: 0.0024171009976810967
Loss at iteration [578]: 0.0024168010500285114
Loss at iteration [579]: 0.002416509428206435
Loss at iteration [580]: 0.002416208556140501
Loss at iteration [581]: 0.0024159201051830452
Loss at iteration [582]: 0.002415653139740209
Loss at iteration [583]: 0.0024153474530842814
Loss at iteration [584]: 0.002415042591597144
Loss at iteration [585]: 0.0024147339242331064
Loss at iteration [586]: 0.0024144071552545474
Loss at iteration [587]: 0.0024142512757658555
Loss at iteration [588]: 0.0024140244880786034
Loss at iteration [589]: 0.002413731842594534
Loss at iteration [590]: 0.0024135156258323553
Loss at iteration [591]: 0.0024133110518354444
Loss at iteration [592]: 0.0024131017696116513
Loss at iteration [593]: 0.002412881273528295
Loss at iteration [594]: 0.002412656725803014
Loss at iteration [595]: 0.002412432292173201
Loss at iteration [596]: 0.0024122054083172643
Loss at iteration [597]: 0.0024119717485961635
Loss at iteration [598]: 0.0024117372742652773
Loss at iteration [599]: 0.002411498502171327
Loss at iteration [600]: 0.002411256136249187
Loss at iteration [601]: 0.002411011456303218
Loss at iteration [602]: 0.002410751383697718
Loss at iteration [603]: 0.0024104941293439696
Loss at iteration [604]: 0.0024102705452268666
Loss at iteration [605]: 0.002410024259424648
Loss at iteration [606]: 0.0024097646841962227
Loss at iteration [607]: 0.002409501141604322
Loss at iteration [608]: 0.002409278275060109
Loss at iteration [609]: 0.002409023987079124
Loss at iteration [610]: 0.0024087609767147453
Loss at iteration [611]: 0.00240852937782454
Loss at iteration [612]: 0.0024082906051104483
Loss at iteration [613]: 0.002408077647825157
Loss at iteration [614]: 0.002407836145173665
Loss at iteration [615]: 0.002407590117643597
Loss at iteration [616]: 0.0024073755123546316
Loss at iteration [617]: 0.0024071445500455065
Loss at iteration [618]: 0.002406885335454763
Loss at iteration [619]: 0.0024066550701881974
Loss at iteration [620]: 0.002406420988726853
Loss at iteration [621]: 0.002406179895513992
Loss at iteration [622]: 0.0024059465632422544
Loss at iteration [623]: 0.002405708488577933
Loss at iteration [624]: 0.002405466513580021
Loss at iteration [625]: 0.002405229343355159
Loss at iteration [626]: 0.0024049872524740962
Loss at iteration [627]: 0.002404744621623599
Loss at iteration [628]: 0.0024045200933113
Loss at iteration [629]: 0.0024042733930121954
Loss at iteration [630]: 0.0024040132437550548
Loss at iteration [631]: 0.0024037903086864447
Loss at iteration [632]: 0.002403549697955828
Loss at iteration [633]: 0.0024032781177848084
Loss at iteration [634]: 0.0024030675932085626
Loss at iteration [635]: 0.0024028254198253597
Loss at iteration [636]: 0.0024025687734362403
Loss at iteration [637]: 0.0024023268240566426
Loss at iteration [638]: 0.0024020945911310313
Loss at iteration [639]: 0.0024018324483018137
Loss at iteration [640]: 0.0024015771565884074
Loss at iteration [641]: 0.0024013464530678497
Loss at iteration [642]: 0.0024010918240940323
Loss at iteration [643]: 0.002400842053478267
Loss at iteration [644]: 0.002400592011697086
Loss at iteration [645]: 0.0024003524598731297
Loss at iteration [646]: 0.002400103865232897
Loss at iteration [647]: 0.0023998582847210514
Loss at iteration [648]: 0.0023996093844676023
Loss at iteration [649]: 0.0023993619590812073
Loss at iteration [650]: 0.0023991130518147205
Loss at iteration [651]: 0.002398874348062866
Loss at iteration [652]: 0.0023986376915299755
Loss at iteration [653]: 0.0023984052571418063
Loss at iteration [654]: 0.002398201726211132
Loss at iteration [655]: 0.0023979168678326564
Loss at iteration [656]: 0.002397667768486801
Loss at iteration [657]: 0.0023974027668907296
Loss at iteration [658]: 0.002397161566221124
Loss at iteration [659]: 0.002396913517483702
Loss at iteration [660]: 0.0023966713850164917
Loss at iteration [661]: 0.0023964230285668654
Loss at iteration [662]: 0.002396176266168902
Loss at iteration [663]: 0.002395925479732835
Loss at iteration [664]: 0.002395687097281523
Loss at iteration [665]: 0.002395437239486294
Loss at iteration [666]: 0.002395198050421634
Loss at iteration [667]: 0.00239494545563952
Loss at iteration [668]: 0.0023947051525056023
Loss at iteration [669]: 0.002394469225124974
Loss at iteration [670]: 0.0023942237312607486
Loss at iteration [671]: 0.0023939818814025715
Loss at iteration [672]: 0.00239374015160364
Loss at iteration [673]: 0.0023934792106717027
Loss at iteration [674]: 0.00239324016502125
Loss at iteration [675]: 0.002392971241605524
Loss at iteration [676]: 0.0023927381323398107
Loss at iteration [677]: 0.002392473169137328
Loss at iteration [678]: 0.002392225890552156
Loss at iteration [679]: 0.002391993823081373
Loss at iteration [680]: 0.002391742022496623
Loss at iteration [681]: 0.0023914893127340497
Loss at iteration [682]: 0.0023912561261072766
Loss at iteration [683]: 0.0023909994183126263
Loss at iteration [684]: 0.002390760055440408
Loss at iteration [685]: 0.002390488203809935
Loss at iteration [686]: 0.002390279707149134
Loss at iteration [687]: 0.002389985332238097
Loss at iteration [688]: 0.002389745080208651
Loss at iteration [689]: 0.002389533710702416
Loss at iteration [690]: 0.002389221337007866
Loss at iteration [691]: 0.002389033877746142
Loss at iteration [692]: 0.002388769551292378
Loss at iteration [693]: 0.0023884580589295613
Loss at iteration [694]: 0.00238818332726585
Loss at iteration [695]: 0.0023877793611287524
Loss at iteration [696]: 0.002387638022956139
Loss at iteration [697]: 0.002387338649510603
Loss at iteration [698]: 0.002387106247137217
Loss at iteration [699]: 0.0023869052033563026
Loss at iteration [700]: 0.0023864965245300827
Loss at iteration [701]: 0.0023864267823229587
Loss at iteration [702]: 0.002386237923107667
Loss at iteration [703]: 0.00238584865880708
Loss at iteration [704]: 0.002385604649465581
Loss at iteration [705]: 0.0023854271896527565
Loss at iteration [706]: 0.002385040333704023
Loss at iteration [707]: 0.0023848779380101362
Loss at iteration [708]: 0.0023846685256113213
Loss at iteration [709]: 0.00238428512970998
Loss at iteration [710]: 0.002384126950440182
Loss at iteration [711]: 0.0023839429657915496
Loss at iteration [712]: 0.002383507938874586
Loss at iteration [713]: 0.00238334064868905
Loss at iteration [714]: 0.002383121641371878
Loss at iteration [715]: 0.0023827398747190255
Loss at iteration [716]: 0.002382607787791303
Loss at iteration [717]: 0.0023823423145435093
Loss at iteration [718]: 0.002381999949433775
Loss at iteration [719]: 0.00238178074766551
Loss at iteration [720]: 0.0023815009092268886
Loss at iteration [721]: 0.0023813042196453344
Loss at iteration [722]: 0.002381004078000425
Loss at iteration [723]: 0.0023808229776636908
Loss at iteration [724]: 0.0023805176040738217
Loss at iteration [725]: 0.0023803128810755032
Loss at iteration [726]: 0.0023800681913246653
Loss at iteration [727]: 0.0023797716620422922
Loss at iteration [728]: 0.002379581720178326
Loss at iteration [729]: 0.002379264964628081
Loss at iteration [730]: 0.0023790631367672536
Loss at iteration [731]: 0.0023787621999419657
Loss at iteration [732]: 0.0023785993764945026
Loss at iteration [733]: 0.002378293886397937
Loss at iteration [734]: 0.002378093117058848
Loss at iteration [735]: 0.00237785167446445
Loss at iteration [736]: 0.002377565889239039
Loss at iteration [737]: 0.002377406662919915
Loss at iteration [738]: 0.002377084968682027
Loss at iteration [739]: 0.002376851994367868
Loss at iteration [740]: 0.0023766026613163196
Loss at iteration [741]: 0.0023763438955746005
Loss at iteration [742]: 0.0023761691111010173
Loss at iteration [743]: 0.0023758553966966613
Loss at iteration [744]: 0.0023756191611781128
Loss at iteration [745]: 0.002375411158293428
Loss at iteration [746]: 0.0023751382119713395
Loss at iteration [747]: 0.0023749033327210875
Loss at iteration [748]: 0.002374688989260802
Loss at iteration [749]: 0.002374407028724042
Loss at iteration [750]: 0.002374192709794554
Loss at iteration [751]: 0.0023739378047954126
Loss at iteration [752]: 0.0023736664630798466
Loss at iteration [753]: 0.0023734550054933614
Loss at iteration [754]: 0.0023731954933610367
Loss at iteration [755]: 0.002372946426226193
Loss at iteration [756]: 0.0023727056128728555
Loss at iteration [757]: 0.0023724645748821813
Loss at iteration [758]: 0.002372220305653932
Loss at iteration [759]: 0.0023719788309601374
Loss at iteration [760]: 0.0023717348763286563
Loss at iteration [761]: 0.002371499455390982
Loss at iteration [762]: 0.0023712609079476797
Loss at iteration [763]: 0.0023710326176885144
Loss at iteration [764]: 0.0023708010166666295
Loss at iteration [765]: 0.002370547693317009
Loss at iteration [766]: 0.0023702996209950724
Loss at iteration [767]: 0.0023700653178709507
Loss at iteration [768]: 0.0023698075262549804
Loss at iteration [769]: 0.002369564235205382
Loss at iteration [770]: 0.0023693304586810153
Loss at iteration [771]: 0.0023690982720821164
Loss at iteration [772]: 0.0023688592320485068
Loss at iteration [773]: 0.0023686101605679315
Loss at iteration [774]: 0.0023683859606739807
Loss at iteration [775]: 0.002368142682023992
Loss at iteration [776]: 0.002367892601734802
Loss at iteration [777]: 0.0023676828901257607
Loss at iteration [778]: 0.0023674211848934462
Loss at iteration [779]: 0.0023671822964679585
Loss at iteration [780]: 0.002366944498417343
Loss at iteration [781]: 0.002366698129354522
Loss at iteration [782]: 0.0023664689780961046
Loss at iteration [783]: 0.0023662056285555055
Loss at iteration [784]: 0.002365956861108629
Loss at iteration [785]: 0.0023657213440448637
Loss at iteration [786]: 0.002365520827526124
Loss at iteration [787]: 0.002365263662176261
Loss at iteration [788]: 0.002365027696930511
Loss at iteration [789]: 0.002364790064048715
Loss at iteration [790]: 0.002364537972870418
Loss at iteration [791]: 0.0023643223740639767
Loss at iteration [792]: 0.002364061811016001
Loss at iteration [793]: 0.002363838403638538
Loss at iteration [794]: 0.0023636011982959447
Loss at iteration [795]: 0.0023633595251110013
Loss at iteration [796]: 0.002363128760281973
Loss at iteration [797]: 0.0023629038629457417
Loss at iteration [798]: 0.00236267693078091
Loss at iteration [799]: 0.002362435081251759
Loss at iteration [800]: 0.0023622107190739757
Loss at iteration [801]: 0.0023620056628975237
Loss at iteration [802]: 0.002361804752927514
Loss at iteration [803]: 0.0023615577417392545
Loss at iteration [804]: 0.0023612876778541987
Loss at iteration [805]: 0.002361146879317284
Loss at iteration [806]: 0.0023608569033480432
Loss at iteration [807]: 0.002360606264720196
Loss at iteration [808]: 0.002360421308092516
Loss at iteration [809]: 0.00236022085179747
Loss at iteration [810]: 0.002359968064295235
Loss at iteration [811]: 0.0023596745815518713
Loss at iteration [812]: 0.002359439090058241
Loss at iteration [813]: 0.002359234591272308
Loss at iteration [814]: 0.002358979761831713
Loss at iteration [815]: 0.0023587358058882453
Loss at iteration [816]: 0.0023584855480073396
Loss at iteration [817]: 0.0023582721401860965
Loss at iteration [818]: 0.002358034957602737
Loss at iteration [819]: 0.0023577998607130163
Loss at iteration [820]: 0.002357566706694042
Loss at iteration [821]: 0.0023573148614181205
Loss at iteration [822]: 0.0023571170598187625
Loss at iteration [823]: 0.002356895309427129
Loss at iteration [824]: 0.0023566330808584396
Loss at iteration [825]: 0.002356396054534895
Loss at iteration [826]: 0.002356186020951894
Loss at iteration [827]: 0.0023559289422129454
Loss at iteration [828]: 0.0023556721272271915
Loss at iteration [829]: 0.002355473153567432
Loss at iteration [830]: 0.002355250605340666
Loss at iteration [831]: 0.002354997029373888
Loss at iteration [832]: 0.0023547600705317757
Loss at iteration [833]: 0.0023545237172269123
Loss at iteration [834]: 0.002354341694925943
Loss at iteration [835]: 0.002354089513771728
Loss at iteration [836]: 0.0023538446919506247
Loss at iteration [837]: 0.0023536338309141096
Loss at iteration [838]: 0.00235340445668944
Loss at iteration [839]: 0.0023531412082287897
Loss at iteration [840]: 0.0023529418439590315
Loss at iteration [841]: 0.002352707826365836
Loss at iteration [842]: 0.0023524603829744545
Loss at iteration [843]: 0.002352237381962949
Loss at iteration [844]: 0.002352001174123167
Loss at iteration [845]: 0.0023517879926694854
Loss at iteration [846]: 0.002351571385288626
Loss at iteration [847]: 0.002351345998499339
Loss at iteration [848]: 0.002351131457951458
Loss at iteration [849]: 0.0023508758851402208
Loss at iteration [850]: 0.0023506552356930086
Loss at iteration [851]: 0.002350438441425871
Loss at iteration [852]: 0.002350216321503588
Loss at iteration [853]: 0.002349992498955612
Loss at iteration [854]: 0.0023497722868477664
Loss at iteration [855]: 0.0023495597909387565
Loss at iteration [856]: 0.002349325134794257
Loss at iteration [857]: 0.0023491053119007034
Loss at iteration [858]: 0.0023488725227701064
Loss at iteration [859]: 0.0023486446551879607
Loss at iteration [860]: 0.002348424671207001
Loss at iteration [861]: 0.0023481993366982045
Loss at iteration [862]: 0.0023479654162533816
Loss at iteration [863]: 0.0023477765484936734
Loss at iteration [864]: 0.002347533295700729
Loss at iteration [865]: 0.002347327622356839
Loss at iteration [866]: 0.0023471052440708186
Loss at iteration [867]: 0.0023468807007650263
Loss at iteration [868]: 0.0023466600835585967
Loss at iteration [869]: 0.002346445294300225
Loss at iteration [870]: 0.0023462428189819492
Loss at iteration [871]: 0.0023460032044160775
Loss at iteration [872]: 0.0023458533626424046
Loss at iteration [873]: 0.0023455989900987217
Loss at iteration [874]: 0.002345388774488698
Loss at iteration [875]: 0.0023451876854337687
Loss at iteration [876]: 0.0023449111723983264
Loss at iteration [877]: 0.002344780852985866
Loss at iteration [878]: 0.002344517002769207
Loss at iteration [879]: 0.002344326799391343
Loss at iteration [880]: 0.002344120716881947
Loss at iteration [881]: 0.00234386851381469
Loss at iteration [882]: 0.002343713655529293
Loss at iteration [883]: 0.002343484803575711
Loss at iteration [884]: 0.002343240253946034
Loss at iteration [885]: 0.0023430763004177076
Loss at iteration [886]: 0.0023428577231130797
Loss at iteration [887]: 0.0023426105491654975
Loss at iteration [888]: 0.0023424254248306593
Loss at iteration [889]: 0.0023421996815475455
Loss at iteration [890]: 0.0023419825200115974
Loss at iteration [891]: 0.002341807159946722
Loss at iteration [892]: 0.002341545277787314
Loss at iteration [893]: 0.0023414197347665436
Loss at iteration [894]: 0.0023411810488272672
Loss at iteration [895]: 0.0023409760034666716
Loss at iteration [896]: 0.0023408173197292633
Loss at iteration [897]: 0.0023405991515662835
Loss at iteration [898]: 0.002340316560304513
Loss at iteration [899]: 0.0023401908515680732
Loss at iteration [900]: 0.002339980039205042
Loss at iteration [901]: 0.0023396922466715066
Loss at iteration [902]: 0.00233952876190623
Loss at iteration [903]: 0.0023393029871176044
Loss at iteration [904]: 0.002339109044370531
Loss at iteration [905]: 0.002338919084043486
Loss at iteration [906]: 0.0023386863206956133
Loss at iteration [907]: 0.002338468807748561
Loss at iteration [908]: 0.002338274734682969
Loss at iteration [909]: 0.0023380287531041327
Loss at iteration [910]: 0.0023378492721518505
Loss at iteration [911]: 0.002337646249172509
Loss at iteration [912]: 0.002337452114005412
Loss at iteration [913]: 0.002337244731180538
Loss at iteration [914]: 0.002337021255285043
Loss at iteration [915]: 0.0023368128725653327
Loss at iteration [916]: 0.0023366451537469137
Loss at iteration [917]: 0.0023364254002361242
Loss at iteration [918]: 0.0023362306793921436
Loss at iteration [919]: 0.0023360283789123667
Loss at iteration [920]: 0.002335860951637249
Loss at iteration [921]: 0.0023356146680942242
Loss at iteration [922]: 0.0023354280769891096
Loss at iteration [923]: 0.002335210644455303
Loss at iteration [924]: 0.00233501077679923
Loss at iteration [925]: 0.002334814161817787
Loss at iteration [926]: 0.002334668736502628
Loss at iteration [927]: 0.002334424182957231
Loss at iteration [928]: 0.0023342305357838223
Loss at iteration [929]: 0.0023340028350363755
Loss at iteration [930]: 0.002333869358078472
Loss at iteration [931]: 0.002333620219072882
Loss at iteration [932]: 0.002333438114461144
Loss at iteration [933]: 0.0023332251278597887
Loss at iteration [934]: 0.002333042116040503
Loss at iteration [935]: 0.002332831723945412
Loss at iteration [936]: 0.002332640640897441
Loss at iteration [937]: 0.0023324247308237045
Loss at iteration [938]: 0.002332239272494313
Loss at iteration [939]: 0.002332031061951181
Loss at iteration [940]: 0.002331821225040934
Loss at iteration [941]: 0.002331681739300064
Loss at iteration [942]: 0.002331449949712501
Loss at iteration [943]: 0.002331274578374405
Loss at iteration [944]: 0.002331067769287344
Loss at iteration [945]: 0.0023308613754255605
Loss at iteration [946]: 0.0023306681511533383
Loss at iteration [947]: 0.0023304758360947328
Loss at iteration [948]: 0.0023302900007260183
Loss at iteration [949]: 0.0023300744388697107
Loss at iteration [950]: 0.0023299518153188604
Loss at iteration [951]: 0.0023296708931869834
Loss at iteration [952]: 0.002329504081760972
Loss at iteration [953]: 0.00232929196737286
Loss at iteration [954]: 0.002329142280747118
Loss at iteration [955]: 0.0023289367626978218
Loss at iteration [956]: 0.002328770199689557
Loss at iteration [957]: 0.002328591329468821
Loss at iteration [958]: 0.002328366088468299
Loss at iteration [959]: 0.0023281623951762523
Loss at iteration [960]: 0.0023279475668436717
Loss at iteration [961]: 0.002327758811908359
Loss at iteration [962]: 0.0023275498012555584
Loss at iteration [963]: 0.00232730061001983
Loss at iteration [964]: 0.002327073224789519
Loss at iteration [965]: 0.0023269077929963084
Loss at iteration [966]: 0.002326755515701425
Loss at iteration [967]: 0.0023265121375058164
Loss at iteration [968]: 0.002326342916021212
Loss at iteration [969]: 0.002326091472222459
Loss at iteration [970]: 0.0023259060998070333
Loss at iteration [971]: 0.0023256843781866276
Loss at iteration [972]: 0.002325455310457263
Loss at iteration [973]: 0.0023253707556606406
Loss at iteration [974]: 0.0023251081699459763
Loss at iteration [975]: 0.002324929943403691
Loss at iteration [976]: 0.002324766095601005
Loss at iteration [977]: 0.00232452145459701
Loss at iteration [978]: 0.002324317806012704
Loss at iteration [979]: 0.002324137081457596
Loss at iteration [980]: 0.0023239298825481307
Loss at iteration [981]: 0.0023237684046484098
Loss at iteration [982]: 0.002323546943477116
Loss at iteration [983]: 0.0023233178503716838
Loss at iteration [984]: 0.002323136816124789
Loss at iteration [985]: 0.0023229458802653128
Loss at iteration [986]: 0.0023227336708310153
Loss at iteration [987]: 0.0023225571226255987
Loss at iteration [988]: 0.002322396078982032
Loss at iteration [989]: 0.0023222134027255366
Loss at iteration [990]: 0.0023220102342376234
Loss at iteration [991]: 0.0023217888010465377
Loss at iteration [992]: 0.002321573194962324
Loss at iteration [993]: 0.002321389383459598
Loss at iteration [994]: 0.00232117228529053
Loss at iteration [995]: 0.002320998480060881
Loss at iteration [996]: 0.0023208147334796135
Loss at iteration [997]: 0.0023206068372212778
Loss at iteration [998]: 0.0023204214144999864
Loss at iteration [999]: 0.002320233086751205
Loss at iteration [1000]: 0.002320045811940368
Loss at iteration [1001]: 0.0023198425948095804
Loss at iteration [1002]: 0.0023196662625993886
Loss at iteration [1003]: 0.0023194563460235346
Loss at iteration [1004]: 0.0023192843262355464
Loss at iteration [1005]: 0.0023190830829809073
Loss at iteration [1006]: 0.002318898178761097
Loss at iteration [1007]: 0.0023187195998417842
Loss at iteration [1008]: 0.0023185420922308634
Loss at iteration [1009]: 0.0023183357145366403
Loss at iteration [1010]: 0.0023181308477441954
Loss at iteration [1011]: 0.002317984081037152
Loss at iteration [1012]: 0.002317760975667296
Loss at iteration [1013]: 0.0023176120199280992
Loss at iteration [1014]: 0.0023174409773041786
Loss at iteration [1015]: 0.0023172267021808845
Loss at iteration [1016]: 0.0023170294270435355
Loss at iteration [1017]: 0.0023168638387204123
Loss at iteration [1018]: 0.002316637677980156
Loss at iteration [1019]: 0.002316474103049037
Loss at iteration [1020]: 0.002316292086477391
Loss at iteration [1021]: 0.002316091804994945
Loss at iteration [1022]: 0.0023159382817303504
Loss at iteration [1023]: 0.002315760056688253
Loss at iteration [1024]: 0.0023155547074751965
Loss at iteration [1025]: 0.002315381992550815
Loss at iteration [1026]: 0.002315197561863397
Loss at iteration [1027]: 0.0023150179704678254
Loss at iteration [1028]: 0.0023149039567541352
Loss at iteration [1029]: 0.0023146791686395767
Loss at iteration [1030]: 0.002314539280542894
Loss at iteration [1031]: 0.002314413282843495
Loss at iteration [1032]: 0.002314223826361868
Loss at iteration [1033]: 0.002314003542227527
Loss at iteration [1034]: 0.0023138432248308994
Loss at iteration [1035]: 0.002313700519923869
Loss at iteration [1036]: 0.0023134700059470924
Loss at iteration [1037]: 0.0023132923205202863
Loss at iteration [1038]: 0.0023131297085451676
Loss at iteration [1039]: 0.002312923423454021
Loss at iteration [1040]: 0.002312743423543772
Loss at iteration [1041]: 0.0023125852655424546
Loss at iteration [1042]: 0.002312414480003956
Loss at iteration [1043]: 0.002312262399668013
Loss at iteration [1044]: 0.002312077713874433
Loss at iteration [1045]: 0.002311901196218298
Loss at iteration [1046]: 0.00231172338212704
Loss at iteration [1047]: 0.0023115409380114775
Loss at iteration [1048]: 0.00231138051296654
Loss at iteration [1049]: 0.0023112130419979207
Loss at iteration [1050]: 0.0023110448168725726
Loss at iteration [1051]: 0.0023108952858972628
Loss at iteration [1052]: 0.002310726953104421
Loss at iteration [1053]: 0.0023105430505346585
Loss at iteration [1054]: 0.0023103767284034624
Loss at iteration [1055]: 0.002310213148329225
Loss at iteration [1056]: 0.002310018441230231
Loss at iteration [1057]: 0.0023098600182859907
Loss at iteration [1058]: 0.0023096948916521017
Loss at iteration [1059]: 0.0023095211147119775
Loss at iteration [1060]: 0.002309339844034839
Loss at iteration [1061]: 0.002309224125142217
Loss at iteration [1062]: 0.002309019499381604
Loss at iteration [1063]: 0.0023088464598470964
Loss at iteration [1064]: 0.002308705036716932
Loss at iteration [1065]: 0.0023085577478009446
Loss at iteration [1066]: 0.0023083494715877085
Loss at iteration [1067]: 0.0023081881457221944
Loss at iteration [1068]: 0.002308036856368919
Loss at iteration [1069]: 0.002307854000975596
Loss at iteration [1070]: 0.0023076845164341294
Loss at iteration [1071]: 0.0023075399597235905
Loss at iteration [1072]: 0.002307345574211742
Loss at iteration [1073]: 0.0023072106770260026
Loss at iteration [1074]: 0.002307035306029863
Loss at iteration [1075]: 0.0023068643816617617
Loss at iteration [1076]: 0.0023067075848377102
Loss at iteration [1077]: 0.0023065282452122996
Loss at iteration [1078]: 0.0023063717302144035
Loss at iteration [1079]: 0.0023062059868681132
Loss at iteration [1080]: 0.002306053380064029
Loss at iteration [1081]: 0.0023058774440537664
Loss at iteration [1082]: 0.0023057131855636994
Loss at iteration [1083]: 0.0023055586740744344
Loss at iteration [1084]: 0.002305406324997926
Loss at iteration [1085]: 0.0023052291479858764
Loss at iteration [1086]: 0.0023051096901735616
Loss at iteration [1087]: 0.002304945506311332
Loss at iteration [1088]: 0.002304752226809586
Loss at iteration [1089]: 0.002304631570577246
Loss at iteration [1090]: 0.0023044303107499268
Loss at iteration [1091]: 0.002304302251417055
Loss at iteration [1092]: 0.0023041362749495467
Loss at iteration [1093]: 0.0023039381441133182
Loss at iteration [1094]: 0.0023038249662017773
Loss at iteration [1095]: 0.002303612526733492
Loss at iteration [1096]: 0.0023034956298774215
Loss at iteration [1097]: 0.0023033269313907524
Loss at iteration [1098]: 0.0023031729855459704
Loss at iteration [1099]: 0.0023030402619818023
Loss at iteration [1100]: 0.002302881324203372
Loss at iteration [1101]: 0.0023027278359079068
Loss at iteration [1102]: 0.0023025887410661867
Loss at iteration [1103]: 0.0023024231201742586
Loss at iteration [1104]: 0.0023022339228373034
Loss at iteration [1105]: 0.0023020372689479133
Loss at iteration [1106]: 0.002301942648601102
Loss at iteration [1107]: 0.0023017425141430574
Loss at iteration [1108]: 0.0023015881470132
Loss at iteration [1109]: 0.0023014281397246445
Loss at iteration [1110]: 0.002301278065820559
Loss at iteration [1111]: 0.0023011080194467026
Loss at iteration [1112]: 0.0023009404771679007
Loss at iteration [1113]: 0.0023008107035626764
Loss at iteration [1114]: 0.0023006951589546955
Loss at iteration [1115]: 0.0023005168947770344
Loss at iteration [1116]: 0.002300345110274543
Loss at iteration [1117]: 0.0023002116613422526
Loss at iteration [1118]: 0.0023000074203392284
Loss at iteration [1119]: 0.0022998634969667916
Loss at iteration [1120]: 0.0022997249719677536
Loss at iteration [1121]: 0.0022995383479291844
Loss at iteration [1122]: 0.0022993787233583094
Loss at iteration [1123]: 0.0022992413491438345
Loss at iteration [1124]: 0.002299074147429026
Loss at iteration [1125]: 0.002298925800973779
Loss at iteration [1126]: 0.0022987653597643103
Loss at iteration [1127]: 0.0022985993770618745
Loss at iteration [1128]: 0.002298448004498923
Loss at iteration [1129]: 0.002298311746961579
Loss at iteration [1130]: 0.00229817590722416
Loss at iteration [1131]: 0.002298043769811619
Loss at iteration [1132]: 0.0022978837110589913
Loss at iteration [1133]: 0.002297719914014588
Loss at iteration [1134]: 0.0022975917449136727
Loss at iteration [1135]: 0.002297403406062877
Loss at iteration [1136]: 0.0022972843311774493
Loss at iteration [1137]: 0.002297138177426942
Loss at iteration [1138]: 0.002296963938361129
Loss at iteration [1139]: 0.0022968552811557315
Loss at iteration [1140]: 0.0022966700675262877
Loss at iteration [1141]: 0.002296548080314472
Loss at iteration [1142]: 0.0022964052366568465
Loss at iteration [1143]: 0.002296235848974871
Loss at iteration [1144]: 0.0022960886240699124
Loss at iteration [1145]: 0.002295957341529841
Loss at iteration [1146]: 0.0022958065444451023
Loss at iteration [1147]: 0.0022956543020017125
Loss at iteration [1148]: 0.0022955164252462367
Loss at iteration [1149]: 0.002295370028632507
Loss at iteration [1150]: 0.0022952052861148517
Loss at iteration [1151]: 0.002295105664601771
Loss at iteration [1152]: 0.002294916123151288
Loss at iteration [1153]: 0.0022947783852371303
Loss at iteration [1154]: 0.0022946355538743263
Loss at iteration [1155]: 0.002294476078113602
Loss at iteration [1156]: 0.0022943475859088843
Loss at iteration [1157]: 0.002294194801763513
Loss at iteration [1158]: 0.0022940646006184384
Loss at iteration [1159]: 0.0022939288451100347
Loss at iteration [1160]: 0.0022937689031779294
Loss at iteration [1161]: 0.002293639898005033
Loss at iteration [1162]: 0.0022934988074506083
Loss at iteration [1163]: 0.002293355486921492
Loss at iteration [1164]: 0.002293181806745888
Loss at iteration [1165]: 0.0022930630428857736
Loss at iteration [1166]: 0.0022929066967762924
Loss at iteration [1167]: 0.002292767056149006
Loss at iteration [1168]: 0.00229263888901151
Loss at iteration [1169]: 0.002292474715395672
Loss at iteration [1170]: 0.002292351572279726
Loss at iteration [1171]: 0.0022922321772806996
Loss at iteration [1172]: 0.0022920894594388283
Loss at iteration [1173]: 0.0022919073200141737
Loss at iteration [1174]: 0.0022917900919721796
Loss at iteration [1175]: 0.0022916167925208122
Loss at iteration [1176]: 0.0022914646355260784
Loss at iteration [1177]: 0.0022913479629396434
Loss at iteration [1178]: 0.0022911987692192243
Loss at iteration [1179]: 0.0022910426830215305
Loss at iteration [1180]: 0.0022909351427741996
Loss at iteration [1181]: 0.0022907638712275392
Loss at iteration [1182]: 0.0022906073754674684
Loss at iteration [1183]: 0.002290453750438378
Loss at iteration [1184]: 0.0022902885257983594
Loss at iteration [1185]: 0.002290163872231829
Loss at iteration [1186]: 0.002290005001083565
Loss at iteration [1187]: 0.0022898975313411043
Loss at iteration [1188]: 0.002289739907655423
Loss at iteration [1189]: 0.002289588391931365
Loss at iteration [1190]: 0.002289445311098926
Loss at iteration [1191]: 0.002289280037028397
Loss at iteration [1192]: 0.002289169660929429
Loss at iteration [1193]: 0.0022889963022115563
Loss at iteration [1194]: 0.0022888523217347964
Loss at iteration [1195]: 0.0022887121793900827
Loss at iteration [1196]: 0.0022885943148815194
Loss at iteration [1197]: 0.0022884548785382284
Loss at iteration [1198]: 0.002288277139683626
Loss at iteration [1199]: 0.0022881181452442
Loss at iteration [1200]: 0.002287973439190208
Loss at iteration [1201]: 0.002287862449583258
Loss at iteration [1202]: 0.0022877377881051816
Loss at iteration [1203]: 0.002287538632746587
Loss at iteration [1204]: 0.0022874424048254394
Loss at iteration [1205]: 0.0022872549794866123
Loss at iteration [1206]: 0.002287087362722321
Loss at iteration [1207]: 0.0022869562635146477
Loss at iteration [1208]: 0.0022868166529099866
Loss at iteration [1209]: 0.0022866521685439185
Loss at iteration [1210]: 0.002286509075655315
Loss at iteration [1211]: 0.0022863720522909967
Loss at iteration [1212]: 0.002286220245965139
Loss at iteration [1213]: 0.0022860861324715963
Loss at iteration [1214]: 0.002285939533123074
Loss at iteration [1215]: 0.0022857944176423992
Loss at iteration [1216]: 0.0022856378994797273
Loss at iteration [1217]: 0.002285505161894317
Loss at iteration [1218]: 0.0022853726311432947
Loss at iteration [1219]: 0.0022852219864180475
Loss at iteration [1220]: 0.002285107803249403
Loss at iteration [1221]: 0.0022849492922737713
Loss at iteration [1222]: 0.0022848371437269544
Loss at iteration [1223]: 0.0022846871127173876
Loss at iteration [1224]: 0.0022845571869077653
Loss at iteration [1225]: 0.002284450209650514
Loss at iteration [1226]: 0.0022843140253586487
Loss at iteration [1227]: 0.002284191674090341
Loss at iteration [1228]: 0.002284018518651497
Loss at iteration [1229]: 0.00228387940953105
Loss at iteration [1230]: 0.002283743028031766
Loss at iteration [1231]: 0.0022835977237063306
Loss at iteration [1232]: 0.0022834729572725155
Loss at iteration [1233]: 0.002283335060035578
Loss at iteration [1234]: 0.00228318995192769
Loss at iteration [1235]: 0.0022830418441554135
Loss at iteration [1236]: 0.00228289250809864
Loss at iteration [1237]: 0.002282770106082638
Loss at iteration [1238]: 0.0022826375437185894
Loss at iteration [1239]: 0.002282510998338708
Loss at iteration [1240]: 0.0022823965262437004
Loss at iteration [1241]: 0.002282264147275072
Loss at iteration [1242]: 0.0022821223411028397
Loss at iteration [1243]: 0.002281986432672115
Loss at iteration [1244]: 0.002281870110523716
Loss at iteration [1245]: 0.0022817139720157412
Loss at iteration [1246]: 0.0022815832921375986
Loss at iteration [1247]: 0.0022814735733883685
Loss at iteration [1248]: 0.0022813393782308523
Loss at iteration [1249]: 0.0022812084159448275
Loss at iteration [1250]: 0.0022810531883868477
Loss at iteration [1251]: 0.002280936889228121
Loss at iteration [1252]: 0.00228079336295132
Loss at iteration [1253]: 0.002280664352684786
Loss at iteration [1254]: 0.0022805498214671545
Loss at iteration [1255]: 0.0022804026118264796
Loss at iteration [1256]: 0.0022803104864310714
Loss at iteration [1257]: 0.002280118686260453
Loss at iteration [1258]: 0.0022800469904690926
Loss at iteration [1259]: 0.002279907311738566
Loss at iteration [1260]: 0.0022797742175061217
Loss at iteration [1261]: 0.00227963444931511
Loss at iteration [1262]: 0.0022794889515836306
Loss at iteration [1263]: 0.002279371221183903
Loss at iteration [1264]: 0.002279225129377334
Loss at iteration [1265]: 0.002279067295985926
Loss at iteration [1266]: 0.002278934608413771
Loss at iteration [1267]: 0.0022788179716689808
Loss at iteration [1268]: 0.0022786771761515176
Loss at iteration [1269]: 0.0022785766210088616
Loss at iteration [1270]: 0.0022784625028387955
Loss at iteration [1271]: 0.0022782960136936126
Loss at iteration [1272]: 0.0022782035668987145
Loss at iteration [1273]: 0.0022780924820808694
Loss at iteration [1274]: 0.0022779447336793417
Loss at iteration [1275]: 0.002277796280910027
Loss at iteration [1276]: 0.0022776573548771413
Loss at iteration [1277]: 0.0022775649764361795
Loss at iteration [1278]: 0.002277406024116205
Loss at iteration [1279]: 0.0022772908123350386
Loss at iteration [1280]: 0.0022771698542451355
Loss at iteration [1281]: 0.0022770037099210526
Loss at iteration [1282]: 0.0022769517643236085
Loss at iteration [1283]: 0.0022767970015184135
Loss at iteration [1284]: 0.0022766359974776148
Loss at iteration [1285]: 0.0022765210060301205
Loss at iteration [1286]: 0.002276426219122596
Loss at iteration [1287]: 0.0022762860320802614
Loss at iteration [1288]: 0.00227614863240428
Loss at iteration [1289]: 0.0022760097684082327
Loss at iteration [1290]: 0.0022758835491793993
Loss at iteration [1291]: 0.0022757649320135865
Loss at iteration [1292]: 0.0022756191968453412
Loss at iteration [1293]: 0.0022754958004517233
Loss at iteration [1294]: 0.0022753862375497567
Loss at iteration [1295]: 0.002275238472870897
Loss at iteration [1296]: 0.0022751058105652503
Loss at iteration [1297]: 0.0022749583739387577
Loss at iteration [1298]: 0.002274902182868266
Loss at iteration [1299]: 0.0022747451788519306
Loss at iteration [1300]: 0.0022745957471384582
Loss at iteration [1301]: 0.0022744807733905375
Loss at iteration [1302]: 0.002274371583092817
Loss at iteration [1303]: 0.0022742309646758756
Loss at iteration [1304]: 0.0022740893135024306
Loss at iteration [1305]: 0.0022739557750671496
Loss at iteration [1306]: 0.0022738793046841207
Loss at iteration [1307]: 0.0022737226692654744
Loss at iteration [1308]: 0.002273574082900527
Loss at iteration [1309]: 0.002273488954912684
Loss at iteration [1310]: 0.0022733765970878463
Loss at iteration [1311]: 0.0022732277988079044
Loss at iteration [1312]: 0.002273082257571252
Loss at iteration [1313]: 0.0022729412880131052
Loss at iteration [1314]: 0.002272849634789719
Loss at iteration [1315]: 0.0022727494743406
Loss at iteration [1316]: 0.0022725860854063105
Loss at iteration [1317]: 0.002272466344138869
Loss at iteration [1318]: 0.0022723483529112167
Loss at iteration [1319]: 0.0022721983086744727
Loss at iteration [1320]: 0.002272097539002201
Loss at iteration [1321]: 0.0022719657490247232
Loss at iteration [1322]: 0.002271836137594635
Loss at iteration [1323]: 0.0022717237102496757
Loss at iteration [1324]: 0.0022716018844356114
Loss at iteration [1325]: 0.0022714706177746134
Loss at iteration [1326]: 0.0022713469814183243
Loss at iteration [1327]: 0.0022711867786397795
Loss at iteration [1328]: 0.0022711064056341146
Loss at iteration [1329]: 0.0022709609701338767
Loss at iteration [1330]: 0.002270834911250436
Loss at iteration [1331]: 0.0022707297329212076
Loss at iteration [1332]: 0.002270589631953326
Loss at iteration [1333]: 0.0022704774019017353
Loss at iteration [1334]: 0.0022703330284655856
Loss at iteration [1335]: 0.002270253569783218
Loss at iteration [1336]: 0.0022701076036673175
Loss at iteration [1337]: 0.002269975512545557
Loss at iteration [1338]: 0.0022698520551190293
Loss at iteration [1339]: 0.0022697263473475393
Loss at iteration [1340]: 0.0022696087202331563
Loss at iteration [1341]: 0.002269485165223292
Loss at iteration [1342]: 0.002269372627422049
Loss at iteration [1343]: 0.0022692369924733817
Loss at iteration [1344]: 0.0022691412404584423
Loss at iteration [1345]: 0.0022690196486328178
Loss at iteration [1346]: 0.0022689150770340847
Loss at iteration [1347]: 0.002268776541090396
Loss at iteration [1348]: 0.002268643774224521
Loss at iteration [1349]: 0.002268570716584472
Loss at iteration [1350]: 0.0022684064968189277
Loss at iteration [1351]: 0.0022682891628090568
Loss at iteration [1352]: 0.0022682203167967664
Loss at iteration [1353]: 0.0022680959459716018
Loss at iteration [1354]: 0.002267946585499504
Loss at iteration [1355]: 0.00226782252480057
Loss at iteration [1356]: 0.0022676740789855346
Loss at iteration [1357]: 0.002267619723886802
Loss at iteration [1358]: 0.002267441927446355
Loss at iteration [1359]: 0.0022673476094439223
Loss at iteration [1360]: 0.002267239014269384
Loss at iteration [1361]: 0.002267112190784518
Loss at iteration [1362]: 0.002267021445230994
Loss at iteration [1363]: 0.0022668520951741092
Loss at iteration [1364]: 0.0022667334609321643
Loss at iteration [1365]: 0.0022666510795854356
Loss at iteration [1366]: 0.002266485489428578
Loss at iteration [1367]: 0.00226639592769012
Loss at iteration [1368]: 0.0022662670064136914
Loss at iteration [1369]: 0.002266176561378822
Loss at iteration [1370]: 0.0022660375940286104
Loss at iteration [1371]: 0.0022659007698071494
Loss at iteration [1372]: 0.0022657638361098537
Loss at iteration [1373]: 0.0022656730725142734
Loss at iteration [1374]: 0.0022655228654443207
Loss at iteration [1375]: 0.0022654125729915575
Loss at iteration [1376]: 0.0022652823513196847
Loss at iteration [1377]: 0.002265166911599426
Loss at iteration [1378]: 0.0022650461731723187
Loss at iteration [1379]: 0.0022649288253067624
Loss at iteration [1380]: 0.0022648192300286744
Loss at iteration [1381]: 0.0022646982199393704
Loss at iteration [1382]: 0.002264611873674026
Loss at iteration [1383]: 0.0022644775212096414
Loss at iteration [1384]: 0.0022643518381495536
Loss at iteration [1385]: 0.0022642316089122112
Loss at iteration [1386]: 0.002264119678372876
Loss at iteration [1387]: 0.002263994699472531
Loss at iteration [1388]: 0.0022638779769581703
Loss at iteration [1389]: 0.0022637554998214387
Loss at iteration [1390]: 0.00226365001291438
Loss at iteration [1391]: 0.0022635432238444168
Loss at iteration [1392]: 0.002263398769180745
Loss at iteration [1393]: 0.0022632949164027153
Loss at iteration [1394]: 0.0022631564075898045
Loss at iteration [1395]: 0.002263059139978159
Loss at iteration [1396]: 0.0022629348886895745
Loss at iteration [1397]: 0.0022628048528958587
Loss at iteration [1398]: 0.0022627444960240614
Loss at iteration [1399]: 0.0022625884813319303
Loss at iteration [1400]: 0.0022624845765392347
Loss at iteration [1401]: 0.0022623431797201632
Loss at iteration [1402]: 0.0022622244207533867
Loss at iteration [1403]: 0.002262111774827516
Loss at iteration [1404]: 0.0022619725849960075
Loss at iteration [1405]: 0.0022618633532617673
Loss at iteration [1406]: 0.00226174542472432
Loss at iteration [1407]: 0.002261627599705773
Loss at iteration [1408]: 0.002261477511221527
Loss at iteration [1409]: 0.0022613651531887048
Loss at iteration [1410]: 0.0022612425330052212
Loss at iteration [1411]: 0.0022611101107202426
Loss at iteration [1412]: 0.0022610012168495226
Loss at iteration [1413]: 0.0022608546185265517
Loss at iteration [1414]: 0.002260767900737519
Loss at iteration [1415]: 0.00226063571608182
Loss at iteration [1416]: 0.002260515647685583
Loss at iteration [1417]: 0.0022603685272141356
Loss at iteration [1418]: 0.002260264550063886
Loss at iteration [1419]: 0.0022601212024363757
Loss at iteration [1420]: 0.002260010570138528
Loss at iteration [1421]: 0.0022599012681505663
Loss at iteration [1422]: 0.0022597749877073313
Loss at iteration [1423]: 0.0022596604052049444
Loss at iteration [1424]: 0.0022595397506054546
Loss at iteration [1425]: 0.002259423599706506
Loss at iteration [1426]: 0.002259299742890783
Loss at iteration [1427]: 0.0022591766959546363
Loss at iteration [1428]: 0.0022590582927058222
Loss at iteration [1429]: 0.00225895084133773
Loss at iteration [1430]: 0.002258817975932086
Loss at iteration [1431]: 0.0022587082359738593
Loss at iteration [1432]: 0.0022585890845732265
Loss at iteration [1433]: 0.0022584801447313213
Loss at iteration [1434]: 0.0022583509957540865
Loss at iteration [1435]: 0.0022582229408017684
Loss at iteration [1436]: 0.002258088270352045
Loss at iteration [1437]: 0.0022580063417323992
Loss at iteration [1438]: 0.002257875198636914
Loss at iteration [1439]: 0.0022577622801419994
Loss at iteration [1440]: 0.0022576865770609087
Loss at iteration [1441]: 0.002257541674810696
Loss at iteration [1442]: 0.0022574485365136274
Loss at iteration [1443]: 0.0022572948782559063
Loss at iteration [1444]: 0.0022571739230628934
Loss at iteration [1445]: 0.0022570336141922395
Loss at iteration [1446]: 0.0022569392157763338
Loss at iteration [1447]: 0.002256831520800403
Loss at iteration [1448]: 0.0022567130580037394
Loss at iteration [1449]: 0.0022565657948459576
Loss at iteration [1450]: 0.002256459103756847
Loss at iteration [1451]: 0.0022563203239621297
Loss at iteration [1452]: 0.0022562321044248827
Loss at iteration [1453]: 0.002256116806903084
Loss at iteration [1454]: 0.0022559899208340435
Loss at iteration [1455]: 0.002255859348575765
Loss at iteration [1456]: 0.0022557612900577814
Loss at iteration [1457]: 0.002255617709701666
Loss at iteration [1458]: 0.002255505159138351
Loss at iteration [1459]: 0.0022554011742540268
Loss at iteration [1460]: 0.0022552519250125685
Loss at iteration [1461]: 0.0022551640749840576
Loss at iteration [1462]: 0.0022550504483731045
Loss at iteration [1463]: 0.002254915816903587
Loss at iteration [1464]: 0.002254805686461607
Loss at iteration [1465]: 0.002254692902991661
Loss at iteration [1466]: 0.0022545399330069216
Loss at iteration [1467]: 0.002254457384867214
Loss at iteration [1468]: 0.002254356662751603
Loss at iteration [1469]: 0.002254231548030346
Loss at iteration [1470]: 0.0022540730602722183
Loss at iteration [1471]: 0.0022539707685177153
Loss at iteration [1472]: 0.0022538557612648644
Loss at iteration [1473]: 0.0022537547491334126
Loss at iteration [1474]: 0.0022536346752074377
Loss at iteration [1475]: 0.0022534975656128987
Loss at iteration [1476]: 0.002253370870990383
Loss at iteration [1477]: 0.002253270075014246
Loss at iteration [1478]: 0.0022531489603730133
Loss at iteration [1479]: 0.002253021427833149
Loss at iteration [1480]: 0.0022529083946012055
Loss at iteration [1481]: 0.0022527839296641516
Loss at iteration [1482]: 0.0022526618451803944
Loss at iteration [1483]: 0.0022525507721086356
Loss at iteration [1484]: 0.0022524183934559665
Loss at iteration [1485]: 0.002252300941941097
Loss at iteration [1486]: 0.0022521635599902063
Loss at iteration [1487]: 0.0022520777331762965
Loss at iteration [1488]: 0.0022519347085683847
Loss at iteration [1489]: 0.00225183942511328
Loss at iteration [1490]: 0.0022516924098307054
Loss at iteration [1491]: 0.0022516273855812665
Loss at iteration [1492]: 0.0022514981973119105
Loss at iteration [1493]: 0.0022513771418733674
Loss at iteration [1494]: 0.0022512753877619415
Loss at iteration [1495]: 0.0022511387134782623
Loss at iteration [1496]: 0.0022510662038755314
Loss at iteration [1497]: 0.0022509092294629367
Loss at iteration [1498]: 0.0022508171397961005
Loss at iteration [1499]: 0.002250657221493172
Loss at iteration [1500]: 0.00225052435631487
Loss at iteration [1501]: 0.0022504366890532396
Loss at iteration [1502]: 0.0022503331366020268
Loss at iteration [1503]: 0.00225021365030058
Loss at iteration [1504]: 0.0022500988691059923
Loss at iteration [1505]: 0.0022500017639252674
Loss at iteration [1506]: 0.0022498538684679495
Loss at iteration [1507]: 0.002249717748107156
Loss at iteration [1508]: 0.0022496820786929903
Loss at iteration [1509]: 0.002249496903304718
Loss at iteration [1510]: 0.0022494083806588985
Loss at iteration [1511]: 0.0022492566493170956
Loss at iteration [1512]: 0.0022491770184858116
Loss at iteration [1513]: 0.0022490047597707283
Loss at iteration [1514]: 0.0022488669974344544
Loss at iteration [1515]: 0.002248818047113083
Loss at iteration [1516]: 0.0022486271427338477
Loss at iteration [1517]: 0.002248533554303969
Loss at iteration [1518]: 0.0022484234469017793
Loss at iteration [1519]: 0.0022482956209521985
Loss at iteration [1520]: 0.0022481623460763097
Loss at iteration [1521]: 0.0022480760112381887
Loss at iteration [1522]: 0.0022479607117363055
Loss at iteration [1523]: 0.0022478410325964273
Loss at iteration [1524]: 0.0022477230028183004
Loss at iteration [1525]: 0.0022475872177448657
Loss at iteration [1526]: 0.0022474605866502112
Loss at iteration [1527]: 0.002247367569904089
Loss at iteration [1528]: 0.0022472542241908163
Loss at iteration [1529]: 0.0022471073830593875
Loss at iteration [1530]: 0.0022469989803728077
Loss at iteration [1531]: 0.0022468848157873576
Loss at iteration [1532]: 0.002246798066659631
Loss at iteration [1533]: 0.002246673505490983
Loss at iteration [1534]: 0.002246549041330504
Loss at iteration [1535]: 0.0022464320469915465
Loss at iteration [1536]: 0.00224630971233716
Loss at iteration [1537]: 0.0022461663819365162
Loss at iteration [1538]: 0.0022460800694496653
Loss at iteration [1539]: 0.0022459490497324943
Loss at iteration [1540]: 0.002245831101562142
Loss at iteration [1541]: 0.002245696342002482
Loss at iteration [1542]: 0.002245577163041171
Loss at iteration [1543]: 0.002245493535352194
Loss at iteration [1544]: 0.002245362458208774
Loss at iteration [1545]: 0.002245260314196757
Loss at iteration [1546]: 0.002245147618470941
Loss at iteration [1547]: 0.0022450096229714753
Loss at iteration [1548]: 0.002244907518974469
Loss at iteration [1549]: 0.0022448012366517527
Loss at iteration [1550]: 0.0022447041868974477
Loss at iteration [1551]: 0.0022445474343882428
Loss at iteration [1552]: 0.002244462172793901
Loss at iteration [1553]: 0.002244283775789898
Loss at iteration [1554]: 0.002244228482299502
Loss at iteration [1555]: 0.002244066049810041
Loss at iteration [1556]: 0.0022439610881441413
Loss at iteration [1557]: 0.0022438413483073295
Loss at iteration [1558]: 0.0022437196291786037
Loss at iteration [1559]: 0.0022435807759968555
Loss at iteration [1560]: 0.0022435450784335014
Loss at iteration [1561]: 0.0022433713218684968
Loss at iteration [1562]: 0.0022432938891989453
Loss at iteration [1563]: 0.0022432144480787506
Loss at iteration [1564]: 0.0022430832939815838
Loss at iteration [1565]: 0.00224291110775465
Loss at iteration [1566]: 0.002242836065282328
Loss at iteration [1567]: 0.002242734977640652
Loss at iteration [1568]: 0.0022425519147021805
Loss at iteration [1569]: 0.0022424407129753533
Loss at iteration [1570]: 0.002242357314197082
Loss at iteration [1571]: 0.002242210381438411
Loss at iteration [1572]: 0.002242070687736747
Loss at iteration [1573]: 0.002241971412926918
Loss at iteration [1574]: 0.0022418397983960795
Loss at iteration [1575]: 0.002241725348972614
Loss at iteration [1576]: 0.0022416132408109852
Loss at iteration [1577]: 0.0022414779004925373
Loss at iteration [1578]: 0.002241394581917725
Loss at iteration [1579]: 0.002241258634183006
Loss at iteration [1580]: 0.0022411587965982427
Loss at iteration [1581]: 0.002241043650033329
Loss at iteration [1582]: 0.002240925059539186
Loss at iteration [1583]: 0.0022408025481546624
Loss at iteration [1584]: 0.0022406960726851824
Loss at iteration [1585]: 0.0022405587029333865
Loss at iteration [1586]: 0.0022404463633777178
Loss at iteration [1587]: 0.002240338356146104
Loss at iteration [1588]: 0.002240205982827455
Loss at iteration [1589]: 0.0022400868069483927
Loss at iteration [1590]: 0.0022399659877864733
Loss at iteration [1591]: 0.0022398297074466963
Loss at iteration [1592]: 0.0022397183558317047
Loss at iteration [1593]: 0.0022395973805493497
Loss at iteration [1594]: 0.0022394861537744252
Loss at iteration [1595]: 0.0022393763872824283
Loss at iteration [1596]: 0.0022392481690779215
Loss at iteration [1597]: 0.00223913188246439
Loss at iteration [1598]: 0.002239016912617744
Loss at iteration [1599]: 0.002238904354244101
Loss at iteration [1600]: 0.0022387675077304068
Loss at iteration [1601]: 0.00223864287151058
Loss at iteration [1602]: 0.002238551489642426
Loss at iteration [1603]: 0.0022384293081334017
Loss at iteration [1604]: 0.002238337449138975
Loss at iteration [1605]: 0.0022382118254790995
Loss at iteration [1606]: 0.002238127149584819
Loss at iteration [1607]: 0.0022380007132005103
Loss at iteration [1608]: 0.0022378813193575956
Loss at iteration [1609]: 0.0022377869987063948
Loss at iteration [1610]: 0.0022376723144363407
Loss at iteration [1611]: 0.0022375515553776253
Loss at iteration [1612]: 0.002237392682342434
Loss at iteration [1613]: 0.002237345626686011
Loss at iteration [1614]: 0.0022371783965701056
Loss at iteration [1615]: 0.0022371224586423626
Loss at iteration [1616]: 0.002236962427984465
Loss at iteration [1617]: 0.0022368905250163286
Loss at iteration [1618]: 0.002236770941596969
Loss at iteration [1619]: 0.0022366041216598516
Loss at iteration [1620]: 0.0022364234200160523
Loss at iteration [1621]: 0.002236344622900771
Loss at iteration [1622]: 0.002236229982196116
Loss at iteration [1623]: 0.002236111708062807
Loss at iteration [1624]: 0.0022359709530239452
Loss at iteration [1625]: 0.002235878896412708
Loss at iteration [1626]: 0.0022358030180219288
Loss at iteration [1627]: 0.0022356283980494796
Loss at iteration [1628]: 0.0022355554994084354
Loss at iteration [1629]: 0.0022354015372537625
Loss at iteration [1630]: 0.0022353055094525144
Loss at iteration [1631]: 0.0022351602312917335
Loss at iteration [1632]: 0.0022350691412046544
Loss at iteration [1633]: 0.002234879338537639
Loss at iteration [1634]: 0.002234870764783612
Loss at iteration [1635]: 0.0022346963601866813
Loss at iteration [1636]: 0.0022346558456554813
Loss at iteration [1637]: 0.0022344892396543256
Loss at iteration [1638]: 0.0022343858733339327
Loss at iteration [1639]: 0.002234266970535987
Loss at iteration [1640]: 0.002234088300197974
Loss at iteration [1641]: 0.0022340624173842506
Loss at iteration [1642]: 0.0022338446396613755
Loss at iteration [1643]: 0.0022338348185354244
Loss at iteration [1644]: 0.00223365032230643
Loss at iteration [1645]: 0.0022336046535281194
Loss at iteration [1646]: 0.0022334287423874696
Loss at iteration [1647]: 0.0022333329747025385
Loss at iteration [1648]: 0.002233179242654379
Loss at iteration [1649]: 0.002233059135421501
Loss at iteration [1650]: 0.0022329236527178697
Loss at iteration [1651]: 0.0022327851913810665
Loss at iteration [1652]: 0.002232654033578009
Loss at iteration [1653]: 0.002232569833059397
Loss at iteration [1654]: 0.0022324245757454407
Loss at iteration [1655]: 0.0022323045016608366
Loss at iteration [1656]: 0.0022321995274949724
Loss at iteration [1657]: 0.0022320956660370215
Loss at iteration [1658]: 0.002231981978529723
Loss at iteration [1659]: 0.0022318568085364617
Loss at iteration [1660]: 0.002231736248718586
Loss at iteration [1661]: 0.0022316236910304944
Loss at iteration [1662]: 0.00223149026416893
Loss at iteration [1663]: 0.002231348857287655
Loss at iteration [1664]: 0.0022312637669309296
Loss at iteration [1665]: 0.0022311625419648403
Loss at iteration [1666]: 0.0022309960136342186
Loss at iteration [1667]: 0.0022309218661965475
Loss at iteration [1668]: 0.002230854424020569
Loss at iteration [1669]: 0.0022306821243242535
Loss at iteration [1670]: 0.002230557660655703
Loss at iteration [1671]: 0.002230479480662883
Loss at iteration [1672]: 0.002230323871389731
Loss at iteration [1673]: 0.002230198597735096
Loss at iteration [1674]: 0.0022300929799105983
Loss at iteration [1675]: 0.0022299837070464115
Loss at iteration [1676]: 0.002229825177807661
Loss at iteration [1677]: 0.0022297390131518977
Loss at iteration [1678]: 0.002229602844753134
Loss at iteration [1679]: 0.0022295147566692027
Loss at iteration [1680]: 0.0022293801953499554
Loss at iteration [1681]: 0.002229278107489301
Loss at iteration [1682]: 0.0022291622547401147
Loss at iteration [1683]: 0.002229089739489824
Loss at iteration [1684]: 0.002228927400465622
Loss at iteration [1685]: 0.0022288141684721
Loss at iteration [1686]: 0.002228687359399899
Loss at iteration [1687]: 0.002228628885812191
Loss at iteration [1688]: 0.002228452113230774
Loss at iteration [1689]: 0.0022283320851645455
Loss at iteration [1690]: 0.0022282179567331877
Loss at iteration [1691]: 0.0022280927860712243
Loss at iteration [1692]: 0.0022279781423431766
Loss at iteration [1693]: 0.00222787517577128
Loss at iteration [1694]: 0.002227723418833906
Loss at iteration [1695]: 0.002227606873661728
Loss at iteration [1696]: 0.0022274803365086963
Loss at iteration [1697]: 0.002227339556844604
Loss at iteration [1698]: 0.002227205166927054
Loss at iteration [1699]: 0.0022271874847975925
Loss at iteration [1700]: 0.0022269819729561235
Loss at iteration [1701]: 0.002226967576184408
Loss at iteration [1702]: 0.0022267909500861467
Loss at iteration [1703]: 0.0022267300443484976
Loss at iteration [1704]: 0.0022265809171155616
Loss at iteration [1705]: 0.0022264781850199707
Loss at iteration [1706]: 0.002226377573674965
Loss at iteration [1707]: 0.002226209683251186
Loss at iteration [1708]: 0.002226135876946022
Loss at iteration [1709]: 0.0022259311483438164
Loss at iteration [1710]: 0.002225868794111597
Loss at iteration [1711]: 0.002225726982929284
Loss at iteration [1712]: 0.0022256347293557652
Loss at iteration [1713]: 0.0022254492470714605
Loss at iteration [1714]: 0.0022253702244541813
Loss at iteration [1715]: 0.002225260046789629
Loss at iteration [1716]: 0.0022251169980796112
Loss at iteration [1717]: 0.0022250499521502824
Loss at iteration [1718]: 0.0022249168470815464
Loss at iteration [1719]: 0.0022248423425657426
Loss at iteration [1720]: 0.0022246567188081365
Loss at iteration [1721]: 0.0022245143991903747
Loss at iteration [1722]: 0.0022243855055318626
Loss at iteration [1723]: 0.002224303581534706
Loss at iteration [1724]: 0.0022241974938539007
Loss at iteration [1725]: 0.002224131720974256
Loss at iteration [1726]: 0.0022239704232538647
Loss at iteration [1727]: 0.0022239050322088202
Loss at iteration [1728]: 0.002223728753839469
Loss at iteration [1729]: 0.0022235747590305314
Loss at iteration [1730]: 0.002223470266497448
Loss at iteration [1731]: 0.002223381245256895
Loss at iteration [1732]: 0.002223210359111324
Loss at iteration [1733]: 0.002223204936442238
Loss at iteration [1734]: 0.0022229787140860876
Loss at iteration [1735]: 0.002222863515995317
Loss at iteration [1736]: 0.0022227417504058432
Loss at iteration [1737]: 0.0022226609620616153
Loss at iteration [1738]: 0.0022225487603300233
Loss at iteration [1739]: 0.0022223284079683284
Loss at iteration [1740]: 0.00222223211184281
Loss at iteration [1741]: 0.002222136765151784
Loss at iteration [1742]: 0.0022219906006251205
Loss at iteration [1743]: 0.002221831017288225
Loss at iteration [1744]: 0.0022217490649954633
Loss at iteration [1745]: 0.0022216344416153577
Loss at iteration [1746]: 0.002221480526633323
Loss at iteration [1747]: 0.002221348149237236
Loss at iteration [1748]: 0.0022212513602584984
Loss at iteration [1749]: 0.0022210933382763942
Loss at iteration [1750]: 0.0022209544703881523
Loss at iteration [1751]: 0.002220818279483581
Loss at iteration [1752]: 0.0022207218609191738
Loss at iteration [1753]: 0.002220579375051408
Loss at iteration [1754]: 0.002220477145587929
Loss at iteration [1755]: 0.0022203964108596834
Loss at iteration [1756]: 0.002220231755289538
Loss at iteration [1757]: 0.0022201764414612347
Loss at iteration [1758]: 0.0022200319767623836
Loss at iteration [1759]: 0.0022199099761254166
Loss at iteration [1760]: 0.0022198313919140683
Loss at iteration [1761]: 0.0022197479176467054
Loss at iteration [1762]: 0.002219564451979597
Loss at iteration [1763]: 0.0022194582877082267
Loss at iteration [1764]: 0.0022192472888032037
Loss at iteration [1765]: 0.002219220816248952
Loss at iteration [1766]: 0.0022190970786584467
Loss at iteration [1767]: 0.0022189621780017606
Loss at iteration [1768]: 0.002218826298686901
Loss at iteration [1769]: 0.0022187539332559542
Loss at iteration [1770]: 0.002218612271744484
Loss at iteration [1771]: 0.00221847150320078
Loss at iteration [1772]: 0.002218295370634685
Loss at iteration [1773]: 0.002218221657321129
Loss at iteration [1774]: 0.0022180487661211975
Loss at iteration [1775]: 0.0022179513614816295
Loss at iteration [1776]: 0.0022178127245865197
Loss at iteration [1777]: 0.0022177254046049557
Loss at iteration [1778]: 0.0022176826112478634
Loss at iteration [1779]: 0.0022175170164036423
Loss at iteration [1780]: 0.002217371741749828
Loss at iteration [1781]: 0.0022172962181613754
Loss at iteration [1782]: 0.002217166355174215
Loss at iteration [1783]: 0.0022170319148801347
Loss at iteration [1784]: 0.0022169499492483667
Loss at iteration [1785]: 0.0022168178089957166
Loss at iteration [1786]: 0.002216711993561828
Loss at iteration [1787]: 0.002216496789223245
Loss at iteration [1788]: 0.0022163611725767875
Loss at iteration [1789]: 0.0022164099414140313
***** Warning: Loss has increased *****
Loss at iteration [1790]: 0.002216174737030064
Loss at iteration [1791]: 0.0022160232108417565
Loss at iteration [1792]: 0.0022160522980770032
***** Warning: Loss has increased *****
Loss at iteration [1793]: 0.0022160056357007443
Loss at iteration [1794]: 0.0022158397631374324
Loss at iteration [1795]: 0.002215655333189814
Loss at iteration [1796]: 0.0022155041828641813
Loss at iteration [1797]: 0.0022152382985760025
Loss at iteration [1798]: 0.002215334173341546
***** Warning: Loss has increased *****
Loss at iteration [1799]: 0.002215145118595118
Loss at iteration [1800]: 0.002214895310471587
Loss at iteration [1801]: 0.0022148503208242235
Loss at iteration [1802]: 0.002214763301932651
Loss at iteration [1803]: 0.002214564511542696
Loss at iteration [1804]: 0.002214436831146085
Loss at iteration [1805]: 0.002214301101394285
Loss at iteration [1806]: 0.0022141914912264674
Loss at iteration [1807]: 0.002213999176476731
Loss at iteration [1808]: 0.0022138956181053005
Loss at iteration [1809]: 0.002213800495033629
Loss at iteration [1810]: 0.0022136684581674977
Loss at iteration [1811]: 0.0022135378442731633
Loss at iteration [1812]: 0.002213393275941015
Loss at iteration [1813]: 0.0022132334183291016
Loss at iteration [1814]: 0.002213164385227017
Loss at iteration [1815]: 0.002213041033893818
Loss at iteration [1816]: 0.002212892439855808
Loss at iteration [1817]: 0.0022128322897504225
Loss at iteration [1818]: 0.002212754872591737
Loss at iteration [1819]: 0.0022125235697411493
Loss at iteration [1820]: 0.0022124215307163762
Loss at iteration [1821]: 0.002212302451162301
Loss at iteration [1822]: 0.0022121442575711837
Loss at iteration [1823]: 0.002212094022704344
Loss at iteration [1824]: 0.0022120185892983424
Loss at iteration [1825]: 0.0022118460654797474
Loss at iteration [1826]: 0.002211740942098314
Loss at iteration [1827]: 0.002211513192051157
Loss at iteration [1828]: 0.0022114554555250697
Loss at iteration [1829]: 0.0022113185821736967
Loss at iteration [1830]: 0.002211186600640465
Loss at iteration [1831]: 0.00221103278246622
Loss at iteration [1832]: 0.0022109415016783513
Loss at iteration [1833]: 0.002210853986838355
Loss at iteration [1834]: 0.002210734926304449
Loss at iteration [1835]: 0.0022106275928474445
Loss at iteration [1836]: 0.0022105026229482573
Loss at iteration [1837]: 0.0022103312349391908
Loss at iteration [1838]: 0.0022101896376052043
Loss at iteration [1839]: 0.0022101227039508786
Loss at iteration [1840]: 0.002209914126812997
Loss at iteration [1841]: 0.002209866612017656
Loss at iteration [1842]: 0.0022097533844150272
Loss at iteration [1843]: 0.002209619080001403
Loss at iteration [1844]: 0.0022095206959027694
Loss at iteration [1845]: 0.0022093532205155523
Loss at iteration [1846]: 0.0022092228118388052
Loss at iteration [1847]: 0.002209211872870981
Loss at iteration [1848]: 0.002209002819875034
Loss at iteration [1849]: 0.0022088500025446733
Loss at iteration [1850]: 0.0022088318367603967
Loss at iteration [1851]: 0.0022086886855752223
Loss at iteration [1852]: 0.002208466450014052
Loss at iteration [1853]: 0.0022084541470003844
Loss at iteration [1854]: 0.0022083379654841487
Loss at iteration [1855]: 0.002208121463601465
Loss at iteration [1856]: 0.0022080799011953596
Loss at iteration [1857]: 0.0022079527131850774
Loss at iteration [1858]: 0.002207746420563859
Loss at iteration [1859]: 0.002207636309257285
Loss at iteration [1860]: 0.002207659807979492
***** Warning: Loss has increased *****
Loss at iteration [1861]: 0.0022074516495451998
Loss at iteration [1862]: 0.0022072487160758806
Loss at iteration [1863]: 0.0022071878322475636
Loss at iteration [1864]: 0.0022070152096924146
Loss at iteration [1865]: 0.0022068727170231144
Loss at iteration [1866]: 0.0022067612576775956
Loss at iteration [1867]: 0.002206701721685807
Loss at iteration [1868]: 0.00220647307174578
Loss at iteration [1869]: 0.0022063987283267653
Loss at iteration [1870]: 0.0022062284914993657
Loss at iteration [1871]: 0.002206131137727235
Loss at iteration [1872]: 0.002205984217442571
Loss at iteration [1873]: 0.0022058648388023007
Loss at iteration [1874]: 0.0022057256207487653
Loss at iteration [1875]: 0.002205593806896082
Loss at iteration [1876]: 0.0022055243759728565
Loss at iteration [1877]: 0.0022053601373260958
Loss at iteration [1878]: 0.0022052569393980837
Loss at iteration [1879]: 0.0022051329790979757
Loss at iteration [1880]: 0.002204983756888779
Loss at iteration [1881]: 0.002204981384628299
Loss at iteration [1882]: 0.002204793526137493
Loss at iteration [1883]: 0.0022046559100048677
Loss at iteration [1884]: 0.0022046519562989543
Loss at iteration [1885]: 0.002204545558857314
Loss at iteration [1886]: 0.002204290356073599
Loss at iteration [1887]: 0.0022041563856419213
Loss at iteration [1888]: 0.0022042884590179846
***** Warning: Loss has increased *****
Loss at iteration [1889]: 0.0022040072422178487
Loss at iteration [1890]: 0.002203890796155569
Loss at iteration [1891]: 0.0022038842590263153
Loss at iteration [1892]: 0.002203715323402301
Loss at iteration [1893]: 0.0022035285274394213
Loss at iteration [1894]: 0.002203373202910877
Loss at iteration [1895]: 0.0022032154585268616
Loss at iteration [1896]: 0.0022031073611101203
Loss at iteration [1897]: 0.0022029448855427755
Loss at iteration [1898]: 0.002202803680467879
Loss at iteration [1899]: 0.0022026874570486438
Loss at iteration [1900]: 0.002202576208981096
Loss at iteration [1901]: 0.002202402542398842
Loss at iteration [1902]: 0.002202289060128671
Loss at iteration [1903]: 0.0022021519732568387
Loss at iteration [1904]: 0.0022020350611846925
Loss at iteration [1905]: 0.002201855948906146
Loss at iteration [1906]: 0.0022017639769990323
Loss at iteration [1907]: 0.002201638211097485
Loss at iteration [1908]: 0.002201474962241453
Loss at iteration [1909]: 0.0022013966176023278
Loss at iteration [1910]: 0.002201234700550506
Loss at iteration [1911]: 0.002201101687613241
Loss at iteration [1912]: 0.0022010188402213143
Loss at iteration [1913]: 0.0022008601645558136
Loss at iteration [1914]: 0.0022008298252681273
Loss at iteration [1915]: 0.002200672298855153
Loss at iteration [1916]: 0.002200521526737547
Loss at iteration [1917]: 0.002200477614842376
Loss at iteration [1918]: 0.002200337193658496
Loss at iteration [1919]: 0.0022001482204640284
Loss at iteration [1920]: 0.002200032434732048
Loss at iteration [1921]: 0.0022000707045877247
***** Warning: Loss has increased *****
Loss at iteration [1922]: 0.0021997710651365404
Loss at iteration [1923]: 0.002199677869465061
Loss at iteration [1924]: 0.002199628019581311
Loss at iteration [1925]: 0.002199609758723454
Loss at iteration [1926]: 0.0021994404430108715
Loss at iteration [1927]: 0.0021991115547056726
Loss at iteration [1928]: 0.002199077529540018
Loss at iteration [1929]: 0.002198880981638394
Loss at iteration [1930]: 0.0021987617225770737
Loss at iteration [1931]: 0.002198732348682321
Loss at iteration [1932]: 0.0021986044709237356
Loss at iteration [1933]: 0.002198430105295141
Loss at iteration [1934]: 0.0021982800984542247
Loss at iteration [1935]: 0.002198268915493871
Loss at iteration [1936]: 0.0021980245716566927
Loss at iteration [1937]: 0.002197910108288165
Loss at iteration [1938]: 0.0021978643641051634
Loss at iteration [1939]: 0.002197786232316493
Loss at iteration [1940]: 0.0021976575046273056
Loss at iteration [1941]: 0.0021973946315244153
Loss at iteration [1942]: 0.002197251360711914
Loss at iteration [1943]: 0.002197168678317259
Loss at iteration [1944]: 0.002196989988407127
Loss at iteration [1945]: 0.0021968795104016217
Loss at iteration [1946]: 0.0021967336501423334
Loss at iteration [1947]: 0.0021966360236934782
Loss at iteration [1948]: 0.0021965257206000267
Loss at iteration [1949]: 0.002196375650006688
Loss at iteration [1950]: 0.002196260580702247
Loss at iteration [1951]: 0.0021961394620201736
Loss at iteration [1952]: 0.0021959648355478006
Loss at iteration [1953]: 0.002195921064810925
Loss at iteration [1954]: 0.0021957215583941047
Loss at iteration [1955]: 0.002195582078878361
Loss at iteration [1956]: 0.00219543970595157
Loss at iteration [1957]: 0.0021954226470414758
Loss at iteration [1958]: 0.0021951823087395517
Loss at iteration [1959]: 0.0021951591163214793
Loss at iteration [1960]: 0.0021949852047714194
Loss at iteration [1961]: 0.0021949131132780143
Loss at iteration [1962]: 0.0021947407458173416
Loss at iteration [1963]: 0.002194676973697964
Loss at iteration [1964]: 0.0021945684761333567
Loss at iteration [1965]: 0.002194376178011145
Loss at iteration [1966]: 0.0021943363088644777
Loss at iteration [1967]: 0.002194102805412091
Loss at iteration [1968]: 0.002193947096399862
Loss at iteration [1969]: 0.0021938282816041247
Loss at iteration [1970]: 0.002193817746006659
Loss at iteration [1971]: 0.002193542730269541
Loss at iteration [1972]: 0.0021935712292893046
***** Warning: Loss has increased *****
Loss at iteration [1973]: 0.002193270925738295
Loss at iteration [1974]: 0.0021932556883717757
Loss at iteration [1975]: 0.0021931129034052955
Loss at iteration [1976]: 0.0021930002737106866
Loss at iteration [1977]: 0.002192781165671765
Loss at iteration [1978]: 0.002192682791810328
Loss at iteration [1979]: 0.0021925357461041496
Loss at iteration [1980]: 0.002192403144060465
Loss at iteration [1981]: 0.002192363442837546
Loss at iteration [1982]: 0.0021921214433026945
Loss at iteration [1983]: 0.0021920382415061613
Loss at iteration [1984]: 0.002191817966131103
Loss at iteration [1985]: 0.0021917200692994996
Loss at iteration [1986]: 0.0021916087505897963
Loss at iteration [1987]: 0.002191404445658816
Loss at iteration [1988]: 0.002191358781497542
Loss at iteration [1989]: 0.0021911845057228044
Loss at iteration [1990]: 0.002190981020337553
Loss at iteration [1991]: 0.002190866018501055
Loss at iteration [1992]: 0.0021907166221248617
Loss at iteration [1993]: 0.0021907014704736208
Loss at iteration [1994]: 0.002190443779977346
Loss at iteration [1995]: 0.0021903866027648514
Loss at iteration [1996]: 0.0021902543866608153
Loss at iteration [1997]: 0.0021900702932155405
Loss at iteration [1998]: 0.0021899922672679313
Loss at iteration [1999]: 0.0021898407884340296
Loss at iteration [2000]: 0.002189677815783561
Loss at iteration [2001]: 0.0021895574225397705
Loss at iteration [2002]: 0.0021894469081804664
Loss at iteration [2003]: 0.0021893159585348303
Loss at iteration [2004]: 0.0021891953419578484
Loss at iteration [2005]: 0.002189119918418967
Loss at iteration [2006]: 0.002188906065678234
Loss at iteration [2007]: 0.002188811751484017
Loss at iteration [2008]: 0.0021886686076725568
Loss at iteration [2009]: 0.002188553673874065
Loss at iteration [2010]: 0.002188483992349738
Loss at iteration [2011]: 0.002188206300594258
Loss at iteration [2012]: 0.0021882152682756884
***** Warning: Loss has increased *****
Loss at iteration [2013]: 0.0021880242070026305
Loss at iteration [2014]: 0.0021879267624306197
Loss at iteration [2015]: 0.002187785846989758
Loss at iteration [2016]: 0.002187655903310767
Loss at iteration [2017]: 0.0021875670615398085
Loss at iteration [2018]: 0.00218731261501852
Loss at iteration [2019]: 0.0021872333044411557
Loss at iteration [2020]: 0.002187113505164132
Loss at iteration [2021]: 0.0021869985709890238
Loss at iteration [2022]: 0.0021868421767433596
Loss at iteration [2023]: 0.002186729494391852
Loss at iteration [2024]: 0.0021866014553667103
Loss at iteration [2025]: 0.00218648484186322
Loss at iteration [2026]: 0.0021864854888012063
***** Warning: Loss has increased *****
